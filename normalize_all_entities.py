#!/usr/bin/env python3
"""
Script para normalizar TODAS as entidades extraÃ­das usando LLM.
"""
import sys
from pathlib import Path
import pickle

# Adicionar src ao path
sys.path.append(str(Path(__file__).parent / "src"))

from knowledge_graph.entity_normalizer import normalize_entities, load_extracted_entities

def main():
    print("ğŸ§  Iniciando normalizaÃ§Ã£o de TODAS as entidades com LLM...")
    print("â±ï¸ Isso pode levar 15-30 minutos...")
    
    try:
        # Carregar todas as entidades
        print("\n1ï¸âƒ£ Carregando entidades extraÃ­das...")
        entities = load_extracted_entities()
        print(f"âœ… {len(entities)} entidades carregadas")
        
        # Processar todas as entidades
        print(f"\n2ï¸âƒ£ Iniciando normalizaÃ§Ã£o completa...")
        print(f"ğŸ“Š Estimativa: ~{len(entities)//20} chamadas LLM necessÃ¡rias")
        
        normalized, stats, summary = normalize_entities(entities)
        
        print(f"\nğŸ‰ NORMALIZAÃ‡ÃƒO COMPLETA CONCLUÃDA!")
        print(f"\nğŸ“Š EstatÃ­sticas finais:")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  â€¢ {key}: {value:.1f}")
            else:
                print(f"  â€¢ {key}: {value:,}")
        
        print(f"\nğŸ“‹ Resumo das entidades normalizadas:")
        for key, value in summary.items():
            if key not in ['top_entities', 'type_distribution']:
                if isinstance(value, float):
                    print(f"  â€¢ {key}: {value:.1f}")
                else:
                    print(f"  â€¢ {key}: {value:,}")
        
        print(f"\nğŸ·ï¸ DistribuiÃ§Ã£o por tipo:")
        for entity_type, count in sorted(summary['type_distribution'].items(), 
                                       key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {entity_type}: {count:,}")
        
        print(f"\nğŸ† Top 15 entidades por frequÃªncia:")
        for i, entity in enumerate(summary.get('top_entities', [])[:15], 1):
            aliases_info = f" ({entity['aliases_count']} aliases)" if entity['aliases_count'] > 0 else ""
            print(f"  {i:2}. {entity['name']} ({entity['type']}): {entity['frequency']} ocorrÃªncias{aliases_info}")
        
        # Salvar resultados
        output_file = Path("data/normalized_entities.pkl")
        print(f"\nğŸ’¾ Salvando resultados finais em: {output_file}")
        
        with open(output_file, 'wb') as f:
            pickle.dump({
                'normalized_entities': normalized,
                'statistics': stats,
                'summary': summary,
                'total_entities_processed': len(entities)
            }, f)
        
        file_size_mb = output_file.stat().st_size / 1024 / 1024
        print(f"âœ… Resultados salvos! Arquivo: {output_file} ({file_size_mb:.1f} MB)")
        
        # Resumo final
        reduction_pct = stats.get('reduction_percentage', 0)
        print(f"\nğŸ¯ RESUMO FINAL:")
        print(f"   ğŸ“¥ Entidades de entrada: {len(entities):,}")
        print(f"   ğŸ“¤ Entidades normalizadas: {len(normalized):,}")
        print(f"   ğŸ“‰ ReduÃ§Ã£o: {reduction_pct:.1f}%")
        print(f"   ğŸ¤– Chamadas LLM: {stats.get('llm_calls', 0):,}")
        
    except Exception as e:
        print(f"âŒ Erro durante processamento: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
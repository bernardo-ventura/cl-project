"""
Response Enhancer: Converte respostas estruturadas em linguagem natural

Este mÃ³dulo usa o LLM (Ollama) para transformar respostas estruturadas
do Knowledge Graph em texto natural fluido e conversacional.
"""

import logging
import json
from typing import Dict, Any, Optional
from dataclasses import dataclass
import ollama

from .response_formatter import FormattedResponse

logger = logging.getLogger(__name__)


@dataclass
class EnhancedResponse:
    """
    Resposta melhorada em linguagem natural
    
    Attributes:
        natural_answer: Resposta em linguagem natural fluida
        structured_data: Dados estruturados originais
        confidence: ConfianÃ§a na resposta (0-1)
        processing_time: Tempo de processamento
    """
    natural_answer: str
    structured_data: str
    confidence: float
    processing_time: float


class ResponseEnhancer:
    """
    Enhancer que usa LLM para converter respostas estruturadas em linguagem natural
    
    Fluxo:
    1. Recebe resposta estruturada do ResponseFormatter
    2. Cria prompt contextualizado para o LLM
    3. Gera resposta em linguagem natural
    4. Combina dados estruturados + resposta natural
    """
    
    def __init__(self, model_name: str = "llama3.2:3b"):
        """
        Inicializa o enhancer com modelo Ollama
        
        Args:
            model_name: Nome do modelo Ollama a usar
        """
        self.model_name = model_name
        self._test_llm_connection()
    
    def _test_llm_connection(self) -> None:
        """Testa conexÃ£o com Ollama"""
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": "Test"}],
                options={"num_predict": 10}
            )
            logger.info(f"âœ… ConexÃ£o com Ollama ({self.model_name}) estabelecida")
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama nÃ£o disponÃ­vel: {e}")
            raise
    
    def enhance_response(self, 
                        formatted_response: FormattedResponse,
                        original_question: str,
                        query_type: str) -> EnhancedResponse:
        """
        Converte resposta estruturada em linguagem natural
        
        Args:
            formatted_response: Resposta estruturada do ResponseFormatter
            original_question: Pergunta original do usuÃ¡rio
            query_type: Tipo de consulta executada
            
        Returns:
            Resposta melhorada em linguagem natural
        """
        import time
        start_time = time.time()
        
        try:
            # Cria prompt contextualizado
            prompt = self._create_enhancement_prompt(
                question=original_question,
                structured_answer=formatted_response.answer,
                query_type=query_type,
                confidence=formatted_response.confidence,
                metadata=formatted_response.metadata
            )
            
            # Gera resposta natural com LLM
            natural_answer = self._generate_natural_response(prompt)
            
            processing_time = time.time() - start_time
            
            return EnhancedResponse(
                natural_answer=natural_answer,
                structured_data=formatted_response.answer,
                confidence=formatted_response.confidence,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"âŒ Erro no enhancement: {e}")
            # Fallback: retorna resposta estruturada original
            return EnhancedResponse(
                natural_answer=formatted_response.answer,
                structured_data=formatted_response.answer,
                confidence=formatted_response.confidence * 0.8,  # Reduz confidence
                processing_time=time.time() - start_time
            )
    
    def _create_enhancement_prompt(self,
                                  question: str,
                                  structured_answer: str,
                                  query_type: str,
                                  confidence: float,
                                  metadata: Dict[str, Any]) -> str:
        """
        Cria prompt contextualizado para o LLM
        
        Adapta o prompt baseado no tipo de consulta e dados disponÃ­veis
        """
        
        # InstruÃ§Ãµes base
        base_instructions = """VocÃª Ã© um assistente especializado em Machine Learning e Deep Learning. 
Sua tarefa Ã© converter informaÃ§Ãµes estruturadas do Knowledge Graph em respostas naturais e conversacionais.

REGRAS IMPORTANTES:
1. Use linguagem clara e didÃ¡tica
2. Mantenha precisÃ£o tÃ©cnica
3. Seja conciso mas informativo
4. Use exemplos quando apropriado
5. Responda em portuguÃªs brasileiro

"""
        
        # InstruÃ§Ãµes especÃ­ficas por tipo de consulta
        query_specific_instructions = {
            "what_is": "Explique o conceito de forma didÃ¡tica, incluindo definiÃ§Ã£o, caracterÃ­sticas principais e aplicaÃ§Ãµes.",
            "what_uses": "Liste e explique brevemente cada item que usa o conceito mencionado.",
            "what_is_type_of": "Explique a hierarquia e classificaÃ§Ã£o do conceito.",
            "who_created": "ForneÃ§a informaÃ§Ãµes sobre os criadores e contexto histÃ³rico.",
            "how_related": "Explique as conexÃµes e relaÃ§Ãµes entre os conceitos.",
            "list_by_type": "Apresente a lista de forma organizada com breves descriÃ§Ãµes.",
            "find_similar": "Compare e explique as similaridades entre os conceitos."
        }
        
        specific_instruction = query_specific_instructions.get(
            query_type, 
            "Responda de forma clara e informativa."
        )
        
        # Monta prompt final
        prompt = f"""{base_instructions}

TIPO DE CONSULTA: {query_type}
INSTRUÃ‡ÃƒO ESPECÃFICA: {specific_instruction}

PERGUNTA DO USUÃRIO: "{question}"

DADOS DO KNOWLEDGE GRAPH:
{structured_answer}

METADADOS: {metadata.get('result_count', 0)} resultados encontrados, confianÃ§a: {confidence:.1%}

TAREFA: Transforme as informaÃ§Ãµes estruturadas acima em uma resposta natural e conversacional que responda Ã  pergunta do usuÃ¡rio. Mantenha a precisÃ£o tÃ©cnica mas use linguagem acessÃ­vel."""

        return prompt
    
    def _generate_natural_response(self, prompt: str) -> str:
        """
        Gera resposta natural usando Ollama
        
        Args:
            prompt: Prompt contextualizado
            
        Returns:
            Resposta em linguagem natural
        """
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{
                    "role": "user", 
                    "content": prompt
                }],
                options={
                    "num_predict": 300,  # Limite de tokens para resposta concisa
                    "temperature": 0.7,   # Criatividade moderada
                    "top_p": 0.9,        # Diversidade de vocabulÃ¡rio
                    "stop": ["TAREFA:", "PERGUNTA:", "DADOS:"]  # Stop tokens
                }
            )
            
            natural_answer = response['message']['content'].strip()
            
            # ValidaÃ§Ã£o bÃ¡sica
            if len(natural_answer) < 10:
                raise ValueError("Resposta muito curta")
            
            return natural_answer
            
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o LLM: {e}")
            raise
    
    def create_combined_response(self,
                               enhanced: EnhancedResponse,
                               show_structured: bool = True) -> str:
        """
        Combina resposta natural com dados estruturados opcionais
        
        Args:
            enhanced: Resposta melhorada
            show_structured: Se deve mostrar dados estruturados tambÃ©m
            
        Returns:
            Resposta final combinada
        """
        response_parts = [
            "ğŸ¤– **Resposta:**",
            enhanced.natural_answer
        ]
        
        if show_structured:
            response_parts.extend([
                "",
                "ğŸ“Š **Dados Estruturados:**",
                enhanced.structured_data
            ])
        
        response_parts.extend([
            "",
            f"ğŸ¯ **ConfianÃ§a**: {enhanced.confidence:.1%}",
            f"â±ï¸ **Tempo LLM**: {enhanced.processing_time:.2f}s"
        ])
        
        return "\n".join(response_parts)


# Factory function
def create_response_enhancer(model_name: str = "llama3.2:3b") -> ResponseEnhancer:
    """
    Factory function para criar instÃ¢ncia do ResponseEnhancer
    
    Args:
        model_name: Modelo Ollama a usar
        
    Returns:
        InstÃ¢ncia configurada do ResponseEnhancer
    """
    return ResponseEnhancer(model_name)


if __name__ == "__main__":
    # Teste do enhancer
    print("ğŸ§ª Testando Response Enhancer...")
    
    try:
        enhancer = create_response_enhancer()
        
        # Mock de resposta estruturada
        from .query_templates import QueryType
        
        mock_formatted = FormattedResponse(
            answer="""ğŸ“‹ **Gradient Descent**
ğŸ·ï¸ **Tipo**: Algorithm
ğŸ“Š **Propriedades**:
   â€¢ **Uses**: backpropagation, optimization
   â€¢ **Is A**: optimization_algorithm""",
            metadata={'result_count': 5},
            raw_results=[],
            confidence=0.9
        )
        
        # Testa enhancement
        enhanced = enhancer.enhance_response(
            formatted_response=mock_formatted,
            original_question="O que Ã© gradient descent?",
            query_type="what_is"
        )
        
        print(f"\nğŸ“ Resposta Natural:")
        print(enhanced.natural_answer)
        print(f"\nğŸ¯ Confidence: {enhanced.confidence:.1%}")
        print(f"â±ï¸ Tempo: {enhanced.processing_time:.2f}s")
        
        print("\nâœ… Enhancer funcionando!")
        
    except Exception as e:
        print(f"âŒ Erro: {e}")
        print("ğŸ’¡ Certifique-se que o Ollama estÃ¡ rodando: ollama serve")
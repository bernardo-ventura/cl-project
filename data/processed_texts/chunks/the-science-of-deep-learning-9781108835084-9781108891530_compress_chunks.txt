=== CHUNK 001 ===
Palavras: 362
Caracteres: 2887
--------------------------------------------------
ISTUDY

TheScience ofDeep Learning
The Science ofDeep Learning emerged from courses taught bythe author that have
provided thousands ofstudents with training and experience for their academic stud -
ies, and prepared them for careers indeep learning, machine learning, and artiﬁcial
intelligence intop companies inindustry and academia The book begins bycovering the foundations ofdeep learning, followed bykey
deep learning architectures Subsequent parts ongenerative models and reinforcement
learning may beused aspart ofadeep learning course oraspart ofacourse on
each topic The book includes state-of-the-art topics such asTransformers, graph
neural networks, variational autoencoders, and deep reinforcement learning, with a
broad range ofapplications The appendices provide equations for computing gradi -
ents inbackpropagation and optimization, and best practices inscientiﬁc writing and
reviewing The text presents anup-to-date guide tothe ﬁeld built upon clear visualizations
using auniﬁed notation and equations, lowering the barrier toentry for the reader The accompanying website provides complementary code and hundreds ofexercises
with solutions Iddo Drori isafaculty member and associate professor atBoston University, alecturer
atMIT, and adjunct associate professor atColumbia University He was avisiting
associate professor atCornell University inoperations research and information engi -
neering, and research scientist and adjunct professor atNYU Center forData Science,
Courant Institute, and NYU Tandon He holds aPhD incomputer science and was
apostdoctoral research fellow atStanford University instatistics He also holds an
MBA inorganizational behavior and entrepreneurship and has adecade ofindustry
research and leadership experience His main research isinmachine learning, AI, and
computer vision, with 70publications and over 5,100 citations, and hehas taught over
35courses incomputer science Hehas won multiple competitions incomputer vision
conferences and received multiple best paper awards inmachine learning conferences TheScience ofDeep Learning
IDDO DRORI
Massachusetts Institute ofT echnology
Columbia University

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New Y ork, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3,Splendor Forum, Jasola District Centre, New Delhi –110025, India
103 Penang Road, #05–06/07, Visioncrest Commercial, Singapore 238467
Cambridge University Press ispart ofthe University ofCambridge Itfurthers the University’s mission bydisseminating knowledge inthe pursuit of
education, learning, and research atthe highest international levels ofexcellence www.cambridge.org
Information onthis title: www.cambridge.org/highereducation/isbn/9781108835084
DOI: 10.1017/9781108891530
©Iddo Drori 2023
This publication isincopyright

============================================================

=== CHUNK 002 ===
Palavras: 83
Caracteres: 720
--------------------------------------------------
Subject tostatutory exception
and tothe provisions ofrelevant collective licensing agreements,
noreproduction ofany part may take place without the written
permission ofCambridge University Press First published 2023
Printed inthe United Kingdom byTJBooks Limited, Padstow, Cornwall, 2023
Acatalogue record for this publication isavailable from the British Library ISBN 978-1-108-83508-4 Hardback
Additional resources forthis publication atwww.cambridge.org/drori
Cambridge University Press has noresponsibility forthe persistence oraccuracy of
URLs forexternal orthird-party internet websites referred tointhis publication
and does not guarantee that any content onsuch websites is,orwill remain,
accurate orappropriate

============================================================

=== CHUNK 003 ===
Palavras: 350
Caracteres: 6542
--------------------------------------------------
Contents Preface pagexv Acknowledgments xvii Abbreviations andNotation Part IFoundations 1Introduction 3 1.1DeepLearning ...........................3 1.2Outline ...............................4 1.2.1PartI:Foundations: Backpropagation, Optimization, andRegularization ....................4 1.2.2PartII:Architectures: CNNs,RNNs,GNNs,andTrans- formers...........................5 1.2.3PartIII:Generative Models: GANs,VAEs,andNor- malizing Flows......................6 1.2.4PartIV:Reinforcement Learning ............6 1.2.5PartV:Applications ...................7 1.2.6Appendices ........................7 1.3Code................................7 1.4Exercises ..............................8 2Forward and Backpropagation 9 2.1Introduction ............................9 2.2FullyConnected NeuralNetwork .................9 2.3Forward Propagation .......................11 2.3.1Algorithm .........................12 2.3.2Example ..........................12 2.3.3LogisticRegression ....................14 2.4Non-linear Activation Functions .................16 2.4.1Sigmoid ..........................16 2.4.2Hyperbolic Tangent ....................16 2.4.3Rectiﬁed LinearUnit...................17 2.4.4Swish............................18xix vi Contents 2.4.5Softmax ..........................18 2.5LossFunctions ...........................19 2.6Backpropagation ..........................21 2.7Diﬀerentiable Programming ...................22 2.8Computation Graph........................22 2.8.1Example ..........................22 2.8.2LogisticRegression ....................24 2.8.3Forward andBackpropagation ..............25 2.9Derivative ofNon-linear Activation Functions .........26 2.10Backpropagation Algorithm ...................28 2.10.1Example ..........................29 2.11ChainRuleforDiﬀerentiation ..................30 2.11.1TwoFunctions inOneDimension ............30 2.11.2ThreeFunctions inOneDimension ...........31 2.11.3TwoFunctions inHigherDimensions ..........31 2.12Gradient ofLossFunction ....................32 2.13Gradient Descent .........................32 2.14Initialization andNormalization .................33 2.15Software Libraries andPlatforms ................33 2.16Summary ..............................33 3Optimization 35 3.1Introduction ............................35 3.2Overview ..............................35 3.2.1Optimization Problem Classes..............35 3.2.2Optimization Solution Methods .............37 3.2.3Derivatives andGradients ................37 3.2.4Gradient Computation ..................38 3.3First-Order Methods .......................39 3.3.1Gradient Descent .....................39 3.3.2StepSize..........................41 3.3.3Mini-Batch Gradient Descent ..............44 3.3.4Stochastic Gradient Descent ...............44 3.3.5Adaptive Gradient Descent ...............45 3.3.6Momentum ........................46 3.3.7Adagrad ..........................47 3.3.8Adam:Adaptive Moment Estimation ..........47 3.3.9Hypergradient Descent ..................48 3.4Second-Order Methods ......................49 3.4.1Newton’s Method .....................49 3.4.2Second-Order TaylorApproximation ..........51 3.4.3Quasi-Newton Methods .................53 3.5Evolution Strategies ........................54 3.6Summary ..............................55 Contents vii 4Regularization 56 4.1Introduction ............................56 4.2Generalization ...........................56 4.3Overﬁtting .............................58 4.4CrossValidation ..........................59 4.5BiasandVariance .........................59 4.6VectorNorms ...........................59 4.7RidgeRegression andLasso...................60 4.8Regularized LossFunctions ....................61 4.9Dropout Regularization ......................62 4.9.1Random LeastSquareswithDropout ..........63 4.9.2LeastSquareswithNoiseInputDistortion .......64 4.10DataAugmentation ........................64 4.11BatchNormalization .......................65 4.12Summary ..............................65 Part IIArchitectures 5Convolutional Neural Networks 69 5.1Introduction ............................69 5.1.1Representations SharingWeights ............69 5.2Convolution ............................70 5.2.1One-Dimensional Convolution ..............70 5.2.2MatrixMultiplication ..................71 5.2.3Two-Dimensional Convolution ..............73 5.2.4Separable Filters.....................76 5.2.5Properties .........................76 5.2.6Composition ........................77 5.2.7Three-Dimensional Convolution .............77 5.3Layers ...............................78 5.3.1Convolution ........................78 5.3.2Pooling ..........................78 5.4Example ..............................79 5.5Architectures ............................80 5.6Applications ............................86 5.7Summary ..............................89 6Sequence Models 91 6.1Introduction ............................91 6.2NaturalLanguage Models.....................91 6.2.1BagofWords.......................91 6.2.2FeatureVector ......................92 6.2.3N-grams ..........................92 viii Contents 6.2.4MarkovModel.......................92 6.2.5StateMachine .......................92 6.2.6Recurrent NeuralNetwork ................93 6.3Recurrent NeuralNetwork ....................93 6.3.1Architectures .......................94 6.3.2LossFunction .......................95 6.3.3DeepRNN.........................97 6.3.4Bidirectional RNN....................98 6.3.5Backpropagation Through Time.............99 6.4GatedRecurrent Unit.......................102 6.4.1UpdateGate.......................104 6.4.2Candidate Activation ...................105 6.4.3ResetGate........................105 6.4.4Function ..........................107 6.5LongShort-Term Memory ....................108 6.5.1ForgetGate........................109 6.5.2InputGate........................110 6.5.3Memory Cell.......................112 6.5.4Candidate Memory ....................113 6.5.5OutputGate.......................113 6.5.6Peephole Connections ..................115 6.5.7GRUvs.LSTM......................115 6.6Sequence toSequence .......................117 6.7Attention ..............................118 6.8Embeddings ............................121 6.9Introduction toTransformers ...................122 6.10Summary ..............................123 7Graph Neural Networks 124 7.1Introduction ............................124 7.2Deﬁnitions .............................126 7.3Embeddings ............................130 7.4NodeSimilarity ..........................132

============================================================

=== CHUNK 004 ===
Palavras: 346
Caracteres: 6578
--------------------------------------------------
7.4.1Adjacency-based Similarity ...............132 7.4.2Multi-hop Similarity ...................132 7.4.3Overlap Similarity ....................132 7.4.4Random WalkEmbedding ................133 7.4.5GraphNeuralNetwork Properties ............135 7.5Neighborhood Aggregation inGraphNeuralNetworks .....136 7.5.1Supervised NodeClassiﬁcation UsingaGNN.....138 7.6GraphNeuralNetwork Variants .................138 7.6.1GraphConvolution Network ...............138 7.6.2GraphSAGE ........................139 7.6.3GatedGraphNeuralNetworks .............139 Contents ix 7.6.4GraphAttention Networks ................140 7.6.5Message-Passing Networks ................140 7.7Applications ............................140 7.8Software Libraries, Benchmarks, andVisualization .......141 7.9Summary ..............................141 8Transformers 142 8.1Introduction ............................142 8.2General-Purpose Transformer-Based Architectures .......143 8.2.1BERT...........................143 8.3Self-Attention ...........................143 8.4Multi-head Attention .......................144 8.5Transformer ............................145 8.5.1Positional Encoding ...................145 8.5.2Encoder ..........................145 8.5.3Decoder ..........................146 8.5.4Pre-training andFine-tuning ..............146 8.6Transformer Models........................146 8.6.1Autoencoding Transformers ...............146 8.6.2Auto-regressive Transformers ..............147 8.6.3Sequence-to-Sequence Transformers ...........147 8.6.4GPT-3...........................148 8.7VisionTransformers ........................148 8.8Multi-modal Transformers ....................149 8.9TextandCodeTransformers ...................149 8.10Summary ..............................150 Part III Generative Models 9Generative Adversarial Networks 153 9.1Introduction ............................153 9.1.1Progress ..........................153 9.1.2GameTheory .......................154 9.1.3Co-evolution ........................155 9.2Minimax Optimization ......................155 9.3Divergence between Distributions ................157 9.3.1LeastSquaresGAN....................158 9.3.2f-GAN...........................158 9.4Optimal Objective Value.....................158 9.5Gradient DescentAscent.....................158 9.6Optimistic Gradient DescentAscent...............159 9.7GANTraining ...........................160 9.7.1Discriminator Training ..................160 x Contents 9.7.2Generator Training ....................160 9.7.3Alternating Discriminator–Generator Training .....161 9.8GANLosses............................162 9.8.1Wasserstein GAN.....................162 9.8.2Unrolled GAN.......................163 9.9GANArchitectures ........................164 9.9.1Progressive GAN.....................164 9.9.2DeepConvolutional GAN................164 9.9.3Semi-Supervised GAN..................164 9.9.4Conditional GAN.....................164 9.9.5Image-to-Image Translation ...............165 9.9.6Cycle-Consistent GAN..................165 9.9.7Registration GAN....................167 9.9.8Self-Attention GANandBigGAN ............167 9.9.9Composition andControlwithGANs..........167 9.9.10Instance Conditioned GAN...............168 9.10Evaluation .............................168 9.10.1Inception Score......................168 9.10.2FrechetInception Distance ................169 9.11Applications ............................169 9.11.1SuperResolution andRestoration ............169 9.11.2StyleSynthesis ......................169 9.11.3ImageCompletion ....................169 9.11.4De-raining .........................170 9.11.5MapSynthesis .......................170 9.11.6PoseSynthesis .......................170 9.11.7FaceEditing........................170 9.11.8Training DataGeneration ................170 9.11.9Text-to-Image Synthesis .................170 9.11.10Medical Imaging .....................171 9.11.11VideoSynthesis ......................171 9.11.12MotionRetargeting ....................171 9.11.133DSynthesis .......................171 9.11.14GraphSynthesis .....................172 9.11.15Autonomous Vehicles ...................172 9.11.16Text-to-Speech Synthesis .................172 9.11.17VoiceConversion .....................172 9.11.18MusicSynthesis ......................172 9.11.19ProteinDesign ......................172 9.11.20NaturalLanguage Synthesis ...............173 9.11.21Cryptography .......................173 9.12Software Libraries, Benchmarks, andVisualization .......173 9.13Summary ..............................173 Contents xi 10Variational Autoencoders 174 10.1Introduction ............................174 10.2Variational Inference .......................174 10.2.1ReverseKL........................176 10.2.2ScoreGradient ......................178 10.2.3Reparameterization Gradient ..............179 10.2.4Forward KL........................180 10.3Variational Autoencoder .....................181 10.3.1Autoencoder ........................181 10.3.2Variational Autoencoder .................182 10.4Generative Flows.........................184 10.5Denoising Diﬀusion Probabilistic Model.............186 10.5.1Forward NoisingProcess .................186 10.5.2ReverseGeneration bySampling ............186 10.6Geometric Variational Inference .................187 10.6.1MoserFlow........................188 10.6.2Riemannian Score-Based Generative Models......188 10.7Software Libraries .........................189 10.8Summary ..............................189 Part IV Reinforcement Learning 11Reinforcement Learning 193 11.1Introduction ............................193 11.2Multi-Armed Bandit .......................193 11.2.1GreedyApproach .....................194 11.2.2 ε-greedyApproach ....................194 11.2.3UpperConﬁdence Bound.................196 11.3StateMachines ...........................196 11.4MarkovProcesses .........................196 11.5MarkovDecision Processes ....................199 11.5.1StateofEnvironment andAgent............200 11.6Deﬁnitions .............................202 11.6.1Policy...........................202 11.6.2StateActionDiagram ..................203 11.6.3StateValueFunction ...................204 11.6.4ActionValueFunction ..................207 11.6.5Reward ..........................209 11.6.6Model...........................210 11.6.7AgentTypes........................210 11.6.8Problem Types......................211 11.6.9AgentRepresentation ofState..............211 11.6.10Bellman Expectation Equation forStateValueFunction 212 xii Contents 11.6.11 BellmanExpectationEquationforActionValueFunction 214 1 1

============================================================

=== CHUNK 005 ===
Palavras: 355
Caracteres: 1135
--------------------------------------------------
7 O p t i m a lP o l i c y 1 O p t i m a lV a l u eF u n c t i o n 215
11.7.2 Bellman Optimality Equation for V⋆ 215
11.7.3 Bellman Optimality Equation for Q⋆ 216
11.8 Planning by Dynamic Programming with a Known MDP 1 I t e r a t i v eP o l i c yE v a l u a t i o n 2 P o l i c yI t e r a t i o n 218
11.8.3 Inﬁnite Horizon Value Iteration 219
11.9 Reinforcement Learning 219
11.9.1 Model-Based Reinforcement Learning 2 P o l i c yS e a r c h 3 M o n t eC a r l oS a m p l i n g 222
11.9.4 Temporal Diﬀerence Sampling 222
11.9.5Q- L e a r n i n g 226
11.9.7 On-Policy vs 227
11.10 Maximum Entropy Reinforcement Learning 228
12 Deep Reinforcement Learning 229
1 2 1 I n t r o d u c t i o n 2 F u n c t i o nA p p r o x i m a t i o n 230
12.2.1 State Value Function Approximation 230
12.2.2 Action Value Function Approximation 3 V a l u e - B a s e dM e t h o d s 1 E x p e r i e n c eR e p l a y 232
12.3.2 Neural Fitted Q- I t e r a t i o n 233
12.3.3 Deep Q- N e t w o r k 4 T a r g e tN e t w o r k 6 P r i o r i t i z e dR e p l a y 8 D u e l i n gN e t w o r k s 4 P o l i c y - B a s e dM e t h o d s 1 P o l i c yG r a d i e n t

============================================================

=== CHUNK 006 ===
Palavras: 353
Caracteres: 1273
--------------------------------------------------
238
12.4.3 Subtracting a Baseline 5 A c t o r – C r i t i cM e t h o d s 1 A d v a n t a g eA c t o r – C r i t i c 240
12.5.2 Asynchronous Advantage Actor–Critic 3 I m p o r t a n c eS a m p l i n g 4 S u r r o g a t eL o s s 241
Contents xiii
1 2 5 N a t u r a lP o l i c yG r a d i e n t 242
12.5.6 Trust Region Policy Optimization 243
12.5.7 Proximal Policy Optimization 244
12.5.8 Deep Deterministic Policy Gradient 244
12.6 Model-Based Reinforcement Learning 1 M o n t eC a r l oT r e eS e a r c h 245
12.6.2 Expert Iteration and AlphaZero 3 W o r l dM o d e l s 7 I m i t a t i o nL e a r n i n g 8 E x p l o r a t i o n 1 S p a r s eR e w a r d s 249
Part V Applications
13 Applications 253
1 3 1 I n t r o d u c t i o n 2 A u t o n o m o u sV e h i c l e s 253
13.3 Climate Change and Climate Monitoring 255
13.3.1 Predicting Ocean Biogeochemistry 255
13.3.2 Predicting Atlantic Multidecadal Variability 258
13.3.3 Predicting Wildﬁre Growth 4 C o m p u t e rV i s i o n 1 K i n s h i pV e r i ﬁ c a t i o n 2 I m a g e - t o - 3 D 263
13.4.3 Image2LEGO ® 264
13.4.4 Imaging through Scattering Media 265
13.4.5 Contrastive Language-Image Pre-training 269
13.5 Speech and Audio Processing 269
13.5.1 Audio Reverb Impulse Response Synthesis 2 V o i c eS w a p p i n g

============================================================

=== CHUNK 007 ===
Palavras: 385
Caracteres: 1599
--------------------------------------------------
270
13.5.3 Explainable Musical Phrase Completion 271
13.6 Natural Language Processing 273
13.6.1 QuantifyingandAlleviatingDistributionShiftsinFoun-
dation Models on Review Classiﬁcation 7 A u t o m a t e dM a c h i n eL e a r n i n g 278
13.8.1 Learning-to-Learn STEM Courses 9 P r o t e o m i c s 280
13.9.1 Protein Structure Prediction 2 P r o t e i nD o c k i n g 1 0C o m b i n a t o r i a lO p t i m i z a t i o n 1P r o b l e m so v e rG r a p h s 288
xiv
13.10.2 Learning Graph Algorithms as Single-Player Games 2 8 9
13.11.1 Pedestrian Wind Estimation in Urban Environments 2F u s i o nP l a s m a 291
Appendix A Matrix Calculus 293
A.1 Gradient Computations for Backpropagation 1 S c a l a rb yV e c t o r 2 S c a l a rb yM a t r i x 3 V e c t o rb yV e c t o r 4 M a t r i xb yS c a l a r 294
A.2 Gradient Computations for Optimization 1 D o tP r o d u c tb yV e c t o r 2 Q u a d r a t i cF o r mb yV e c t o r 295
Appendix B Scientiﬁc Writing and Reviewing Best Practices 296
B 1 W r i t i n gB e s tP r a c t i c e s 1 I n t r o d u c t i o n 3 F i g u r e sa n dT a b l e s 297
B.1.5 Abbreviations and Notation 297
B.2 Reviewing Best Practices 298
References 299
Index 335Contents
Preface
This book provides comprehensive and clear coverage of deep learning, which has
transformed the ﬁeld of artiﬁcial intelligence The book is distinctive in that it
uses a uniﬁed notation, high-quality illustrated ﬁgures and the most up-to-date
material in the ﬁeld, and is accompanied by hundreds of code samples, exercises,
and solutions on each topic, automatically generated by program synthesis

============================================================

=== CHUNK 008 ===
Palavras: 356
Caracteres: 2458
--------------------------------------------------
The
Science of Deep Learning emerged from courses taught by the author in the past
ﬁve years that have provided thousands of students with training and experi-
ence for their academic studies, and prepared them for careers in deep learning,
machine learning, and artiﬁcial intelligence in leading companies in industry
and academia The motivation for the book is to provide a guide to the ﬁeld
built upon clear visualizations using a uniﬁed notation and equations The con-
tent is self-contained, using a uniﬁed language so that students, teachers, and
researchers in academia and industry can use the book without having to over-
come the barriers to entry of the speciﬁc language and notation of each topic Introductory topics are represented using both basic linear algebra and graphs
simultaneously, along with the corresponding algorithms Coverage
The material is presented in ﬁve main parts:
1 Part I, on the foundations of deep learning, includes Chapters 1–4, which
covers core deep learning material on forward and backpropagation, opti-
mization, and regularization Part II, on deep learning architectures, includes Chapters 5–8 This covers
key architectures including convolutional neural networks (CNNs), recur-
rent neural networks (RNNs), long short-term memory (LSTMs), gated
recurrent units (GRUs), graph neural networks (GNNs), and Transform-
ers Part III, on generative models, comprises Chapters 9 and 10, which cover
generativeadversarialnetworks(GANs)andvariationalautoencoders(VAEs) Part IV addresses reinforcement learning and deep reinforcement learning
(Chapters 11 and 12) Part V, on applications (Chapter 13), covers a broad range of deep learning
xvi Preface
applications, which are also distributed among the chapters by topic and
relevance Appendices provide equations for computing gradients in backpropagation
and optimization, and best practices in scientiﬁc writing and reviewing Contribution
The book contributes to the literature in the ﬁeld in that it uses rigorous math
with a uniﬁed notation In addition, over the past ﬁve years, during the course
of instruction, advanced topics have been simpliﬁed to become part of the core,
whilebringinginnewtopicsintheﬁeldasadvancedtopics.Thekeyadvantagesof
this book are that it is up-to-date, with the latest advances in the ﬁeld including
unique content; the math is rigorous, using a uniﬁed notation; and the book
presents comprehensive algorithms and uses high-quality ﬁgures

============================================================

=== CHUNK 009 ===
Palavras: 351
Caracteres: 2248
--------------------------------------------------
Audience and Prerequisite Knowledge
The book is intended for students and researchers in academia and industry, as
well as lecturers in academia The book is primarily intended for computer sci-
ence undergraduate and graduate students, as well as advanced PhD students This book has been used for teaching students mainly in computer science, elec-
trical engineering, data science, statistics, and operations research The required
backgroundislinearalgebraandcalculus.Optionalbackgroundismachinelearn-
ing and programming experience The book is also applicable for a wide audience
of students pursuing degrees in STEM ﬁelds with the required background The
book is useful for researchers in academia and industry, as well as data scientists
and algorithm developers of artiﬁcial intelligence Finally, the book may be used
by lecturers in academia for teaching a course on deep learning, and chapters
may be used in teaching topics in courses on machine learning, data science, op-
timization, and reinforcement learning at the undergraduate and graduate levels Usage
The ﬁrst four parts of the book have been used as a textbook in courses on deep
learning The third part, on generative models, may be used as part of a course
on unsupervised learning The fourth part may be used as part of a course on
reinforcement learning or deep reinforcement learning Appendix A is useful for
computing the gradients in backpropagation and optimization Appendix B may
be used in project-based courses for providing best practices in scientiﬁc writing
and reviewing Acknowledgments
I developed the material for this book over the past ﬁve years while teach-
ing a dozen deep learning classes The book is now used as the new textbook
in deep learning at Columbia University I would like to thank Columbia Uni-
versity students that read each chapter before class and reported errata that
I ﬁxed and improved the book, MIT students and colleagues for reading the
book and providing feedback, Leslie Goodman and Gary Daniel Smith for copy
editing, Michal Solel Elnekave and Nikhil Singh, who were helpful in producing
high-quality ﬁgures in Illustrator from sketches, and Maggie Jeﬀers and Lauren
Cowles, my editors who encouraged me to see the book to completion

============================================================

=== CHUNK 010 ===
Palavras: 88
Caracteres: 515
--------------------------------------------------
I wish
to thank numerous colleagues, particularly Kyunghyun Cho and Claudio Silva
of NYU, Nakul Verma and Itsik Pe’er of Columbia University, Tonio Buonassisi
and Gilbert Strang of MIT, Dov Te’eni of Tel Aviv University, and Madeleine
Udell and David Williamson of Cornell University Finally, I’d like to thank my
family, Adi, Danielle, Yael, Sharon, and Gilly, my sister Dr Tali Drori Snir, mom
Nili and special thanks to my dad, Prof Israel Drori, who has published over a
dozen books and gets to read my ﬁrst one

============================================================

=== CHUNK 011 ===
Palavras: 350
Caracteres: 2749
--------------------------------------------------
Abbreviations and Notation xix Abbreviations and Notation Abbreviations A2C advantage actor–critic A3C asynchronous advantage actor–critic AMV Atlantic Multidecadal Variability AVI amortized variational inference BBVI black-box variational inference BCE binary cross entropy BERT bidirectional encoder representations from Transformers BFGS Broyden–Fletcher–Goldfarb–Shanno (correction) CFD computational ﬂuid dynamics CGAN conditional GAN CLIP contrastive language-image pre-training CNN convolutional neural network DAG directed acyclic graph DCGAN deep convolutional generative adversarial network DDPG deep deterministic policy gradient DDPM denoising diﬀusion probabilistic model DFP Davidon–Fletcher–Powell (correction) DMD digital micromirror device DQN deep Q-network ELBO evidence lower bound EMD Earth mover’s distance ENSO El Ni˜ no-Southern Oscillation ESM Earth system model FID Frechet inception distance FKL forward KL GAE generalized advantage estimation GAN generative adversarial network GAT graph attention network GCN graph convolutional network GDA Gradient descent ascent GNN graph neural network GPT-3 generative pre-trained Transformer 3 GRU gated recurrent unit IID independent and identically xx Abbreviations and Notation IR impulse response IS inception score JS Jensen–Shannon (divergence) KL Kullback–Leibler LSTM long short-term memory MAE mean absolute error MC Monte Carlo MCMC Markov chain Monte Carlo MCTS Monte Carlo tree search MDP Markov decision process MFVI mean-ﬁeld variational inference MST minimum spanning tree NAS neural architecture search NPCC negative Pearson correlation coeﬃcient NPG natural policy gradient OGDA optimistic gradient descent ascent PCA principle component analysis PPO proximal policy optimization ReLU rectiﬁed linear unit function RFIW Recognizing Families in the Wild RKL reverse KL RNN recurrent neural network seq2seq sequence-to-sequence (models) SGAN semi-supervised GAN SGD stochastic gradient descent SLM spatial light modulator SSP single-source shortest paths SSS sea surface salinity SST sea-surface temperatures TD temporal diﬀerence TRPO trust region policy optimization TSP traveling salesman problem UCB upper conﬁdence bound VAE variational autoencoder VI variational inference VQ-VAE vector quantized variational autoencoder VRN Volumetric Regression Network VRP vehicle routing problem WGAN Wasserstein GAN Abbreviations and Notation xxi Notation General E expectation R real numbers I identity matrix XTmatrix transpose /bardblx/bardblp/lscriptpnorm of vector x ∩ intersection ∪ union || concatenation of vectors N(μ,σ) Gaussian distribution with mean μand standard deviation σ ∇xygradient of ywith respect to x Neural Networks αlearning rate θlearning parameter

============================================================

=== CHUNK 012 ===
Palavras: 283
Caracteres: 2081
--------------------------------------------------
/lscriptneural network layer index W/lscriptweight matrix of layer z/lscriptpre-activation vector of layer Z/lscriptpre-activation matrix of layer a/lscriptactivation vector of layer A/lscriptactivation matrix of layer f/lscriptnon-linear activation function of layer σsigmoid function Lloss function Rregularization function Convolutional Neural Networks f⋆gconvolution of functions fandg Sequence Models xtinput vector at time t hthidden vector at time t ytoutput vector at time t Uweight matrix applied to input vector shared across time Vweight matrix applied to hidden vector shared across time Wweight matrix applied to previous hidden vector shared across time xxii Abbreviations and Notation Graph Neural Networks Ggraph Vgraph nodes Egraph edges N(i) neighbors of node i Agraph adjacency matrix Dgraph diagonal degree matrix Lgraph Laplacian matrix Lsymsymmetric normalized Laplacian matrix Lrwrandom walk normalized Laplacian matrix h/lscriptembedding vector of layer /lscript Generative Models DGAN discriminator GGAN generator DKLKullback–Leibler divergence DJSJenson–Shannon divergence Reinforcement Learning s state S set of states a action A set of actions T(s,a,s/prime) transition function from state sand action ato next state s/prime r reward R(s,a) reward for state sand action a γ reward discount factor gt return at time step t π policy π(a|s) probability of taking action ain statesunder policy π h horizon Vh π(s) state value function with respect to policy π with horizon hof states Qh π(s,a) action value function with respect to policy π with horizon hof statesand action a π⋆optimal policy V⋆(s) state value function with respect to optimal policy π⋆ for state s Q⋆(s,a) action value function with respect to optimal policy π⋆ for state sand action a Part I Foundations 1 Introduction Intheﬁfteenthcentury,theprintingpressrevolutionizedtheworldbyovercoming the genomic bottleneck that allows for only two billion characters of our DNA to bepassedonfromgenerationtogeneration.Theprintedtextallowsforunlimited knowledge to be passed on between generations

============================================================

=== CHUNK 013 ===
Palavras: 358
Caracteres: 2695
--------------------------------------------------
Acommondistinctionbetweenthecapabilitiesofhumansandmachinesisthat
humansaregeneralistsandmachinesarespecialists.Thedeeplearningrevolution
has resulted in many specialized machine learning systems with super-human
capabilities under the title AlphaX A few noteworthy examples are AlphaGo
(Silver et al., 2016) for playing Go, AlphaZero (Silver et al., 2017) for playing
chess, AlphaHoldem (Zhao et al., 2022) for playing poker, AlphaD3M (Drori,
Krishnamurthy, Rampin, Lourenco, One, Cho, Silva and Freire, 2018) for auto-
mated machine learning, AlphaStock (Wang, Zhang, Tang, Wu and Xiong, 2019)
for trading stocks, AlphaStar (Vinyals et al., 2019) for playing multi-player strat-
egy games, AlphaDogﬁght (Pope et al., 2021) for ﬂying ﬁghter jets, AlphaFold
(Jumper et al., 2021) for protein structure prediction, and AlphaCode (Li et al.,
2022) for competition-level code generation In contrast, recent deep learning Transformers, also called foundation models,
trainedwithonetrillionparameters,aregeneralists.Considerthetaskoflearning
a university-level course A human may learn at most a few hundred courses with
great eﬀort during an entire lifetime, whereas a foundation model is soon able
to learn all courses in days with super-human performance Understanding such
a machine is very diﬀerent from that of a human Deep learning and artiﬁcial intelligence (AI) are revolutionizing the world
again in the twenty-ﬁrst century by overcoming the human perception of reality,
which is limited by our brains and senses Machines are revealing to humans
insights and new understandings of reality, in which, by comparison, individual
human capabilities are mundane 1.1 Deep Learning
Deep learning is narrowly deﬁned as optimizing neural networks that have many
layers In the broader sense, deep learning encompasses all methods, architec-
tures, and applications involving neural network representations Deep neural
4 1 Introduction
networks are inspired by neurons and their connections in the brain The back-
propagation algorithm is the most commonly used approach for optimizing deep
neuralnetworks.Backpropagationisbasedoncomputinggradientsofalossfunc-
tion using the chain rule in reverse mode diﬀerentiation Backpropagation and
gradient-based methods for optimizing neural networks are very diﬀerent from
biological learning mechanisms in the brain Deep neural networks may also be
optimized using genetic algorithms or Hebbian learning rules, which are inspired
by learning in biological neural networks This book focuses on the broad deﬁ-
nition of deep learning, encompassing methods, architectures, and applications
that use neural network representations optimized using backpropagation

============================================================

=== CHUNK 014 ===
Palavras: 427
Caracteres: 2956
--------------------------------------------------
1.2 Outline
The book is divided into ﬁve parts: (1) Foundations ,( 2 )Architectures ,( 3 )Gen-
erative Models ,( 4 )Reinforcement Learning , and (5) Applications 1.2.1 Part I: Foundations: Backpropagation, Optimization, and Regularization
Part I,Foundations ,consistsofthreechapters.Chapter2deﬁnesneuralnetworks
and presents forward propagation and backpropagation Neural networks are de-
ﬁned as a composition of functions consisting of a linear and a non-linear part The chapter deﬁnes the network inputs, pre-activations, non-linear activation
functions, activation units, and outputs These are used to introduce forward
propagation in neural networks Next, the chapter presents loss functions and
their gradients, derivatives of non-linear activation functions, and the chain rule These are used to explain backpropagation in a neural network, which is the
cornerstone of training neural networks by gradient descent Multiple examples
illustrate the algorithms and provide the backpropagation derivations using the
chain rule in reverse mode diﬀerentiation Finally, the chapter presents initializa-
tion and normalization strategies for neural networks and the key deep learning
software libraries and platforms Chapter 3 presents optimization in deep learning, focusing on gradient descent
which iteratively ﬁnds a local minimum by taking steps in the direction of the
steepest descent Three main problems with training neural networks using gra-
dient descent and their solutions are discussed: (1) the total loss function with
respect to the neural network weights, which is a sum of many individual losses
for many samples – the solution is mini-batch or stochastic gradient descent;
(2) the derivative of the total loss, which is computed with respect to all of
the network weights – the solution is backpropagation; and (3) the directions
of gradients for consecutive time steps which follow optimized step sizes are or-
thogonal, forming a zig-zag pattern, which is slow, especially in ﬂat regions The
solution is adaptive gradient descent methods that use previous gradients to de-
terminethestepsize.Next,thechapterpresentssecond-ordermethods,including
1.2 Outline 5
practical quasi-Newton approaches Finally, the chapter discusses gradient-free
optimization approaches such as evolution strategies Chapter 4 presents regularization as a technique that can be used to prevent
overﬁtting and explains generalization, bias, and variance The chapter presents
three methods for regularization: (1) adding a penalty term to the cost function –
the penalty term is usually a function of the number of parameters in the model;
(2) dropout, which is a technique that randomly sets several of the weights in
a neural network to zero, which helps to prevent overﬁtting by reducing the
variance of the network; and (3) data augmentation, which is a technique that
involves modifying the input data to the neural network by applying random
transformations

============================================================

=== CHUNK 015 ===
Palavras: 363
Caracteres: 2557
--------------------------------------------------
This technique also helps prevent overﬁtting by increasing the
size of the training set 1.2.2 Part II: Architectures: CNNs, RNNs, GNNs, and Transformers
Part II,Architectures , is about deep learning architectures and consists of four
chapters The ﬁrst three chapters in this part present successful deep learning
representations since they share weights across space, time, or neighborhoods Chapter 5 presents convolutional neural networks (CNNs), which are a type
of neural network that is designed to recognize patterns in images The network
comprises a series of layers, with each layer performing a speciﬁc function The
ﬁrst layer is typically a convolutional layer, which performs a convolution opera-
tion on the input image The convolution operation is a mathematical operation
that extracts information from the input image The output of the convolutional
layer is then passed to a pooling layer, which reduces the number of neurons
in the network Multiple convolutions and pooling layers are followed by a se-
ries of fully connected layers responsible for classiﬁcation or other applications
performed on the image Convolutional neural networks perform well in practice
across a broad range of applications since they share weights at multiple scales
across space Finally, the chapter describes CNN architectures such as residual
neural networks (ResNets), DenseNets, and ODENets Chapter 6 introduces recurrent neural networks (RNNs), which share weights
across time This chapter describes backpropagation through time, its limita-
tions, and the solutions in the form of long short-term memory (LSTM) and
gated-recurrent unit (GRU) Next, the chapter describes sequence-to-sequence
models, followed by encoder–decoder attention and self-attention and embed-
dings Chapter 7 presents graph neural networks (GNNs), which share weights across
neighborhoods.Thechapterbeginswiththedeﬁnitionsofgraphsandtheirrepre-
sentations Graph neural networks are introduced and applied to irregular struc-
tures such as networks They are used for three tasks: (1) predicting properties
of nodes; (2) predicting properties of edges; and (3) predicting properties of
sub-graphs or properties of entire graphs The second part of the book concludes with Chapter 8, which covers state-of-
6 1 Introduction
the-art Transformers, also known as foundation models, which have become a
mainstream architecture in deep learning Transformers have disrupted various
ﬁelds, including natural language processing, computer vision, audio process-
ing, programming, and education

============================================================

=== CHUNK 016 ===
Palavras: 353
Caracteres: 2564
--------------------------------------------------
Large Transformer models currently consist
of more than one trillion parameters, and the number of parameters of Trans-
formers is increasing by orders of magnitude each year; it is on track to surpass
the number of connections in the human brain Transformers may be classiﬁed
into three types of architectures: (1) autoencoding Transformers, which is a stack
of encoders; (2) auto-regressive Transformers, which is a stack of decoders; and
(3) sequence-to-sequence Transformers, which is a stack of encoders connected
to a stack of decoders New scalable deep learning architectures such as Trans-
formers are revolutionizing how machines perceive the world, make decisions,
and generate novel output 1.2.3 Part III: Generative Models: GANs, VAEs, and Normalizing Flows
Thetaskofclassiﬁcationmapsasetofexamplestoalabel.Incontrast,generative
models map a label to a set of examples Part III, Generative Models , consists
of two chapters Chapter 9 introduces generative adversarial network (GAN) theory, practice,
and applications The chapter begins by describing the roles of the generator and
discriminator Next, the advantages and limitations of diﬀerent loss functions
are described Generative adversarial network training algorithms are presented,
discussing the issues of mode collapse and vanishing gradients while providing
state-of-the-art solutions Finally, the chapter concludes with a broad range of
applications of GANs Chapter 10 introduces variational inference and its extension to black-box
variational inference used in practice for inference on large datasets Both reverse
Kullback–Leibler (KL) and forward KL approaches are presented The chapter
coversthevariationalautoencoderalgorithm,whichconsistsofanencoderneural
network for inference and a decoder network for generation, trained end-to-end
by backpropagation The chapter describes how the variational approximation
of the posterior is improved using a series of invertible transformations, known
as normalizing ﬂows, in both discrete and continuous domains Finally, state-of-
the-art examples of deep variational inference on manifolds are presented 1.2.4 Part IV: Reinforcement Learning
Part IV covers Reinforcement Learning , a type of machine learning in which an
agent learns by interacting with an environment Chapter 11 begins by deﬁning a stateless multi-armed bandit, presenting the
trade-oﬀ between exploration and exploitation Next, the chapter covers ba-
sic principles of state machines and Markov decision processes (MDPs) with
1.3 Code 7
known transition and reward functions

============================================================

=== CHUNK 017 ===
Palavras: 356
Caracteres: 2735
--------------------------------------------------
Finally, the chapter presents reinforce-
ment learning in which the transition and reward functions are unknown, and
therefore the agent interacts with the environment by sampling the world Monte
Carlo sampling and temporal diﬀerence sampling are described with examples The chapter concludes by presenting the Q-learning algorithm Chapter12presentsdeepreinforcementlearningthroughvalue-basedmethods,
policy-based methods, and actor–critic methods Value-based methods covered
include deep Q-networks and present prioritized replay Policy-based methods
described include policy gradients and REINFORCE Next, the chapter covers
actor–critic methods, including advantage actor–critic and asynchronous advan-
tage actor–critic Advanced hybrid approaches, such as natural policy gradient,
trust region policy optimization, proximal policy optimization, and a deep de-
terministic policy gradient, are presented Next, the chapter covers model-based
reinforcement learning approaches, including Monte Carlo tree search (MCTS),
AlphaZero, and world models The chapter concludes by presenting imitation
learning and exploration strategies for environments with sparse rewards 1.2.5 Part V: Applications
The book concludes with Part V, which covers a dozen state-of-the-art applica-
tions of deep learning in a broad range of domains: autonomous vehicles, climate
changeandmonitoring,computervision,audioprocessing,voiceswapping,music
synthesis, natural language processing, automated machine learning, learning-
to-learn courses, protein structure prediction and docking, combinatorial opti-
mization, computational ﬂuid dynamics, and plasma physics Each deep learning
application is brieﬂy described, along with a visualization or system architecture 1.2.6 Appendices
The ﬁrst appendix, Matrix Calculus, deﬁnes the partial derivatives of a function
with respect to variables and is helpful for gradient computations in backprop-
agation and optimization The second appendix summarizes best practices in
scientiﬁc writing and reviewing A section on scientiﬁc writing addresses the ab-
stract, introduction, related work, the structure of the text, ﬁgures, captions,
results, discussion, and the reader’s perspective and provides this book’s style
sheet A section on reviewing explains the review process, including best prac-
tices for evaluating and rating scientiﬁc work and writing a rebuttal 1.3 Code
Hundreds of Python functions are automatically generated on each topic for
each chapter by program synthesis using deep learning All of the code is made
available on the book’s website at www.dlbook.org 8 1 Introduction
1.4 Exercises
Each chapter has around a dozen human-generated theoretical and programming
exercises and their solutions

============================================================

=== CHUNK 018 ===
Palavras: 353
Caracteres: 2660
--------------------------------------------------
In addition, hundreds of questions and solutions on
each topic are automatically generated by program synthesis using deep learn-
ing All questions and solutions are made available on the book’s website at
www.dlbook.org 2 Forward and Backpropagation
2.1 Introduction
Thischapterdeﬁnesneuralnetworksandpresentsforwardpropagationandback-
propagation.Neuralnetworksaredeﬁnedasacompositionoffunctionsconsisting
ofalinearandanon-linearpart.Thelinearpartismatrixmultiplication,andthe
non-linear part is a non-linear activation function We deﬁne the network inputs
x, pre-activations z, non-linear activation functions f, activation units a,a n d
outputsy These are used to introduce forward propagation in a neural network Next, we introduce loss functions and their gradients, derivatives of non-linear
activation functions, and the chain rule These are used to present backpropaga-
tion in a neural network, which is the cornerstone of training neural networks by
gradient descent Next, we present initialization and normalization strategies for
neural networks Finally, the chapter introduces the key deep learning software
libraries and platforms 2.2 Fully Connected Neural Network
A neural network as shown in Figure 2.1 is a composition of functions:
FL(...F1(F0(x))) (2.1)
F/lscriptrepresents layers /lscript=0,...,L, where each function F/lscriptconsists of a linear
part which is a matrix multiplication W(green) and non-linear part which is
pointwise application of a non-linear function f(blue) Figure 2.2 shows a fully
connectedneuralnetwork.Eachcolumnofpre-activationsisdenotedby z/lscript,which
is the result of multiplying the input to the layer by a matrix W/lscriptT A non-linear
function fis applied pointwise to the coeﬃcients of the pre-activations z/lscriptto
form activation units a/lscript Together, these operations form layer /lscriptof the network Layer/lscript= 0 is the input layer with input example xand layer /lscript=Lis the output
layer with output y The number of activation units in layer /lscriptfor/lscript=0,...,Lis
denoted by n/lscriptwhereL=3 Layer/lscriptcontains pre-activations z/lscriptand activation units a/lscript, inputsx=a0,a n d
outputsy=aL, as shown in Figure 2.2 The input x=a0is a vector denoting
a single sample For example, if xis a color w×himage, then a0is ﬂattened
10 2 Forward and Backpropagation
Figure 2.1 Three-layer fully connected neural network Each layer of the network
consists of a linear part (in green) and a non-linear part f(in blue) The inputs are a
3×1 vector x=(x1,x2,x3)Twhich are multiplied by a 5 ×3 matrix W1Tto produce
a5×1 vector of pre-activations z1=(z1
1,z1
2,z1
3,z1
4,z1
5)T=W1Tx

============================================================

=== CHUNK 019 ===
Palavras: 356
Caracteres: 2151
--------------------------------------------------
A non-linear
activation function fis applied point-wise to each element z1
iof the pre-activation
vector to yield a 5 ×1 activation vector a1=(a1
1,a1
2,a1
3,a1
4,a1
5)Tsuch that a1
i=f(z1
i) Together, the linear part of matrix multiplication (in green) and non-linear part of
point-wise non-linear activation function (in blue) form the ﬁrst layer of the neural
network The output activations a1of the ﬁrst layer serve as the inputs to the second
layer of the network and the process is repeated In the second layer the 5 ×1
activations a1are multiplied by the 4 ×5 weight matrix W2Tto yield a 4 ×1
pre-activations vector z2=W2Ta1, which is passed through a pointwise non-linear
activation function fto yield a 4 ×1 vector of activations a2=f(z2) The outputs a2
of the second layer form the input to the third layer, which yields a 3 ×1
pre-activation vector z3=W3Ta2followed by a 3 ×1 activation vector a3=f(z3),
which constitute the outputs y=a3of the network and represented as a 3 wh×1 vector, where each color channel (red, green, and
blue) constitutes whcoeﬃcients The layers of a neural network form a Markov
chain, as shown in Figure 2.3, where each layer ldepends only on the previous
layerl−1 In this example the input, pre-activations, activations, and output
are represented by vectors The matrix X=A0is ann0×mmatrix whose columns are examples a0i
fori=1...m.T h em a t r i x A0=/bracketleftbig
a01···a0m/bracketrightbig
hasn0rows, which are features,
andmcolumns, which are examples The output ˆ y=aLis a vector for a single
example and the matrix ˆY=ALis a matrix of outputs for mexamples Here,
AL=[a31,...,a3m]f o rL= 3 and mexamples For layer /lscript,A/lscriptis ann/lscript×m
matrix 2.3 Forward Propagation 11
Figure 2.2 Fully connected neural network Each layer of the network consists of
multiple nodes (orange) In this example, the input is a 3 ×1 vector xwhich is
considered layer 0 with n0= 3 nodes The ﬁrst hidden layer consists of n1= 5 nodes The second layer has n2= 4 nodes, and the third layer with n3= 3 nodes in this
example is the output y Figure 2.3 Neural network layers form a Markov chain with operations on vectors

============================================================

=== CHUNK 020 ===
Palavras: 372
Caracteres: 3024
--------------------------------------------------
Starting from the input vector x=a0, which is considered layer /lscript= 0, each layer
/lscript=1,...,Lconsists of a pre-activation vector z/lscript=W/lscriptTa/lscript−1and activation vector
a/lscript=f(z/lscript) and only depends on the previous layer output vector a/lscript−1 The output is
y=aL,w h e r eL= 3 in this example denotes the number of layers 2.3 Forward Propagation
Forward propagation of activations from layer /lscript−1t ol a y e r /lscriptis a mapping F/lscript
fromA/lscript−1toA/lscript:
A/lscript=F/lscript(A/lscript−1) (2.2)
where each F/lscriptis composed of two parts: a linear function and a non-linear
function The linear function is deﬁned as:
Z/lscript=W/lscriptTA/lscript−1+b/lscript(2.3)
whereW/lscriptis ann/lscript−1×n/lscriptmatrix of weights for layer /lscript,W/lscriptTis then/lscript×n/lscript−1
transpose of W/lscriptandb/lscriptis ann/lscript×1 bias vector for layer /lscript Both act on all
examples We absorb the bias vector b/lscriptintoW/lscriptTby appending it as the last column,
makingW/lscriptTann/lscript×(n/lscript−1+ 1) matrix, and appending a 1 to the activation
vectora/lscript−1or equivalently appending a row of 1s to the activation matrix,
12 2 Forward and Backpropagation
makingA/lscript−1an (n/lscript−1+1)×mmatrix Without loss of generality we continue
tousethenotationof W/lscriptTandA/lscript−1todenotetheaugmentedmatrixandvector For example, if W/lscript=⎛
⎝w11w12
w21w22
w31w32⎞
⎠andAl−1=⎛
⎝a1
a2
a3⎞
⎠andbl=/parenleftbiggb1
b2/parenrightbigg
, then
W/lscriptTA/lscript−1+b/lscript=/parenleftbiggw11w21w31
w12w22w32/parenrightbigg⎛
⎝a1
a2
a3⎞
⎠+/parenleftbiggb1
b2/parenrightbigg
=/parenleftbiggw11a1+w21a2+w31a3+b1
w12a1+w22a2+w32a3+b2/parenrightbigg
=/parenleftbiggw11w21w31b1
w12w22w32b2/parenrightbigg⎛
⎜⎜⎝a1
a2
a3
1⎞
⎟⎟⎠
Equation 2.3 is then rewritten as matrix multiplication:
Z/lscript=W/lscriptTA/lscript−1(2.4)
The non-linear part is a non-linear activation function f/lscript, for layer /lscript, which
operates on each element of the matrix Z/lscriptseparately:
A/lscript=f/lscript(Z/lscript) (2.5)
2.3.1 Algorithm
Unrolling the forward propagation for the network shown in Figure 2.3, we get:
ˆY=A3=f3(W3Tf2(W2Tf1(W1TX))) (2.6)
Iffis the identity then Fis linear in x Algorithm 2.1 provides the forward propagation pseudocode Algorithm 2.1 Forward propagation giveninitial weights W1,...,WL
givendata example vector x1
foreach layer /lscript=1,...,L do:
x/lscript+1=f/lscript(W/lscriptTx/lscript)
2.3.2 Example
As an example of forward propagation, consider the neural network shown in
Figure 2.4 with input vector x=[x1,x2,x3]T The 3×1 input is x=a0and the 3 ×2 weight matrix of the ﬁrst layer is
2.3 Forward Propagation 13
Figure 2.4 Neural network example The input to the network is a 3 ×1 vector
x=(x1,x2,x3)T Figure 2.5 Example of forward propagation in a neural network The 3 ×1 input
vectorxis multiplied by a 2 ×3 matrix W1Tto form a 2 ×1 pre-activations vector
z1=W1Tx, which is the linear part (in green) of the ﬁrst layer of the network

============================================================

=== CHUNK 021 ===
Palavras: 386
Caracteres: 2536
--------------------------------------------------
W1=⎛
⎝w1
11w1
12
w1
21w1
22
w1
31w1
32⎞
⎠.T h e3×1 input is multiplied by the 2 ×3 transpose W1T
to yield the 2 ×1 pre-activations z1, as shown in Figure 2.5:
z1=/parenleftbiggz1
1
z1
2/parenrightbigg
=W1Tx=/parenleftbiggw1
11w1
21w1
31
w1
12w1
22w1
32/parenrightbigg⎛
⎝x1
x2
x3⎞
⎠=/parenleftbiggw1
11x1+w1
21x2+w1
31x3
w1
12x1+w1
22x2+w1
32x3/parenrightbigg
(2.7)
The pre-activation vector z1is followed by a non-linear function f1applied
pointwise to yield the activation vector a1=[a1
1,a1
2]Tof the ﬁrst layer, as shown
in Figure 2.6:
a1=/parenleftbiggf(z1
1)
f(z1
2)/parenrightbigg
=/parenleftbigga1
1
a1
2/parenrightbigg
(2.8)
Next, as shown in Figure 2.7, the activation vector a1of the ﬁrst layer is
multiplied by the 2 ×2 weight matrix W2Tof the second layer:
z2=W2Ta1=/parenleftbiggw2
11w2
21
w2
12w2
22/parenrightbigg/parenleftbigga1
1
a1
2/parenrightbigg
=/parenleftbiggw2
11a1
1+w2
21a1
2
w2
12a1
1+w2
22a1
2/parenrightbigg
=/parenleftbiggz2
1
z2
2/parenrightbigg
(2.9)
followed by a non-linear function fof the second layer applied pointwise to yield
14 2 Forward and Backpropagation
Figure 2.6 Example of forward propagation in a neural network highlighting the ﬁrst
layer of the network After the linear part (in green), the 2 ×1 pre-activation vector
z1is passed through a point-wise non-linear activation function fto form a 2 ×1
activation vector a1(in blue) Together, the linear part (in green) and a non-linear
part (in blue) constitute the ﬁrst layer of the network Figure 2.7 Example of forward propagation in a neural network highlighting the
second layer of the network The outputs of the ﬁrst layer a1are multiplied by the
weight matrix W2Tto form the 2 ×1 pre-activation vector z2=W2Ta1(in green),
which is passed through a pointwise non-linear activation function fto form a 2 ×1
activation vector a2=f(z2) (in blue) Together, the linear part (in green) and a
non-linear part (in blue) constitute the second layer of the network the activation vector a2of the second layer:
a2=/parenleftbiggf(z2
1)
f(z2
2)/parenrightbigg
=/parenleftbigga2
1
a2
2/parenrightbigg
(2.10)
Finally, the activation vector a2of the second layer is multiplied by the weight
matrixW3Tof the third layer to yield the output y, as shown in Figure 2.8:
y=/parenleftbig
w3
1w3
2/parenrightbig/parenleftbigga2
1
a2
2/parenrightbigg
=w3
1a2
1+w3
2a2
2 (2.11)
In this example, the network output is a real value used for regression rather
than classiﬁcation, and therefore the last layer does not consist of a non-linear
activation function

============================================================

=== CHUNK 022 ===
Palavras: 353
Caracteres: 2406
--------------------------------------------------
Next, various non-linear activation functions are described 2.3.3 Logistic Regression
As a second example of forward propagation in a general computation graph,
we consider simple logistic regression, which is deﬁned when the log-odds of the
2.3 Forward Propagation 15
Figure 2.8 Example of forward propagation in a neural network highlighting the third
layer of the network In this example, the network output is a real value used for
regression rather than classiﬁcation and therefore the last layer consists only of
multiplication of the 2 ×1 activations a2by a 1×2 weight matrix W3Tto yield a
scalar output y=W3Ta2(in green) and does not include a non-linear activation
function Figure 2.9 Logistic regression computation graph forward propagation class is a linear function:
f(x)=l o g/parenleftbiggp(x)
1−p(x)/parenrightbigg
=wTx (2.12)
which yields the sigmoid:
p(x)=1
1+e−f(x)(2.13)
which may represent mapping the input to a probability in (0 ,1) Therefore,
ﬁtting a logistic regression model to data involves computing the likelihood that
each example belongs to a class given the weights:
g(x,w)=/braceleftBigg
p(x),ifx∈C
1−p(x),ifx/negationslash ∈C(2.14)
Next,wesum/summationtext
ig(xi,w)forallexamples i=1,...,m,sothatthemodeloverall
possible weights wwhich gives the largest sum is the maximum likelihood model Stacking logistic regression functions results in a highly non-linear parametric
function, which is a neural network In our example, the inputs to the graph are the variables x,w, and the bias
bif not absorbed into the weights, and the output is L(y,f(x,w,b)) where a=
f(x,w,b)=g(wTx+b)a n dL(y,a)=−yloga−(1−y)log(1−a), as shown in
Figure 2.9 16 2 Forward and Backpropagation
2.4 Non-linear Activation Functions
The pre-activation zof a neural network may be followed by a diﬀerentiable
non-linear activation function f:R/mapsto →R Typical non-linear activation functions
include the sigmoid, hyperbolic tangent, rectiﬁed linear unit (ReLU), Swish, and
softmax 2.4.1 Sigmoid
For a probability p(x) the log-odds is deﬁned by:
logp(x)
1−p(x)(2.15)
For example, for a probability1
2the odds are 50:50 or 1, and the log-odds is 0 For a probability of 0 .9, the odds are 90:10 or 9, and the log-odds equal 2 .19 A
linear classiﬁer is given by:
fw(x)=wTx (2.16)
Setting the log-odds to be a linear classiﬁer:
logp(x)
1−p(x)=wTx (2.17)
and solving for p(x) results in the sigmoid function

============================================================

=== CHUNK 023 ===
Palavras: 355
Caracteres: 2295
--------------------------------------------------
The sigmoid function shown
in Figure 2.10 maps the input zto (0,1) by:
f(z)=1
1+e−z(2.18)
The function asymptotes at lim
z→∞f(z) = 1 and lim
z→−∞f(z) = 0, and crosses zero
atf(0) = 0.5 The sigmoid function is commonly used in logistic regression for
building a classiﬁer by taking the sigmoid of linear regression 2.4.2 Hyperbolic Tangent
The hyperbolic tangent function tanhshown in Figure 2.11 maps the input zto
(−1,1) by:
f(z)=ez−e−z
ez+e−z(2.19)
The function asymptotes at lim
z→∞f(z) = 1 and lim
z→−∞f(z)=−1, and crosses
zero atf(0) = 0 2.4 Non-linear Activation Functions 17
Figure 2.10 Sigmoid function: f(z)=1
1+e−zcrosses zero at f(0) = 0.5 and asymptotes
at lim
z→∞f(z) = 1 and lim
z→−∞f(z)=0 Figure 2.11 Hyperbolic tangent function: f(z)=ez−e−z
ez+e−zcrosses zero at f(0) = 0 and
asymptotes at lim
z→∞f(z) = 1 and lim
z→−∞f(z)=−1 2.4.3 Rectiﬁed Linear Unit
The ReLU, shown in Figure 2.12, maps negative values to zero:
g(z)=z+=m a x ( 0 ,z) (2.20)
Ifgis a ReLU non-linear activation function ReLU( x)=m a x {0,x}, thenfis
continuous and piecewise linear in xand the graph of fconsists of hyper-planes
with folds The leaky ReLU shown in Figure 2.13 is deﬁned for α≥0b y :
g(z)=z+−αz−=m a x ( 0 ,z)−αmax(0,−z) (2.21)
18 2 Forward and Backpropagation
Figure 2.12 Rectiﬁed linear unit (ReLU) function Figure 2.13 Leaky ReLU function 2.4.4 Swish
The Swish function, shown in Figure 2.14, is a non-linear activation function
found by automatically searching the space of activation functions (Ramachan-
dranetal.,2017)forafunctionwithgoodperformancewhichempiricallyoutper-
forms the ReLU It is deﬁned by f(x)=xσ(βx) whereσis the sigmoid function As lim β→∞f(x) the Swish becomes the ReLU; however, unlike the ReLU, which
has a stepwise derivative, the Swish has a smooth derivative for various values
ofβ 2.4.5 Softmax
For binary classiﬁcation the labels are yi∈{0,1}for example i For multiple
classes, the labels are yi∈{1,2,...,k}for example i The softmax function
2.5 Loss Functions 19
Figure 2.14 Swish function with β=1 extends logistic regression from binary to multi-class classiﬁcation:
fw(x)=⎛
⎜⎝p(y=1|x;w) p(y=k|x;w)⎞
⎟⎠=1
/summationtextk
c=1ewT
cx⎛
⎜⎜⎝ewT
1x ewT
kx⎞
⎟⎟⎠(2.22)
The softmax is a non-linear activation function used in the last layer Lfor
multi-class classiﬁcation

============================================================

=== CHUNK 024 ===
Palavras: 364
Caracteres: 2317
--------------------------------------------------
The softmax function is a generalization of the sigmoid
function, from scalars to vectors, mapping a vector z∈Rkto a vector f(z)∈
[0,1]kwhich sums to 1:/summationtextk
c=1f(z)c=1 ,w h e r e kis the number of classes The
softmax function for class iis given by:
fL(zL)i=ezL
i
/summationtextk
c=1ezL
c(2.23)
The softmax is used for multi-class classiﬁcation and computes a probability in
(0,1) for each class c=1,...,k,w h i c hs u mt o1 2.5 Loss Functions
Compare output f(x)=ˆy=aLwith ground truth label yfor an input x For example, consider the probability of an image being an object Compare all
outputsf(X)=ˆY=ALwith all ground truth labels Yfor all inputs X.T h el o s s
function for a single input example and output label is L(y,F(x)) The average
loss over all examples, or cost, is:
1
mm/summationdisplay
i=1L(yi,ˆyi) (2.24)
Our goal is to minimize the loss function so that the predictions agree with
20 2 Forward and Backpropagation
Figure 2.15 Loss functions |y−a|pfor various values of pwithy=0 The loss function space may be highly non-linear as a func-
tion space of the network weights The weights constitute a point in a high-
dimensional space, and moving in that space corresponds to changing the clas-
siﬁer and hence the predicted labels Given the weights of the network Wand forward propagating the inputs x
through the network to get the output labels ˆ yi=F(xi,W) our goal is to ﬁnd
weightsWsuch that:
minimize
W1
mm/summationdisplay
i=1L(yi,F(xi,W)) (2.25)
A common regularization term R(W) may be added to the loss function to prefer
simple models and avoid overﬁtting In general, the loss function is not convex with respect to W; therefore, solving
Equation 2.25 does not guarantee a global minimum We therefore use gradient
descent to ﬁnd a local minimum Common loss functions are the mean squared error, with L(yi,ˆyi) deﬁned by:
L(yi,ˆyi)=(yi−ˆyi)2(2.26)
The loss function |y−a|pfor various values of pwithy= 0 is shown in Figure
2.15 2.6 Backpropagation 21
The logistic regression loss is deﬁned by:
L(yi,ˆyi)=−yilog(ˆyi)−(1−yi)log(1−ˆyi) (2.27)
which is the special case for binary classiﬁcation k=2 :
L(yi,ˆyi)=−k/summationdisplay
c=1I{yi=k}logp(y=k|xi,W) (2.28)
forkclasses, where Iis the indicator function such that I{true}= 1 and
I{false}= 0, and p(y=k|xi,W) is a softmax coeﬃcient

============================================================

=== CHUNK 025 ===
Palavras: 354
Caracteres: 2475
--------------------------------------------------
For the special case
of logistic regression, the mean squared error is not convex, whereas the logistic
regression loss is convex 2.6 Backpropagation
The goal of backpropagation is to eﬃciently compute the derivatives of the total
loss function with respect to all of the network weights Backpropagation, also
known as automatic reverse diﬀerentiation, eﬃciently computes the gradient of
a function Fwith respect to all of the parameters∂F
∂W/lscriptfor all/lscript Once the output x/lscriptof the last layer is computed by forward propagation, the
loss between the ground truth labels yand network output is minimized:
minimize
WLW(x/lscript,y) (2.29)
Denote the application of a single layer of the network by:
x/lscript+1=f(W/lscriptTx/lscript) (2.30)
Then, diﬀerentiating both sides, we get:
dx/lscript+1=f/prime/lscript(dW/lscriptx/lscript+W/lscriptdx/lscript) (2.31)
and in vector and matrix form:
dx=DdW+Ldx (2.32)
wheredxis a vector of derivatives, Dis a diagonal matrix, Lis a lower triangular
matrix, and dWis the derivative with respect to the network weights Solving
fordx,w eg e t :
(I−L)dx=DdW (2.33)
and, therefore:
dx=(I−L)−1DdW (2.34)
22 2 Forward and Backpropagation
Figure 2.16 Computation graph forward propagation 2.7 Diﬀerentiable Programming
Backpropagation is a special case of diﬀerentiable programming (Wengert, 1964;
Bellman et al., 1965) for neural networks Derivatives of variables corresponding
to nodes in a computational directed acyclic graph (DAG) can be computed with
respect to the output using the chain rule in two opposite directions Diﬀerentia-
tioninaforwardpassthroughthegraphusingthechainruleresultsinthepartial
derivative of the output with respect to a single input variable, whereas diﬀeren-
tiation in a backward pass using the chain rule results in the partial derivative
of the output with respect to all input variables, namely the gradient, in a single
pass This O(n) factor in computational eﬃciency is signiﬁcant in data science,
in a similar fashion to the log( n) factor of the fast Fourier transform to convolu-
tion in signal processing and has broad implications First is the special case of
backpropagation in neural networks (Rumelhart et al., 1986), namely computing
the gradient of the loss function with respect to the weights in a single backward
pass Perhaps most importantly, any number of complex diﬀerentiable functions
can be composed into a computational DAG, and optimized using diﬀerentiable
programming

============================================================

=== CHUNK 026 ===
Palavras: 367
Caracteres: 2365
--------------------------------------------------
2.8 Computation Graph
2.8.1 Example
We begin with a simple toy example illustrating forward and backpropagation
using a computation graph The inputs to the graph shown in Figure 2.16 are
three constants a=3 ,b= 2, and c= 1 Nodes in the graph denote arithmetic
operations The graph computes the output f(a,b,c)=y=5u=5 (v+c)=
5(ab+c) = 35 by propagating the input forward through the nodes Next, we compute the derivatives of the output with respect to each input by
2.8 Computation Graph 23
Figure 2.17 Computation graph backward propagation, third layer Figure 2.18 Computation graph backward propagation, second layer applying the chain rule of diﬀerentiations As shown in Figure 2.17, beginning
withy=5uthe derivative with respect to the intermediate value uisdy
du=5 Thederivativeof ywithrespecttointermediatevalue visdy
dv=dy
dudu
dv=5×1=
5 and the derivative of ywith respect to the input cisdy
dc=dy
dudu
dc=5×1=5 ,
as shown in Figure 2.18 Finally, the derivative of ywith respect to the inputs aandbare respectively
dy
da=dy
dvdv
da=5×2 = 10 anddy
db=dy
dvdv
db=5×3 = 15, as shown in Figure 2.19 Notice that the computation at each node is local Each node receives an input
from the next layer, performs a local computation of its partial derivative, and
provides output to the previous layer This allows us to build complex graphs
consisting of many simple local computations The deep learning frameworks
TensorFlow and PyTorch perform such simple local computations on complex
graphs 24 2 Forward and Backpropagation
Figure 2.19 Computation graph backward propagation, ﬁrst layer Figure 2.20 Logistic regression computation graph backpropagation, third layer 2.8.2 Logistic Regression
As a second example, we consider backpropagation in logistic regression We ﬁrst
compute the derivative of the loss with respect to the predicted outputdL
da,a s
shown in Figure 2.20 Next we compute the derivative of the loss with respect to zsuch thatdL
dz=
dL
dada
dz, as shown in Figure 2.21 Finally, we compute the derivative of the loss with respect to the weights w
Figure 2.21 Logistic regression computation graph backpropagation, second layer 2.8 Computation Graph 25
Figure 2.22 Logistic regression computation graph backpropagation Figure 2.23 Backpropagation such thatdL
dw=dL
dzdz
dwanddL
db=dL
dzdz
db, as shown in Figure 2.22, all by applying
the chain rule

============================================================

=== CHUNK 027 ===
Palavras: 350
Caracteres: 3089
--------------------------------------------------
Plugging in the derivativedL
da=−y
a+1−y
1−a, and setting a=g(z)=
1
1+e−zto be the sigmoid function, we plug in the local valueda
dz=ez
(1+ez)z=
g(z)(1−g(z)) and reach the derivatives of the output with respect to each of the
input variables 2.8.3 Forward and Backpropagation
In a neural network, forward propagation maps the activations from layer /lscript−1
forward to layer /lscriptby matrix multiplications followed by an elementwise non-
linear activation function The forward mapping from A/lscript−1→A/lscriptis given by:
Z/lscript=W/lscriptTA/lscript−1(2.35a)
A/lscript=f/lscript(Z/lscript) (2.35b)
which means that each activation layer A/lscriptis a function F(A/lscript−1,W/lscriptT)o ft h e
previous activation layer A/lscript−1and a weight matrix W/lscript, as shown in Figure 2.3 Backpropagation maps the derivatives from layer /lscriptback to layer /lscript−1 with
respect to both activations and weights, as shown in Figure 2.23 Given∂L
∂A/lscript,ourgoalistocomputethepartialderivativeofthelosswithrespect
to the previous layer’s activations∂L
∂A/lscript−1and with respect to the weights∂L 26 2 Forward and Backpropagation
Figure 2.24 Sigmoid derivative Using∂L
∂A/lscriptand∂L
∂Z/lscript, the backward mapping from∂L
∂A/lscript→∂L
∂A/lscript−1is given by:
∂L
∂Z/lscript=∂L
∂A/lscript×f/prime/lscript(Z/lscript) (2.36a)
∂L
∂A/lscript−1=(W/lscript)T∂L
∂Z/lscript(2.36b)
which follows from the chain rule for diﬀerentiation:
∂L
∂Z/lscript=∂L
∂A/lscript∂A/lscript
∂Z/lscript(2.37)
and
∂L
∂A/lscript−1=∂L
∂Z/lscript∂Z/lscript
∂A/lscript−1(2.38)
Next,wediﬀerentiatethelosswithrespecttotheweights.Since A/lscript=F(A/lscript−1)=
f/lscript(Z/lscript)=f/lscript(W/lscriptTA/lscript−1), therefore by the chain rule we have:
∂L
∂W/lscript=∂L
∂A/lscript∂F(A/lscript−1)
∂W/lscript=∂L
∂A/lscript∂f/lscript(W/lscriptTA/lscript−1)
∂W/lscript=∂L
∂A/lscriptf/prime/lscript(A/lscript−1)T(2.39)
Finally, the weights W/lscriptare updated using∂L
∂W/lscriptby:
W/lscript=W/lscript−α∂L
∂Wl(2.40)
2.9 Derivative of Non-linear Activation Functions
The derivative of the sigmoid function is:
f/prime(z)=ez
(1+ez)2=f(z)(1−f(z)) (2.41)
as shown in Figure 2.24 The sigmoid derivative is computed eﬃciently by using
the sigmoid itself 2.9 Derivative of Non-linear Activation Functions 27
Figure 2.25 Hyperbolic tangent derivative Figure 2.26 Rectiﬁed linear unit (ReLU) derivative The derivative of the hyperbolic tangent function is:
f/prime(z)=4
(e−z+ez)2=1−f(z)2(2.42)
asshowninFigure2.25.Thehyperbolictangentderivativeiscomputedeﬃciently
using the hyperbolic tangent function The derivative of the ReLU is deﬁned for z/negationslash=0 :
f/prime(z)=/braceleftBigg
0,ifz<0
1,ifz>0(2.43)
asshowninFigure2.26.ThederivativeoftheReLUisnotdeﬁnedatzero,though
for practical purposes, machine ﬂoating-point numbers may not be exactly zero If they are, we add a tiny oﬀset to deﬁne the derivative The derivative of the
ReLU is either 0 or 1, which is signiﬁcantly simpler than the derivatives of the
sigmoid and tanh activation functions 28 2 Forward and Backpropagation
Figure 2.27 Swish derivative for β=1

============================================================

=== CHUNK 028 ===
Palavras: 350
Caracteres: 2583
--------------------------------------------------
Similarly, the derivative of the leaky ReLU is deﬁned for α≥0a n dz/negationslash=0b y :
f/prime(z)=/braceleftBigg
α,ifz<0
1,ifz>0(2.44)
While similar to the ReLU, the Swish function (Ramachandran et al., 2017)
has the advantage that its derivative is well deﬁned and smooth:
f/prime(z)=σ(βz)+βzσ(1−σ(βz)) (2.45)
as shown in Figure 2.27 2.10 Backpropagation Algorithm
Given an input example and a ground-truth label, we perform the following three
steps iteratively:
1 Forward propagation: Forward propagate the activations through all layers
from input to output, reaching a prediction Compute loss function: Compute the error between the prediction and
ground truth Backpropagation:Usethechainrulefordiﬀerentiationforbackpropagating
the gradients through the layers in the opposite direction from the output
to the input Algorithm2.2providespseudocodefortraininganeuralnetworkusingstochas-
tic gradient descent (SGD) using forward and backpropagation 2.10 Backpropagation Algorithm 29
Algorithm 2.2 Training a neural network by SGD using forward and backprop-
agation fori=1,...,n do
randomly sample an input–label pair ( x=a0,y)
foreach layer /lscript=1,...,L do:
z/lscript=W/lscripta/lscript−1
a/lscript=f/lscript(z/lscript)
foreach layer /lscript=L,...,1do:
∂L(y,F(x,W))
∂z/lscript=∂L(y,F(x,W))
∂a/lscript×f/prime/lscript(z/lscript)
∂L(y,F(x,W))
∂W/lscript=∂L(y,F(x,W))
∂z/lscripta/lscript−1
W/lscript=W/lscript−α∂L
∂W/lscript
Figure 2.28 Backpropagation example: third layer 2.10.1 Example
As an example of backpropagation, consider the neural network shown in Figure
2.4 with sigmoid non-linear activation functions in the ﬁrst and second layers,
f1=f2=σ First, as shown in Figure 2.28, the derivative of the output y=
w3
1a2
1+w3
2a2
2is computed with respect to z2
1:
∂y
∂z2
1=w3
1∂a2
1
∂z2
1=w3
1σ(z2
1)(1−σ(z2
1)) =w3
1a2
1(1−a2
1) (2.46)
andz2
2, such that the derivative of the output ywith respect to z2is:
∂y
∂z2=∂y
∂a2∂a2
∂z2=/parenleftbiggw3
1a2
1(1−a2
1)
w3
2a2
2(1−a2
2)/parenrightbigg
(2.47)
Next, as shown in Figure 2.29, the derivative of the output ywith respect to
z1is computed by using the value∂y
∂z2of the derivative of the output ywith
respect to z2, which was previously computed, and the chain rule:
∂y
∂z1=∂y
∂z2∂z2
∂z1=∂y
∂z2∂z2
∂a1∂a1
∂z1=/parenleftbiggw2
11w2
12
w2
21w2
22/parenrightbigg∂y
∂z2a1(1−a1) (2.48)
Finally, as shown in Figure 2.30, the derivative of the output yis computed
with respect to each of the weight matrix coeﬃcients of the ﬁrst layer by using
30 2 Forward and Backpropagation
Figure 2.29 Backpropagation example: second layer

============================================================

=== CHUNK 029 ===
Palavras: 353
Caracteres: 2618
--------------------------------------------------
Figure 2.30 Backpropagation example: ﬁrst layer the value∂y
∂z1of the derivative of the output ywith respect to z1,w h i c hw a s
previously computed:
∂y
∂w1
11=∂y
∂z1∂z1
∂w1
11=∂y
∂z1/parenleftbiggx1
0/parenrightbigg
(2.49)
In a single backward pass, the derivatives of the output ywith respect to each
of the network weights are computed eﬃciently by a series of local computations
using the chain rule for diﬀerentiation 2.11 Chain Rule for Diﬀerentiation
We deﬁne the chain rule for diﬀerentiation for two and three functions in one
dimension and two functions in higher dimensions 2.11.1 Two Functions in One Dimension
First, consider the simple case of the derivative of the composition of two func-
tions in one dimension:
(f◦g)/prime=(f/prime◦g)g/prime(2.50)
which is:
f(g(x))/prime=f/prime(g(x))g/prime(x) (2.51)
2.11 Chain Rule for Diﬀerentiation 31
Therefore, for y=g(x)a n dz=f(y)=f(g(x)) we get:
dz
dx=dz
dydy
dx=f/prime(y)g/prime(x)=f/prime(g(x))g/prime(x) (2.52)
2.11.2 Three Functions in One Dimension
Next, consider the case of the derivative of the composition of three functions in
one dimension:
(f◦g◦h)/prime(2.53)
which is:
f(g(h(x)))/prime=f/prime(g(h(x)))g(h(x))/prime=f/prime(g(h(x)))g/prime(h(x))h/prime(x) (2.54)
Lety=f(u)a n du=g(v)a n dv=h(x); then we get:
dy
dx=dy
dudu
dvdv
dx(2.55)
2.11.3 Two Functions in Higher Dimensions
Letf:Rm→Rkandg:Rn→Rm.L e tz=f(y)a n dy=g(x) Then
z=f(g(x)) maps the n×1 vector xto thek×1 vector zthrough the m×1
vectory Forf:Rn→Rmandy=f(x) mapping an n×1 vector xto anm×1 vector
y, for example by multiplying by A,i sa nm×nmatrix such that y=Ax Deﬁne
the Jacobian matrix as:
Jf=/bracketleftBig
∂f
∂x1···∂f
∂xn/bracketrightBig
=⎡
⎢⎣∂f1
∂x1···∂f1
∂xn ∂fm
∂x1···∂fm
∂xn⎤
⎥⎦ (2.56)
andJij=∂fi
∂xj For example, for the function:
y=/bracketleftbiggy1
y2/bracketrightbigg
=/bracketleftbiggf1(x1,x2)
f2(x1,x2)/bracketrightbigg
=/bracketleftbiggx2
1x2
5x1+sin(x2)=f(x1,x2)/bracketrightbigg
(2.57)
The Jacobian is:
Jf=/bracketleftbigg2x1x2 x2
1
5 cos( x2)=f(x1,x2)/bracketrightbigg
(2.58)
and in the general case, in a similar fashion to the chain rule in one dimension
in Equation 2.50, the chain rule in higher dimensions is:
Jf◦g(x)=Jf(g(x))J(x) (2.59)
32 2 Forward and Backpropagation
Forz=f(y)=(f1(y),...,f k(y)) andy=g(x)=(g1(x),...,g m(x)) applying
the chain rule we get:
∂(z1,...,z k)
∂(x1,...,x n)=∂(z1,...,z k)
∂(y1,...,y m)∂(y1,...,y m)
∂(x1,...,x n)(2.60)
which is Equation 2.52 in higher dimensions Storing and computing the Jacobian of large matrices in neural networks is
very ineﬃcient Therefore, we use the expressions derived in Equations 2.36 and
2.39

============================================================

=== CHUNK 030 ===
Palavras: 365
Caracteres: 2614
--------------------------------------------------
2.12 Gradient of Loss Function
For a neural network, given a single training example, consider the loss function
L(y,F(x,W)) Writing ∂L=∂L(y,F(x,W)) then by the chain rule of diﬀeren-
tiation for the output Lwe get:
δL=∂L(y,F(x,W))
∂zL=∂L(y,F(x,W))
∂aL×f/primeL(zL) (2.61)
and for a squared error loss:
L(y,F(x,W)) =1
2/bardbly−F(x,W)/bardbl2
2 (2.62)
where the derivative in Equation 2.61 is δL=−(y−aL)×f/primeL(zL) 2.13 Gradient Descent
A practical method for minimizing the loss function is gradient descent Gradient
descent iteratively ﬁnds a local minimum by taking steps in the direction of the
steepestdescent,asshowninAlgorithm2.3.Itdoesnotguaranteetoﬁndaglobal
minimum, and two diﬀerent starting points may result in two diﬀerent local
minima Local minimum points are rare in high dimensions since they require
that the partial derivatives in all dimensions be zero Therefore, having saddle
points or plateaus in high dimensions is more common than a local minimum Algorithm 2.3 Gradient descent givenas t a r t i n gp o i n t x∈domf
repeat:
determine descent direction −∇f(x)
choose scalar α
updatex:=x−α∇f(x)
untilstopping criterion is satisﬁed
2.14 Initialization and Normalization 33
Givenmtraining examples, the gradient of the loss function with respect to
the weights is given by:
∂L
∂W/lscript=1
mm/summationdisplay
i=1∂L(yi,F(xi,W))
∂W/lscript(2.63)
The gradient descent update is then given by W/lscript:=W/lscript−α∂L
∂W/lscriptfor layers
/lscript=1,...,L−1 2.14 Initialization and Normalization
Given input x, a standard practice is to normalize the data to the standard
score by x=x−μ
σwhereμ=1
n/summationtextn
i=1xiis the mean and σ2=1
n/summationtextn
i=1(xi−μ)2is
the variance Input normalization improves the convergence of gradient descent,
turning narrow ravines in the loss function into even level sets In a similar
fashion, batch normalization (Ioﬀe and Szegedy, 2015) normalizes each batch of
input for each layer of the network, making the optimization landscape smoother
(Santurkar et al., 2018) Gradient descent methods require an initial value for the weights; however,
since the layers are symmetric with respect to the weights into each activation
we cannot initialize to zero Therefore, a common practice is to initialize the
weights to small random values by using a normal distributionN(0,1)√
(n), wherenis
thenumberofconnectionsintoanactivationunit.Anothercommoninitialization
is a normal distribution with zero mean and variance σ2=2/(nl−1+nl), where
nl−1andnlare the number of activation units in the previous and current layers
of the weights (Glorot and Bengio, 2010)

============================================================

=== CHUNK 031 ===
Palavras: 365
Caracteres: 2610
--------------------------------------------------
2.15 Software Libraries and Platforms
The most notable and commonly used deep learning platforms are the open-
source libraries PyTorch (Paszke et al., 2017) from Facebook and TensorFlow
(Abadi et al., 2016) from Google Both libraries implement automatic diﬀerenti-
ation on general computation graphs Keras (Chollet, 2015) is a high-level API
based on TensorFlow that allows rapid prototyping 2.16 Summary
This chapter presents forward and backpropagation in neural networks using lin-
earalgebra,calculus,andalgorithms.Multipleexamplesillustratethealgorithms
andprovidethebackpropagationderivationsusingthechainruleinreversemode
diﬀerentiation Key advantages of backpropagation are:
34 2 Forward and Backpropagation
•Eﬃciency: It allows computing derivatives of the total loss function with re-
spect to all network weights in linear time in a single backward pass •Extendable: Backpropagation in neural networks is a special case of diﬀeren-
tiable programming and extends to a general computation graph with nodes
representing diﬀerentiable functions •Gradient-based learning: Backpropagation minimizes a total loss function by
updatingtheneuralnetworkparametersbasedonthegradientsofthetotalloss
with respect to each of the parameters Stochastic gradient descent, described
in the next chapter, is key in neural network optimization 3 Optimization
3.1 Introduction
Why have a basic understanding of the optimization under the hood of deep
learning A key reason is that diﬀerent algorithms perform diﬀerently, and un-
derstanding their performance is important in practice An optimization goal when training neural networks is to minimize the loss
function The loss function measures the error between the output of the net-
work, such as predictions, and target values, such as ground-truth values The
optimization problem is that of ﬁnding the minimum of a function:
minimize
x∈Xf(x) (3.1)
forxin a feasible set X.T h ep o i n t x⋆= argmin
xf(x) is a local minimum of the
function if there exists δ>0 such that f(x⋆)≤f(x) for allxwhere/bardblx⋆−x/bardbl≤δ In the case of neural networks, fmay be the loss function and xthe parameters
of the network, and the optimization goal is to ﬁnd the parameter values x∗that
minimize the loss f(x) The optimization problem for neural networks is usually solved using gradient
descent, an iterative method for ﬁnding a local minimum of a function It starts
with an initial point and, at each iteration, updates the parameter values in the
directionofthenegativegradient.Gradientdescentconvergestoalocalminimum
of the loss function if the learning rate is small enough

============================================================

=== CHUNK 032 ===
Palavras: 367
Caracteres: 2442
--------------------------------------------------
The gradient descent
algorithm may be implemented using backpropagation, which is an eﬃcient way
to compute the gradient of a loss function with respect to the parameters of a
neural network 3.2 Overview
3.2.1 Optimization Problem Classes
Optimization methods may be coarsely divided into two important classes: con-
vex and non-convex optimization problems, as shown in Figure 3.1 Convex opti-
mizationproblemsareatinysubsetofnon-convexproblems(andmay,inturn,be
ﬁnely classiﬁed into linear programs, quadratic programs, second-order cone pro-
36 3 Optimization
Figure 3.1 Optimization problem classes grams, semideﬁnite programs, and conic programs, each subsuming the other) A setSis convex if for any points x,y∈Sthe line that connects them is in the
setαx+(1−α)y∈Sfor allα∈(0,1) A function fis convex if the set of points
above the function graph is convex: f(αx+(1−α)y)≤αf(x)+(1−α)f(y) The
maximum of convex functions is convex, and a convex function is the maximum
of its tangent functions A convex optimization problem is deﬁned for a convex
function over a convex set Perhaps most importantly, the set of points that are a
minimum of a convex problem is convex, which means that any local minimum is
a global minimum Unfortunately, training neural networks involves optimizing
non-convex optimization problems, so a local minimum will, in general, not be
a global minimum The most common non-convex optimization methods are gradient descent and
quasi-Newton methods Gradient descent is a method for minimizing a function
fby taking steps proportional to the negative of the gradient of f The gradient
descent algorithm proceeds by iteratively updating the variables in proportion
to their partial derivatives with respect to the objective function This algorithm
converges to a local minimum, but it may converge slowly or not if the function
is not convex Quasi-Newton methods are a family of algorithms that use Newton’s method
to approximate the gradient descent algorithm Newton’s method is an iterative
procedure for ﬁnding the roots of a function by computing successive approxi-
mations The quasi-Newton methods use a Taylor expansion to approximate the
derivative of fat a given point The algorithm then uses this approximation to
compute the next step in the gradient descent algorithm This approach con-
verges much faster than gradient descent, but it does not guarantee that the
algorithm will converge to a global minimum

============================================================

=== CHUNK 033 ===
Palavras: 353
Caracteres: 2431
--------------------------------------------------
3.2 Overview 37
3.2.2 Optimization Solution Methods
In this chapter, we will review three types of optimization solution methods for
non-convex optimization problems, based on the degree of derivatives they use:
•First-order methods depend on ﬁrst derivatives, including gradient descent,
which is the most simple and commonly used Gradient descent is a ﬁrst-order
method and the simplest optimization method It is based on the gradient
of the cost function, which is calculated by taking partial derivatives of the
cost function with respect to all of the weights and biases in the network The
gradientdescentalgorithmstartswithaninitialguessforalloftheweightsand
biases in the network Then it iteratively updates these values by moving in a
direction that reduces error We compute a gradient vector in each iteration,
which points in a direction that reduces error We then move a small step
in that direction and repeat until we have moved far enough to reduce error
signiﬁcantly •Second-order methods depend on ﬁrst derivatives and their rate of change These include Newton’s method (which is impractical for directly optimizing
neural networks) and quasi-Newton methods (practical for optimizing neural
networks) These methods have faster convergence than ﬁrst-order methods
but, even so, are less frequently used Quasi-Newton methods are based on
approximatingtheHessianmatrixofthefunctiontobeoptimized.TheHessian
matrix is a square matrix of second derivatives, and it is costly to compute
directly Quasi-Newton methods approximate the Hessian using a diagonal
approximation or an approximation that only depends on ﬁrst derivatives The
most common quasi-Newton method used for neural network optimization is
Broyden–Fletcher–Goldfarb–Shanno (BFGS) •Evolution strategies: Instead of optimizing an individual point toward a lo-
cal minimum, evolution strategies consider multiple points sampled from a
probability distribution, which progress as a group toward the minimum The
algorithm is a variant of the genetic algorithm, with the main diﬀerence being
that instead of using a population of individuals, it uses a population of points The points are generated from a probability distribution over the search space The probability distribution is updated during the optimization process, and
each point is evaluated for its ﬁtness The points are then sorted by ﬁtness
and used to form a new population for the next iteration

============================================================

=== CHUNK 034 ===
Palavras: 379
Caracteres: 2471
--------------------------------------------------
3.2.3 Derivatives and Gradients
The derivative f/prime(x) of a univariate function fatxi st h er a t eo fc h a n g eo ft h e
function at x:
f/prime(x)=df(x)
dx(3.2)
which is the slope of the tangent line to the function at x The second derivative
is the rate of change of the ﬁrst derivative The second derivative at xis deﬁned
38 3 Optimization
Figure 3.2 Extreme points of a function where the derivative is zero by:
f/prime/prime(x)=df/prime(x)
dx=d2f
dx2(3.3)
A univariate function ffor which the ﬁrst derivative f/prime(x) = 0 can be classiﬁed
as one of the three types of extreme points, as shown in Figure 3.2: (1) a point
x⋆is a local minimum if the ﬁrst derivative is zero f/prime(x) = 0 and the second
derivative is positive f/prime/prime(x)>0; (2) a point x⋆is a local maximum if the ﬁrst
derivative is zero f/prime(x) = 0 and the second derivative is negative f/prime/prime(x)<0; (3)
ap o i n tx⋆is a saddle-point if the ﬁrst and second derivatives are zero f/prime(x)=0
andf/prime/prime(x) = 0 The gradient ∇f(x) of a multivariate function f(x1,...,x n)i s
the vector in which each component is the partial derivative of fwith respect
to that component:
∇f(x)=/parenleftbigg∂f(x)
∂x1,...,∂f(x)
∂xn/parenrightbigg
(3.4)
The Hessian ∇2f(x) of a multivariate function fis the matrix of the second
partial derivatives of the function:
∇2f(x)=⎡
⎢⎢⎣∂2f(x)
∂x1∂x1...∂2f(x)
∂x1∂xn ∂2f(x)
∂xn∂x1...∂2f(x)
∂xn∂xn⎤
⎥⎥⎦(3.5)
Similar to the univariate function, for a multivariate function the point x⋆is a
local minimum if the gradient of the function is zero ∇f(x) = 0 and the Hessian
∇2f(x) is positive deﬁnite 3.2.4 Gradient Computation
The derivative of a function can be computed numerically or analytically When
it is available, the analytic derivation is exact and fast, whereas the numerical
computation is approximate and slow We, therefore, derive the analytic gradient
for training neural networks and use the numerical gradient computation only
3.3 First-Order Methods 39
for checking our implementation There are several numerical ﬁnite diﬀerence
approximations of the derivative, the most common being the forward diﬀerence:
f/prime(x)≈f(x+ε)−f(x)
ε(3.6)
backward diﬀerence:
f/prime(x)≈f(x)−f(x−ε)
ε(3.7)
and (two-sided) central diﬀerence, which is their average:
f/prime(x)≈f(x+ε)−f(x−ε)
2ε(3.8)
Compared with the analytic derivation, the numeric approximation using the
central diﬀerence is helpful for debugging purposes when training a neural net-
work

============================================================

=== CHUNK 035 ===
Palavras: 387
Caracteres: 2601
--------------------------------------------------
In practice, we replace J(θ,x) with the loss of the neural network and
compute its derivative 3.3 First-Order Methods
First-order methods use the gradient gt=∇f(xt) to direct the search towards a
local minimum We begin with gradient descent, in which we follow the direction
of the steepest descent 3.3.1 Gradient Descent
Gradient descent minimizes a function based on iterative steps in the direction
of steepest descent, which is the opposite direction of the gradient, as shown in
Algorithm 3.1 Algorithm 3.1 Gradient descent givenobjective function f(x)
givenstarting point x1∈domf
givenlearning rate α
whilenot converged do:
gt=∇f(xt) gradient
xt+1=xt−αgtupdate
At every descent step, we compute the function’s gradient and update the
valuexin the negative direction scaled by the learning rate α.S ol o n ga st h e
gradient is not zero, this improves for a smooth function and a suﬃciently small
step size Gradient descent terminates once a stopping criterion is met The
stopping criteria may be set to a ﬁxed number of iterations, or once the change
in the function values between successive iterations f(x)−f(x) is smaller
40 3 Optimization
than either a constant threshold s=εor threshold which depends on the func-
tion magnitude s=ε/bardblf(xt)/bardbl, or once the gradient magnitude is smaller than a
threshold /bardblgt/bardbl<s Our primary choices in gradient descent methods are determining the descent
direction based on past gradients and choosing the step size Gradient descent is
a simple and basic optimization algorithm most commonly used to train machine
learning models, speciﬁcally neural networks Consider the special case of applying gradient descent to logistic regression The objective function is:
J=1
nn/summationdisplay
i=1L(ˆyi,yi)+λ
2/bardblw/bardbl2(3.9)
where ˆyi=σ(wTxi+b) is the sigmoid of the dot product between the parameter
vectorwTand the input point xiplus the bias term b Using the negative log
likelihood loss:
L(ˆyi,yi)=−yilog(ˆyi)−(1−yi)log(1−ˆyi) (3.10)
the gradient of the optimization objective with respect to the weight vector w
is:
∂J
∂w=1
nn/summationdisplay
i=1(ˆyi−yi)xi+λw (3.11)
and the gradient of the optimization objective with respect to the bias bis:
∂J
∂b=1
nn/summationdisplay
i=1(ˆyi−yi) (3.12)
We can then use gradient descent to ﬁnd the optimal parameters When optimizing neural networks, the algorithm minimizes the total loss func-
tionf(x)=L(x) based on a step in the direction of steepest descent:
xt+1=xt−αt∇L(x) (3.13)
In neural networks the total loss function is the sum of errors in classifying
each of the training examples

============================================================

=== CHUNK 036 ===
Palavras: 356
Caracteres: 2316
--------------------------------------------------
Examples of loss functions are the square loss:
L(x)=1
mm/summationdisplay
i=1/bardblf(x,ai)−yi/bardbl2(3.14)
for weights x, examples ai, and ground-truth labels yi; the hinge loss function
is:
L(x)=1
mm/summationdisplay
i=1max(0,1−tf(x)) (3.15)
wheret=1o rt=−1 for classiﬁcation, and the cross-entropy loss is:
L(x)=−1
mm/summationdisplay
i=1yilog(ˆyi)+(1−yi)log(1−ˆyi) (3.16)
3.3 First-Order Methods 41
Figure 3.3 When the step size is too big, gradient descent may diverge and never
converge where ˆyis the prediction There are two main computational problems with the gradient descent algo-
rithm applied to optimizing deep neural networks The ﬁrst problem is that the
total loss function L(x) with respect to the neural network weights xis the sum
of many individual losses L(x,ai), one for each training example ai:
L(x)=1
mm/summationdisplay
i=1L(x,ai) (3.17)
and therefore computing the gradient with respect to the total loss is computa-
tionallyexpensive.Theproblemissolvedbyusingonlyamini-batchofsamplesat
each step or random samples, also known as stochastic gradient descent (SGD),
which we will discuss further The second problem is computing the derivative of
the total loss with respect to all network weights x, as there are many weights The backpropagation algorithm solves this second problem 3.3.2 Step Size
Having computed or approximated the gradient of the function fatx, our next
task is to choose the learning rate αfor updating x Notice that the step size
α/bardblgt/bardblis the product of the learning rate αand the gradient vector length /bardblgt/bardbl If the step size is too big, then the optimization may never converge, as shown
in Figure 3.3 On the other hand, if the step size is too small, convergence may
be very slow, as shown in Figure 3.4 When the step size is just right, gradient
descent converges nicely to a local minimum in a relatively short time, as shown
in Figure 3.5 The learning rate αis an important hyperparameter in training neural net-
works and may control how quickly the model learns The learning rate may
42 3 Optimization
Figure 3.4 When the step size is too small, gradient descent convergence may be slow Figure 3.5 When the step size is just right, gradient descent converges nicely to a local
minimum be manually set to a small constant in most cases

============================================================

=== CHUNK 037 ===
Palavras: 373
Caracteres: 2533
--------------------------------------------------
However, we may want to
decrease the learning rate as training progresses to prevent overﬁtting for a vast
dataset or training on a GPU This can be done by reducing the learning rate
by constant amounts or by using an adaptive learning rate schedule, decreasing
the learning rate by a decay factor as a function of iteration t:
αt=α11
1+tβ(3.18)
forβclose to 0; or by exponential decay setting
αt=α1γt(3.19)
3.3 First-Order Methods 43
Figure 3.6 Gradient descent convergence path using a backtracking line search step
size forγclose to 1; or by setting
αt=α1exp−βt(3.20)
Backtrack line search : A simple and practical algorithm for computing an
adaptive step size is the backtrack line search algorithm The algorithm con-
tinuously halves αuntil a criterion is met, such as decreasing the value of the
function:
f(xt+1)≤f(xt)+βαgt (3.21)
forβ∈[0,1] The result of backtrack line search is shown in Figure 3.6 Exact line search : An alternative is to optimize for the best step size along
the negative gradient direction dt=−gt
/bardblgt/bardbl:
minimize
αf(xt+αdt) (3.22)
Computing the derivatives of this optimization function may be performed by
setting the derivative to zero:
∇f(xt+αdt)Tdt= 0 (3.23)
where∇dtf(xt)=∇f(xt)Tdt Sincedt+1=−gt+1
/bardblgt+1/bardbland the next gradient is
gt+1=∇f(xt+αdt)w eg e t :
dt+1=−∇f(xt+αdt)
/bardbl∇f(xt+αdt)/bardbl(3.24)
and therefore dT
t+1dt= 0, which means that the directions of the gradients for
consecutive steps tandt+1 which follow optimized step sizes αtare perpendic-
ular, forming a zig-zag pattern Optimizing for the exact step size is computa-
tionally expensive, and rarely used in practice 44 3 Optimization
3.3.3 Mini-Batch Gradient Descent
Inmini-batchgradientdescent,thecostfunctioniscomputedforeachexamplein
amini-batchofexamples,andthentheweightsareupdatedusingthesegradients The size of the mini-batch can be ﬁxed or variable Mini-batch methods are a
natural way to parallelize the gradient computation since the gradient can be
computed in parallel on each of the ksubsets The mini-batch method is also
more robust to outliers since it is less likely that all ksubsets will contain an
outlier The standard error of a sample mean ˆ μof a population mean μis given
bySE(ˆμ)=σ√nwhereσis the standard deviation of the population and the
sizenis the number of observations of the sample To see this, note that if
x1...xnarenindependent samples, then the variance of T=/summationtext
ixiisnσ2.T h e
variance of the sample mean ˆ μ=T
nisσ2
nand the standard deviation of ˆ μisσ√n

============================================================

=== CHUNK 038 ===
Palavras: 370
Caracteres: 2310
--------------------------------------------------
This motivates us to estimate the gradient from samples rather than the entire
dataset For example, estimating the gradient from 100 samples instead of from
10,000 samples reduces the standard error of the mean only by a factor of 10,
while reducing computation time and memory by a factor of 100 Optimization
algorithms that use the entire dataset are termed batch methods In contrast,
mini-batch methods use a sample of the data, splitting the data inton
kdisjoint
sets of size k 3.3.4 Stochastic Gradient Descent
Stochastic gradient descent (Robbins and Monro, 1951) is a special case of mini-
batch methods in which the mini-batch is of size 1, using a single random sample
aiat a time:
xt+1=xt−αt∇xL(xt,ai) (3.25)
where the learning rate, or step-size parameter, αtis dependent on iteration t
and∇fi(xt)=∇xL(xt,ai), as shown in Algorithm 3.2 Algorithm 3.2 Stochastic gradient descent givenobjective function f(x)
givenstarting point x1∈domf
given{αt}T
t=1learning rates
whilenot converged do:
Randomly shuﬄe training set a1,...,an
fori=1,...,n do:
gt=∇fi(xt) gradient of a single sample i
xt+1=xt−αtgtupdate
Two diﬀerences between the SGD and gradient descent algorithms are that (1)
in SGD, we approximate the gradient using a single sample iby∇finstead of
3.3 First-Order Methods 45
all samples by ∇f, and (2) the learning rate αtis dependent upon the iterations
trather than being a ﬁxed α SGD is also suitable as an online method for
handling a stream of data one example at a time Notice that near saddle-points,
the gradient is close to zero, and therefore the step size in gradient descent may
be too small to be eﬃcient A common practice is to add noise εtto the gradient
descent update:
xt+1=xt−αtgt+εt (3.26)
whereεt∼N(0,σt) Stochastic gradient descent is by itself a noisy estimate
of the true gradient, which increases the chances of ﬁnding a global minimum Saddle-points in which the gradient is close to zero may cause gradient descent to
make a step which is too small To alleviate this problem we can add noise εt∼
N(0,σt) to the update in gradient descent xt+1=xt−αgt+εtwithσtdecreasing
in timet Fortunately, the noisy estimate of the gradient in SGD eliminates the
need to add noise to the gradient update, and also has the advantage of being a
very eﬃcient approximation of the gradient

============================================================

=== CHUNK 039 ===
Palavras: 367
Caracteres: 2739
--------------------------------------------------
Averaging the outputs of multiple
steps of SGD, also known as stochastic weight averaging (Izmailov et al., 2018),
improves generalization In practice, SGD may be implemented by going though
a random ordering of the training examples A training epoch amounts to a pass
through the training data The data are then re-randomized for the next epoch,
and so on, until convergence Sincetrainingneuralnetworksmaybeaverytime-consumingprocess,stochas-
tic gradient descent is often used in practice This means that we sample the
gradient value at a new point and then update our parameters using this sam-
ple If we want to use a mini-batch of samples, we need to compute the gradient
over the mini-batch An excellent property of SGD is that these samples are un-
biased estimators of the actual gradient This means that if we repeat updating
the parameters for many iterations, we will eventually get close to the optimal
parameters 3.3.5 Adaptive Gradient Descent
The third problem with gradient descent is that it takes many steps in ﬂat
regions since the directions of the gradients in consecutive iterations for optimal
step sizes are perpendicular, forming a slow zig-zag pattern A solution is to use the gradients from previous steps for faster convergence Adaptive gradient descent methods use gradients from earlier steps to compute
the current update A critic of adaptive gradient descent methods (Wilson et al.,
2017) shows that they may result in solutions that are diﬀerent from that of
gradient descent and SGD A motivation for using the gradients computed in
previousiterationstoaﬀectthecurrentupdateistomovefasteralongdimensions
of low curvature and slower along dimensions with oscillations 46 3 Optimization
3.3.6 Momentum
One of the simplest adaptive gradient descent methods is called gradient descent
with momentum The idea is to add a fraction of the previous step’s gradient
to the current step’s update, where the fraction is a parameter that is tuned This results in faster convergence in ﬂat regions and slower convergence in steep
regions Themomentumvectoraccumulatesgradientsfrompreviousiterationsforcom-
putingthecurrentgradient(Sutskeveretal.,2013).Inaweightedmovingaverage
the weights decrease arithmetically, normalized by the sum of weights:
at=nat+(n−1)at−1+···+at−n+1
n+(n−1)+···+1(3.27)
Forst=/summationtextt
i=t−n+1aiwe have st+1=st+at+1−at−n+1 Therefore:
at+1=nat+1+(n−1)at+···+at−n+2
n+(n−1)+···+1
=at+nat+1−at−···−at−n+1
n+(n−1)+···+1=at+nat+1−st,n
n(n+1)
2(3.28)
In an exponentially weighted moving average the weights decrease exponen-
tially:
mt=αat+(1−α)mt−1
=αat+(1−α)(αat−1+(1−α)(αat−2+(1−α)(···)))
=α(at+(1−α)at−1+(1−α)2at−2+···)(3.29)
and by unrolling the telescopic sum, the weight of at−iisα(1−α)i

============================================================

=== CHUNK 040 ===
Palavras: 424
Caracteres: 3019
--------------------------------------------------
Settingato be the gradient and choosing the parameter β=1−α∈[0,1)
close to 1, and step sizes {αt}T
t=1, we compute the gradient for each iteration
using momentum as an exponentially weighted moving average of gradients:
gt=∇f(xt)
mt=βmt−1+gt
xt+1=xt−αtmt(3.30)
The special base of β= 0 reduces to gradient descent The ﬁrst problem with
momentum is that the step sizes may not decrease once we have reached close
to the minimum that may cause oscillations, which can be remedied by using
Nesterov momentum (Dozat, 2016) that replaces the gradient with the gradient
after computing momentum (Dozat, 2016):
gt=∇f(xt−αβmt−1)
mt=βmt−1+gt
xt+1=xt−αtmt(3.31)
3.3 First-Order Methods 47
3.3.7 Adagrad
A second problem with momentum is that it updates all components of xtusing
the same learning rate α Therefore, adaptive subgradient descent, or Adagrad
(Duchi et al., 2011), uses adaptive updates for diﬀerent components of the learn-
ing rate, making the method less sensitive to α:
gt=∇f(xt)
st=βst−1+g2
t
xt+1=xt−αtgt√st+ε(3.32)
whereg2
t=gt⊙gtis a pointwise multiplication If g2
tis large then1√st+εis small This is a limitation since the learning rate may be monotonically decreasing,
decaying to zero lim st→∞1√st+ε=0 Improvements upon Adagrad for overcoming this limitation include RMSProp
and AdaDelta:
•RMSProp (Tieleman and Hinton, 2012) is Adagrad using a weighted moving
average, replacing:
st=βst−1+g2
t (3.33)
with
st=βst−1+(1−β)g2
t (3.34)
•AdaDelta (Zeiler, 2012) is Adagrad using an exponential decaying average of
square updates without a learning rate, replacing:
xt+1=xt−αt1√st+εgt (3.35)
with:
xt+1=xt−√ut+ε√st+εgt
ut+1=γut+(1−γ)Δx2(3.36)
where Δx2=(xt+1−xt)⊙(xt+1−xt) is a pointwise multiplication 3.3.8 Adam: Adaptive Moment Estimation
Adaptive moment estimation, or Adam (Kingma and Ba, 2014), combines the
best of both momentum updates and Adagrad-based methods, as shown in Al-
gorithm 3.3 Like momentum updates, Adam does not rely on a pre-speciﬁed
learning rate, but it also does not suﬀer from the same ﬂaw of overﬁtting the ini-
tial stage of training Typical hyperparameter values are β1=0.9a n dβ2=0.99 Several improvements upon Adam include the following:
•NAdam (Dozat, 2016) is Adam with Nesterov momentum 48 3 Optimization
Algorithm 3.3 Adam: Adaptive moment estimation givenstarting point x1∈domf
givenlearning rates {αt}T
t=1
givendecay rates β1,β2∈[0,1) close to 1
givensmallε>0
initm0=0 ,v0=0
whilenot converged do:
gt=∇f(xt) gradient
mt=β1mt−1+(1−β1)gtﬁrst momentum
vt=β2vt−1+(1−β2)g2
tsecond momentum
xt+1=xt−αtmt√vt+εupdate
•Yogi (Zaheer et al., 2018) is Adam with an improvement to the second mo-
mentum term, which is rewritten as:
vt=vt−1−(1−β2)(vt−1−g2
t) (3.37)
and replaced with:
vt=vt−1−(1−β2)sign(vt−1−g2
t)g2
t (3.38)
•AMSGrad (Reddi et al., 2018) is Adam with the following improvement:
ˆvt=m a x (ˆvt−1,vt)
xt+1=xt−αtmt√ˆvt(3.39)
3.3.9 Hypergradient Descent
Hypergradient descent (Baydin et al., 2018) performs gradient descent on the
learning rate within gradient descent

============================================================

=== CHUNK 041 ===
Palavras: 358
Caracteres: 2414
--------------------------------------------------
This improves the convergence of the var-
ious gradient descent methods and may be applied to any adaptive stochastic
gradient descent method Computing the derivative of fwith respect to αtis
used to update:
ht=∂f(xt)
∂α=gT
t∂
∂α(xt−1−αgt−1)
αt+1=αt−βht=αt+βgT
tgt−1(3.40)
Algorithm 3.4 shows the application to gradient descent The above methods are usually used with annealing schedules, which provide
a schedule of the learning rate, and are applied when the network’s performance
does not improve The schedule may lower the learning rate when the optimiza-
tion gets stuck in a local minimum and increase the learning rate when the
network is progressing well 3.4 Second-Order Methods 49
Algorithm 3.4 Hypergradient descent givenobjective function f(x)
givenstarting point x1∈domf
givenstarting learning rate α1
givenhypergradient learning rate β
whilenot converged do:
gt=∇f(xt) gradient
ht=∂f(xt)
∂αtgradient
αt+1=αt−βhtupdate
xt+1=xt−αt+1gtupdate
3.4 Second-Order Methods
First-order methods are easier to implement and understand but have a slower
convergence rate than second-order methods Second-order methods use the ﬁrst
and second derivatives of a univariate function or the gradient and Hessian of
a multivariate function to compute the step direction and size Second-order
methods approximate the objective function using a quadratic, resulting in faster
convergence than ﬁrst-order methods The second-order information allows us to
identify a local minimum among extreme points 3.4.1 Newton’s Method
Newton’s method for zero values or for ﬁnding roots of a function ﬁnds a ﬁrst-
order approximation:
f/prime(xt)=f(xt)
xt−xt+1(3.41)
xt+1=xt−f(xt)
f/prime(xt)(3.42)
as shown in Figure 3.7 Newton’s method is an iterative process To ﬁnd the root
of a function, the method takes the sample point and guesses a function value
at that position It then makes a new guess based on the current guess at the
function value Similarly, Newton’s method for optimization or for ﬁnding roots of the deriva-
tive of a function ﬁnds a second-order approximation:
xt+1=xt−f/prime(xt)
f/prime/prime(xt)(3.43)
as shown in Figure 3.8 Newton’s method is a modiﬁcation of the secant method
applicable for ﬁnding the zero of the derivative of a function at a special point This special point is a point where the function’s derivative is equal to zero This
is the method that is used by Newton’s technique for optimization

============================================================

=== CHUNK 042 ===
Palavras: 367
Caracteres: 3030
--------------------------------------------------
50 3 Optimization
Figure 3.7 First-order ﬁt: line Figure 3.8 Second-order ﬁt: quadratic Our goal is to ﬁnd xwhich minimizes the objective function f:
x⋆= argmin
xf(x) (3.44)
Givenxtwe would like to take a step toward xt+1that is closer to x⋆:
f/prime(xt+1)≈f/prime(xt)+f/prime/prime(xt)(xt+1−xt) (3.45)
such that f/prime(xt+1) = 0, resulting in Equation 3.43, deﬁned for f/prime/prime(x)/negationslash=0 3.4 Second-Order Methods 51
3.4.2 Second-Order Taylor Approximation
The relationship between a function and its derivative is deﬁned by:
f(b)−f(a)=/integraldisplayb
af/prime(x)dx (3.46)
and therefore:
f(x+h)−f(x)=/integraldisplayh
0f/prime(x+a)dx (3.47)
and the Taylor series of faroundxis:
f(x+h)=f(x)+/integraldisplayh
0f/prime(x+a)dx
=f(x)+/integraldisplayh
0/parenleftbigg
f/prime(x)+/integraldisplaya
0f/prime/prime(x+b)db/parenrightbigg
da=···
=f(x)+f/prime(x)
1!h+f/prime/prime(x)
2!h2+···=∞/summationdisplay
n=0f(n)(x)
n!hn(3.48)
Plugging in aforx,a n d(x−a)f o rh, results in the Taylor series of f(x) around
a, given by:
f(x)=∞/summationdisplay
n=0f(n)(a)
n!(x−a)n=f(a)+f/prime(a)
1!(x−a)+f/prime/prime(a)
2!(x−a)2+···(3.49)
The second-order Taylor approximation of a univariate function by a quadratic
is:
f(x)≈f(a)+f/prime(a)(x−a)+f/prime/prime(a)1
2(x−a)2(3.50)
To ﬁnd the minimum, we set the derivative with respect to xto zero and get:
f/prime(x)≈f/prime(a)+f/prime/prime(a)(x−a) = 0 (3.51)
and solving for x:
x=a−f/prime(a)
f/prime/prime(a)(3.52)
which is the update of Newton’s method, and is deﬁned when f/prime/prime(a)/negationslash=0 For a multivariate function f:Rn→Rthe approximation of the Taylor
expansion is:
f(x)≈f(a)+∇f(a)(x−a)T+1
2(x−a)T∇2f(a)(x−a) (3.53)
Denoting the gradient gand Hessian H:
∇f(a)=g
∇2f(a)=/parenleftbigg∂2f(a)
∂xi∂xj/parenrightbigg
=H(3.54)
52 3 Optimization
Table 3.1 Comparison of properties of gradient descent vs Gradient descent Newton’s method
Order First Second
Convergence Linear Quadratic
Memory O(n) O(n2)
Computation O(n) O(n3)
Conditioning Degrades
Robustness More sensitive
we get
f(x)≈f(a)+(x−a)g+1
2(x−a)TH(x−a) (3.55)
Regrouping the second-order, ﬁrst-order, and constant terms we get:
q(x)=1
2xTHx+(g−Ha)Tx+c (3.56)
Since the gradients ∇xbTx=band∇xxTAx=(A+AT)x, and since the Hessian
is a symmetric matrix, solving for a critical point of the function by setting its
gradient to zero results in:
∇q(x)=Hx+(g−Ha) = 0 (3.57)
The solution is x⋆=a−H−1g, which is the Newton–Raphson update rule:
xt+1=xt−H−1
tgt (3.58)
Notice that replacing the Hessian Hwith the identity Imatrix times a scalar
αreduces Newton’s method to the special case of gradient descent since xt+1=
xt−αgt For a deep neural network:
gt=1
m∇θ/summationdisplay
L(yi,ˆyi)
Ht=1
m∇2
θ/summationdisplay
L(yi,ˆyi)
θt+1=θt−H−1gt(3.59)
Table 3.1 and Figure 3.9 compare gradient descent with Newton’s method If the second derivative is unknown, we can approximate it by using the ﬁrst
derivatives:
f/prime/prime(x)≈f/prime(x)−f/prime(a)
x−a(3.60)
which brings us to quasi-Newton methods, described next

============================================================

=== CHUNK 043 ===
Palavras: 366
Caracteres: 2646
--------------------------------------------------
3.4 Second-Order Methods 53
Figure 3.9 Comparison of convergence path of gradient descent vs Newton’s method
(illustrated by a shorter line through the function’s level sets) 3.4.3 Quasi-Newton Methods
Quasi-Newton methods, which provide an iterative approximation to the in-
verse Hessian H−1, avoid computing the second derivatives, avoid inverting the
Hessian and may also avoid storing the Hessian matrix Quasi-Newton methods
typically converge faster than Newton methods For a convex quadratic function:
f(x)=1
2xTHx+bTx+c (3.61)
where the Hessian His positive deﬁnite, it holds that:
∇f(x)=Hx+b
∇2f(x)=H(3.62)
Therefore, the Hessian satisﬁes:
∇f(x)−∇f(y)=H(x−y) (3.63)
Multiplying both sides by Q=H−1, we get the secant condition:
Q(∇f(x)−∇f(y)) =x−y (3.64)
If the matrix Qsatisﬁes the secant condition and the function fcan be approxi-
mated by a quadratic function, then its inverse Hessian would be approximated
byQ We initialize Q0=Ito the identity and iteratively update the matrix,
satisfying:
Qt+1(∇f(xt+1)−∇f(xt)) =xt+1−xt (3.65)
To deﬁne the iterative updates, we ﬁrst deﬁne the diﬀerences:
δt=xt+1−xt
ht=gt+1−gt=∇f(x)−∇f(x)(3.66)
54 3 Optimization
and letzt=Qtht Next, we update the inverse Hessian approximation using the
following three methods: SR1, DFP, or BFGS These methods are similar in that
they all begin by initializing the inverse Hessian to the identity matrix and then
iteratively updating the inverse Hessian These three update rules diﬀer in that
their convergence properties improve upon one another The ﬁrst method, called
an SR1 update, is a rank one correction and is deﬁned by:
Qt+1=Qt+(δt−zt)(δt−zt)T
(δt−zt)Tht(3.67)
The second method for updating the inverse Hessian approximation is the
Davidon–Fletcher–Powell(DFP)correction(Davidon,1991;FletcherandPowell,
1963) and is deﬁned by:
Qt+1=Qt+δtδT
t
hT
tδt−ztzT
t
hT
tzt(3.68)
The third method updates the inverse Hessian approximation by the BFGS cor-
rection, deﬁned by:
Qt+1=Qt+(ztδT
t)+(ztδT
t)
hT
tzt−/parenleftbigg
1+hT
tδt
hT
tzt/parenrightbiggztzT
t
hT
tzt(3.69)
In summary, quasi-Newton methods avoid computing the inverse Hessian H−1
matrix and instead iteratively approximate Each iteration involves O(n2) opera-
tions, without O(n3) operations such as solving linear systems or matrix–matrix
operations The iterative algorithm is robust, with fast convergence Storing the inverse Hessian approximation matrix itself for large dimensions
may be prohibitive Therefore, limited memory BFGS (L-BFGS) (Liu and No-
cedal, 1989) avoids storing the inverse Hessian approximation by unrolling the
approximation and using limited memory for storing updates

============================================================

=== CHUNK 044 ===
Palavras: 369
Caracteres: 2547
--------------------------------------------------
The L-BFGS ap-
proximates BFGS by storing only the last updates of δt,ht,a n dzt Finally,
quasi-Newton methods are easy to implement 3.5 Evolution Strategies
Rather than incrementally improving a single point toward a local minimum,
evolution strategies use a probability distribution and many sample points in
the search space or parameter space to ﬁnd a local minimum, which may be eas-
ily distributed An advantage of using a probability distribution is that it allows
the algorithm to escape local minima The cross-entropy method stores a proba-
bility distribution over the search space and samples points from this probability
distribution The algorithm proceeds iteratively: Each iteration samples from the
probability distribution and then updates the probability distribution to ﬁt the
best samples by the cross-entropy Typically, a multivariate normal distribution
is used as the probability distribution, maintaining a mean vector and covariance
3.6 Summary 55
matrix Neural evolution strategies use a probability distribution over the search
space as well; however, instead of ﬁtting the probability distribution to the best
samples, it uses gradient descent where the gradient is computed from the sam-
ples (Salimans et al., 2017) Covariance matrix adaptation (Hansen, 2006) stores
a covariance matrix and updates a probability distribution iteratively based on
samples from a multivariate Gaussian distribution 3.6 Summary
Gradient descent iteratively ﬁnds a local minimum by taking steps in the direc-
tion of the steepest descent Three main problems with training neural networks
using gradient descent and their solutions are:
•The total loss function with respect to the neural network weights is a sum of
many individual losses for many samples The solution is mini-batch or SGD •Thederivativeofthetotallossiscomputedwithrespecttoallnetworkweights The solution is backpropagation •The directions of gradients for consecutive time steps which follow optimized
step sizes are orthogonal, forming a zig-zag pattern, which is slow, especially
in ﬂat regions The solution is adaptive gradient descent Speciﬁcally, adding momentum avoids the zig-zag directions when using the
optimal step size and improves progress toward the local minimum Adaptive
methods update the learning rate in each dimension individually and are com-
bined with momentum Stochastic gradient descent approximates the gradient
by random samples, which is highly eﬃcient for training neural networks with
many examples, and its randomness improves optimization

============================================================

=== CHUNK 045 ===
Palavras: 350
Caracteres: 2191
--------------------------------------------------
Using the second
derivative speeds up convergence, and quasi-Newton methods approximate the
second derivative when unavailable or approximate the inverse Hessian, avoid-
ing its computation and storage In contrast to gradient descent methods, which
advance a single point toward a local minimum, evolution strategies update a
probability distribution, from which multiple points are sampled, lending itself
to a highly eﬃcient distributed computation 4 Regularization
4.1 Introduction
Regularization is a technique that helps prevent overﬁtting by penalizing the
complexity of the network In this chapter, we will describe three forms of regu-
larization: (1) penalty term, which is adding a penalty term to the cost function,
which encourages the model to decrease the weights and has fewer parameters
and a simpler structure; (2) dropout, which randomly removes a percentage of
neurons in each layer from the network – this causes the network to be less ac-
curate but also makes it more robust to overﬁtting; and (3) data augmentation,
which generates new training data points that are similar to the existing train-
ing data points, though not identical Overﬁtting occurs when a model is too
closely tailored to the training data and does not generalize well to unseen data;
augmentation may avoid overﬁtting and allow the model to generalize better to
new data Overﬁtting data, as shown in Figure 4.1, happens when the model is too com-
plex and captures noise in the data To avoid overﬁtting, we may (1) add a
penalty term to the loss function; (2) use dropout, which is a regularization
technique that randomly sets some of the network activations to zero; or (3)
augment the data These three methods are all forms of regularization whose
goal is to prevent the network from overﬁtting the training data Figure 4.2 shows the improvement in image classiﬁcation performance as ef-
ﬁcient methods were developed for training deeper neural networks This ﬁgure
shows the error of the best-published network for each year, while Figure 4.3
shows the corresponding number of neural network layers As shown in the ﬁg-
ures, the error decreases as the number of neural network layers increases

============================================================

=== CHUNK 046 ===
Palavras: 358
Caracteres: 2284
--------------------------------------------------
This
continues until a certain depth, at which adding residual connections between
layers is required to continue this trend and avoid vanishing gradients 4.2 Generalization
Training data is a sample from a population We want our neural network model
to generalize well to unseen test data drawn from the same population Speciﬁ-
cally, the generalization gap Gis deﬁned as the diﬀerence between the expected
4.2 Generalization 57
Figure 4.1 Underﬁtting (left): The model is not complex enough to capture the
underlying pattern in the data Overﬁtting (center): The model is too complex and
captures noise in the data Regularization (right): The green line represents a model
which is neither too simple nor too complex Regularization helps prevent overﬁtting
by penalizing the complexity of the model Figure 4.2 ImageNet classiﬁcation error by year loss when sampling from the population P, which the test error may approxi-
mate, and the empirical loss, which is the training error of the training samples
(xi,yi)f o ri=1,...,m:
G(f(X,W)) = E(X,Y)∼P(L(yi,f(xi,W))−1
mm/summationdisplay
i=1L(yi,f(xi,W))) (4.1)
We can use the generalization gap to measure how well our neural network
model ﬁts the training data If the generalization gap is low, the neural network
model is a good ﬁt for the training data If the generalization gap is high, the
neural network may be overﬁtting 58 4 Regularization
Figure 4.3 Number of neural network layers for corresponding best models on
ImageNet by year Learning curves plot the test accuracy as a function of the amount of training
data (X,Y) for various models Adding more training data ( X,Y) increases the
generalizationaccuracyuptoalimit,reducingthegeneralizationgapasexpected
E(X,Y)∼Pby Equation 4.1 4.3 Overﬁtting
A neural network model tailored to training data and does not generalize well
to unseen test data has a high generalization error and is said to be overﬁtting
the data The training error decreases as we increase the network complexity for
a given dataset, and the test error also decreases until a certain point and then
increases due to overﬁtting If we increase the network complexity even further,
then at the limit, the test error begins to decrease again, a phenomenon that is
known as double descent (Belkin et al., 2019)

============================================================

=== CHUNK 047 ===
Palavras: 356
Caracteres: 2072
--------------------------------------------------
Unless we have suﬃcient data, a
very complex neural network may ﬁt the training data very well at the expense of
a poor ﬁt to the test data, resulting in a large gap between the training error and
test error, which is overﬁtting Since training data is a population sample, it may
be overﬁtting; applying our model to test data is the way to detect overﬁtting 4.4 Cross Validation 59
4.4 Cross Validation
Cross validation allows us to compute the mean and variance of the general-
ization error We randomly split the data into kfolds and iteratively take the
training data to be k−1 out ofkfolds for building a model that is tested on the
remaining fold After testing each model, we compute the mean and variance of
the generalization error over all kmodels The mean generalization error is the average generalization error over all k
models and is a good indicator of how well a model performs on unseen data A
common practice is to use cross validation to select hyperparameters for training
a model For example, when training a neural network with stochastic gradient
descent (SGD), we can use cross validation to ﬁnd the optimal learning rate and
momentum We can also use cross validation to select features for building a
model We can build many models with diﬀerent subsets of features and then
compute their mean and variance of the generalization error to determine which
subset performs best 4.5 Bias and Variance
Practical steps in training neural networks include reducing bias by training a
deeper and wider network and reducing variance by obtaining more data or by
regularization Our goal is to reduce both bias and variance In order to reduce
the bias, we can train a deeper and wider network with more parameters to learn
from the training data In other words, a deeper and wider network can learn
more complex functions from the training data In order to reduce the variance,
we can obtain more data or use regularization The more data we have, the less
variance in our model Regularization is a method of preventing overﬁtting by
penalizing complex models

============================================================

=== CHUNK 048 ===
Palavras: 354
Caracteres: 2832
--------------------------------------------------
4.6 Vector Norms
We deﬁne vector norms before discussing regularization using diﬀerent norms For all vectors x,yand scalars α: (1) all vector norms of a non-zero vector are
a positive scalar; (2) norms maintain the triangle inequality; and (3) all vectors
and scalars have a rescaling property, as follows:
1./bardblx/bardbl≥0a n d/bardblx/bardbl=0i ﬀx=0
2./bardblx+y/bardbl≤/bardblx/bardbl+/bardbly/bardbl
3./bardblαx/bardbl=|α|/bardblx/bardbl
Special norms are illustrated in Figure 4.4; these are the /lscript1norm deﬁned as:
/bardblx/bardbl1=|x1|+···+|xn|=n/summationdisplay
|xi| (4.2)
60 4 Regularization
Figure 4.4 Unit circle {x∈R2:/bardblx/bardbl=1}for each vector norm: /lscript1,/lscript2,/lscript∞,/lscriptp the/lscript2norm:
/bardblx/bardbl2=/radicalBig
x2
1+···+x2n=/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1x2
i (4.3)
and the/lscript∞norm:
/bardblx/bardbl∞=m a x
i(|x1|,...,|xn|)=m a x
i|xi| (4.4)
which is the maximum norm – the maximum possible distance between any two
vectors The /lscript∞norm is also the maximum of all norms More generally, we may
deﬁne the /lscriptpnorm by:
/bardblx/bardblp=/parenleftBiggn/summationdisplay
i=1(/bardblxi/bardblp)/parenrightBigg1
p
(4.5)
for 1≤p≤∞ Notice that the number of non-zero elements in a vector, often
informally referred to as the /lscript0“norm,” is not a norm by the properties above 4.7 Ridge Regression and Lasso
Weformulatedmachinelearningasanoptimizationobjectiveofthegeneralform:
J(θ)=1
mm/summationdisplay
i=1L(fθ(xi,yi))+λR(θ) (4.6)
We may make multiple design choices regarding this general optimization objec-
tive: (1) deﬁne the hypothesis class or model denoted by the function fθ(xi,yi)
andit’sparameters θ;(2)deﬁnethelossfunctiondenotedby L;and(3)deﬁnethe
type of regularization function R(θ) For example, choosing a linear model with
a squared loss function and an /lscript2regularization term results in the objective:
Jridge(θ)=1
mm/summationdisplay
i=1(θTxi−yi)2+λ/bardblθ/bardbl (4.7)
also known as ridge regression, where λ>0 is a regularization hyperparameter Ridge regression is a form of linear regression that penalizes the squared distance
4.8 Regularized Loss Functions 61
between the predicted and observed values, which may also be written in matrix
form as:
Jridge(θ)=1
m(Xθ−Y)T(Xθ−Y)+λ/bardblθ/bardbl (4.8)
whereXis ad×mmatrix whose columns are the data points, and Yis ann×1
vector of labels To ﬁnd the minimum, we compute the derivative of the loss with
respect to the parameters θ:
∇θJridge=2
mXT(Xθ−Y)+2λθ (4.9)
and set the gradient to zero:
∇θJridge=1
mXTXθ−1
mXTY+λθ= 0 (4.10)
which has an analytic solution:
θ=(XTX+nλI)−1XTY (4.11)
A diﬀerent choice of objective is called Lasso, which is a linear regression model
that penalizes the absolute value of the diﬀerence between the prediction and
the mean

============================================================

=== CHUNK 049 ===
Palavras: 353
Caracteres: 2343
--------------------------------------------------
4.8 Regularized Loss Functions
Regularized loss functions is used in neural networks to prevent overﬁtting This
type of regularization is also called weight decay for its eﬀect of decreasing the
weights Regularized loss functions are used to penalize the network for large
weights and large activations A regularization term R(W) is added to the loss
function:
1
mm/summationdisplay
i=1L(yi,fW(xi))+R(W) (4.12)
whereR(W)=λ/bardblW/bardblpfor a regularization parameter λ>0, which is a scalar
and an/lscriptpnorm The regularization parameter λcontrols how much we want to
penalize large weights If λ= 0, then no regularization occurs If λ=1 ,t h e n
all weights are penalized equally A value between 0 and 1 gives us a trade-
oﬀ between ﬁtting complex models and ﬁtting simple models The value of the
hyperparameter λmay be set by cross validation Speciﬁcally, by splitting the
training data into multiple folds k, training k−1 folds and validating on the kth
fold for a set of possible λvalues This results in kvalues of the loss for each
diﬀerent value of λ We can then compute the average of the klosses for each λ
value, and choose the best λfor our data Commontypesofregularizationare /lscript1and/lscript2.Theeﬀectofeachofthesenorms
when used for regularizing fully connected neural networks may be interactively
62 4 Regularization
Figure 4.5 /lscript1regularization The solution is called a sparse solution since the diamond
shape of the /lscript1norm intersects with the loss function on a sharp point where
coeﬃcients are zero visualized (Smilkov and Carter, n.d.) Setting p= 1 results in the /lscript1norm, and
is called /lscript1regularization The eﬀect of /lscript1regularization is a solution with zero
coeﬃcients, also called a sparse solution, since the diamond shape of the norm,
as shown in Figure 4.5, intersects with the loss function on a sharp point where
coeﬃcients are zero Settingp= 2 results in the /lscript2norm, called /lscript2regularization The diﬀerence
between /lscript1and/lscript2regularization is that /lscript1regularization is a penalty on the
sum of absolute weights, which promotes sparsity, whereas /lscript2regularization is a
penalty on the sum of the squares of the weights The eﬀect of adding a regularization term to a neural network loss may be
observed directly by thechangein theupdatestep of SGD

============================================================

=== CHUNK 050 ===
Palavras: 353
Caracteres: 2639
--------------------------------------------------
Consider thegradient
of the/lscript2regularized loss with respect to a single random sample in SGD:
L(yi,fW(xi))+λ/bardblW/bardbl2(4.13)
The weight update is then:
Wt+1=Wt−α∇Wt(L(yi,fWt(xi))+λ/bardblWt/bardbl2)
=Wt(1−2αλWt)−α∇Wt(L(yi,fWt(xi))(4.14)
which demonstrates the eﬀect of shrinking the weights 4.9 Dropout Regularization
Dropout is a regularization technique used in neural networks to prevent over-
ﬁtting (Srivastava et al., 2014) It is a technique that randomly removes a per-
centage of the neurons in each layer from the network This causes the network
to be less accurate, making it more robust to overﬁtting 4.9 Dropout Regularization 63
Figure 4.6 Fully connected neural network Figure 4.7 Dropout regularization Activations are randomly removed with probability
pat training time Activations are randomly set to zero during training Testing is done without
dropout For layer l, set dropout probability pl For each activation al
j=al
jIl
j
forj=1,...,n l, where:
Il
j=/braceleftBigg
0 with probability pl
1
1−plwith probability 1 −pl(4.15)
such that activations that remain stand in for activations dropped out Figure 4.6 shows a fully connected neural network before dropout Figure 4.7
showsthenetworkafterdropoutwhereactivations(inred)arerandomlyremoved
with probability pat training time The dropout rate pis a hyperparameter 4.9.1 Random Least Squares with Dropout
Dropout is not unique to neural networks and was used earlier in least squares Random least squares with dropout is equivalent to ridge regression:
LI(β)=1
2m/summationdisplay
i=1(yi−k/summationdisplay
Xi,jIi,jβj)2(4.16)
64 4 Regularization
where
Ii,j=/braceleftBigg
0 with probability p
1
1−pwith probability 1 −p(4.17)
set:
E/parenleftbigg∂LI(β)
∂β/parenrightbigg
=−XTy+XTXβ+p
1+pDβ= 0 (4.18)
withD=d i a g{/bardblx1/bardbl2,...,/bardblxk/bardbl2} The solution is ˆβ=(XTX+p
1+pD)−1XTy,
which is ridge regression 4.9.2 Least Squares with Noise Input Distortion
Least squares with noise input distortion is also equivalent to ridge regression:
LN(β)=1
2m/summationdisplay
i=1(yi−k/summationdisplay
j=1(Xi,j+ni,j)βj)2(4.19)
We add random noise to the prediction N(0,λ), by setting:
E/parenleftbigg∂LN(β)
∂β/parenrightbigg
=−XTy+XTXβ+λβ= 0 (4.20)
where E(n2
i,j)=λ The solution is ˆβ=(XTX+p
1+pλ)−1XTy, which is ridge
regression 4.10 Data Augmentation
Data augmentation is the process of generating new data points by transform-
ing existing ones This is done to improve the performance of machine learning
algorithms For example, if a dataset has many images of cars, data augmenta-
tion might generate new images by rotating them or changing their color

============================================================

=== CHUNK 051 ===
Palavras: 364
Caracteres: 2320
--------------------------------------------------
The
augmented training data is then used to train a neural network Data augmentation may be used to reduce overﬁtting Overﬁtting occurs when
a model is too closely tailored to the training data and does not generalize well
to new data Data augmentation can be used to generate new training data
points that are similar to the existing training data points but are not identical
copies This helps the model avoid overﬁtting and generalizing better to new
data Since data augmentation adds more data for training, it may be used as a
regularization technique for reducing variance in models We augment the training data by replacing each example pair ( xi,yi)w i t ha
collection {x∗b
i,yi}B
b=1, where each x∗b
iis a version of xi Transformations com-
monly used for data augmentation include rotation, reﬂection, translation, shear,
crop, color transformation, and added noise 4.11 Batch Normalization 65
4.11 Batch Normalization
Explodinggradientsisaproblemthatmayoccurwhentraininganeuralnetwork They occur when the gradient of the cost function with respect to the network’s
weights is too large This can cause the weights to change too quickly and cause
the neural network to diverge Vanishing gradients is a phenomenon in neural
networks where the gradient of the error function becomes small or zero This
means that the network cannot learn anymore and is stuck at a local minimum Covariate shift occurs in neural networks when the magnitudes of the inputs to
a layer change during training, making it challenging to learn the weights of a
subsequent layer while accounting for the change in magnitude of the inputs Batch normalization is a technique for training neural networks that are used
to counteract the problems of exploding and vanishing gradients, as well as co-
variate shift It does this by scaling the input data by a factor of1√n, wheren
is the batch size This makes the gradient values more stable and prevents them
from changing too quickly 4.12 Summary
Regularization is a technique that can be used to prevent overﬁtting Regulariza-
tion can be achieved by adding a penalty term to the cost function The penalty
termisusuallyafunctionofthenumberofparametersinthemodel.Dropoutand
data augmentation are also forms of regularization in neural networks because
these methods help to prevent overﬁtting

============================================================

=== CHUNK 052 ===
Palavras: 379
Caracteres: 2553
--------------------------------------------------
Dropout is a technique that randomly
sets several weights in a neural network to zero This technique helps to pre-
vent overﬁtting by reducing the variance of the network Data augmentation is
a technique that involves modifying the input data to the neural network by
applying random transformations This technique also helps prevent overﬁtting
by increasing the size of the training set Part II
Architectures

5 Convolutional Neural Networks
5.1 Introduction
The ﬁrst step in the visual pathway is the retina, which consists of a layer of pho-
toreceptor cells: the rods and cones The retina’s output is an optic nerve consist-
ing of one million ﬁbers connected to regions of the brain Each of these regions is
connected in turn to other regions The primary visual cortex reacts to low-level
visual stimuli such as oriented lines Hubel and Wiesel won the Nobel Prize for
mapping the function of receptor cells along the visual pathways of cats from the
retina to the cortex (Hubel and Wiesel, 1968) Processing proceeds in a hierar-
chical fashion of layers (Felleman and Van Essen, 1991) Fukushima introduced
the Neocognitron architecture (Fukushima, 1988), which led to the development
of modern convolutional neural networks (CNNs) Initially, computer vision re-
searchers handcrafted ﬁlters, whereas optimizing CNNs automatically calculates
the weights of ﬁlter banks in multiple layers by backpropagation Similarly, deep
learning researchers initially handcrafted CNN architectures, whereas neural ar-
chitecture search (NAS) automatically ﬁnds CNN architectures from basic layers
and building blocks by optimization Finally, computational power allows gener-
ating entire wirings of graph neural networks (GNNs) and vast amounts of data
allow training vision Transformer networks without handcrafting the inductive
bias into the network architecture Convolutional neural networks are a special
case of these vision Transformer networks, which supersede CNN performance on
common tasks and benchmarks Today, 60 years after Hubel and Wiesel’s (1959)
discoveries, there is an understanding of the visual pathways in the human brain,
and common neural networks trained on millions of images outperform humans
in visual recognition 5.1.1 Representations Sharing Weights
The most successful deep learning representations share weights: CNNs, de-
scribed in this chapter, share weights across space; recurrent neural networks
(RNNs), described in Chapter 6, share weights across time; and GNNs, described
in Chapter 7, share weights across neighborhoods

============================================================

=== CHUNK 053 ===
Palavras: 361
Caracteres: 2384
--------------------------------------------------
In a fully connected neural network, as presented in Chapter 2, the dimension
of the matrix of weights between two layers with nactivation units each is
70 5 Convolutional Neural Networks
the multiplication of the layer sizes, n×n Performing a computation with time
complexitysquareinthenumberofactivationunitsinalayer O(n2)isprohibitive
for wide layers Images are regular grids of pixels, and it is beneﬁcial to perform
the same operation locally on diﬀerent parts of the image Using a local ﬁlter and
sharing these weights spatially across the image reduces the number of weights
to a constant Convolutionalneuralnetworksreducethenumberofweightsbysharingweights
for diﬀerent parts of an image They may learn many sets of weights, namely
multiple ﬁlters, for each layer in the neural network, thereby capturing multiple
features such as edges, corners, and textures Each layer captures features at dif-
ferent scales, from low-level features, such as edges, through mid-level features,
such as textures, to high-level features, such as entire objects When we process data of a particular type, such as images with locality, or
when the mapping between input and output spaces has known properties, such
as invariance to transformations, incorporating this inductive bias into the neu-
ral network architecture, for example, in the form of a CNN, makes sense For
example, image pixels depend on neighboring pixels or fragments, and objects
are detected or classiﬁed with the same label under aﬃne transformations 5.2 Convolution
Convolution is the process of multiplying two functions together A very simple
example of convolution is multiplying an image with a kernel The convolution
kernel is a function that is typically represented by a small matrix that operates
on the image Speciﬁcally, the kernel is applied to the image by local pointwise
multiplication of the image with the kernel and by summing the contributions The convolution of an image with a kernel is a weighted average of the image 5.2.1 One-Dimensional Convolution
The deﬁnition of the discrete one-dimensional convolution of two functions fand
gis:
(f⋆g)(i)=s/summationdisplay
u=−sg(u)f(i−u) (5.1)
Figure 5.1 illustrates the process of sliding a 3 ×1 ﬁlterkover a 10 ×1 input
x, and for each position multiplying corresponding values of the input with the
ﬁlter and taking their sum, resulting in an output value

============================================================

=== CHUNK 054 ===
Palavras: 418
Caracteres: 2682
--------------------------------------------------
For example, yu+1=/summationtext3
i=1kixi+u−1foru=1,...,8, where we omit the reﬂection of the ﬁlter Convolution padding may be performed so that the output size is the same
as the input size Figure 5.2 illustrates zero-padding by padding the boundaries
with zeros Figure 5.3 illustrates reﬂection-padding by padding the boundaries
using values that are a reﬂection of the signal 5.2 Convolution 71
Figure 5.1 One-dimensional convolution Figure 5.2 One-dimensional convolution with zero-padding Filteringwiththeidentitykernel,forexample k=[ 0,1,0],resultsintheoutput
being equal to the input, as shown in Figure 5.3 Filtering with an averaging
kernel, for example, k=1
3[1,1,1], results in the output is equal to a local average
of the input Notice that we normalize the ﬁlter by dividing each ﬁlter coeﬃcient
by the sum of the coeﬃcients Filters are commonly used for blurring or sharpening a signal For example,
the ﬁlter kernel k=1
4[1,2,1] approximates a Gaussian blur, whereas the ﬁlter
kernelk=[−1,2,−1] sharpens the signal Figure 5.4 shows an example of a one-dimensional convolution, with a bias of
+1, followed by applying a non-linear function, the rectiﬁed linear unit (ReLU)
function, pointwise 5.2.2 Matrix Multiplication
Convolution is a linear operation that may be represented by matrix multiplica-
tion In one dimension we represent a ﬁlter kusing a Toeplitz matrix (B¨ ottcher
72 5 Convolutional Neural Networks
Figure 5.3 One-dimensional convolution with the identity ﬁlter Figure 5.4 Example of one-dimensional convolution with bias, followed by a ReLU and Grudsky, 2005) such that the matrix–vector product Kxis the convolution
operation k⋆x.T h em a t r i x Kmay be expressed as a linear combination of di-
agonal matrices, with 1s on the diagonal, multiplied by the ﬁlter weights For a
3×1 ﬁlterk=(k1,k2,k3)T: The ﬁrst matrix S1has 1s on the diagonal below
the main diagonal and is multiplied by the ﬁlter coeﬃcient k1; the second matrix
S2has 1s on the main diagonal and is multiplied by k2; and the third matrix S3
has 1s on the diagonal above the main diagonal and is multiplied by k3:
y=k⋆x=Kx=3/summationdisplay
i=1kiSix (5.2)
The derivative of the output ywith respect to each of the kernel weights is:
dy
dki=Six (5.3)
5.2 Convolution 73
For example, convolution of a ﬁlter k=(k1,k2,k3)Twith a signal x=
(x1,...,x 5)Texpressed as y=k⋆xis equivalent to multiplication y=Kxby
a band diagonal matrix Kcalled a Toeplitz matrix, with the kernel kreplicated
along the diagonal and all other elements zero:
⎡
⎣y1
y2
y3⎤
⎦=⎡
⎣k1k2k300
0k1k2k30
00k1k2k3⎤
⎦⎡
⎢⎢⎢⎢⎣x1
x2
x3
x4
x5⎤
⎥⎥⎥⎥⎦(5.4)
In this case the input dimension is 5, whereas the output dimension is 3

============================================================

=== CHUNK 055 ===
Palavras: 366
Caracteres: 2313
--------------------------------------------------
To have
the input and output dimensions be the same, we pad the signal x One form of
padding is zero-padding by adding zeros to the beginning and end of the signal
and adding the corresponding rows and columns of the matrix:
⎡
⎢⎢⎢⎢⎣y1
y2
y3
y4
y5⎤
⎥⎥⎥⎥⎦=⎡
⎢⎢⎢⎢⎣k1k2k30000
0k1k2k3000
00k1k2k300
000 k1k2k30
0000 k1k2k3⎤
⎥⎥⎥⎥⎦⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣0
x1
x2
x3
x4
x5
0⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(5.5)
Padding results in inputs and outputs of equal dimensions, which may be con-
venient to work with In this case we perform convolution of the ﬁlter with the signal by sliding
the ﬁlter across the signal one step, or stride, at a time We can also perform
the convolution with diﬀerent strides, for example with a stride of two in which
case the kernel matrix is/bracketleftbiggk1k2k300
00k1k2k3/bracketrightbigg Increasing the stride to two
decreases the output length by a factor of two Overall, the number of weights to
be considered is reduced signiﬁcantly, from the entire matrix dimensions to just
the kernel’s size In contrast, in a fully connected neural network, the number
of weights between two adjacent layers is the multiplication of the number of
activation units in the layers 5.2.3 Two-Dimensional Convolution
The two-dimensional convolution of a ﬁlter fwith a signal gis deﬁned as:
(f⋆g)(i,j)=s/summationdisplay
u=−ss/summationdisplay
v=−sg(u,v)f(i−u,j−v) (5.6)
A two-dimensional input Xrepresenting an image may be represented as a
regular two-dimensional grid with local connectivity of each pixel connected to
its neighbors, as shown in Figure 5.5 74 5 Convolutional Neural Networks
Figure 5.5 Regular two-dimensional grid Figure 5.6 Two-dimensional convolution For such a two-dimensional input Xrepresenting an image, we ﬁrst ﬂatten
Xinto a vector xby concatenating the rows of Xinto a single vector x.T h e
convolution of the two-dimensional ﬁlter Kwith the two-dimensional input X
is equivalent to a matrix–vector multiplication For example, for a 3 ×3 ﬁlterk
and a 7×7 imageXﬂattened into a vector x, the result of convolution is a 5 ×5
imageY, as shown in Figure 5.6, where each coeﬃcient is a linear combination
of nine products of the kernel weights centered upon the corresponding image
position For example, the value of the output y22computed by centering the ﬁlter k
on input x22is:
y22=k11x11+k12x12+k13x+···+k x

============================================================

=== CHUNK 056 ===
Palavras: 378
Caracteres: 2609
--------------------------------------------------
(5.7)
5.2 Convolution 75
As another example, consider the matrix form of the convolution of a two-
dimensional 3 ×3 ﬁlter:
k=⎡
⎣k11k12k13
k21k22k23
k31k32k33⎤
⎦ (5.8)
with a 5 ×5 image:
x=⎡
⎢⎢⎢⎢⎣x11x12x13x14x15
x21x22x23x24x25
x31x32x33x34x35
x41x42x43x44x45
x51x52x53x54x55⎤
⎥⎥⎥⎥⎦(5.9)
which is expressed as multiplication of a block Toeplitz matrix K:
K=⎡
⎣K1K2K300
0K1K2K30
00 K1K2K3⎤
⎦ (5.10)
whose blocks are themselves Toeplitz matrices Kifori=1,2,3:
Ki=⎡
⎣ki1ki2ki300
0ki1ki2ki30
00 ki1ki2ki3⎤
⎦ (5.11)
with the ﬂattened vector representation x=(x11,...,x 55)Tof the image In-
creasing the stride from one to two in each dimension decreases the output size
by a factor of two in each dimension Similarly to one-dimensional convolution,
padding can be done with zeros, as shown in Figure 5.7, or reﬂected values in
two-dimensions, as shown in Figure 5.8 Figure 5.9 shows an example of two-dimmensional convolution followed by
applyingtheReLU.Theresultofthisoperationisﬁndingavalueof1surrounded
by zeros Considering that an image may contain millions of pixels, the number of
weights between layers using a fully connected network may increase to billions,
whereas sharing all weights using a CNN results in a small constant number
of weights Between every two layers of the CNN we can learn multiple two-
dimensional ﬁlters, and for each ﬁlter have a constant number of weights The
convolution of a ﬁlter with each local neighborhood of the image results in a
response that captures diverse features from multiple ﬁlters and features at mul-
tiple scales from multiple network layers Before convolutional neural networks, image ﬁlters were manually designed
for speciﬁc purposes Examples of well-known ﬁlters include the box ﬁlter, ap-
proximation of Gaussian blur, sharpen ﬁlter, horizontal and vertical Sobel edge
detection, and Prewitt edge detection A discrete approximation of the Gaussian
ﬁlter is given by a two-dimensional matrix For a 3 ×3 discrete approximation,
76 5 Convolutional Neural Networks
Figure 5.7 Two-dimensional convolution with zero-padding the ﬁlter coeﬃcients are:
1
16⎡
⎣121
242
121⎤
⎦=1
4⎡
⎣1
2
1⎤
⎦⊗1
4/bracketleftbig
121/bracketrightbig
(5.12)
5.2.4 Separable Filters
In special cases such as the approximation of a Gaussian ﬁlter, above, two-
dimensionalconvolutionisequivalenttotheouterproductoftwoone-dimensional
ﬁlters, as shown in Figure 5.10 In such cases the computation is more eﬃcient:
Performing two one-dimensional convolutions of size kwith an image of size n×n
takes time O(n2k), whereas performing one two-dimensional convolution of size
k×ktakes time O(n2k2)

============================================================

=== CHUNK 057 ===
Palavras: 356
Caracteres: 2320
--------------------------------------------------
5.2.5 Properties
The convolution operation is commutative, associative, distributive, and diﬀer-
entiable:
•Commutative: f⋆g=g⋆f
5.2 Convolution 77
Figure 5.8 Two-dimensional convolution with reﬂection-padding •Associative: f⋆(g⋆h)=(f⋆g)⋆h
•Distributive: f⋆(g+h)=f⋆g+f⋆h
•Diﬀerentiation:d
dx(f⋆g)=df
dx⋆g=f⋆dg
dx
5.2.6 Composition
Repeated convolutions with a small kernel are equivalent to a single convolution
with a large kernel; however, they are more eﬃcient For example, two repeated
convolutions with a 3 ×3 kernel may be equivalent and more eﬃcient than a 2D
convolution with a 5 ×5 kernel, and three repeated convolutions with a 3 ×3
kernel may be equivalent and more eﬃcient than 2D convolution with a 7 ×7
kernel An example of inputs and outputs of a repeated convolution with a 3 ×3
kernel are shown in Figure 5.11 5.2.7 Three-Dimensional Convolution
Images are often represented by three color channels of red, green, and blue
(RGB) Two-dimensional convolution can be performed on each channel sepa-
rately, as shown in Figure 5.12, or a three-dimensional ﬁlter can be convolved
78 5 Convolutional Neural Networks
Figure 5.9 Two-dimensional convolution followed by a ReLU with the 3D volume consisting of the three channels, as shown in Figures 5.13
and 5.14 5.3 Layers
5.3.1 Convolution
Convolution of an n×n×3 color image with a k×k×3 ﬁlter, with padding,
results in an n×noutput, as shown in Figure 5.15 Convolution of an n×n×3
color image with four such ﬁlters, with padding, results in an n×n×4 volume
of activations, as shown in Figure 5.16 Convolution of an n×n×3 color image
withfﬁlters, with padding, results in an n×n×fvolume of activations, as
shown in Figure 5.17 Each set of such ﬁltering operations constitutes the linear part of a convolution
layer Repeated ﬁlteringmay increasethenumber ofchannels Asdescribed next,
a common way to reduce the spatial dimension is by pooling 5.3.2 Pooling
Pooling is an operation that reduces the dimensionality of the input by tak-
ing a function value from a set of locations Max pooling takes the maximum
over image patches, for example over 2 ×2 grids of neighboring pixels m=
max{x1,x2,x3,x4}, reducing dimensionality to half in each spatial dimension,
as shown in Figure 5.18 5.4 Example 79
Figure 5.10 Two-dimensional convolution with separable ﬁlters

============================================================

=== CHUNK 058 ===
Palavras: 356
Caracteres: 2265
--------------------------------------------------
One-Dimensional Convolution
One-dimensional convolution with fﬁlters also allows reducing the number of
channels, as shown in Figure 5.19 5.4 Example
Combining the above components in sequential layers of convolution and non-
linear functions followed by pooling results in a very simple CNN architecture, as
shown in Figure 5.20 In this example, the input is a 28 ×28 grayscale image, and
the output is 1 of 10 classes, such as the digits 0–9 The ﬁrst convolutional layer
consistsof 32ﬁlters, such as5 ×5ﬁlters applied totheimage with padding, which
yields a 28 ×28times32 volume Next, a non-linear function, such as the ReLU,
is applied pointwise to each element in the volume The ﬁrst convolution layer of
the network shown in Figure 5.20 is followed by a 2 ×2 max pooling operation
that reduces dimensionality to half in each spatial dimension, to 14 ×14×32 The
second convolutional layer increases dimensionality to 14 ×14×64 by applying
a second set of 64 ﬁlters, such as 3 ×3 ﬁlters, and the second pooling layer
reduces dimensionality, again to half in each dimension, to 7 ×7×64 The
80 5 Convolutional Neural Networks
Figure 5.11 Repeated convolutions with a 3 ×3 kernel are equivalent to a single
convolution with a 7 ×7 kernel, and more eﬃcient resulting volume is then ﬂattened to form a 3 ,136 dimensional vector which is
fed into two fully connected layers The ﬁrst fully connected layer consists of
1,024 activations, followed by a second fully connected layer with 10 activations,
one for each output class 5.5 Architectures
Convolutional Neural Network
We composed a CNN using ﬁlters and convolutional and pooling layers The
simple convolutional neural network example described above consists of a few
convolutional layers The Neocognitron (Fukushima, 1988) introduced CNNs 5.5 Architectures 81
Figure 5.12 Two-dimensional convolution with three channels A deeper network of eight layers may resemble the cortical visual pathways
in the brain (Cichy et al., 2016) Early implementations of CNN architectures
were handcrafted for speciﬁc image classiﬁcation tasks These include LeNet
(LeCun et al., 2010), AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan
and Zisserman, 2014), GoogLeNet (Szegedy et al., 2015) and Inception (Szegedy
et al., 2016)

============================================================

=== CHUNK 059 ===
Palavras: 368
Caracteres: 2449
--------------------------------------------------
Figure 5.21 schematically illustrates a CNN with multiple layers
and the connections between volumes of activations ResNet
The deep residual neural network (ResNet) architecture (He et al., 2016 a,b),
introduced skip connections between consecutive layers, as shown in Figure 5.22 Skip connections allow training deeper neural networks by avoiding vanishing
gradients The ResNet architecture enables training very deep neural networks
82 5 Convolutional Neural Networks
Figure 5.13 Three-dimensional convolution with three channels with hundreds of layers The original ResNet with skip connections between
layers is shown in Figure 5.23 In a neural network, the activations are the layer
outputs:
al+1=f(Wl,al) (5.13)
wherefis the non-linear function, Wlthe weights of layer l,a n dal, the input
activations to layer l In a ResNet, due to the skip connections, the activations
are the sum of the previous activations and the layer outputs:
al+1=al+f(Wl,al) (5.14)
Adding a new layer to a neural network with a skip connection does not reduce
its representation power Adding a residual layer results in the network repre-
senting all the functions that the network was able to represent before adding the
5.5 Architectures 83
Figure 5.14 Three-dimensional convolution layer plus additional functions, thus increasing the space of functions A neural
network is a composition of functions fwith a linear part and a non-linear part For a three-layer network, the composition is:
F(x)=f(f(f(x))) (5.15)
A ResNet is a composition of functions where each function is the sum f(x)+
x, and each layer represents a residual f(x)−x For a three-layer ResNet the
function is:
F(x)=f(f(f(x)+x)+(f(x)+x))+f(f(x)+x)+(f(x)+x) (5.16)
DenseNet
A DenseNet (Huang, Liu, van der Maaten and Weinberger, 2017) layer concate-
nates the input xand output f(x) of each layer to form the next layer [ f(x),x] Since a neural network is a composition of functions this results in a dense con-
nection between each layer and its previous layers For a three-layer network the
composition is:
F(x)=f([f([f(x),x]),[f(x),x]]),f([f(x),x]),[f(x),x] (5.17)
Figure 5.24 shows a block diagram of a DenseNet 84 5 Convolutional Neural Networks
Figure 5.15 Convolution layer with one ﬁlter Figure 5.16 Convolution layer with four ﬁlters SENet
Squeeze and excitation networks, or SENet (Hu et al., 2018), take into account
the relationships between channels by weighting the channels of layers

============================================================

=== CHUNK 060 ===
Palavras: 351
Caracteres: 2416
--------------------------------------------------
5.5 Architectures 85
Figure 5.17 Convolution layer with fﬁlters Figure 5.18 Max pooling MobileNets
MobileNets(Howardetal.,2017)areCNNswithalightweightbackboneandhigh
performancegearedtowardmobilephones.MobileNetimprovementsincludesep-
arable convolution ﬁlters, bottleneck blocks, and inverted residual blocks Shuf-
ﬂeNets (Zhang, Zhou, Lin and Sun, 2018) optimize the CNN architecture for
computation and memory access and allow for parallel computation (Ma et al.,
2018) ODENet
ODENet (Chen, Rubanova, Bettencourt and Duvenaud, 2018) introduce a con-
tinuous formulation for CNNs equivalent to any number of layers 86 5 Convolutional Neural Networks
Figure 5.19 Convolution layer with one-dimensional ﬁlters Invertible Networks
Invertible residual neural networks (Behrmann et al., 2019) use the same net-
work for both analysis and synthesis An invertible ResNet is used for both
classiﬁcation and generation in the inverse direction Space–Time CNNs
Convolutional neural networks may be extended from input images to video by
processing multiple video frames simultaneously, as shown in Figure 5.25 5.6 Applications
Convolutional neural networks have broad applications in computer vision and
beyond Such a wide range of applications means that any architectural advance
impacts multiple application domains The applications in computer vision in-
clude classiﬁcation, recognition, localization, counting, object detection, segmen-
tation, image completion, and pose estimation Beyond computer vision, CNNs
are used to eﬃciently represent data that may be represented as an array or
volume For example, in game playing, AlphaZero (Silver et al., 2017) uses a ResNet
as the representation of the board, pieces, and their possible moves A policy
π(a|s), which is the probability of any action ataking place given any board
statesis represented by an 8 ×8×73 volume, as shown in Figure 5.26 There
5.6 Applications 87
Figure 5.20 Convolutional neural network Input is a 28 ×28 grayscale image, and the
output is 1 of 10 classes, such as the digits 0–9 The ﬁrst convolutional layer consists
of 32 ﬁlters, such as 5 ×5 ﬁlters applied to the image with padding, which yields a
28×28times32 volume Next, a non-linear function, such as the ReLU, is applied
pointwise to each element in the volume This is followed by a 2 ×2 max pooling
operation, which reduces dimensionality to half in each spatial dimension, to
14×14×32

============================================================

=== CHUNK 061 ===
Palavras: 351
Caracteres: 2226
--------------------------------------------------
The second convolutional layer increases dimensionality to 14 ×14×64
by applying a second set of 64 ﬁlters, such as 3 ×3 ﬁlters, and the second pooling layer
reduces dimensionality, again to half in each dimension, to 7 ×7×64 The resulting
volume is then ﬂattened to form a 3 ,136 dimensional vector which is fed into two fully
connected layers The ﬁrst fully connected layer consists of 1 ,024 activations, followed
by a second fully connected layer with 10 activations, one for each output class are 8×8 board positions from which to pick up a piece Figure 5.27 shows that
in chess, there are 56 queen moves in 8 possible directions times seven maximum
steps, eight knight moves marked by blue squares, and nine under promotions,
for a total of 73 possibilities 88 5 Convolutional Neural Networks
Figure 5.21 Convolutional neural network activations Figure 5.22 Residual neural network (ResNet) activations with skip connections
between layers Figure 5.23 Residual neural network (ResNet) 5.7 Summary 89
Figure 5.24 Dense neural network (DenseNet) Figure 5.25 Space–time CNN 5.7 Summary
In summary, CNNs are a type of neural network designed to recognize patterns
in images The network comprises a series of layers, with each layer performing a
speciﬁc function The ﬁrst layer is typically a convolutional layer, which performs
90 5 Convolutional Neural Networks
Figure 5.26 AlphaZero board and move representation Figure 5.27 AlphaZero chess move representation a convolution operation on the input image The convolution operation is a
mathematical operation that extracts information from the input image The
output of the convolutional layer is then passed to a pooling layer, which reduces
the number of neurons in the network Multiple convolutions and pooling layers
are followed by a series of fully connected layers responsible for the classiﬁcation
or other applications performed on the image Convolutional neural networks
perform well in practice across a broad range of applications since they share
weights at multiple scales across space 6 Sequence Models
6.1 Introduction
Time series may be used for representing any temporal sequence, such as a sen-
tence of words, video of image frames, or audio spectrogram

============================================================

=== CHUNK 062 ===
Palavras: 374
Caracteres: 2403
--------------------------------------------------
There are many
questions we can answer about sequences using deep learning Applications us-
ing sequence models include machine translation, protein structure prediction,
DNA sequence analysis, speech recognition, music synthesis, image captioning,
sentiment classiﬁcation, video action recognition, handwriting recognition, self-
driving cars, and many other applications involving time series In a similar
fashion that representations for images share weights across space, many deep
learning representations for sequences share weights across time 6.2 Natural Language Models
Language models are among the most common sequence models and therefore
we begin with their description Representing language requires a natural lan-
guage model which may be a probability distribution over strings We begin the
presentation of language models, starting from the simplest model and increas-
ing model complexity to reach recurrent neural networks (RNNs), which model
long-term dependencies 6.2.1 Bag of Words
Perhaps the simplest model of language is a multi-set, also known as a bag of
words In a bag of words we count how prevalent each term xis in a single docu-
mentd, which is the term frequency TF(x,d) Words are commonly normalized
to lowercase and stemmed by removing their suﬃxes; common stopwords (such
as a, an, the, etc.) are removed The inverse document frequency may be used
to boost terms that are rare in an entire corpus of documents The inverse docu-
ment frequency of a word appearing in a document and of a word not appearing
in a document measure together the entropy of the word:
IDF(x)=1+l o g/parenleftbiggtotal number of documents
number of documents containing x/parenrightbigg
(6.1)
92 6 Sequence Models
The product of the term frequency and the inverse document frequency may
be used to form a vector TFIDF(x,d)=TF(x,d)×IDF(x) and used for
measuring similarity between a query and a document TFIDFcan be used as
weighting in search and data mining A bag of words representation does not
preserve order information For example, representing the sentence “Alice sent
a message to Bob” as a bag of words does not distinguish between the sender
and receiver of the message, and has an equivalent representation as the sentence
“Bob sent a message to Alice.”
6.2.2 Feature Vector
In contrast to a bag of words, using a feature vector to represent a sentence
preserves order information

============================================================

=== CHUNK 063 ===
Palavras: 362
Caracteres: 2266
--------------------------------------------------
A limitation of a feature vector representation is
that it requires learning each word order separately, even if two sentences are
equivalent, such as the sentences “Alice sent a message on Sunday” and “On
Sunday Alice sent a message,” which may be ineﬃcient 6.2.3 N-grams
A sequence of nadjacent words is called an n-gram A bag of words is the
1-gram or unigram model in which p(x1,...,x n)≈/producttextp(xi) A Markov model
is a 2-gram or bi-gram model in which p(xn|x1,...,x n−1)≈p(xn|xn−1) The
probability of a word given the previous word may be computed by counting
p(xn|xn−1)=count(xn−1xn)
count(xn−1) Usually 3-,4-,5- or k-gram models are computed
p(xn|x1,...,x n−1)≈p(xn|xn−1,...,x n−k+1) and stored, given a large corpus 6.2.4 Markov Model
The special case of a bi-gram is a Markov model given by p(xn|xn−1,...,x 1)≈
p(xn|xn−1) and does not model long-term dependencies For example, in the sen-
tence “Alice and Bob communicate Alice sent Bob a message,” the probability
of the last word “message,” given a Markov model, depends only on the previous
word “a,” which does not provide much information, rather than taking into
account the relevant preﬁx that “Alice and Bob communicate.” A limitation of
a Markov model is that it does not model long-term dependencies 6.2.5 State Machine
A state machine is deﬁned by a set of possible states S, a set of possible inputs
X, a transition function f:S×X/mapsto →S that maps from state and input to state,
a set of possible outputs Y, a mapping g:S /mapsto → Yfrom states to outputs, and
an initial state s0 An example of a state machine is shown in Figure 6.1 6.3 Recurrent Neural Network 93
Figure 6.1 State machine example Possible states are S={standing ,moving},as e to f
possible inputs are X={slow,fast}, transition function f:S×X/mapsto →S denoted by
arrows, mapping gthat in this example is the identity, and an initial state
s0=standing Starting from the initial state s0we iteratively compute:
st=f(st−1,xt)
yt=g(st)(6.2)
for time steps t≥1 For a sequence of inputs xtthe outputs ytare of the form:
yt=g(f(...(f(f(s0,x1),x2),...),xt)) (6.3)
Recurrent neural networks are state machines with speciﬁc deﬁnitions of tran-
sition function fand mapping g, in which the states, inputs and outputs are
vectors

============================================================

=== CHUNK 064 ===
Palavras: 379
Caracteres: 2502
--------------------------------------------------
6.2.6 Recurrent Neural Network
Recurrent neural networks both maintain word order and model long-term de-
pendencies by sharing parameters across time They allow for the example inputs
and the label outputs to be of diﬀerent lengths Bidirectional RNNs model both
forward and backward sequence dependencies, and deep RNNs use multiple hid-
den layers The limitation of plain RNNs is that they are diﬃcult to train since
the error signals ﬂowing back in time explode or vanish Therefore, the hidden
units are replaced by simple gates that are easily trained 6.3 Recurrent Neural Network
An RNN processes input sequences x1,...,x tvia hidden units h0,h1,...,h tto
form outputs y1,...,y tby sharing parameter matrices U,W,Vacross time:
ht=f(ht−1,xt)=g(Wht−1+Uxt)
yt=Vht(6.4)
as shown in Figure 6.2, where the matrices U,W,Vare shared across all time
stepst Each component of the sequence, xt,ht,ytis a vector The matrices Uare
appliedtotheinputunits xtandthetransformedinput Uxservesasinputtothe
94 6 Sequence Models
Figure 6.2 Recurrent neural network (RNN): forward propagation, sharing weights
across time The matrices Ware applied to the recurrent hidden units ht−1
and the transformed hidden unit Wht−1serves as an input to the next hidden
unitht The matrices Vare applied to the hidden units htand the transformed
hidden unit Vhtserves as an input to the predicted output yt The function
gis a non-linear pointwise operation such as the tanh activation function The
matrices WandUmay be concatenated to form a matrix [ W;U] and the hidden
unitsht−1and input units xtmay be concatenated to form a single column
vector [ht−1;xt]T, resulting in a single matrix–vector multiplication followed by
the non-linear function gfor updating the hidden state ht 6.3.1 Architectures
Recurrent neural networks map input sequences to output sequences of varying
lengths This mapping may be a one-to-many, many-to-one, or a many-to-many
mapping A one-to-many mapping from x1toy1,...,y tis shown in Figure 6.3 An example of such a mapping applied to image captioning is receiving the
representation of an image by a convolutional neural network (CNN) as an input
vectorx1for generating a sequence of words as output y1,...,y t, describing the
image A many-to-one mapping from x1,...,x ttoytis shown in Figure 6.4 An ex-
ample of such a mapping applied to sentiment classiﬁcation is taking a sequence
of word representations x1,...,x tas input for computing a number ytwhich
denotes the sentiment of the input sentence

============================================================

=== CHUNK 065 ===
Palavras: 357
Caracteres: 2185
--------------------------------------------------
This can be applied to a book or to
restaurant reviews, wheretheinput isthereviewin words, each word represented
by an embedding as described in Section 6.8, and the output is the number of
stars in the rating A many-to-many mapping from x1,...,x ttoy1,...,y tis shown in Figure 6.5 An example of such a mapping applied to video action classiﬁcation maps a
sequence, which is the representations of video frames given by a CNN, to a
sequence of classes, which is used for classifying the action in the video A many-
to-many model can also be used for named entity recognition, for example by
6.3 Recurrent Neural Network 95
Figure 6.3 Recurrent neural network with a one-to-many mapping, which may be used
in image captioning Figure 6.4 Recurrent neural network with a many-to-one mapping, which may be used
in sentiment classiﬁcation denoting the output corresponding to named entities as 1 and 0 otherwise, and
learning the many-to-many mapping given multiple labeled sentences A many-to-many mapping can also be used in an encoder–decoder architec-
ture, as shown in Figure 6.6 and described in Section 6.6 Mapping between
sequences of words may be used in machine translation, in which the input se-
quence that is a sentence in one language is encoded and then decoded to an
output sentence in another language 6.3.2 Loss Function
To complete our deﬁnition of the RNN architecture requires incorporating a loss
function, so that we can train our models, as shown in Figure 6.7 The loss is a function of the predicted outputs ot, and ground-truth labels
yt, as shown in Figure 6.8, and may be the cross-entropy loss Using a softmax
activation function we deﬁne the total loss between the softmax of the predicted
values ˆyt= softmax( ot), and ground-truth labels ytas the sum of losses in indi-
vidual time steps L=/summationtext
tLt Speciﬁcally, we deﬁned the hidden units, outputs,
96 6 Sequence Models
Figure 6.5 Recurrent neural network with a many-to-many mapping, which may be
used in action recognition and named entity recognition Figure 6.6 Recurrent neural network with a many-to-many mapping using an
encoder–decoder architecture which may be used in machine translation

============================================================

=== CHUNK 066 ===
Palavras: 380
Caracteres: 2539
--------------------------------------------------
predictions, and ground-truth value for each time step tby:
ht=g(Wht−1+Uxt)
ot=Vht
ˆyt= softmax( ot)
yt= ground-truth label(6.5)
Given input sequences xiand predicted sequences ˆ yifori=1,...,m, each
pair of inputs and predicted outputs of length li, and ground-truth outputs yi,
deﬁne a loss of a single sequence as the sum of element losses where each element
may be a character or word:
Lsequence(ˆyi,yi)=li/summationdisplay
t=1Lelement(ˆyi
t,yi
t) (6.6)
and the total loss over all sequences as the sum of sequence losses:
Ltotal(ˆy,y)=m/summationdisplay
i=1Lsequences(ˆyi,yi) (6.7)
6.3 Recurrent Neural Network 97
Figure 6.7 Recurrent neural network total loss Figure 6.8 Recurrent neural network individual losses where ˆyi=Fθ(xi) is the prediction of the RNN for input xiwith network
parameters θ 6.3.3 Deep RNN
Stacking multiple hidden layers and connecting them:
hl
t=g(Wlhl
t−1+Ulhl−1
t) (6.8)
for layers l=1,...,L, results in a deep RNN, as shown in Figure 6.9 Matrices
Wl,Ulfor each layer lare shared across all time steps t 98 6 Sequence Models
Figure 6.9 Deep recurrent neural network 6.3.4 Bidirectional RNN
Often the output depends both on past and future values of the sequence For
example, in speech and handwriting recognition a word or character may de-
pend both on previous and following words or characters We therefore deﬁne a
bidirectional RNN by:
ht=g(Wht−1+Uxt)
¯ht=g(¯W¯ht+1+Uxt)
ot=V[ht;¯ht]T(6.9)
where the hidden units htmove forward in time using the shared matrix W,a n d
the hidden units ¯htmove backward in time from using the shared matrix ¯W,a s
shown in Figure 6.10 The inputs xtare fed into both the forward hidden units
htand backward hidden units ¯ht, and in turn both the forward and backward
hidden units are fed into the output ot Stacking multiple bidirectional hidden layers results in a deep bidirectional
RNN, as shown in Figure 6.11 6.3 Recurrent Neural Network 99
Figure 6.10 Bidirectional recurrent neural network 6.3.5 Backpropagation Through Time
Having deﬁned the RNN architectures and loss function, our goal is to train
the RNN Neural networks are trained using the backpropagation algorithm,
and RNNs are trained using backpropagation through time Given a non-linear
activation function gsuch as the tanh activation function, and softmax loss, the
loss for element or time step jmay be deﬁned as Lelement(yj,ˆyj)=−yjlogˆyj
and the total loss of the sequence:
Lsequence(y,ˆy)=l/summationdisplay
j=1Lelement(yj,ˆyj)=−l/summationdisplay
j=1yjlogˆyj (6.10)
as illustrated in Figure 6.12

============================================================

=== CHUNK 067 ===
Palavras: 354
Caracteres: 2484
--------------------------------------------------
Expressing the gradient of a sequence loss Lsequence(ˆy,y)w i t hr e s p e c tt oa l l
RNN weights θby the sum of gradients of element losses and then using the
chain rule results in:
dLsequence(ˆy,y)
dθ=l/summationdisplay
j=1dLelement(ˆyj,yj)
dθ=l/summationdisplay
j=1l/summationdisplay
t=1∂Lelement(ˆyj,yj)
∂ht∂ht
∂θ(6.11)
Taking the derivatives of the loss for time twith respect to the matrix Vonly
depends on time t For example:
∂L3
∂V=∂L3
∂ˆy3∂ˆy3
∂V=∂L3
∂ˆy3∂ˆy3
∂z3∂z3
∂V(6.12)
wherez3=Vh3 However, the derivative for time step twith respect to Walso
100 6 Sequence Models
Figure 6.11 Deep bidirectional recurrent neural network Figure 6.12 Recurrent neural network losses depends on the previous time step t−1, which in turn depends on t−2; for
example:
∂L3
∂W=∂L3
∂ˆy3∂ˆy3
∂h3∂h3
∂W=3/summationdisplay
i=1∂L3
∂ˆy3∂ˆy3
∂h3∂h3
∂hi∂hi
∂W(6.13)
6.3 Recurrent Neural Network 101
Figure 6.13 Recurrent neural network: backpropagation through time whereh3= tanh(Wh2+Ux3), as shown in Figure 6.13 Since:
∂h3
∂h1=∂h3
∂h2∂h2
∂h1(6.14)
we get:
∂L3
∂W=3/summationdisplay
i=1∂L3
∂ˆy3∂ˆy3
∂h3⎛
⎝3/productdisplay
j=i+1∂hj
∂hj−1⎞
⎠∂hi
∂W(6.15)
wheretheproductinparenthesesinEquation6.15equals/producttextWTdiag(tanh/prime(ht−1)) Thus, backpropagation through time involves raising the matrix WTto a high
power Therefore, if the eigenvalues are less than 1 the corresponding terms will
vanish, whereas if they are greater than 1 they will explode While exploding
gradients may be handled by clipping, vanishing gradients make plain RNNs
diﬃcult to train Backpropagation through time is described in pseudocode in Algorithm 6.1 Notice that the last line of the backpropagation loop involves a matrix multi-
plication with WT, which means that performing the loop results in taking the
kth power of the matrix, which is equivalent to the analytic derivation Thus,
backpropagation in time will result in the eigenvalue of the matrix being either
less than 1, in which case the gradient will vanish, or more than 1, in which case
the gradients will explode Algorithm 6.1 RNN backward propagation through time fort=k,...,1do:
dot=e/prime(ot)dL(zt;yt)
dzt
dV=dV+dothT
t
dht=dht+VTdot
dzt=g/prime(zt)dht
dU=dU+dztxT
t
dW=dW+dzthT
t−1
dht−1=WTdzt
102 6 Sequence Models
Figure 6.14 Recurrent neural network: hidden unit Figure 6.15 Gated recurrent unit inputs and output In summary, RNNs allow us to process variable-length sequences and model
long-term dependencies by sharing parameters across time

============================================================

=== CHUNK 068 ===
Palavras: 371
Caracteres: 2226
--------------------------------------------------
Each hidden state
depends on the corresponding input and current state, as shown in Figure 6.14 An input may alter the network at a later time step The main limitation of
plain RNNs is that training is diﬃcult and results in vanishing and exploding
gradients The solution is to model the hidden units using gates, which are easy
to train, such as the long short-term memory (LSTM) (Hochreiter and Schmid-
huber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014), as described
next 6.4 Gated Recurrent Unit
The GRU (Cho et al., 2014) is simple and easy to train It has the same inputs
and outputs as the RNN At each time step tthe GRU receives as input the
current state xtand the hidden state ht−1of the previous time step, and outputs
the hidden state htat timet, as shown in Figure 6.15 Gated recurrent units are placed one after the other, sequentially, where the
hiddenstateoutput htofoneunitservesasthehiddenstateinputofthefollowing
unit, as shown in Figure 6.16 In a similar fashion to the RNN, the weights from
the current states and from the hidden states are shared across time The GRU consists of an update gate zt, a reset gate rtand a candidate ˜ht,a s
shown in Figure 6.17 6.4 Gated Recurrent Unit 103
Figure 6.16 Gated recurrent units used sequentially with the outputs of one unit
serving as the input to the next Figure 6.17 Gated recurrent unit The GRU gates are deﬁned by:
zt=σ(Wzht−1+Uzxt) update gate
rt=σ(Wrht−1+Urxt) reset gate
˜ht=φ(W(rt⊙ht−1)+Uxt) candidate
ht=zt⊙ht−1+(1−zt)⊙˜htoutput(6.16)
As an analogy, consider the input xtto be the weather today, the hidden unit
ht−1to be the clothes we wore yesterday, ˜htto be the candidate clothes we
prepared to wear today and htto be the actual clothes we wear today Usually,
we wear clothes based on the weather, based on what we wore yesterday, and
based on our mood or what we prepared to wear today The update and reset
gates determine to what extent we take into account these factors: Do we ignore
104 6 Sequence Models
Figure 6.18 Gated recurrent unit update gate used for interpolation The current
hidden state htis a linear interpolation of the previous hidden state ht−1and the new
candidate ˜htbased on the value of zt,ht=zt⊙ht−1+(1−zt)⊙˜ht

============================================================

=== CHUNK 069 ===
Palavras: 360
Caracteres: 2250
--------------------------------------------------
Figure 6.19 Gated recurrent unit update gate zt=σ(Wzht−1+Uzxt) the weather xtcompletely Do we forget what we wore yesterday ht−1 And do
we take into account our candidate clothes we prepared ˜ht, and to what extent 6.4.1 Update Gate
The activation htat timetis a linear interpolation between the previous acti-
vationht−1and the current candidate ˜ht, which is controlled by an update gate
zt:
ht=zt⊙ht−1+(1−zt)⊙˜ht (6.17)
as shown in Figure 6.18 The update gate ztis a non-linear function, a sigmoid, of a combination of the
current state xtand the previous hidden state ht−1:
zt=σ(Wzht−1+Uzxt) (6.18)
as shown in Figure 6.19 If the update gate is set to zt= 0, then the output is the new candidate, as
shown in Figure 6.20 6.4 Gated Recurrent Unit 105
Figure 6.20 Gated recurrent unit with update gate set to zt= 0 The current hidden
statehtis the new candidate ˜ht,ht=˜ht=φ(W(rt⊙ht−1)+Uxt) If the update gate is set to zt= 1, then the output is the previous hidden
state, ignoring both the candidate and current state altogether, as shown in
Figure 6.21 6.4.2 Candidate Activation
Thecandidateactivation ˜htisanon-linearfunction,a φ= tanh,ofacombination
of the current state xtand the previous hidden state ht−1modulated by the reset
gatert:
˜ht=φ(W(rt⊙ht−1)+Uxt) (6.19)
as shown in Figure 6.22 6.4.3 Reset Gate
The reset gate rtis a non-linear function, a sigmoid, of a combination of the
current state xtand the previous hidden state ht−1:
rt=σ(Wrht−1+Urxt) (6.20)
as shown in Figure 6.23 106 6 Sequence Models
Figure 6.21 Gated recurrent unit with update gate set to zt= 1 The current hidden
statehtequals the previous hidden state ht−1,ht=ht−1, ignoring both the current
statextand the new candidate ˜ht Figure 6.22 Gated recurrent unit candidate activation is a non-linear function of a
combination of the current state xtand previous hidden state ht−1modulated by the
reset gate rt,˜ht=φ(W(rt⊙ht−1)+Uxt) If the reset gate is set to rt= 0 then the candidate ˜htis a function of the
current state xtsuch that ˜ht=φ(Uxt), forgetting the previous hidden state
ht−1, as shown in Figure 6.24 If the reset gate is set to rt= 1 then the candidate is a function of the previous
hidden state ht−1, ignoring the current state x, as shown in Figure 6.25

============================================================

=== CHUNK 070 ===
Palavras: 363
Caracteres: 2070
--------------------------------------------------
6.4 Gated Recurrent Unit 107
Figure 6.23 Gated recurrent unit reset gate rt=σ(Wrht−1+Urxt) Figure 6.24 Gated recurrent unit with reset gate set to rt= 0 The candidate is a
function of the current state ˜ht=φ(Uxt) 6.4.4 Function
Ifzt= 0 and rt= 0 then the output hidden state is only dependent on the
current state φ(Uxt), forgetting the previous hidden state ht−1,a ss h o w ni n
Figure 6.26 Ifzt= 0 and rt= 1, as shown in Figure 6.27, then the GRU is reduced to an
RNN, as shown in Figure 6.28 The GRU avoids the RNN problem of vanishing or exploding gradients by an
addition before the output, as highlighted in Figure 6.29, which interrupts the
repeated multiplication during backpropagation through time 108 6 Sequence Models
Figure 6.25 Gated recurrent unit with reset gate set to 1, rt= 1 The candidate ˜htis a
function of the previous hidden state ht−1ignoring the current state xtsuch that
˜ht=φ(Wht−1+Uxt) 6.5 Long Short-Term Memory
The long short-term memory (Hochreiter and Schmidhuber, 1997) was intro-
duced two decades before the GRU (Cho et al., 2014) The LSTM is easy to
train, and includes an additional input and output compared with the RNN and
GRU At each time step tthe LSTM receives as input the current state xt, the
hidden state ht−1, and memory cell ct−1of the previous time step, and outputs
the hidden state htand memory cell ctat timet, as shown in Figure 6.30 The
memory cells propagate information from the previous state to the next, whereas
the hidden states determine the way in which that information is propagated The LSTM units are placed one after the other, sequentially, where the hidden
statehtand memory cell ctoutputs of one unit serve as the hidden state and
memory cell inputs of the following unit, as shown in Figure 6.31 In a similar
fashion to the RNN and GRU, the weights from the current states and from the
hidden states, as well as the weights from the memory cells, are shared across
time The LSTM consists of a forget gate ft, an input gate it, an output gate ot,
and a candidate memory ˜ ct, as shown in Figure 6.32

============================================================

=== CHUNK 071 ===
Palavras: 384
Caracteres: 2377
--------------------------------------------------
6.5 Long Short-Term Memory 109
Figure 6.26 Gated recurrent unit with update gate set to zt= 0, and reset gate set to
rt= 0 The output hidden state htdoes not take into account the previous hidden
stateht−1, such that ht=˜ht=φ(Uxt) The LSTM gates are deﬁned by:
ft=σ(Wfht−1+Ufxt) forget gate
it=σ(Wiht−1+Uixt) input gate
ot=σ(Woht−1+Uoxt) output gate
˜ct=φ(Wht−1+Uxt) candidate memory
ct=ftct−1+it˜ct memory cell
ht=otφ(ct) output gated memory(6.21)
6.5.1 Forget Gate
The forget gate ftis a non-linear function, a sigmoid, of a combination of the
current state xtand the previous hidden state ht−1:
ft=σ(Wfht−1+Ufxt) (6.22)
as shown in Figure 6.33 Ifft= 0 then the previous memory cell ct−1is ignored, as shown in Figure
6.34 110 6 Sequence Models
Figure 6.27 Gated recurrent unit with update gate set to zt= 0, and reset gate set to
rt= 1 The GRU is reduced to an RNN such that ht=˜ht=φ(Wht−1+Uxt) Figure 6.28 Gated recurrent unit reduced to an RNN when the update gate is set to
zt= 0 and the reset gate is set to rt= 1, such that ht=φ(Wht−1+Uxt) 6.5.2 Input Gate
The input gate itis a non-linear function, a sigmoid, of a combination of the
current state xtand the previous hidden state ht−1:
it=σ(Wiht−1+Uixt) (6.23)
shown in Figure 6.35 Ifit= 0 then the new candidate memory ˜ ctis ignored, as shown in Figure
6.36 6.5 Long Short-Term Memory 111
Figure 6.29 Gated recurrent unit addition before output, as highlighted, avoids the
vanishing or exploding gradient problem during training, interrupting the repeated
multiplication during backpropagation through time Figure 6.30 The LSTM inputs and outputs Figure 6.31 The LSTM units used sequentially, with the outputs of one unit serving as
inputs to the next 112 6 Sequence Models
Figure 6.32 Long short-term memory Figure 6.33 The LSTM forget gate ft=σ(Wfht−1+Ufxt) 6.5.3 Memory Cell
The memory ctis updated by partially forgetting the previous memory ct−1and
adding the new candidate memory ˜ ct:
ct=ftct−1+ii˜ct (6.24)
as shown in Figure 6.37 6.5 Long Short-Term Memory 113
Figure 6.34 The LSTM forget gate set to ft= 0, ignoring previous memory cell ct−1 Figure 6.35 The LSTM input gate it=σ(Wiht−1+Uixt) 6.5.4 Candidate Memory
The new candidate memory ˜ ctis a non-linear function, a sigmoid, of a combina-
tion of the current state xtand the previous hidden state ht−1:
˜ct=σ(Wht−1+Uxt) (6.25)
as shown in Figure 6.38

============================================================

=== CHUNK 072 ===
Palavras: 374
Caracteres: 2444
--------------------------------------------------
6.5.5 Output Gate
The output gate otis a non-linear function, a sigmoid, of a combination of the
current state xtand the previous hidden state ht−1:
ot=σ(Woht−+U x) (6.26)
114 6 Sequence Models
Figure 6.36 The LSTM input gate set to it= 0, ignoring new candidate memory ˜ ct Figure 6.37 The LSTM interpolation of previous memory ct−1and new candidate
memory ˜ ctby forget gate ftand input gate it, such that ct=ftct−1+ii˜ct as shown in Figure 6.39 The hidden state at time tis a pointwise multiplication
of the output otand a non-linear function, a tanh of new memory cell ct:
ht=otφ(ct) (6.27)
6.5 Long Short-Term Memory 115
Figure 6.38 The LSTM candidate memory ˜ ctis a function of the previous hidden state
ht−1and the current state xt, such that ˜ ct=σ(Wht−1+Uxt) Figure 6.39 The LSTM output gate ˜ ct=σ(Wht−1+Uxt) 6.5.6 Peephole Connections
Notice that the LSTM unit performs a read after write, as shown in Figure 6.40 This is avoided by adding peephole connections from the previous memory cell
ct−1orct:
ft=σ(Wfht−1+Pfct−1+Ufxt) forget gate
it=σ(Wiht−1+Pict−1+Uixt) input gate
ot=σ(Woht−1+Poct+Uoxt) output gate(6.28)
as shown in Figure 6.41 LSTM
Comparingtheadditionbeforethenewhiddenstate htintheGRUashighlighted
in Figure 6.29 with the addition before the new memory cell ctin the LSTM as
highlighted in Figure 6.42, both units avoid repeated multiplications that cause
vanishing or exploding gradients by a similarly positioned addition Comparing the interpolation of the new candidate in the GRU as highlighted
in Figure 6.43 with the interpolation of the new memory cell in the LSTM as
highlighted in Figure 6.44 shows that the update gate zcontrols the amount
116 6 Sequence Models
Figure 6.40 The LSTM read after writing Figure 6.41 The LSTM with peepholes 6.6 Sequence to Sequence 117
Figure 6.42 The LSTM addition, as highlighted, avoiding repeated multiplication,
which causes vanishing or exploding gradients of the new candidate to pass in the GRU, whereas the input gate controls the
amount of the new candidate memory to pass in the LSTM Interpolation in the
GRU is controlled by a single parameter zt, whereas in the LSTM interpolation
is controlled by two separate parameters itandft Comparing the GRU reset gate controlling the candidate hidden state, as
highlighted in Figure 6.45, with the LSTM input gate controlling the candidate
memorycell,ashighlightedinFigure6.46,showsthemodulationofthecandidate
in both units

============================================================

=== CHUNK 073 ===
Palavras: 359
Caracteres: 2458
--------------------------------------------------
Finally,anempiricalevaluationof10,000architectures(Jozefowiczetal.,2015)
for these units demonstrates that the best results are obtained by similar GRU
variants In summary, perhaps the exact gate conﬁguration is less important
than its overall structure and function 6.6 Sequence to Sequence
Building a language model involves a distribution over words in the language In applications such as machine translation, question answering, story synthesis,
and protein structure prediction, it is important to generate entire sequences In
these applications the long-term dependencies between words in the vocabulary,
sentences and even paragraphs are important Sequence-to-sequence (seq2seq)
models (Sutskever et al., 2014) consider entire sequences, or sentences, as inputs
and outputs Seq2seq models consist of an encoder and a decoder The encoder is
118 6 Sequence Models
Figure 6.43 Gated recurrent unit candidate interpolation by zt⊙˜ht a GRU or LSTM, which may be bidirectional and deep, which encodes the input
sequence ( x1,...,x s) into a context vector as output z= encoder( x1,...,x s) The decoder is also a GRU or LSTM that receives the context vector zas input,
as the ﬁrst hidden state vector, and generates an output sequence ( y1,...,y t)=
decoder( z) The encoder and decoder models are trained end-to-end such that:
(y1,...,y t) = decoder(encoder( x1,...,x s)) (6.29)
6.7 Attention
When humans translate or write sentences and paragraphs we do not store a
representationoftheentiresentenceorparagraphbeforewebeginitstranslation When writing sentences and paragraphs we edit diﬀerent parts of the sentence,
going back to other parts These processes are not only sequential and back-
to-back as in the encoder–decoder architecture described earlier A simple form
of attention is applied to regression Given samples xi,yiof a function, we can
estimate the value y=f(x) by weighting each yias a function of α(x,xi), which
decreases the farther apart xandxiare For example, we can estimate:
y=f(x)=/summationdisplay
iα(x,xi)yi (6.30)
6.7 Attention 119
Figure 6.44 The LSTM candidate memory interpolation by it⊙˜ct Figure 6.45 Gated recurrent unit reset rtand candidate hidden state ˜ht where in attention xis called the query, xiis called a key, and yiis a value A
speciﬁc choice is using kernel kas a local weighting function:
α(x,xi)=k(x,xi)/summationtext
jk(x,x)yi (6.31)
120 6 Sequence Models
Figure 6.46 The LSTM input itand candidate memory ˜ ct

============================================================

=== CHUNK 074 ===
Palavras: 354
Caracteres: 2347
--------------------------------------------------
also known as the Nadaraya–Watson estimator (Nadaraya, 1964; Watson, 1964) Morerecently,seq2seqmodelshavebeenimprovedbymodelsofattention.Seq2seq
models incorporating attention have diﬀerent parts of the output sequence pay
attention to diﬀerent parts of the input sequence (Bahdanau et al., 2015; Luong
et al., 2015) Instead of having the decoder receive as input the encoder’s out-
put in a sequential process, the decoder takes into account the entire encoding
sequence, as shown in Figure 6.47 The input to the decoder hidden units are
context vectors cifor each time step icomputed as:
ci=/summationdisplay
jαi,j[hj;¯hj]T(6.32)
for a bidirectional LSTM The weights αi,jsum to 1,/summationtext
jαi,j= 1, and are the
amount of attention the output word oigives the input word xj:
αi,j=exp(si,j)/summationtext
kexp(si,j)(6.33)
wheresi,jis a function of the encoder hidden units [ ht;¯ht]Tand decoder hidden
units˜ht−1 The decoder hidden units ˜htare a function of the context vectors ct
and output at the previous time step ot−1 This form of attention is also known
as encoder–decoder attention Self-attention (Lin, Feng, Santos, Yu, Xiang, Zhou
and Bengio, 2017) improves upon the encoding process by having each word in a
sequence, in the encoder, consider the eﬀect of all other words in the sequence A
self-attention matrix is then used to represent the embedding, where each entry
i,jcorresponds to the self-attention of word itojin the same sequence 6.8 Embeddings 121
Figure 6.47 Machine translation with attention 6.8 Embeddings
The representation of the input to our sequence model is an important decision,
and for language we would like a meaningful representation for words If we sim-
plyrepresentwordsinalanguagebyone-hotencoding xi=( 0,...,0,1,0,...,0)T
then there will be no meaning to the relationship between words, since in this
simple representation two words xiandxjare either the same when their dot
product is 1, xT
ixj= 1, or diﬀerent when their dot product is 0, xT
ixj= 0 For
example, using a one-hot representation of the words man, woman, king, queen
and ball, all words will either be the same or diﬀerent based on their dot product,
without modeling any relationship between the words In contrast, in language
there are relationships and similarity between words such as synonyms and nega-
tions, as well as analogies

============================================================

=== CHUNK 075 ===
Palavras: 372
Caracteres: 2516
--------------------------------------------------
Therefore, we would like to use a representation of
words that allows us to model their relationships, by learning a word embedding
trained using a neural network (Bengio et al., 2003) Word embeddings can be
used to model analogies A:A/prime::B:B/prime, answering questions such as manis to
womanlikekingis to ?, for which the answer is queen We would like similar
words to be close in a low-dimensional embedding space Moreover, often we per-
form analysis using a limited training set in scope or breadth, whereas we would
like to use representations that allow for transfer learning from a very large cor-
122 6 Sequence Models
pus with a broad vocabulary Transfer learning from a pre-trained model is useful
in many applications such as sentiment analysis, named entity recognition, text
summarization, and parsing Next, we deﬁne the notion of an embedding and its training For words in
a vocabulary Vrepresented by one-hot encoded |V|dimensional vectors w,w e
learn an n×|V|dimensional embedding matrix Esuch that x=Ewis then-
dimensional vector embedding of w Using the embedding and cosine similarity
between embedded words, we can ﬁnd analogies such as xman:xwoman::xking: by:
argmax
x= similarity( x,xking−xman+xwoman) (6.34)
using the cosine similarity of two word representations xiandxjdeﬁned as:
cosine similarity( xi,xj)=xT
ixj
/bardblxi/bardbl2/bardblxj/bardbl2(6.35)
to ﬁndx=xqueen To learn the embedding E, we use a large unsupervised corpus
of sentences, which we turn into supervised training pairs by considering target
wordsandtheirsurroundingcontexts.Theembeddingwillrepresenteachwordin
the corpus wiby its embedding xi=Ewi The skip-gram model (Mikolov, Chen,
Corrado and Dean, 2013) randomly selects a context word wc, while ensuring
coverage of all words, and then randomly selects a target word wtin a window
around the context Next, a neural network maps each context word wcto its
embedding xc=Ewcand maps the embedding to a fully connected output
layerofollowed by a softmax mapping to the probabilities of all words in the
vocabulary to the target word wt:
p(wt|wc)=eθT
cxc
/summationtext|V|
j=1eθT
jxc(6.36)
The network is trained by minimizing the loss between the softmax probabili-
ties and the true targets, solving for the embedding matrix Eweights and the
network parameters θend-to-end by backpropagation In practice, the eﬃciency
of training the skip-gram model is improved by using negative sampling and a
hierarchical softmax (Mikolov, Sutskever, Chen, Corrado and Dean, 2013)

============================================================

=== CHUNK 076 ===
Palavras: 355
Caracteres: 2351
--------------------------------------------------
Fi-
nally, the embedding is used in the various tasks by computing the embedding
of individual words xt=Ewt, which serve as inputs to a sequence model Word embeddings have been extended to sentence embeddings (Kiros et al.,
2015) and paragraph embeddings (Le and Mikolov, 2014) Embedding from lan-
guage models (ELMo; (Peters et al., 2018)) represents word vectors as the hidden
states of deep bidirectional LSTMs pre-trained on a very large corpus 6.9 Introduction to Transformers
Transformers,describedindetailinChapter8,arebasedonlyonattentionmech-
anisms (Vaswani et al., 2017) without using RNNs or CNNs The transformer
6.10 Summary 123
consists of a stack of encoders connected to a stack of decoders The input to the
ﬁrst encoder is a word embedding and a position embedding Each encoder con-
sists of a self-attention layer and a neural network passing its output as input to
the next encoder in the stack of encoders Each decoder contains a self-attention
layer, followed by an encoder–decoder attention layer, followed by a neural net-
work Each decoder passes its output as input to the next decoder in the stack
of decoders Using self-attention in the encoders and both encoder–decoder and
self-attention in the decoders results in state-of-the-art results in machine trans-
lation The transformer architecture does not use RNNs nor CNNs, which results
in faster training time A limitation of the Transformer architecture is that it
processes the entire sequence at once, which may be a very long sequence of
words Recent Transformer models split a long sequence of words into segments
and add a recurrent layer between Transformers, allowing processing ﬁxed-sized
inputs while modeling long-term relationships The position of words is required
for computing the attention score Therefore, the Transformer uses an absolute
position embedding, whereas the Transformer XL that breaks the sequence into
segments embeds the relative distance between words 6.10 Summary
This chapter introduces RNNs and their extension to bidirectional and deep
RNNs We present backpropagation through time, its limitations, and the so-
lutions in the form of LSTM and GRU The chapter describes seq2seq models,
followed by encoder–decoder attention and self-attention Finally, the chapter
presents word embeddings and their extension to sentences and paragraphs

============================================================

=== CHUNK 077 ===
Palavras: 354
Caracteres: 2284
--------------------------------------------------
Convolutional neural networks share weights across space, while RNNs share
weights across time Chapter 7 presents graph neural networks (GNNs), which
share weights across neighborhoods 7 Graph Neural Networks
7.1 Introduction
Graphs are the mathematical representation of networks Networks describe in-
teractions between entities; for example, a social network of friends, bonds be-
tween atoms in a molecule or protein, the internet of web pages, the cellular
communication network between users, a ﬁnancial transaction network between
bank clients, protein-to-protein interaction networks, or the neural networks be-
tween neurons in our brains Speciﬁcally, the human brain consists of around 100
billion neurons (for comparison, the cat brain consists of around one billion neu-
rons) with 100 trillion connections between neurons Each neuron is connected
to 5,000–200,000 other neurons, and there are around 10,000 diﬀerent types of
neurons Perhaps most importantly, around 1,000 neurons are generated each
day of our lives Modeling such networks requires a dynamic graph structure Each node in a network may have an associated feature vector, as shown in
Figure 7.1 For example, in a graph representing a molecule or protein, the nodes
are the atoms, the bonds between atoms are the edges, and each node has an
associated feature vector of atom properties In a social network, each node may
represent a user The edges are connections between users Each node may have
a feature vector, including the user’s age, gender, status, country, occupation,
interests, likes, etc Network data is often messy or incomplete, and therefore we would like to be
Figure 7.1 A graph with nodes Each node i∈Vis associated with a feature vector
vi∈Rn 7.1 Introduction 125
Figure 7.2 Graph node classiﬁcation The goal is to classify the uncolored nodes to
one of two classes, green or blue able to perform operations on graphs, such as completing missing information
in the graph Common tasks on networks include node classiﬁcation for pre-
dicting the type of nodes, as shown in Figure 7.2, link prediction for predicting
whether two nodes are connected, ﬁnding clusters for detecting communities,
and measuring the similarity between nodes for embedding node features into
a low-dimensional space

============================================================

=== CHUNK 078 ===
Palavras: 356
Caracteres: 2139
--------------------------------------------------
Speciﬁcally, we will use deep learning for performing
three key operations on graphs:
1 Node prediction: predicting a property of a graph node Link prediction: predicting a property of a graph edge For example, in a
social network, we can predict whether two people will become friends Graph or sub-graph prediction: predicting a property of the entire graph
or a sub-graph For example, given a graph representation of a protein, we
can predict its function as an enzyme or not Given a molecule represented
as a graph, we can predict whether it will bind to a given receptor Notice that if we only have node information and the task is edge prediction, we
may pool the information from the graph nodes Similarly, if we only have edge
information and the task is node prediction, we may pool information from the
graph edges A fundamental property common to neural network representations that work
well is that they all share weights Chapter 5 on convolutional neural networks
(CNNs) describes neural networks applied to images of ﬁxed size and regular
grids, sharing weights across space, as shown on the left of Figure 7.3, by using
a CNN or ResNet or ODENet Chapter 6, on sequence models, describes neural
networks applied to sequences, sharing weights across time, as shown in the
center of Figure 7.3, by using a recurrent neural network (RNN), long short-term
memory (LSTM), or gated recurrent unit (GRU) In this chapter, we describe
graph neural networks (GNNs), applied to networks or general graphs sharing
weights across neighborhoods, as shown in the right of Figure 7.3 A key insight
in GNNs is that, similarly to CNNs or RNNs, nodes in the graph may aggregate
information from neighboring nodes 126 7 Graph Neural Networks
Figure 7.3 Neural network representations sharing weights A CNN shares weights
across space (left); an RNN shares weights across time (center); and a GNN shares
weights across neighborhoods (right) 7.2 Deﬁnitions
We begin with basic graph deﬁnitions A graph G=(V,E) contains a set of n
vertices (or nodes) Vand a set of medgesEbetween vertices The edges of the
graph can either be undirected or directed

============================================================

=== CHUNK 079 ===
Palavras: 365
Caracteres: 2248
--------------------------------------------------
A common duality of modeling problems in computer science is using graph
theory by a graph representation or linear algebra by a matrix representation Moving back and forth between graph theory and linear algebra allows us to
apply algorithms from both Two basic graph representations are an adjacency matrix and adjacency list An adjacency matrix Aof dimensions n×nis deﬁned such that:
Ai,j=/braceleftBigg
1,if there is an edge between vertices iandj
0,otherwise(7.1)
If the edges have weights then the 1s in the adjacency matrix are replaced with
edge weights wi,j For an undirected graph the matrix Ais symmetric The adjacency matrix of the example graph in Figure 7.4 with 9 nodes and 11
edges is the 9 ×9 matrix:
A=⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣011100000
100110000
100100000
111011000
010100000
000100100
000001011
000000100
000000100⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(7.2)
where the number of 1s in matrix Ais twice the number of edges in the graph Notice that diﬀerent permutations of the node labels result in diﬀerent ad-
jacency matrices In contrast, an adjacency list of the edges in the graph is
invariant to node permutations Storing an adjacency matrix takes O(n2) mem-
7.2 Deﬁnitions 127
Figure 7.4 Example graph with 9 nodes and 11 edges ory, where nis the number of nodes in the graph; storing an adjacency list takes
onlyO(m+n), where mis the number of edges in the graph The degree of a node represents the number of edges incident to that node, and
the average degree of a graph is the average degree over all its nodes1
n/summationtextn
i=1di,
which equals2m
nfor an undirected graph andm
nfor a directed graph In a
complete undirected graph, there is an edge between every two vertices for a
total ofn(n−1)
2edges The degree matrix Dof the adjacency matrix Ais a diagonal matrix such
that:
Di,i= degree( vi)=di=n/summationdisplay
j=1Ai,j (7.3)
The neighbors of a node i∈Vare its adjacent nodes N(i), and the degree of a
node is its number of neighbors di=|N(i)| The degree matrix of the graph in
Figure 7.4 is the 9 ×9 diagonal matrix:
D=⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣300000000
030000000
002000000
000500000
000020000
000002000
000000300
000000010
000000001⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(7.4)
In a regular graph, each node has the same number of neighbors, which is the
degree of a node

============================================================

=== CHUNK 080 ===
Palavras: 355
Caracteres: 2151
--------------------------------------------------
The graph Laplacian matrix Lis the diﬀerence between the degree matrix and
128 7 Graph Neural Networks
adjacency matrix L=D−A The Laplacian matrix of the example graph in
Figure 7.4 is given by the matrix:
L=D−A=⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣3−1−1−1 00000
−13 0 −1−1 0000
−10 2 −1 00000
−1−1−15 −1−10 0 0
0−10 −1 20000
000 −10 2 −10 0
00000 −13 −1−1
000000 −11 0
000000 −10 1⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(7.5)
The adjacency matrix and the degree matrix are symmetric, and therefore,
the Laplacian matrix is symmetric Normalizing the Laplacian matrix makes
diagonal elements equal 1 and scales oﬀ-diagonal entries The graph symmetric
normalized Laplacian matrix is:
Lsym=D−1
2LD−1
2=I−D−1
2AD−1
2 (7.6)
whereD−1
2is a diagonal matrix with entries D−1
2
i,i=1√
di Nodes without neigh-
bors are not normalized to avoid division by zero The symmetric normalized
Laplacian matrix elements are given by:
Lsym
i,j=⎧
⎪⎪⎨
⎪⎪⎩1, ifi=jand di/negationslash=0
−1√
didj,ifi/negationslash=jand node iis adjacent to node j
0, otherwise(7.7)
The symmetric normalized Laplacian matrix of the example graph in Figure 7.4
is given by the matrix:
Lsym=⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣1−1
3−1√
6−1√
150 0000
−1
310 −1√
15−1√
60000
−1√
601 −1√
100 0000
−1√
15−1√
15−1√
101−1√
10−1√
10000
0−1√
60−1√
101 0000
000 −1√
1001 −1√
600
00000 −1√
61−1√
3−1√
3
000000 −1√
310
000000 −1√
301⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(7.8)
which is a symmetric matrix The random walk normalized Laplacian matrix is a transition matrix for a
random walk on a graph with non-negative weights and is deﬁned as:
Lrw=D−1L=I−D−1A (7.9)
7.2 Deﬁnitions 129
whereD−1is a diagonal matrix with entries D−1
i,i=1
di The random walk nor-
malized Laplacian matrix elements are given by:
Lrw
i,j=⎧
⎪⎪⎨
⎪⎪⎩1,ifi=jand di/negationslash=0
−1
di,ifi/negationslash=jand node iis adjacent to node j
0,otherwise(7.10)
The random walk normalized Laplacian matrix of the example graph in Figure
7.4 is given by the matrix:
Lrw=⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣1−1
3−1
3−1
300000
−1
310 −1
3−1
30000
−1
201 −1
200000
−1
5−1
5−1
51−1
5−1
5000
0−1
20−1
210000
000 −1
201 −1
200
00000 −1
31−1
3−1
3
000000 −11 0
000000 −10 1⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(7.11)
which is not symmetric and each row sums to zero

============================================================

=== CHUNK 081 ===
Palavras: 351
Caracteres: 2142
--------------------------------------------------
The matrices LrwandLsym
are similar and therefore have the same eigenvalues Agraphwith nnodeshas neigenvectorswitheigenvaluesthatarenon-negative
since the Laplacian matrix Lhas non-negative eigenvalues A sub-graph of a
graph is a subset of edges and all their nodes in the graph If there is at least one
path between each pair of nodes in the sub-graph, it is a connected component The number of zero eigenvalues of the Laplacian matrix of a graph is the number
of its connected components A walk on a graph begins with a node i∈Vand ends with a node j∈Vand
traverses a sequence of edges and nodes between nodes iandj If the nodes are
distinct, the walk is a path; if the edges are distinct, the walk is a trail In the
matrix,Ak, which is the adjacency matrix to the power of k,e a c he n t r y Ak
i,jis
the number of walks of length kin the graph between the node in row iand the
node in column j Graph nodes may consist of features x For example, a binary feature xfor
the graph shown in Figure 7.4 may be deﬁned by appending a column to the
130 7 Graph Neural Networks
Figure 7.5 Graph node embedding A node i∈Vwith associated feature vector
vi∈Rnwhich is embedded into a low-dimensional space zi∈Rdby an embedding
f:vi/mapsto →zi adjacency matrix:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ABCDEFGHIx
0111000000
1001100001
1001000001
1110110000
0101000000
0001001001
0000010110
0000001000
0000001001⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(7.12)
or, for example, a graph in which the nodes are papers, the edges are the other
papers they cite, and the features are the paper abstract or the language embed-
ding of the abstract Mostgraphsaresparse,withfeweredgesthansquarenodes m/lessmuchn2;therefore,
adjacency lists are an alternative representation for eﬃcient storage A linked list
represents each vertex and all its edges and adjacent vertices 7.3 Embeddings
An example of embedding a node in a graph into Rnis an embedding such that
similar nodes in the graph along with their features are embedded to nearby
nodes in the embedding space Our embedding objective may not be limited to
similarity and may be deﬁned with respect to other properties of the graph and
embedding space

============================================================

=== CHUNK 082 ===
Palavras: 352
Caracteres: 2173
--------------------------------------------------
We deﬁne an encoder fof a node i∈V, such that f(i) is the
embedding of the node feature vector vias shown in Figure 7.5 7.3 Embeddings 131
Figure 7.6 Sub-graph embedding by taking the sum of the embeddings of the nodes in
the sub-graph If each node i∈Vhas an associated feature vector vithen a node embed-
ding, maintaining similarity, maps node feature vectors vito vectors ziin a
low-dimensional space such that the similarity between nodes iandj, denoted
bys(i,j), is maintained in the embedding space For example, we may opti-
mize for the similarity between nodes iandj, such that their similarity s(i,j)i s
maintained after the embedding f(i)Tf(j), where fdenotes the encoder which
embeds node feature vectors A shallow node embedding uses an n×1-dimensional one-hot encoding eiof
each node ito look up the embedded node The one-hot encoding eiof a node
i∈Vis ann×1 zero vector except for a single 1 in position i An embedding
matrixWof dimensionality d×n, wheredis the dimensionality of a node feature
vectorviandnis the number of nodes, is formed such that each column of Wis
the embedding of a diﬀerent node Multiplying the d×nembedding matrix Wby
then×1 one-hot encoded vector eirepresenting a node iresults in Wei, which
is thed×1ith column of the matrix Wrepresenting the node in the embedding
space This results in a problem with shallow embeddings: they do not share
weights As demonstrated earlier, the success of neural networks stems from
representations sharing weights across space in CNNs or across time in RNNs This motivates the sharing of weights by aggregating graph neighborhoods in
GNNs, as described in Section 7.5 We may embed a sub-graph S∈G, either by taking the sum of the embeddings
of the nodes in the sub-graph/summationtext
i∈Sf(i), or by taking a representative node j
of the sub-graph and setting the sub-graph embedding to be f(j)a ss h o w ni n
Figures 7.6 and 7.7 132 7 Graph Neural Networks
Figure 7.7 Sub-graph embedding by taking a representative node of the sub-graph 7.4 Node Similarity
7.4.1 Adjacency-based Similarity
In node embeddings we deﬁne pairwise node similarity and optimize an embed-
ding to approximate similarities

============================================================

=== CHUNK 083 ===
Palavras: 372
Caracteres: 2432
--------------------------------------------------
Going beyond shallow node embeddings, we
can deﬁne diﬀerent measures of similarity For example, we deﬁne the similarity
between nodes iandjto be the weight on the edge between them, s(i,j)=Ai,j,
whereAis the weighted adjacency matrix We then ﬁnd the matrix Wwith
dimensions d×nwhich minimizes the loss:
L=/summationdisplay
(i,j)∈V×V/bardblf(i)Tf(j)−Ai,j/bardbl2(7.13)
over all pairs of nodes in the graph 7.4.2 Multi-hop Similarity
The ﬁrst ring of neighbors of a node, as shown in Figure 7.8, is the node’s
neighborhood Let Adenote the adjacency matrix of 1-hop neighbors, A2denote
the adjacency matrix of 2-hop neighbors and in general Akthe adjacency matrix
ofk-hop neighbors Then, we can minimize the loss:
L=/summationdisplay
(i,j)∈V×V/bardblf(i)Tf(j)−Ak
i,j/bardbl2(7.14)
7.4.3 Overlap Similarity
Another measure of similarity is the overlap between node neighborhoods, as
shown in Figure 7.9 Suppose nodes iandjshare common nodes in the social
network of mutual friends We can then minimize the loss function measuring
the overlap between neighborhoods:
L=/summationdisplay
(i,j)V×V/bardblf(i)Tf(j)−Si,j/bardbl2(7.15)
7.4 Node Similarity 133
Figure 7.8 Graph neighborhoods Given a root node shown in white, the 1-hop ring of
neighbors is shown in blue, the 2-hop neighbors are shown in green, and the 3-hop
neighbors are in purple Figure 7.9 Mutual nodes (shown in purple) are the overlap between node
neighborhoods of nodes iandj whereSi,jmeasures the overlap between the neighbors N(i)o fn od e iand neigh-
borsN(j) of node j The overlap may be measured using the overlap coeﬃcient
|N(i)∩N(j)|
min{|N(i)|,|N(j)|}or Jaccard similarity|N(i)∩N(j)|
|N(i)∪N(j)| 7.4.4 Random Walk Embedding
We can deﬁne an embedding using a random walk from nodes in the graph, as
shown in Figure 7.10 A random walk in a graph begins with a node i∈Vand
134 7 Graph Neural Networks
Figure 7.10 Graph random walk (shown in blue) consists of the nodes on a random
path starting from node i∈Vand ending in node j∈V repeatedly walks to one of its neighbors N(i) with probability1
d(i)fortsteps
until reaching an end at node jon the graph Running random walks may start from each graph node imultiple times We
collect all the nodes visited for each node in the walk, and then optimize the
embedding deﬁned by:
f(i)Tf(j)∝P(iandjco-occur on the random walk) = p(i|j) (7.16)
which is the probability that we reach node jstarting a random walk from node
i

============================================================

=== CHUNK 084 ===
Palavras: 377
Caracteres: 2479
--------------------------------------------------
Using the loss function:
L=/summationdisplay
i∈V/summationdisplay
j∈N(i)−logp(j|f(i)) (7.17)
wherep(j|f(i)) is given by the softmax:
p(j|f(i)) =exp(f(i)Tf(j))/summationtext
j∈N(i)exp(f(i)Tf(j))(7.18)
DeepWalk (Perozzi et al., 2014) uses a skip-gram model of random walks on a
graph to classify nodes of a graph Node2vec (Grover and Leskovec, 2016) uses
a random walk on a graph based on both the current node iand the previous
nodes that led to node i Instead of moving from node ito another node with
probability1
d(i), node2vec deﬁnes a random walk with probability based on the
length of the shortest path between the previous node and the next node LINE
(Tang et al., 2015) embeds graph nodes into a low-dimensional space applied to
the task of node classiﬁcation and link prediction 7.4 Node Similarity 135
Figure 7.11 Regular graph structure representing neighboring pixels of an image Figure 7.12 Irregular graph structure of real-world graphs representing networks 7.4.5 Graph Neural Network Properties
A CNN has a regular grid structure, as shown in Figure 7.11, which is suitable
for images; however, it is not suitable for real-world graphs, which have irregular
structure, as shown in Figure 7.12 Anaiveapproachforrepresentingageneralgraphistoconcatenateeachnode’s
feature vectors to the adjacency matrix and encode each node by the correspond-
ingrowoftheadjacencymatrixanditsfeatures.Afullyconnectednetworkarchi-
tecture given a node’s row in the adjacency matrix and features is unsuitable for
graph representation Having such a vector representation be the input to a fully
connected neural network has numerous limitations In such a naive network, the
number of parameters is linear in the size of the graph, the network is dependent
on the order of the nodes, and it does not accommodate dynamic graphs We
want to be able to add or remove nodes to real-world graphs, such as the social
network, without changing the network architecture The desired properties of
our graph neural network architecture are that the number of parameters is in-
dependent of the graph size, scaling to graphs with billions of nodes, that the
136 7 Graph Neural Networks
Figure 7.13 Each node aggregates information from its ring of neighbors network is invariant to node ordering, that the operations be local depending
on neighborhoods, that the model accommodates any graph structure, and that
once we learn the properties of one graph, we can transfer them to a new unseen
graph

============================================================

=== CHUNK 085 ===
Palavras: 377
Caracteres: 2408
--------------------------------------------------
7.5 Neighborhood Aggregation in Graph Neural Networks
We consider GNNs that take into account neighbors of each node, aggregating
information from neighboring nodes similar to breadth-ﬁrst search (BFS); and
the other graph neural network which considers chains from a node, similar to
depth-ﬁrst search (DFS) In the ﬁrst architecture, we consider each node in the
graph and pick up the graph from that node as the root, allowing all other nodes
to dangle, building a computation graph where that node is the root Once
we determine the node computation graph, we will propagate and transform
information from its neighbors, its neighbors’ neighbors, and so on, as shown in
Figure 7.13, where each node consists of a vector containing the features of the
node Most GNNs are based on aggregating information into each node from its
neighboring nodes in a layer /lscriptand combining that information with the node
features in that layer:
h/lscript
i= combine/lscript{h/lscript−1
i,aggregate/lscript{h/lscript−1
j,j∈N(i)}} (7.19)
whereh/lscript
iis the feature representation of node iat layer/lscript Consider the graph shown in Figure 7.14 We generate embeddings based on
local neighborhoods and aggregate information from neighbors using the neural
7.5 Neighborhood Aggregation in Graph Neural Networks 137
Figure 7.14 A computational graph is constructed for each node, aggregating its
neighbors and in turn from each neighbor Figure 7.15 Computational graphs starting from each node in the graph, sharing
weights between diﬀerent computational graphs for all 1-hop neighbors, 2-hop
neighbors, and 3-hop neighbors The roots of the computational graphs for each node
form the last layer of the GNN, whereas the leaves and their node features form the
ﬁrst layer Consider node A; its neighbors are nodes B,C,a n dD, and in turn B’s
neighbors are nodes AandC,C’s neighbors are nodes A,B,E,a n dF,a n dD’s
neighbor is node A Next, we consider each node in turn and generate a computation graph for
each node where that node is the root Finally, we will share the aggregation
parameters across all nodes for every layer of neighbors, as shown in Figure 7.15 The gray boxes in each layer in Figure 7.15 represent aggregation parameters,
denoted in the special case below by shared matrices W/lscriptandB/lscriptfor layer /lscript, such
that the aggregation boxes in each layer are identical and shared across nodes

============================================================

=== CHUNK 086 ===
Palavras: 362
Caracteres: 2677
--------------------------------------------------
138 7 Graph Neural Networks
In summary, the nodes have embeddings at each layer, and the network shares
aggregation parameters across all nodes in a layer We denote the feature vector of a node ibyh0
i=xi A feature vector h/lscript
iwill
be an aggregation of the feature vectors h/lscript−1
jof the neighbors j∈N(i)o fiand
the feature vector h/lscript−1
iof the previous layer embedding An example of a choice
of aggregation and combination function is:
h/lscript
i=σ⎛
⎝W/lscript/summationdisplay
j∈N(i)h/lscript−1
j
|N(i)|+B/lscripth/lscript−1
i⎞
⎠ (7.20)
whereh/lscript
iis the/lscriptth layer embedding of i,σis a non-linear activation function and
/summationtext
j∈N(i)h/lscript−1
j
|N(i)|is the average of neighbors in the previous layer embedding We
have two types of weight matrices: W/lscriptis a matrix of weights for neighborhood
embeddings, and B/lscriptis a matrix of weights for self-embedding These matrices
are shared for each layer /lscriptacross all nodes 7.5.1 Supervised Node Classiﬁcation Using a GNN
For the task of node classiﬁcation, given mlabeled nodes iwith labels yiwe
train a GNN by minimizing the objective:
1
mm/summationdisplay
i=1L(yi,ˆyi) (7.21)
where the predictions ˆ yiare the softmax of the node representations at the last
layer 7.6 Graph Neural Network Variants
7.6.1 Graph Convolution Network
A graph convolution network (GCN; Kipf and Welling (2017)) has a similar
formulation using a single matrix for both the neighborhood and self-embeddings
normalized by the product of square roots of node degrees:
h/lscript
i=σ⎛
⎝W/lscript/summationdisplay
j∈i∪N(i)ˆAi,j/radicalBig
ˆdjˆdih/lscript−1
j⎞
⎠
=σ⎛
⎝/summationdisplay
j∈N(i)ˆAi,j/radicalBig
ˆdjˆdiW/lscripth/lscript−1
j+1
ˆdiW/lscripth/lscript−1
i⎞
⎠(7.22)
whereˆA=A+Iis the adjacency matrix including self-loops, ˆdii st h ed e g r e ei n
the graph with self-loops, and σis a non-linear activation function Aggregation
is deﬁned by the term on the left and the combination on the right 7.6 Graph Neural Network Variants 139
An equivalent formulation (Wu et al., 2019) is given by:
H/lscript+1=ˆD−1
2ˆAˆD−1
2H/lscriptW/lscript(7.23)
whereˆDi,i=/summationtext
jˆAi,j 7.6.2 GraphSAGE
GraphSAGE (Hamilton et al., 2017) concatenates the neighborhood embedding
and self-embedding:
h/lscript
i=σ([W/lscriptaggregate( {h/lscript−1
j,j∈N(i)}),B/lscripth/lscript−1
i]) (7.24)
The graph neighborhood aggregation function can be the mean, pooling, or
an LSTM sequence model:
Mean aggregation:/summationdisplay
j∈N(i)h/lscript−1
j
|N(i)|(7.25)
Pooling: γ({Qh/lscript−1
j,j∈N(i)}) (7.26)
LSTM: LSTM([ h/lscript−1
j,j∈π(N(i))]) (7.27)
and the network learns the parameters for aggregating information

============================================================

=== CHUNK 087 ===
Palavras: 302
Caracteres: 2063
--------------------------------------------------
In the training process, we have an output embedding after Llayersei=hL
i
and we learn the weight matrices W/lscriptfor the neighborhood embedding and B/lscript
for self-embedding We deﬁne a neighborhood aggregation function and a loss
function on embedding and train on a set of nodes generating embeddings for
nodes This is useful since once we train the GNN, and compute the aggregation
parameters, namely the weight matrices, we can generalize to new nodes We
generate a computation graph for a new node and transfer the weight matrices
to the new node and compute a forward pass for prediction In addition, given an
entire new graph, we can transfer the aggregation weight matrices computed on
one graph to a new graph and compute the forward pass to perform prediction 7.6.3 Gated Graph Neural Networks
The second architecture, similar to DFS, shares weights across all the layers in
each computation graph, instead of sharing weights across neighborhoods In
gated graph neural networks (Li et al., 2016) nodes aggregate messages from
neighbors using a neural network, and similar to RNNs parameter sharing is
across layers:
m/lscript
i=W/summationdisplay
j∈N(i)h/lscript−1
j (7.28)
h/lscript
i=G R U (h/lscript−1
i,m/lscript) (7.29)
140 7 Graph Neural Networks
7.6.4 Graph Attention Networks
In graph attention networks (GATs) (Veliˇ ckovi´ c et al., 2018) we use attention-
based neighborhood aggregation The attention function adaptively controls the
contribution of neighbor jto nodei:
h/lscript
i=σ⎛
⎝/summationdisplay
j∈i∪N(i)αi,jWh/lscript−1
j⎞
⎠ (7.30)
whereαi,jare the attention coeﬃcients that deﬁne a distribution over node i
and its neighbors k∈N(i) using the softmax:
αi,j=exp(ei,j)/summationtext
k∈i∪N(i)exp(ei,k)(7.31)
andei,jis a function of h/lscript−1
iandh/lscript−1
j:
ei,j= ReLU( vT(Wh/lscript−1
i||Wh/lscript−1
j) (7.32)
where||is the concatenation operation, and vandWare a learned vector and
weight matrix When using multiple attention heads, h/lscript
iis the aggregation of
multiple contributions, each of the form of Equation 7.30

============================================================

=== CHUNK 088 ===
Palavras: 377
Caracteres: 2587
--------------------------------------------------
7.6.5 Message-Passing Networks
In a similar fashion to using aggregation and combination, a message-passing
graph neural network is deﬁned by messages between nodes across edges (aggre-
gation) and node updates (combination):
h/lscript
i= update/lscript(h/lscript−1
i,/summationdisplay
j∈N(i)message/lscript(h/lscript−1
i,h/lscript−1
j,ei,j)) (7.33)
7.7 Applications
Graph neural networks are used in a wide range of applications, including (1) im-
age retrieval; (2) computer vision for scene understanding (Santoro et al., 2017);
(3) computer graphics for 3D shape analysis (Monti et al., 2017) and for learn-
ingpoint-cloudrepresentations(Wang,Sun,Liu,Sarma,BronsteinandSolomon,
2019); (4) social networks for link prediction; (5) recommender systems and few-
shot learning (Garcia and Bruna, 2018); (6) combinatorial optimization (Ma
et al., 2020); (7) physics for learning the dynamics and interactions of physical
objects (Battaglia et al., 2016; Chang et al., 2017; Watters et al., 2017; Sanchez-
Gonzalez et al., 2018; Van Steenkiste et al., 2018); (8) chemistry for molecule
classiﬁcation (Duvenaud et al., 2015; Gilmer et al., 2017), deﬁning a graph in
which molecules are nodes and edges represent bonds between molecules, and
7.8 Software Libraries, Benchmarks, and Visualization 141
molecule design (Jin et al., 2018); (9) biology for drug discovery, protein func-
tion prediction, and protein–protein interactions; (10) for representing computer
programs; (11) in natural language processing; (12) for traﬃc applications such
as ride hailing and ﬂight classiﬁcation; and (13) in stock trading 7.8 Software Libraries, Benchmarks, and Visualization
PyTorch Geometric (Fey and Lenssen, 2019) is a library for deep learning on
graphs in PyTorch DGL (Wang, Yu, Gan, Zhaoogle, Gai, Ye, Li, Zhou, Huang,
Zheng, Lin, Ma, Deng, Guo, Zhang and Huang, 2020) is an optimized library
for deep learning on graphs in PyTorch and MXNet OGB (Liu et al., 2020) is a
collection of benchmark datasets, data-loaders and evaluators for deep learning
on graphs 7.9 Summary
Graph neural networks (Hamilton et al., 2017; Kipf and Welling, 2017; Veliˇ ckovi´ c
et al., 2018; Xu et al., 2019) are applied to irregular structures such as networks
represented by graphs They commonly share weights across neighborhoods, sim-
ilar to how CNNs share weights across space and RNNs share weights across
time Graph neural networks are used for three main tasks: (1) predicting prop-
erties of particular nodes, (2) predicting edges between nodes, and (3) predicting
properties of sub-graphs or entire graphs

============================================================

=== CHUNK 089 ===
Palavras: 463
Caracteres: 3383
--------------------------------------------------
8 Transformers
8.1 Introduction
This chapter presents Transformer models, which have state-of-the-art perfor-
mance across many tasks and datasets in a broad range of domains, including
natural language processing, computer vision, audio processing, and program
synthesis The largest Transformer-based language model to date consists of
around a trillion parameters These models are trained using vast unlabeled cor-
pora Replacing labels are objectives based on context at multiple resolutions:
where words occur in a sentence, whether sentences follow each other, and where
sentences occur in a document Very large Transformer-based models and im-
provements in usage of unlabeled data have led to results that supersede the
largest available supervised counterparts and signiﬁcant progress in real-world
applications Transformers are based only on attention mechanisms (Vaswani et al., 2017)
without using RNNs or CNNs Transformers may be classiﬁed into three types
of architectures: (1) autoencoding Transformers, which is a stack of encoders;
(2) auto-regressive Transformers, which is a stack of decoders; or (3) sequence-
to-sequence Transformers, which is a stack of encoders connected to a stack of
decoders In the latter case, the input to the ﬁrst encoder is a word embed-
ding and a position embedding Each encoder consists of a self-attention layer
and a neural network passing its output as input to the next encoder in the
stack of encoders Each decoder contains a self-attention layer, followed by an
encoder–decoder attention layer, followed by a neural network Each decoder
passes its output as input to the next decoder in the stack of decoders Us-
ing self-attention in the encoders and both encoder–decoder and self-attention
in the decoders results in state-of-the-art results in machine translation The
Transformer architecture does not use RNNs or CNNs, which results in faster
training time 8.2 General-Purpose Transformer-Based Architectures 143
8.2 General-Purpose Transformer-Based Architectures
8.2.1 BERT
Bidirectional encoder representations from Transformers (Devlin et al., 2019),
knownasBERT,isageneral-purposeTransformer-basedarchitecturethatachieves
state-of-the-art performance on many natural language processing tasks and
datasets:
•multi-genre natural language inference of sentence entailment, contradiction,
or neutral pairs (Williams et al., 2018);
•Quora question pairs which classiﬁes semantically equivalent sentences (Chen,
Zhang, Zhang and Zhao, 2018);
•question natural language inference of labeled question–answer pairs (Wang,
Singh, Michael, Hill, Levy and Bowman, 2019);
•Stanford sentiment treebank of movie review sentiments (Socher et al., 2013);
•corpus of linguistic acceptability of sentences (Warstadt et al., 2019);
•semantic textual similarity benchmark (Cer et al., 2017);
•Microsoft research paraphrase corpus annotating semantically equivalent sen-
tences (Dolan and Brockett, 2005);
•recognizing a textual entailment task of bidirectional entailment (Bentivogli
et al., 2009);
•Winograd NLI dataset for language inference (Morgenstern and Ortiz, 2015);
•Stanford question answering dataset (SQuAD) (Rajpurkar et al., 2016);
•CoNLL 2003 named entity recognition (NER) dataset (Sang and Meulder,
2003);
•situations with adversarial generations (SWAG) dataset of common-sense sen-
tence completion (Zellers et al., 2018)

============================================================

=== CHUNK 090 ===
Palavras: 402
Caracteres: 3022
--------------------------------------------------
The BERT architecture is a deep bidirectional autoencoding Transformer In
BERT,eachinputword,ortoken,isrepresentedbyatokenembedding,asegment
embedding, and a position embedding Next, a small fraction of all tokens in
each sentence is randomly masked, and the goal of encoding self-attention is to
complete the masked words In addition, BERT learns to predict whether the
relationship between two consecutive sentences is random or not, which is useful
for question answering by concatenating the two sentences in random order with
aseparatortokenbetweenthem.Finally,theoutputsentenceisrepresentedusing
the hidden state of a classiﬁcation token, which serves as input to a classiﬁer that
is ﬁne-tuned on top of BERT 8.3 Self-Attention
Sequence models, described in Chapter 6, perform computations on the input se-
quentially,whereasTransformersperformcomputationsinparallel.Transformers
144 8 Transformers
are based on self-attention (Vaswani et al., 2017), which generates a representa-
tion for each word in the input in parallel We begin by representing nwords in a
sentence using word embedding so that each embedded word is a d-dimensional
vectorxi∈Rdand the sentence X=[x1,...,x n]T∈Rn×dis ann×dmatrix This representation does not take into account the surroundings of the word in a
speciﬁc sentence Therefore, for a sentence with nwords, self-attention computes
nself-attention representations A1,...,A nfor thenwords This computation is
performed in parallel For each embedded word xi∈Rdwe compute a query qi∈Rd,ak e yki∈Rd
and a value vi∈Rd, represented as row vectors (of matrices X,Q,K,a n dV),
by linearly projecting Xinto three d-dimensional spaces of keys, queries, and
values:
Q=XWQ,K=XWK,V=XWV(8.1)
whereQ,K,Varen×d-dimensional matrices whose rows iare the queries qi=
xiWQ, keyski=xiWK, and values vi=xiWVfor each embedded word X∈
Rn×dis then×dmatrix representing the sequence, or sentence, of embedded
words, and WQ,WK,a n dWVare learned d×dmatrices We compute the inner
product between qiandkjfor each j=1,...,n Next, we compute the softmax
multiplied by word values vito get the self-attention representation Ai∈Rdfor
embedded word xi:
Ai(q,K,V)=n/summationdisplay
i=1exp(qki)/summationtext
jexp(qkj)vi (8.2)
which may be summarized for all words i=1,...,nas:
A(X)=A(Q,K,V)=s o f t m a x/parenleftbiggQKT
√dk/parenrightbigg
V (8.3)
whereA(X)i sa nn×dmatrix and dis a scaling factor of the dot product
attention 8.4 Multi-head Attention
Multi-head attention performs self-attention multiple times Instead of inner
products, the query, key, and value vectors are multiplied by matrices WQ
hqj,
WK
hkj,WV
hvjforh=1,...,mand the attention representations:
Ah(X)=Ah(Q,K,V)=Ah(WQ
hQ,WK
hK,WV
hV)=s o f t m a x/parenleftbiggQhKT
h√
d/parenrightbigg
Vh
(8.4)
are computed for each head h=1,...,m, for each embedded word i.T h em
multi-head attention representations Ah(X)f o rh=1,...,mare concatenated
and multiplied:
multiheadattn( X)=[A1(X),...,A(X)]Wo(8.5)
8.5 Transformer 145
Figure 8.1 Multi-head attention

============================================================

=== CHUNK 091 ===
Palavras: 352
Caracteres: 2376
--------------------------------------------------
Given an embedded sequence Xof dimensions n×d
and position embedding as input, for each attention head we compute queries Q, keys
K, and values Vrepresented by d×dmatrices, which are passed to the multi-head
attention layer whereWois anmd×dlearned matrix Multi-head attention is illustrated in
Figure 8.1 We ﬁrst compute the sum of an embedded sequence Xof dimensions
n×dandpositionencoding Pasinput.Next,foreachattentionhead,wecompute
queriesQ, keysK, and values Vrepresented by d×dmatrices, which are passed
to the multi-head attention layer whose output is of dimensions n×d 8.5 Transformer
Given sentences of embedded words, a Transformer may be used for diverse
tasks such as natural language understanding, text generation, and translation
of a sentence from one language to another A Transformer consists of encoder
or decoder blocks or both 8.5.1 Positional Encoding
Thepositionofwordsisrequiredforcomputingtheattentionscore.Thepositions
of the words in the sentence are encoded as a position embedding by sine and
cosine functions and added to X(Vaswani et al., 2017) Speciﬁcally, an n×d
position embedding matrix Pis deﬁned by:
Ppos,2i= sin/parenleftbiggpos
10,0002i
d/parenrightbigg
,Ppos,2i+1=c o s/parenleftbiggpos
10,0002i
d/parenrightbigg
(8.6)
whereposis the position of a word in a sentence, dis the dimension of a word
embedding index i=1,...,d Adding PtoXallows the model to learn to attend
to relative positions 8.5.2 Encoder
The encoder block takes as input a matrix Xof embedded words The position
encoding is added to the embedded words to form the input X+P We then
146 8 Transformers
compute queries Q, keysK, and values V, and pass them through a multi-head
attention layer whose output is fed to a feed-forward neural network A block
consisting of multi-head attention and feed-forward neural network is repeated
multiple times 8.5.3 Decoder
The output of the encoder is fed into a decoder block, which predicts the trans-
lated sentence The decoder also consists of multiple blocks of multi-head at-
tention, which are fed into feed-forward neural networks and add positional em-
beddings to the inputs Both the encoder and decoder may consist of residual
connections between blocks and add and norm layers for normalization before
the feed-forward neural networks The output of the decoder is fed through a
linear layer followed by a softmax layer

============================================================

=== CHUNK 092 ===
Palavras: 388
Caracteres: 2724
--------------------------------------------------
During generation, the decoder predicts
new words, whereas during training, the decoder predicts masked words from
the input 8.5.4 Pre-training and Fine-tuning
Pre-training a Transformer is computationally expensive and most often involves
vast amounts of unlabeled data The most common optimization objectives for
pre-training language models are (1) masked word prediction, which is predict-
ing a random deleted word in a sentence or predicting the next word; and (2)
classifying whether two sentences follow each other or not This computation-
ally expensive step is usually done once, followed by a relatively fast ﬁne-tuning
step In ﬁne-tuning, the pre-trained model is tuned on a speciﬁc dataset and
task Fine-tuning may be performed on a relatively small dataset very eﬃciently
for speciﬁc usage Pre-training followed by ﬁne-tuning is referred to as transfer
learning 8.6 Transformer Models
Transformers may be roughly split into three classes: (1) autoencoding Trans-
former models that use only an encoder, which is suitable for natural language
understanding; (2) auto-regressive Transformer models that use only a decoder,
which is suitable for text-generation tasks; and (3) sequence-to-sequence Trans-
former models that use both an encoder and decoder 8.6.1 Autoencoding Transformers
Encoder Transformers, also known as autoencoding models, use only an en-
coder These models are suitable for natural language understanding tasks such
8.6 Transformer Models 147
as question answering, sentence classiﬁcation, and other tasks that require un-
derstanding entire sentences These Transformers include BERT, ALBERT (Lan
et al., 2020), DistilBERT (Sanh et al., 2019), and RoBERTa (Liu, Ott, Goyal,
Du, Joshi, Chen, Levy, Lewis, Zettlemoyer and Stoyanov, 2019) BERT has been
extended and improved in several ways: RoBERTa (Liu, Ott, Goyal, Du, Joshi,
Chen, Levy, Lewis, Zettlemoyer and Stoyanov, 2019) improves BERT training
and results by ﬁne-tuning hyperparameters; ALBERT (Lan et al., 2020) adds
a self-supervised loss to model inter-sequence coherence; and DistilBERT (Sanh
et al., 2019) reducestheBERTmodel size whilemaintainingperformance BERT
is based on the Transformer architecture and uses a mask token for training but
notfortesting.BERTpredictsmultiplemasktokensinparallelwithoutmodeling
direct dependencies between diﬀerent predictions 8.6.2 Auto-regressive Transformers
Decoder Transformers, also known as auto-regressive models, use only a de-
coder These models are well suited for text generation and include GPT (Rad-
ford et al., 2018), GPT-2 (Radford et al., 2019), CTRL (Keskar et al., 2019),
Transformer XL (Dai et al., 2019), and XLNet (Yang, Dai, Yang, Carbonell,
Salakhutdinov and Le, 2019)

============================================================

=== CHUNK 093 ===
Palavras: 382
Caracteres: 2646
--------------------------------------------------
A limitation of the Transformer architecture is
that it processes the entire sequence at once, which may be a very long se-
quence of words The Transformer XL (Dai et al., 2019) splits a long sequence
of words into segments and adds a recurrent layer between Transformers, al-
lowing us to process ﬁxed-sized inputs while modeling long-term relationships The Transformer uses an absolute position embedding, whereas the Transformer
XL breaks the sequence into segments and embeds the relative distance between
words.XLNet(Yang,Dai,Yang,Carbonell,SalakhutdinovandLe,2019)isbased
on the Transformer XL architecture and models the dependencies between mul-
tiple predictions by predicting tokens in a random order sequentially, improving
performance over BERT across the diﬀerent natural language processing tasks GPT is a pre-trained auto-regressive Transformer ﬁne-tuned on multiple natural
language processing tasks GPT-2 (Radford et al., 2019) is trained with 1.5 bil-
lion parameters CTRL (Keskar et al., 2019) is a conditional Transformer-based
model providing control over the generated text style and content, trained with
1.63 billion parameters 8.6.3 Sequence-to-Sequence Transformers
BART (Lewis et al., 2020) and T5 (Raﬀel et al., 2020) are sequence-to-sequence
Transformer models that use both an encoder and decoder Such models are
suitable for translation, summarization, paraphrasing, and question answering
that involve generating sentences from input 148 8 Transformers
8.6.4 GPT-3
InaracetobuildmorepowerfulTransformer-basedlanguagemodels,thenumber
of model parameters has increased by orders of magnitude GPT-3 (Brown et al.,
2020) is trained with 175 billion parameters and performs well on multiple tasks
even without ﬁne-tuning, by zero-shot (without any examples) or few-shot (given
a few examples) learning Building upon the Megatron-LM model (Shoeybi et al., 2019), trained with 8.3
billion parameters, and the Turing NLG model, trained with 17 billion parame-
ters, Nvidia and Microsoft joined forces to train Megatron-Turing NLG, which
is currently one of the largest natural language processing Transformers, with
530 billion parameters The number of parameters of the largest Transformer
models is growing tenfold each year Transformers will soon reach 100 trillion
parameters at this rate, which is a signiﬁcant milestone since that is roughly the
number of connections in a human brain 8.7 Vision Transformers
In a similar fashion to language models trained by predicting masked words in
a sentence or the next word, vision Transformers (ViT) may be trained by pre-
dicting a masked pixel in a patch or the next pixel

============================================================

=== CHUNK 094 ===
Palavras: 362
Caracteres: 2580
--------------------------------------------------
Vision Transformers have
superseded convolutional neural networks (CNNs) in scalability and performance
across various applications, including object detection, recognition, and segmen-
tation Early eﬀorts include the Image Transformer (Parmar et al., 2018), used
for image generation The Vision Transformer (Dosovitskiy et al., 2020) is used
for image recognition at scale It splits an image into non-overlapping patches,
embeds the patches, and passes them through a Transformer architecture In
early computer vision, scientists manually designed ﬁlters Next, CNNs learned
these ﬁlters automatically by backpropagation; however, they required humans
to design a suitable architecture The use of Transformers in computer vision is
yet another step forward The Transformer does not require speciﬁc inductive
biases in the architecture Nevertheless, itsperformance supersedes CNNs, whose
architecture is based on a strong inductive bias for processing images Various Transformer architectures have been used for object recognition Be-
ginning with a CNN architecture and replacing part of the layers with Trans-
former layers results in hybrid architectures such as VT (Wu et al., 2021) and
BotNet (Srinivas et al., 2021), whose performance improves upon CNNs Begin-
ning with a Transformer architecture and introducing inductive biases by local
computations,asperformedinCNNlayers,resultsinhybridarchitecturessuchas
the data-eﬃcient image Transformer (Touvron et al., 2021) (DeiT) and ConViT
(d’Ascoli et al., 2021) Hierarchical Transformer architectures reduce computa-
tional complexity while maintaining performance These include the pyramid
vision transformer (Wang, Xie, Li, Fan, Song, Liang, Lu, Luo and Shao, 2021),
8.8 Multi-modal Transformers 149
which uses a hierarchical pyramid architecture with non-overlapping patches,
and the shifted windows Transformer (Liu, Lin, Cao, Hu, Wei, Zhang, Lin and
Guo, 2021), which uses local self-attention Detection with Transformer (Carion et al., 2020) (DETR) combines a CNN
with an encoder–decoder Transformer architecture, avoiding manually designed
object detection components such as sample selection and non-maximum sup-
pression Deformable DETR (Zhu et al., 2021) improves the computational ef-
ﬁciency of DETR by applying attention to a sparse set of sampled locations To avoid the collection of a large labeled training set, unsupervised pre-training
DETR (Dai et al., 2021) pre-trains a model on random image patches that serve
as queries for the decoder Video may be treated as a volume consisting of image slices in time

============================================================

=== CHUNK 095 ===
Palavras: 364
Caracteres: 2587
--------------------------------------------------
Trajecto-
ries may also model the temporal correspondence of physically moving objects
in a video Video Transformer (Patrick et al., 2021) uses self-attention of trajec-
tories for the task of video action recognition 8.8 Multi-modal Transformers
Rather than modeling language and vision independently, multi-modal Trans-
formersusethesemodalitiestogether toformmulti-modal Transformerswith ap-
plicationsinmulti-modalsearchandgeneration.Forexample,DALL-E(Ramesh,
Pavlov, Goh and Gray, 2021; Ramesh et al., 2022) is a generative model that is
trained jointly on both text and images The model then receives text describing
an image as input and generates an image matching the description Transformers have been used in a variety of multi-modal settings, including
(1) text and images, by models such as VilBERT (Lu et al., 2019), Vl-BERT (Su
et al., 2020), LXMERT (Tan and Bansal, 2019), VisualBERT (Li et al., 2020),
and Vokenization (Tan and Bansal, 2020); (2) text and video, by models such as
VATT (Akbari et al., 2021), VideoBERT (Sun et al., 2019), and video question
answering (Kant et al., 2020); and (3) audio and video, such as by a model
for learning contextual multi-lingual multi-modal representations (Huang et al.,
2021) Sharing parameters across these modalities (Lee et al., 2021) signiﬁcantly
reduces the number of parameters of such multi-modal Transformers 8.9 Text and Code Transformers
OpenAI Codex (Chen et al., 2021) is a Transformer model trained on text and
ﬁne-tuned on code Codex is used within GitHub-Copilot (OpenAI, 2021) to
guide programming by completing and writing code and documentation Codex
isusedforsolvingmanyuniversity-levelmath,statisticsandotherSTEMcourses
(Drori et al., n.d.; Shporer et al., 2022; Tang et al., 2022) by program synthesis
and few-shot learning Question solutions and programs share an underlying tree
150 8 Transformers
representation Codex correctly solves questions by specifying both question and
programming contexts, such as which programming packages to load In addition
to generating code that solves problems, the resulting code generates useful plots
for understanding the solutions 8.10 Summary
Transformers, also referred to as foundation models (Bommasani et al., 2021),
have become a mainstream architecture in deep learning Huggingface (Wolf
et al., 2020) is a commonly used open-source Transformer platform that consists
of multiple models, datasets, and libraries Transformers have disrupted various
ﬁelds, including natural language processing, computer vision, audio process-
ing, programming, and education

============================================================

=== CHUNK 096 ===
Palavras: 394
Caracteres: 2782
--------------------------------------------------
The number of parameters of Transformers
is increasing by an order of magnitude each year and is on track to surpass the
number of connections in the human brain New scalable deep learning archi-
tectures such as the Transformer are poised to revolutionize the way machines
perceive the world, make decisions (Chen, Lu, Rajeswaran, Lee, Grover, Laskin,
Abbeel, Srinivas and Mordatch, 2021), and generate novel output Part III
Generative Models

9 Generative Adversarial Networks
9.1 Introduction
Generative adversarial networks (GANs) are an unsupervised generative model
used for generating samples that are similar to training examples They have
many applications, including image, video, 3D, trajectory, audio, protein and
language synthesis Common image synthesis applications include image trans-
lation, super-resolution, style synthesis, image completion, pose synthesis, image
editing,text-to-imagesynthesis,andmedicalimaging.Widelyusedaudiosynthe-
sis applications include text-to-speech synthesis and music synthesis In addition
to these applications, GANs have also been used for generating images from text,
audio from images, and images from audio The task of classiﬁcation maps a set of examples to a label In contrast, gener-
ative models such as GANs map a label to a set of examples Since there may be
many examples with the same label, this generative mapping is stochastic and
therefore this generative process involves sampling from a random distribution GANs were introduced in 2014 (Goodfellow et al., 2014) as a minimax opti-
mization problem or a zero-sum game in which two agents, a generator and a
discriminator, compete with each other The generator is trained to produce fake
examples that fool the discriminator The generator learns to synthesize samples
which are indistinguishable from real data The generator’s synthesized samples
serve as negative examples for the discriminator The discriminator learns to
distinguish between the generator’s fake synthesized samples and real data The
discriminator penalizes the generator for synthesizing samples which it is able to
classify correctly The generator and the discriminator are trained alternately The generator
is trained to produce samples that are indistinguishable from real data The
discriminator is trained to distinguish between the generator’s fake samples and
realdata.Generativeadversarialnetworkshavebeenshowntobeabletogenerate
high-quality samples in a wide variety of applications 9.1.1 Progress
The ﬁeld of GANs has seen exponential growth in research and applications,
improving the results of GANs in quality and diversity, while stabilizing GAN
154 9 Generative Adversarial Networks
Figure 9.1 Photo-realistic faces synthesized using GANs: the images are of high
quality and diverse

============================================================

=== CHUNK 097 ===
Palavras: 351
Caracteres: 2383
--------------------------------------------------
training, and understanding the game-theoretic foundations From initial low-
resolution blurry image results in 2014, GANs have reached a level of photo-
realistic synthesis results (Wang, 2019), as shown in Figure 9.1 Given data from
a real distribution, the goal of the generator is to synthesize additional sam-
ples from the same distribution Milestones in the development of GANs since
their introduction (Goodfellow et al., 2014) include architectures such as deep
convolutional generative adversarial networks (DCGAN) (Radford et al., 2015),
progressive GAN (Karras et al., 2018), conditional GAN (CGAN) (Isola et al.,
2017), cycle-consistent GAN (CycleGAN) (Zhu et al., 2017), single image GAN
(Shaham et al., 2019); using the Wasserstein loss (Arjovsky et al., 2017) and the
optimistic gradient descent ascent (OGDA) algorithm (Daskalakis et al., 2017)
for training GANs Recent GANs have overcome their initial limitations, includ-
ing their diﬃculty in training, and their lack of exploration of the probability
space 9.1.2 Game Theory
Generative adversarial networks are a class of generative models that aim to
learn a distribution from training data and then generate new samples from this
distribution The setting is that of two neural networks: a generator network
Gand a discriminator network D The generator is trained to produce samples
9.2 Minimax Optimization 155
that are indistinguishable from the training data The discriminator is trained
to distinguish between real and fake samples The generator and discriminator
are trained simultaneously in an adversarial setting, where the generator tries
to fool the discriminator by producing realistic samples, while the discriminator
tries to distinguish real from fake samples The discriminator and generator form two dueling networks with opposing
objectives This unsupervised setting eliminates the need for labels, since the
label is whether the sample is real or not Real data acquired from the real world
without any label is abundant From a game-theoretic viewpoint, we have two
neural networks with a minimax objective 9.1.3 Co-evolution
From a biological viewpoint, GANs are two neural networks that evolve by co-
evolution An example of co-evolution in nature is the evolution of a predator,
suchasthecheetah,andprey,suchastheantelope,thatco-evolvedforrapidﬁght
and herd ﬂight in an evolutionary arms race

============================================================

=== CHUNK 098 ===
Palavras: 367
Caracteres: 2374
--------------------------------------------------
Another example of co-evolution
in nature is the long-beaked hummingbird and ﬂowers with long petals, which
co-evolved for pollination and feeding The cheetah–antelope arms race and the hummingbird–ﬂower arms race are
examples of co-evolution in nature, where one species evolves in response to
changes in the other species There have been many studies of co-evolution in
nature, and the theory of co-evolution has been very successful in describing how
species co-evolve in nature However, there has been a limitation on the appli-
cation of the theory of co-evolution in nature to artiﬁcial systems because of the
lack of co-evolution in the interaction between two neural networks Generative
adversarial networks are two neural networks that evolve by co-evolution, which
can be regarded as a generalization of the theory of co-evolution in nature, and
can be applied to artiﬁcial systems In other words, GANs are one of the ﬁrst
types of artiﬁcial systems in which co-evolution occurs In the case of GANs, the two neural networks are called the generator and
the discriminator The generator learns to produce images that are similar to
the training data The discriminator learns to distinguish between real images
and fake images produced by the generator The generator and discriminator are
trained together by a joint optimization algorithm 9.2 Minimax Optimization
A minimax optimization problem or saddle-point problem is deﬁned by:
min
xmax
yf(x,y) (9.1)
which is a zero-sum game Generative adversarial networks (Goodfellow et al., 2014) as illustrated in
156 9 Generative Adversarial Networks
Figure 9.2 Generative adversarial network Figure 9.2, were formulated as a minimax optimization problem or a zero-sum
game in which two agents, a generator Gand a discriminator D, compete:
min
Gmax
DV(G,D)=Ex[logD(x)]+Ez[log(1−D(G(z)))] (9.2)
The minimax game is a zero-sum game The discriminator’s loss is the genera-
tor’s gain; the generator’s loss is the discriminator’s gain The term D(x) is the
discriminator’s estimated probability that real data xis real, and Exis the ex-
pectation over the real data The term G(z) is the output of the generator given
random noise z The term D(G(z)) is the discriminator’s estimated probability
that a synthesized sample is real, and Ezis the expectation over random noise
input, that is over the generator’s synthesized samples

============================================================

=== CHUNK 099 ===
Palavras: 360
Caracteres: 2482
--------------------------------------------------
The goal of the gener-
ator is to generate a signal from random noise z∼P(z)i naw a yt h a ti tw i l l
be diﬃcult for the discriminator to distinguish between the generated and real
datax∼Pdata The goal of the discriminator is to classify correctly real and
generated data The game between the generator and discriminator is a minimax
optimization problem Representing the generator Gby a neural network with parameters θand the
discriminator Dby a neural network with parameters φyields:
min
θmax
φV(Gθ,Dφ)=Ex∼Pdata[logDφ(x)]+Ez∼Pz[log(1−Dφ(Gθ(z)))] (9.3)
=Ex∼Pdata[logDφ(x)]+Ex∼PG[log(1−Dφ(x))] (9.4)
Since both the generator and discriminator are represented by neural networks
the problem is non-convex non-concave (Lin et al., 2020) This formulation as a
zero-sum game has been called a saturating GAN (Goodfellow et al., 2014) since
initially it did not work due to saturation of gradients which become small and
do not converge to a solution The ﬁrst term of log D(x) is independent of the
generatorandthereforethegeneratorminimizesthefunctionlog(1 −D(G(z))).To
9.3 Divergence between Distributions 157
ﬁx this saturation problem, a non-saturating GAN formulation was introduced
(Goodfellow et al., 2014) which is not a zero-sum game, changing the generator
loss to maximize log D(G(z)) instead The goal of a GAN is to mimic a probability distribution and therefore it
uses a loss function that represents the distance between the distribution of the
synthesized samples and the distribution of the real data A GAN has two loss
functions, one for the discriminator and the other for the generator, and both are
derived from a measure of similarity between the distribution of the synthesized
samplesPGandthedistributionoftherealdata Pdata.TheﬁrstterminEquation
9.2 depends only on the real data, and therefore training of the generator only
involves the second term of Equation 9.2, which depends on the synthesized
samples 9.3 Divergence between Distributions
The relative entropy or Kullback–Leibler (KL) divergence DKLis a measure of
how one probability distribution pdiverges from another probability distribution
qand is deﬁned by:
DKL(q(x)||p(x)) =/integraldisplay
q(x)logq(x)
p(x)dx (9.5)
which is non-negative and asymmetric The Jensen–Shannon (JS) divergence
DJSis a symmetric smooth version of the KL divergence deﬁned by:
DJS(q||p)=1
2DKL(q||m)+1
2DKL(p||m) (9.6)
wherem=1
2(p+q) The KL divergence and JS divergence are both special
cases of the Bregman divergence

============================================================

=== CHUNK 100 ===
Palavras: 382
Caracteres: 2941
--------------------------------------------------
The Bregman divergence is deﬁned by a convex
function Fand is a measure of distance between two points pandqdeﬁned by:
DF(p,q)=F(p)−F(q)−/angbracketleft∇F(q),p−q/angbracketright (9.7)
Each convex function Fdeﬁnes a diﬀerent divergence Diﬀerent divergences
are explored with the goals of overcoming the problem of vanishing gradients and
improving GAN training stability and diversity For the special case of F(p)=
plog(p)w eg e t :
DF(p,q)=plog(p)−qlog(q)−(log(q)+1)(p−q)
=plog/parenleftbiggp
q/parenrightbigg
+(q−p)(9.8)
which is the generalized KL divergence For the special case of F(p)=plog(p)−
(p+ 1)log( p+ 1) we get the JS divergence, which leads to the original GAN
formulation (Goodfellow et al., 2014) 158 9 Generative Adversarial Networks
9.3.1 Least Squares GAN
The special case of F=( 1−p)2results in the Pearson χ2divergence leading to
the least-squares GAN (LS-GAN) formulation (Mao et al., 2017), which uses a
least squares loss function for the discriminator:
Ex[(D(x)−1)2]−Ez[D(G(z))2] (9.9)
and generator:
Ez[(D(G(z))−1)2] (9.10)
providing a smoother loss 9.3.2 f-GAN
Choosing diﬀerent convex functions Fleads to diﬀerent GAN formulations, also
known as f-GANs (Nowozin et al., 2016) 9.4 Optimal Objective Value
Setting the generator Gto be ﬁxed and optimizing the discriminator by setting
the derivative of:
LD(x)=PdatalogD(x)+PGlog(1−D(x)) (9.11)
to be zero, results in the optimal discriminator D⋆:
D⋆(x)=Pdata
Pdata+PG(9.12)
Plugging the optimal discriminator D⋆into Equation 9.2 results in:
min
GV(G,D⋆)=2DJS(Pdata||PG)−2log2 (9.13)
where the JS divergence DJS(Pdata,PG)i s :
DJS(Pdata,PG)=1
2(DKL(Pdata,Pdata+PG
2)+DKL(PG,Pdata+PG
2)) (9.14)
Therefore, the optimal value of Vis obtained when the distribution of real data
is equal to the generator distribution Pdata=PG In this case the discriminator
cannot distinguish between real and synthesized data, namely D⋆(x)=1
2,a n d
the JS divergence DJSis zero, optimizing the GAN objective 9.5 Gradient Descent Ascent
A common algorithm used to solve the minimax optimization problem in Equa-
tion 9.1 is gradient descent ascent (GDA), which alternates between gradient
9.6 Optimistic Gradient Descent Ascent 159
descent on xand gradient ascent on y The minimization variable is updated by
gradient descent:
xt+1=xt−ηx∇xf(xt,yt) (9.15)
and the maximization variable is updated by gradient ascent:
yt+1=yt+ηy∇yf(xt,yt) (9.16)
whereηxandηyare the learning rates In our setting we use a stochastic variant of GDA with mini-batches, in which
the descent update ∇xf(xt,yt) for the generator neural network is:
∇θV(Gθ,Dφ)=1
m∇θm/summationdisplay
i=1log(1−Dφ(Gθ(zi))) (9.17)
and the ascent update ∇yf(xt,yt) for the discriminator neural network is:
∇φV(Gθ,Dφ)=1
m∇φm/summationdisplay
i=1(logDφ(xi)+log(1 −Dφ(Gθ(zi)))) (9.18)
Iffwereconvex–concavethenplayingthegamesimultaneouslyorinasequential
orderwouldnotmatter;however,inourcase fisnon-convexnon-concaveandthe
order matters

============================================================

=== CHUNK 101 ===
Palavras: 356
Caracteres: 2449
--------------------------------------------------
Therefore, the updates are performed sequentially in our setting,
which is a zero-sum sequential game, also known as a Stackelberg game In
practice the algorithm takes multiple ascent steps, denoted by γ, for each descent
step, denoted by γ-GDA Unfortunately, GDA may converge to points that are not local minimax or fail
to converge to a local minimax A modiﬁcation of GDA (Wang, Zhang and Ba,
2020) which partially addresses this issue is:
yt+1=yt+η∇yf(xt,yt)+ηH−1
yyHyx∇xf(xt,yt) (9.19)
which converges and only converges to local minimax points, driving the gradient
quickly to zero and improving GAN convergence 9.6 Optimistic Gradient Descent Ascent
When introduced, GANs were implemented using momentum However, later on
the implementations did not use momentum, and using a negative momentum
made the saturating GAN work An algorithm that solves the minimax opti-
mization problem by using negative momentum is optimistic gradient descent
ascent; (Daskalakis et al., 2017) This adds negative momentum terms to the
gradient updates:
xt+1=xt−ηx∇xf(xt,yt)−ηx(∇xf(xt,yt)−∇xf(xt−1,yt−1)) (9.20)
yt+1=yt+ηy∇yf(xt,yt)+ηy(∇yf(xt,yt)−∇yf(xt−1,yt−1)) (9.21)
160 9 Generative Adversarial Networks
Optimistic gradient descent ascent yields better empirical results than GDA, and
can be interpreted as an approximation of the proximal point method 9.7 GAN Training
In the beginning of training of the generator and discriminator, the generator
synthesizes samples that are not similar to real data and the discriminator easily
classiﬁes the generated samples as fake As training progresses, the generator
improves and synthesizes samples that are able to fool the discriminator into
classifying them as real data When the generator training is successful, the
discriminator cannot distinguish between real data and fake samples synthesized
by the generator The generator and discriminator are represented by neural
networks and are both trained by backpropagation The output of the generator
serves as input to the discriminator, as shown in Figure 9.2 9.7.1 Discriminator Training
The discriminator loss serves as a signal to the generator for updating its param-
eters by backpropagation The discriminator shown in Figure 9.3 is a classiﬁer
that tries to distinguish between real data and samples synthesized by the gener-
ator Training the discriminator uses real data as positive examples and samples
synthesized by the generator as negative examples

============================================================

=== CHUNK 102 ===
Palavras: 375
Caracteres: 2843
--------------------------------------------------
When the discriminator is
trained,thegeneratorisnottrainedanditsparametersareheldﬁxed.Thegener-
atorsynthesizessamplessothatthediscriminatorcantrainusingthesegenerated
samples When the discriminator is training it ignores the generator’s loss func-
tion and uses only its own loss function, classifying real data and fake samples
synthesized by the generator The discriminator loss penalizes the discriminator
for mis-classifying real data as fake and vice versa, and the discriminator weights
are updated by backpropagation 9.7.2 Generator Training
The generator shown in Figure 9.4 learns to synthesize realistic samples by the
feedback it receives from the discriminator The input to the generator is ran-
dom noise, which it learns to transform to synthesized samples randomly spread
across the output distribution Usually the input random noise is sampled from
a lower dimensional space than the output synthesized sample During generator
training the discriminator parameters are held ﬁxed The discriminator network
classiﬁes the synthesized samples and the generator’s loss function penalizes the
generator if it does not succeed in fooling the discriminator into classifying its
synthesized samples as real During generator training the gradients are back-
propagated through both the discriminator network and the generator network 9.7 GAN Training 161
Figure 9.3 GAN discriminator network Figure 9.4 GAN generator network Even though the discriminator weights are not updated during generator train-
ing, the discriminator’s ﬁxed weights inﬂuence the update of generator parame-
ters 9.7.3 Alternating Discriminator–Generator Training
Generative adversarial network training alternates between training the discrim-
inatorandthegenerator.Thediscriminatorlossfunctionisusuallydiﬀerentfrom
the generator loss function The discriminator trains for several epochs, then the
generator trains for several epochs This alternating training is repeated Algo-
rithm 9.1 provides pseudocode for GAN training During training, as the generator increases the similarity between the synthe-
sized samples and real data, the discriminator classiﬁcation accuracy decreases If the generator training is successful, the discriminator classiﬁcation accuracy is
random This in turn results in providing uninformative feedback to the discrimi-
nator, which illustrates the diﬃculty of convergence of this saddle-point problem Solutions to the convergence problem include adding noise to the discriminator’s
162 9 Generative Adversarial Networks
Algorithm 9.1 Alternating GAN training During training of the discriminator
the generator parameters are held ﬁxed and vice versa foreach epoch i=1,...,n do:
Sample mini-batch from real data x1,...,xm∼Px
Sample mini-batch from noise z1,...,zm∼Pz
Take gradient ascent step on discriminator parameters φb yE q .9

============================================================

=== CHUNK 103 ===
Palavras: 362
Caracteres: 2524
--------------------------------------------------
1 8
Take gradient descent step on generator parameters θb yE q .9 1 7
input or penalizing the discriminator weights, which are regularization methods
that improve GAN convergence 9.8 GAN Losses
As described, diﬀerent Bregman divergences and loss functions have been ex-
plored with the goals of improving GAN training stability and diversity Notably,
using the Earth Mover’s Distance (EMD) in the Wasserstein GAN formulation
has had a fundamental contribution to improving GAN training 9.8.1 Wasserstein GAN
Iftherealdatadistributionandgeneratordistributiondonotoverlap,thentheJS
divergence is zero, DJS= 0, which occurs even if both distributions are identical
but translated This demonstrates the problem with using the JS divergence for
optimizing GANs when distributions have non-overlapping support Fortunately,
this issue has been resolved by using the EMD or Wasserstein-1 distance:
W(P,Q)= m i n
γ∈/producttext(P,Q)E(x,y)∼γ/bardblx−y/bardbl (9.22)
whereγdenotes how much mass, or earth, must be moved from xtoyin order
to transform distribution Pinto distribution Q,a n d/producttext(P,Q) denotes the set of
all disjoint distributions with marginals PandQ The EMD is the cost of the
optimal transport plan and has nicer properties for GAN optimization than the
JS divergence Computing W(P,Q) is intractable since it requires considering all
possiblecombinationsofpairsofpointsbetweenthetwodistributions,computing
themeandistanceofallpairsineachcombination,andtakingtheminimummean
distance across all combinations Fortunately, an alternative is to solve a dual
maximization problem that is tractable, which results in the Wasserstein loss A GAN uses the minimax loss in which the discriminator outputs a probability
in [0,1] of a sample being real or synthesized In contrast, a WGAN (Arjovsky
et al., 2017) uses a Wasserstein loss formulation for the discriminator, which
outputs a real value that is larger for real data than synthesized samples The
WGAN discriminator is called a critic since it does not output values in [0 ,1] for
9.8 GAN Losses 163
performing classiﬁcation There is no sigmoid in the ﬁnal layer of the discrimi-
nator, and the range is [ −∞,∞] The Wasserstein loss function is:
min
Gmax
DV(G,D)=Ex[D(x)]−Ez[D(G(z))] (9.23)
whereD(x) is the critic output given real data, G(z) is the generator output
given noise and D(G(z)) is the output of the critic given synthesized sam-
ples This means that the critic maximizes the diﬀerence between its expected
output on real data and synthesized samples

============================================================

=== CHUNK 104 ===
Palavras: 370
Caracteres: 2503
--------------------------------------------------
The generator loss function is
−Ez∼P(z)[D(G(z))], which means that the generator minimizes the negative out-
put of the critic on samples synthesized by the generator The Wasserstein loss function is derived from the EMD between the distri-
bution of the real data and the distribution of the synthesized samples An
advantage of using the EMD is a metric between distributions, which handles
disjoint distributions without overlapping support The weights in a WGAN are
clipped to within a constant range, and the WGAN avoids vanishing gradients The Wasserstein loss avoids vanishing gradients even if the critic is optimally
trained Wewantthegeneratortosynthesizediversesamples,forexample,tosynthesize
a diﬀerent sample for each diﬀerent random input The generator may learn to
synthesize a small set of samples very well, which the discriminator fails on If
the generator repeatedly synthesizes the same samples, the discriminator may
learn to reject those samples However, suppose the discriminator gets stuck
in a local minimum and does not ﬁnd the optimal strategy The generator may
optimize the output that will fail the discriminator in the next generator-training
iteration If, at each iteration, the generator optimizes for a speciﬁc discriminator and
the discriminator cannot correctly classify the synthesized samples as fake, the
generator will synthesize a small set of samples, not diverse samples, known as
mode collapse The Wasserstein loss trains the critic toward optimality with-
out vanishing gradients When the critic does not get stuck in local minima, it
learns to reject the generator’s repeated samples, encouraging the generator to
synthesize new samples and diversify the result 9.8.2 Unrolled GAN
In order to avoid mode collapse and encourage the generator to diversify the
synthesized samples and not optimize for a constant discriminator, the genera-
tor loss function may be modiﬁed to include multiple subsequent discriminators
(Metz et al., 2017) There is a classical tradeoﬀ between the approximation qual-
ity of the generator loss and the computation time, which is linear in the number
of unrolling steps 164 9 Generative Adversarial Networks
9.9 GAN Architectures
9.9.1 Progressive GAN
A coarse-to-ﬁne approach for training allows generating images at increasing res-
olutions Progressive GANs (Karras et al., 2018) begin by training the generator
and discriminator using low-resolution images and incrementally add layers of
higher-resolution images during training

============================================================

=== CHUNK 105 ===
Palavras: 366
Caracteres: 2445
--------------------------------------------------
Proceeding from coarse to ﬁne achieves
high-resolution results while maintaining training stability 9.9.2 Deep Convolutional GAN
Deep convolutional GANs are a type of GAN that use convolutional neural net-
works (CNNs) as the generator and discriminator In a similar fashion that using
CNNs signiﬁcantly improves classiﬁcation accuracy over fully connected neural
networks, using a CNN as the discriminator network and a deconvolution neural
network as the generator, known as a DCGAN (Radford et al., 2015), signif-
icantly improves the quality of the synthesized results over a fully connected
GAN (Goodfellow et al., 2014) Deep convolutional GANs are capable of gener-
ating high-resolution images with realistic textures and have been extended by
methods that improve and stabilize GAN training (Salimans et al., 2016) 9.9.3 Semi-Supervised GAN
Instead of having the discriminator be a binary classiﬁer for real or fake samples,
in a semi-supervised GAN (SGAN) the discriminator is a multi-class classiﬁer
(Salimans et al., 2016; Kumar et al., 2017; Odena et al., 2017; Oliver et al.,
2018) The discriminator outputs the likelihood of a sample to be synthesized
or real, and if the sample is classiﬁed as real then the discriminator outputs the
probability of the kclasses, estimating to which class the sample belongs In the
semi-supervised setting (Odena, 2016) the SGAN discriminator receives three
types of inputs rather than two: fake samples synthesized by the generator; real
samples without class labels; and real samples with class labels, thus improving
the generated results for speciﬁc classes Training is improved by having the
SGAN discriminator trained using two loss functions (Salimans et al., 2016)
rather than a single loss function: an unsupervised loss and a supervised loss
function 9.9.4 Conditional GAN
A conditional GAN (Mirza and Osindero, 2014) models the conditional proba-
bility distribution P(x|y) by training the generator and discriminator on labeled
data Replacing D(x) withD(x|y)a n dG(z) withG(z|y) in Equation 9.2:
min
Gmax
DV(G,D)=Ex[logD(x|y)]+Ez[log(1−D(G(z|y)))] (9.24)
9.9 GAN Architectures 165
turns a GAN into a conditional GAN Providing labels allows us to synthesize
samples in a speciﬁc class or with a speciﬁc attribute, providing a level of control
over synthesis 9.9.5 Image-to-Image Translation
Image analogies (Hertzmann et al., 2001) provide a framework for synthesizing
images by example

============================================================

=== CHUNK 106 ===
Palavras: 359
Caracteres: 2492
--------------------------------------------------
Given a training set of unﬁltered and ﬁltered image pairs
A:A/primeand a new unﬁltered image B, the output is a ﬁltered image B/primesuch that
the analogy A:A/prime::B:B/primeis maintained Image-to-image translation also known as Pix2Pix (Isola et al., 2017; Huang
et al., 2018; Wang, Liu, Zhu, Tao, Kautz and Catanzaro, 2018; Liu, Huang,
Mallya, Karras, Aila, Lehtinen and Kautz, 2019; Park et al., 2019) applies this
concept using GANs An input image is mapped to a synthesized image with
diﬀerent properties The loss function is a combination of the conditional GAN
loss with an additional loss term, which is a pixelwise loss that encourages the
generator to match the source image:
min
Gmax
DV(G,D)=Ex,y[logD(x,y)]+Ex,z[log(1−D(x,G(x,z)))]
+λEx,y,z[/bardbly−G(x,z)/bardbl1](9.25)
weighted by λ 9.9.6 Cycle-Consistent GAN
Motivatedbystyleandcontentseparation(TenenbaumandFreeman,2000;Drori
et al., 2003 a), cycle-consistent GAN (Zhu et al., 2017) learns unpaired image-to-
image translation using GANs without pixelwise correspondence The training
data are image sets X∈AandY∈A/primefrom two diﬀerent domains AandA/prime
without pixelwise correspondence between the images in XandY The advan-
tage of this unsupervised approach is that images in correspondence may be
expensive to acquire or may not be available altogether Cycle-Consistent GAN (CycleGAN) consists of two generators G(X)=ˆYand
F(Y)=ˆXand two discriminators DYandDX The generator Gmaps a real
imageXto a synthesized sample ˆYand the discriminator DYcompares be-
tween them The generator Fmaps a real image Yto a synthesized sample ˆX
and the discriminator DXcompares between them CycleGAN maintains two
approximate cycle consistencies The ﬁrst cycle consistency F(G(X))≈Xap-
proximately maintains that mapping a real image Xto a synthesized image
ˆYand back is similar to X, and the second cycle consistency G(F(Y))≈Y
approximately maintains that mapping a real image Yto a synthesized image
ˆXand back is similar to Y Consider learning the translation between English
and Chinese by applying one generator that translates the sentence from En-
glish to Chinese followed by a second generator that translates the result back
166 9 Generative Adversarial Networks
Figure 9.5 CycleGAN for horses and zebras Generators are shown in green, critics in
red, real images in orange, and fake images in gray from Chinese to English, while maintaining that the ﬁnal result is similar to the
original English sentence, and vice versa

============================================================

=== CHUNK 107 ===
Palavras: 355
Caracteres: 2548
--------------------------------------------------
The discriminators make sure that the
generators do not learn the identity function, and synthesize diverse samples The overall loss function is deﬁned by (Zhu et al., 2017):
min
G,Fmax
DX,DYL(G,F,DX,DY)=LGAN(G,DY,X,Y)+LGAN(F,DX,Y,X)
+λLcyc(G,F)(9.26)
where the cycle-consistency loss is deﬁned by:
Lcyc(G,F)=Ex∼PX(/bardblF(G(X))−X/bardbl1)+EY∼PY(/bardblG(F(Y))−Y/bardbl1) (9.27)
weighted by λ For example, consider Xto be horse images and Yto be images of zebras Clearly,thereisnopixelwisecorrespondencebetweenimagesofhorsesandzebras
in the wild A CycleGAN for horses and zebras (Zhu et al., 2017) is illustrated
in Figure 9.5 One generator maps horses to zebras and the other maps zebras
to horses One discriminator distinguishes between real and fake horses and the
other distinguishes between real and fake zebras One cycle maintains that, given
a real horse, a generator synthesizes a fake zebra which the other generator
transforms back to a horse matching the original horse A second cycle maintains
that, given a real zebra, a generator synthesizes a fake horse which the other
generator transforms back to a zebra matching the original zebra One critic
learns to distinguish between real and fake horses and another critic learns to
distinguish between real and fake zebras StarGAN (Choi et al., 2018) extends
CycleGAN to more than two domains If the generators GandFwere invertible mappings F=G−1then exact cycle
consistencies would be maintained such that F(G(X)) =YandG(F(Y)) =X This is achieved by modeling each domain using normalizing ﬂows (Rezende and
9.9 GAN Architectures 167
Mohamed, 2015), and maintaining exact cycle consistency improves the quality
of the synthesized results (Grover et al., 2020) 9.9.7 Registration GAN
Registration GAN (Kong et al., n.d.) uses a registration network Rafter the
generator G, treating the misaligned target images as noisy labels and correcting
the result A correction loss is deﬁned by:
min
G,RLcor(G,R)=Ex,˜y,z[/bardbly−G(x,z)◦R(G(x,z),˜y)/bardbl1],(9.28)
where◦represents resampling and R(G(x,z),˜y) is a deformation ﬁeld that dis-
places each pixel A smoothness loss on the deformation ﬁeld is deﬁned by:
min
RLsmooth(R)=Ex,˜y,z[/bardbl∇R(G(x,z),˜y)/bardbl2] (9.29)
and the total loss is the sum of the GAN, correction and smoothness losses:
min
G,Rmax
DL(G,R,D)=LGAN(G,D)+Lcor(G,R)+Lsmooth(R) (9.30)
Registration GAN (RegGAN) outperforms both Pix2Pix on aligned images and
CycleGAN on unpaired images, speciﬁcally on medical images where the noise
may be considered as a deformation ﬁeld

============================================================

=== CHUNK 108 ===
Palavras: 366
Caracteres: 2549
--------------------------------------------------
9.9.8 Self-Attention GAN and BigGAN
Self-attention GAN (Zhang, Goodfellow, Metaxas and Odena, 2019) incorpo-
rates an attention mechanism in both the generator and discriminator networks
to capture long-range spatial dependencies between pixels Using attention im-
proves the diversity of the synthesized images Self-attention GAN (SAGAN) is
improved in BigGAN (Brock et al., 2019) by increasing the batch size to im-
prove quality, by using a truncated normal distribution for zduring training
which trades oﬀ diversity for quality, and by incorporating zinto each layer of
the generator for further improving quality 9.9.9 Composition and Control with GANs
Generative adversarial networks are used for synthesis by sampling a latent vari-
ablezpassed to generator Gto generate an output x Until recently, control-
ling the output synthesized by GANs, for example the pose, illumination and
composition of multiple objects in a scene, has been challenging Recent work
(Niemeyer and Geiger, 2021) adds control over the synthesized scene by incorpo-
rating 3D scene composition into the model During the forward pass, individual
shape and appearance variables for each object and background are sampled, for
example, sampling the pose for each object, then applying the transformation,
and rendering the scene During training, objects and their poses are randomly
sampled 168 9 Generative Adversarial Networks
9.9.10 Instance Conditioned GAN
Instance conditioned GAN (Casanova et al., 2021) learns multiple local distri-
butions deﬁned by clusters of data points along with their nearest neighbors Given an unlabeled dataset of points x(i), their nearest neighbors N(i)a r ed e -
ﬁned based on the cosine similarity of a set of features f(x(i)) A discriminator
Ddistinguishes between real neighbors x(n)sampled uniformly from N(i)a n d
generated neighbors A generator Gsynthesizes samples from the distribution
p(x|f(x(i))) given Gaussian noise z The adversarial objective is then deﬁned by:
min
Gmax
DEx(i)∼Pdata,x(n)∼U(N(i))[logD(x(n),f(x(i)))]
+Ex(i)∼Pdata,z∼Pz[log(1−D(G(z,f(x(i))),f(x(i))))](9.31)
During training, all data points are used for conditioning the model The dis-
criminator and generator are conditioned on instance features, and therefore by
changing instances the model transfers to unseen datasets Given labels, Equa-
tion 9.31 may be extended to be conditioned on classes In this case the discrimi-
nator and generator are conditioned on class labels and the generator synthesizes
samples from the distribution p(x|f(x(i)),y(j))

============================================================

=== CHUNK 109 ===
Palavras: 352
Caracteres: 2434
--------------------------------------------------
Instance conditioned GAN (IC-
GAN) outperforms traditional GANs and conditional GANs, and the trained
models transfer well to new unseen datasets without retraining 9.10 Evaluation
The inception score (IS) and Frechet inception distance (FID) measure the qual-
ity of synthesized examples using pre-trained neural network classiﬁers The
geometry score (Khrulkov and Oseledets, 2018) measures the quality of synthe-
sized examples by comparing the manifold of the synthesized samples with the
manifold of the real data Recent evaluation measures aim to capture both the
quality and diversity of synthesized results 9.10.1 Inception Score
The IS (Salimans et al., 2016) automatically evaluates the quality of images syn-
thesized by the generator by using the pre-trained Inception v3 model (Szegedy
et al., 2016) for classiﬁcation The probabilities of many synthesized images be-
longing to each class are used to compute the score based on the conditional
label distribution p(y|x) and the marginal label distribution p(y):
IS(G)=e x p ( Ex∼pG[DKL(p(y|x)||p(y))] (9.32)
A higher IS is better, which corresponds to a larger KL divergence between the
distributions 9.11 Applications 169
9.10.2 Frechet Inception Distance
The FID (Heusel et al., 2017) is also based on the Inception v3 modally The
FID uses the feature vectors of the last layer for real and synthesized images to
generate multivariate Gaussians that model the real and synthesized distribu-
tions The FID is computed as the diﬀerence between these Gaussians measured
using the Wasserstein-2 distance by:
FID =/bardblμreal−μgenerated/bardbl2+tr(Σ real+Σgenerated−2(ΣrealΣgenerated)1
2) (9.33)
A lower FID is better, which corresponds to similar real and synthesized distri-
butions 9.11 Applications
9.11.1 Super Resolution and Restoration
Super-resolution by example (Freeman et al., 2002) increases the resolution of an
image given corresponding low-resolution and high-resolution training example
pairs Super Resolution GAN (Ledig et al., 2017) uses a GAN framework for
super-resolution SinGAN (Shaham et al., 2019) uses the self-similarity of image
patches within an image for synthesizing versions of an image Building upon
SinGAN, KerGAN (Bell-Kligler et al., 2019) performs blind super resolution
without any training examples by utilizing the self-similarity of image patches
across scales to learn an image-speciﬁc down-sampling kernel used for super-
resolution

============================================================

=== CHUNK 110 ===
Palavras: 355
Caracteres: 2378
--------------------------------------------------
Generative Facial Prior (GFP) GAN (Wang, Li, Zhang and Shan,
2021) performs blind face restoration using U-Nets and a pre-trained face GAN
with excellent results 9.11.2 Style Synthesis
As described, CycleGAN (Zhu et al., 2017) has been used for style synthesis StyleGAN (Karras et al., 2019) combines progressive GANs (Karras et al., 2018)
with style transfer (Huang and Belongie, 2017) based on CNNs with adaptive
normalization layers to disentangle the latent factors controlling image style syn-
thesis Hyper-LifelongGAN (Zhai et al., 2021) provides a lifelong learning frame-
work for image-conditioned generation HistoGAN (Aﬁﬁ et al., 2021) learns to
change image colors based on histogram features ComoGAN (Pizzati et al.,
2021) learns non-linear continuous image translation with unsupervised target
data using physics-inspired models 9.11.3 Image Completion
Image completion (Drori et al., 2003 b) ﬁlls in missing regions of an image Gen-
erative adversarial networks have been used for image completion (Pathak et al.,
170 9 Generative Adversarial Networks
2016; Iizuka et al., 2017; Yu et al., 2019; Liu, Wan, Huang, Song, Han and Liao,
2021), face completion (Li, Liu, Yang and Yang, 2017; Yeh et al., 2017), and
fashion image completion (Han et al., 2019) 9.11.4 De-raining
Conditional GANs have been applied to realistically remove rain streaks from
images with rain (Zhang, Sindagi and Patel, 2019) 9.11.5 Map Synthesis
Generative adversarial networks have been applied to synthesize texture and
high-resolution maps without any noticeable artifacts (Fr¨ uhst¨ uck et al., 2019) 9.11.6 Pose Synthesis
Generative adversarial networks have been used for synthesizing humans in ar-
bitrary target poses (Ma et al., 2017) 9.11.7 Face Editing
Generative adversarial networks have been applied for synthesizing faces with
varying facial expressions, gender, hair styles and colors, glasses (Liu and Tuzel,
2016; Brock et al., 2017), and ages (Antipov et al., 2017; Zhang, Song and Qi,
2017) PairedCycleGAN (Chang et al., 2018) extends CycleGAN to style control
for the application and removal of makeup GANmut (d’Apolito et al., 2021)
learns an interpretable and expressive conditional space of facial emotions rather
than conditioning on handcrafted labels AnycostGANs (Lin et al., 2021) uses
adaptive sampling and multi-resolution to achieve interactive face synthesis

============================================================

=== CHUNK 111 ===
Palavras: 365
Caracteres: 2492
--------------------------------------------------
A
single face image may be suﬃcient for generating a normalized 3D avatar of a
person’s head (Luo et al., 2021) 9.11.8 Training Data Generation
Generative adversarial networks have been used for learning to synthesize pho-
torealistic training examples from synthetic eye and hand images (Shrivastava
et al., 2017) 9.11.9 Text-to-Image Synthesis
StackGAN (Zhang, Xu, Li, Zhang, Wang, Huang and Metaxas, 2017, 2018) and
AttentionalGAN (Xu et al., 2018) receive text as input and synthesize an image
described by the text, which works well for speciﬁc classes of images Text-guided
diverse image generation and manipulation using a GAN (Xia et al., 2021) maps
9.11 Applications 171
text and sketches in the latent space of a StyleGAN for controlling generated
face images by text 9.11.10 Medical Imaging
Generative adversarial networks have been applied to a wide range of medical
imageanalysistasks,includingclassiﬁcation,detection,segmentation,de-noising,
and reconstruction (Kazeminia et al., 2020) Speciﬁcally, CycleGAN has been
applied for magnetic resonance to computed tomography (MR-to-CT) synthesis
(Wolterink et al., 2017) 9.11.11 Video Synthesis
Generative adversarial networks have been applied for video prediction (Von-
drick et al., 2016) using spatio-temporal CNNs that separate moving foreground
objects from static backgrounds Image-to-image transfer has been extended to
video-to-video transfer, learning a mapping between a segmentation map and
real street video with photorealistic results (Wang, Liu, Zhu, Liu, Tao, Kautz
and Catanzaro, 2018) Video portraits (Kim, Carrido, Tewari, Xu, Thies, Niess-
ner, P´ erez, Richardt, Zollh¨ ofer and Theobalt, 2018) use GANs to transfer head
position and rotation, face expression, eye gaze, and blinking from one person to
a portrait video of another person, reanimating a person’s face Self-supervised
video GANs (Hyun et al., 2021) represent video as a composition of appearance
and motions, synthesizing video with temporal coherence 9.11.12 Motion Retargeting
CycleGAN has been applied to retarget a given motion to a new cartoon char-
acter (Villegas et al., 2018) Image transfer has been extended to video transfer,
retargeting the body motion of one person to a new person, achieving videore-
alistic results (Chan et al., 2019) 9.11.13 3D Synthesis
3D-GANs (Wu et al., 2016) use GANs to synthesize high-quality 3D objects
and learn an object representation useful for interpolating between objects and
3D object recognition

============================================================

=== CHUNK 112 ===
Palavras: 356
Caracteres: 2551
--------------------------------------------------
Roof-GAN (Qian et al., 2021) learns to generate roof
geometry and relations for residential housing ShapeInversion (Zhang et al.,
2021) uses a GAN pre-trained on complete shapes to search for a vector in the
latentspacethatresultsinacompletedshapethatreconstructsthepartialinput This results in diverse 3D shape completions without using training pairs 172 9 Generative Adversarial Networks
9.11.14 Graph Synthesis
Graphs are the underlying representation of networks with many applications:
social networks of friends, the internet of web pages, cellular communication net-
works of users, ﬁnancial transaction networks of bank clients, protein-to-protein
interaction networks, or neural networks of brains NetGAN (Bojchevski et al.,
2018) synthesizes graphs by learning the distribution of random walks of a given
graph dataset, which can then be used for link prediction 9.11.15 Autonomous Vehicles
Generative adversarial networks have been used in reinforcement learning to
learn human driving behaviors from human driving demonstrations (Li, Song
and Ermon, 2017) by imitation learning in an unsupervised fashion SocialGAN
combines sequence models and GANs to predict plausible human motion trajec-
tories (Gupta et al., 2018) for accurate prediction and collision avoidance 9.11.16 Text-to-Speech Synthesis
Generative adversarial networks have been applied to synthesize speech from
text, achieving results that are perceptually close to natural speech (Yang, Xie,
Chen, Lou, Zhu, Huang and Li, 2017) DriveGAN (Kim et al., 2021) learns to
simulate a controllable and dynamic driving environment from video 9.11.17 Voice Conversion
CycleGAN has been applied to voice conversion (Fang et al., 2018; Kaneko et al.,
2019) by modifying a speech signal of one speaker to match that of another
speaker 9.11.18 Music Synthesis
MuseGAN generates long, polyphonic music for multiple instruments (Dong
et al., 2018), including pop song phrases with bass, drums, guitar, and piano,
taking into account chords, style, melody, and groove The synthesized music is
coherent, with pleasant harmony and uniﬁed rhythm GANSynth (Engel et al.,
2019) uses a progressive GAN to synthesize an audio sequence from a latent
vector, producing coherent results 9.11.19 Protein Design
Proteinstructuredeterminesfunction;therefore,predictingproteinstructureand
function is important in protein design for drug discovery Generative adversarial
networks have been applied for synthesizing distance matrices between protein
atoms (Anand and Huang, 2018) to aid in protein design

============================================================

=== CHUNK 113 ===
Palavras: 359
Caracteres: 2646
--------------------------------------------------
9.12 Software Libraries, Benchmarks, and Visualization 173
Table 9.1 Summary of discriminator and generator loss functions for diﬀerent GANs GAN Discriminator loss (maximize) Generator loss (minimize)
Original E x[logD(x)]+Ez[log(1−D(G(z)))] E z[log(1−D(G(z)))]
Least squares E x[(D(x)−1)2]−Ez[D(G(z))2]E z[(D(G(z))−1)2]
Wasserstein E x[D(x)]−Ez[D(G(z))] -E z[D(G(z))]
9.11.20 Natural Language Synthesis
Generative adversarial networks have been applied to natural language The
generator Ggenerates language and the discriminator Ddistinguishes between
real text and generated text (Lin, Li, He, Zhang and Sun, 2017; Yu et al., 2017;
Fedus et al., 2018; Guo et al., 2018) Computing derivatives through discrete text
tokens is a challenge (Caccia et al., 2018), and there is often a trade-oﬀ between
the quality and the diversity of the generated text 9.11.21 Cryptography
CycleGAN has been applied to infer simple ciphers given unpaired examples
of ciphertext and plaintext (Gomez et al., 2018) The texts were encoded using
simpleshiftorVigenereciphersanddecodedusingCycleGANinasimilarfashion
to language translation 9.12 Software Libraries, Benchmarks, and Visualization
TF-GAN (Shor et al., 2020) is a library for training and evaluating GANs in
TensorFlow TorchGAN (PyTorch, 2021) is a framework for eﬃcient training of
GANs based on PyTorch Compare GAN (Google, 2020) is a library for com-
paring between GAN architectures, loss functions, and evaluation metrics GAN
Lab (Kahng et al., 2018) is an interactive visualization of GANs available online 9.13 Summary
This chapter introduces GAN theory, practice, and applications We present the
most signiﬁcant GAN architectures, loss functions, training algorithms, and ap-
plications Table 9.1 summarizes the discriminator and generator loss functions
for diﬀerent GANs The roles of the generator and discriminator and the ad-
vantages and limitations of diﬀerent loss functions are important to understand Issues include mode collapse and vanishing gradients, and various solutions are
available.GANshaveabroadrangeofapplicationswithcodelibrariesandbench-
marks in the ﬁeld 10 Variational Autoencoders
10.1 Introduction
This chapter begins with a review of variational inference (VI) as a fast approx-
imation alternative to Markov chain Monte Carlo (MCMC) methods, solving
an optimization problem for approximating the posterior Variational inference
using both the mode-seeking reverse Kullback–Leibler (RKL) divergence and
mass-covering forward Kullback–Leibler (FKL) divergence are presented Re-
verse KL is covered in detail since it is more reliable and stable than FKL in
high dimensions

============================================================

=== CHUNK 114 ===
Palavras: 363
Caracteres: 2539
--------------------------------------------------
Variational inference is scaled to stochastic variational infer-
ence and generalized to black-box variational inference (BBVI) Amortized VI
leads tothe variational autoencoder (VAE) framework, which is introduced using
deep neural networks and graphical models and used for learning representations
and generative modeling Finally, we explore generative ﬂows, the latent space
manifold, and Riemannian geometry of generative models 10.2 Variational Inference
We begin with observed data x, continuous or discrete, and suppose that the
process generating the data involved hidden latent variables z For example, x
may be an image of a face and za hidden vector describing latent variables
such as pose, illumination, gender, or emotion A probabilistic model is a joint
densityp(z,x) of the hidden variables zand the observed variables x Our goal
is to estimate the posterior p(z|x) to explain the observed variables xby the
hidden variables z, for example, answering the question of what are the hidden
latent variables zfor a given image x Inference about the hidden variables is
given by the posterior conditional distribution p(z|x) of hidden variables, given
observations By deﬁnition:
p(z,x)=p(z|x)p(x)=p(x|z)p(z)=p(x,z) (10.1)
wherep(z,x) is the joint density, p(z|x) the posterior, p(x) the evidence or
marginal density, p(z) the prior density, and p(x|z) the likelihood function We
may extend p(x|z)p(z) to multiple layers by:
p(x|z1)p(z1|z2),...,p(z|z)p(z) (10.2)
10.2 Variational Inference 175
byusingdeepgenerativemodels.Fornowwewillfocusonasinglelayer p(x|z)p(z) Rearranging terms, we get Bayes rule:
p(z|x)=p(x|z)p(z)
p(x)(10.3)
For most models the denominator p(x) is a high-dimensional intractable inte-
gral that requires integrating over an exponential number of terms for z:
p(x)=/integraldisplay
p(x|z)p(z)dz (10.4)
Therefore, instead of computing p(z|x) the key insight of VI (Jordan et al.,
1999; Opper and Saad, 2001; Bishop, 2006; Wainwright and Jordan, 2008; Blei
et al., 2017; Kim, Wiseman and Rush, 2018; Zhang, Butepage, Kjellstrom and
Mandt, 2018) is to approximate the posterior by a variational distribution qφ(z)
from a family of distributions Q, deﬁned by variational parameters φsuch that
qφ(z)∈Q A choice for Qis the exponential family of distributions In summary,
we choose a parameterized family of distributions Qand ﬁnd the distribution
qφ⋆(z)∈Qwhich is closest to p(z|x) Once found, we use this approximation
qφ⋆(z) instead of the true posterior p(z|x), as illustrated in the left side of Figure
10.1

============================================================

=== CHUNK 115 ===
Palavras: 373
Caracteres: 2676
--------------------------------------------------
The posterior p(z|x) is often intractable to compute analytically For example,
ifzis a vector of length d, thenp(z|x)i sad×dmatrix, and the posterior is
a function of the parameters of the model p(z|x,θ) In machine learning, the
parameters θare often learned from the data xusing a learning algorithm The
goal of inference is to estimate the posterior p(z|x) from the data xusing a com-
putationally tractable approximation q(z|x) The approximation q(z|x) is called
the variational distribution The variational distribution q(z|x) is deﬁned as the
solution to a variational inference problem The variational inference problem is
a mathematical optimization problem of ﬁnding the parameters of q(z|x)t h a t
minimize the lower bound of a divergence Dbetween the variational distribution
q(z|x) and the posterior p(z|x) Compared with this formulation, methods such as mean-ﬁeld variational in-
ference (MFVI) (Opper and Saad, 2001; Giordano et al., 2018) and MCMC
sampling have several shortcomings The mean-ﬁeld method (Parisi, 1988) as-
sumes a full factorization of variables, which is inaccurate Stochastic variational
inference scales MFVI to large datasets (Hoﬀman et al., 2013) Markov chain
Monte Carlo sampling methods (Brooks et al., 2011), such as the Metropolis–
Hastings algorithm, may not be scalable to very large datasets and may require
manually specifying a proposal distribution Thef-divergence from a probability distribution q(z) to a probability distri-
butionp(z) is deﬁned by:
Df(q(z)||p(z)) =/integraldisplay
f/parenleftbiggq(z)
p(z)/parenrightbigg
p(z)dz=Ep/bracketleftbigg
f/parenleftbiggq(z)
p(z)/parenrightbigg/bracketrightbigg
(10.5)
wherefis a convex function with f(1) = 0 For f(t)=tlog(t)w eg e tt h e
176 10 Variational Autoencoders
Figure 10.1 Variational inference using RKL (left) and FKL (right) between
distributions pandq In RKL we optimize an approximation qφ⋆(z)∈Qclosest to
the posterior p(z|x) The KL is non-negative DKL(p||q)≥0 and is not symmetric
DKL(q||p)/negationslash=DKL(p||q), hence the KL is not a distance The FKL divergence between distributions pandqis deﬁned by:
DKL(p(x)||q(x)) =/integraldisplay
p(x)logp(x)
q(x)dx (10.6)
whereas the RKL divergence is deﬁned by:
DKL(q(x)||p(x)) =/integraldisplay
q(x)logq(x)
p(x)dx (10.7)
The RKL divergence is mode-seeking, whereas the FKL divergence is mass-
covering (Jerfel et al., 2021; Zhang et al., 2022) Therefore the RKL is easier
to optimize and will be described in detail Other divergences may be used; for
example,theKLdivergenceisthespecialcaseofthe α-divergence(LiandTurner,
2016) with α= 1, and the special case of the Bregman divergence generated by
the entropy function

============================================================

=== CHUNK 116 ===
Palavras: 372
Caracteres: 2777
--------------------------------------------------
10.2.1 Reverse KL
Making the choice of an exponential family and RKL divergence, we minimize
the KL divergence between q(z)a n dp(z|x):
minimize
φDKL(qφ(z)||p(z|x)) = minimize
φ/integraldisplay
qφ(z)logqφ(z)
p(z|x)(10.8)
Therefore, when qφ(z) is close to zero then logqφ(z)
p(z|x)does not contribute to the
integral, ignoring p(z|x) When qφ(z) is large and p(z|x) is close to zero there is
signiﬁcant contribution to the integral We ﬁnd the approximate posterior:
qφ⋆(z) = argmin
qφ(z)DKL(qφ(z)||p(z|x)) (10.9)
10.2 Variational Inference 177
as illustrated on the left side of Figure 10.1, where:
DKL(qφ(z)||p(z|x)) = Eqφ(z)[logqφ(z)]−Eqφ(z)[logp(z|x)] (10.10)
therefore, plugging the RKL into Equation 10.9 we get:
qφ⋆(z) = argmin
qφ(z)Eqφ(z)[logqφ(z)]−Eqφ(z)[logp(z|x)] (10.11)
and replacing minimization by maximization yields:
qφ⋆(z) = argmax
qφ(z)Eqφ(z)[logp(z|x)]+Eqφ(z)[−logqφ(z)] (10.12)
which promotes that wherever qφhas high probability, p(z|x) also has high prob-
ability, known as mode-seeking Speciﬁcally, using the deﬁnition of the RKL divergence in Equation 10.7 for
the variational distribution and posterior we get:
DKL(q(z)||p(z|x)) =/integraldisplay
q(z)logq(z)
p(z|x)dz (10.13)
Unfortunately, the denominator contains the posterior p(z|x), which is the term
that we would like to approximate So how can we get close to the posterior
without knowing the posterior By using Bayes rule, replacing the posterior in
Equation 10.13 using Equation 10.1, we get:
/integraldisplay
q(z)logq(z)
p(z|x)dz=/integraldisplay
q(z)logq(z)p(x)
p(z,x)dz (10.14)
Separating the log p(x) term and replacing the log of the ratio with a diﬀerence
yields:
/integraldisplay
q(z)logq(z)p(x)
p(z,x)dz=l o gp(x)−/integraldisplay
q(z)logp(z,x)
q(z)dz (10.15)
In summary, minimizing the reverse KL divergence between p(z|x)a n dq(z)i s
equivalent to minimizing the diﬀerence:
logp(x)−/integraldisplay
q(z)logp(z,x)
q(z)dz≥0 (10.16)
whichisnon-negativesincetheKLdivergenceisnon-negative.Rearrangingterms
we get:
logp(x)≥/integraldisplay
q(z)logp(z,x)
q(z)dz:=L (10.17)
The term on the right, denoted by L, is known as the evidence lower bound
(ELBO) Therefore, minimizing the KL divergence is equivalent to maximizing
the ELBO We have turned the problem of approximating the posterior p(z|x)
into an optimization problem of maximizing the ELBO, which consists of two
terms:
L=Eqφ(z)[logp(x,z)]−E[logq(z)] (10.18)
178 10 Variational Autoencoders
The term on the left is the expected log likelihood, and the term on the right
is the negative entropy Therefore, when optimizing the ELBO, there is a trade-
oﬀ between these two terms The ﬁrst term places mass on the maximum a-
posteriori (MAP) estimate, whereas the second term encourages diﬀusion, or
spreading the variational distribution

============================================================

=== CHUNK 117 ===
Palavras: 356
Caracteres: 2659
--------------------------------------------------
In variational inference we maximize the
ELBO in Equation 10.18 to ﬁnd qφ⋆(z)∈Qclosest to the posterior p(z|x) 10.2.2 Score Gradient
Now that our objective is to maximize the ELBO, we turn to practical optimiza-
tion methods The ELBO is not convex, so we can hope to ﬁnd a local maximum We would like to scale up to large data xwith many hidden variables z A practi-
cal optimization method which scales to large data is stochastic gradient descent
(Robbins and Monro, 1951; Bottou, 2010) Gradient descent optimization is a
ﬁrst-order method which requires computing the gradient Therefore, our prob-
lem is computing the gradient of the ELBO:
∇φL=∇Eqφ(z)[logp(x,z)−logqφ(z)] (10.19)
We would like to compute the gradients of the expectations ∇φEqφ(z)[fφ(z)] of
a cost function fφ(z)=l o gp(x,z)−logqφ(z) by expanding the gradient as:
∇φEqφ(z)[fφ(z)] =∇φ/integraldisplay
qφ(z)fφ(z)dz (10.20)
By using the chain rule this expands to:
∇φ/integraldisplay
qφ(z)fφ(z)dz=/integraldisplay
(∇φqφ(z))fφ(z)+qφ(z)(∇φfφ(z))dz(10.21)
We cannot compute the expectation with respect to qφ(z), which involves the
unknown term ∇φqφ(z), and therefore we will take Monte Carlo estimates of the
gradient by sampling from qand use the score function estimator as described
next Score Function
The score function is the derivative of the log-likelihood function:
∇φlogqφ(z)=∇φqφ(z)
qφ(z)(10.22)
Score Function Estimator
Using Equation 10.20 and multiplying by the identity we get:
∇φ/integraldisplay
qφ(z)fφ(z)dz=/integraldisplayqφ(z)
qφ(z)∇φqφ(z)fφ(z)dz (10.23)
and plugging in Equation 10.22 we derive:
/integraldisplayqφ(z)
qφ(z)∇φqφ(z)fφ(z)dz=/integraldisplay
qφ(z)∇φlogqφ(z)fφ(z)dz (10.24)
10.2 Variational Inference 179
which equals:
/integraldisplay
qφ(z)∇φlogqφ(z)fφ(z)dz=Eqφ(z)[fφ(z)∇φlogqφ(z)] (10.25)
In summary, by using the score function, we have passed the gradient through
the expectation:
∇φEqφ(z)[fφ(z)] = Eqφ(z)[fφ(z)∇φlogqφ(z)] (10.26)
Score Gradient
The gradient of the ELBO with respect to the variational distribution ∇φLis
computed using Equation 10.26 as:
∇φL=Eqφ(z)[(logp(x,z)−logqφ(z))∇φlogqφ(z)] (10.27)
Now that the gradient is inside the expectation we can evaluate using Monte
Carlo sampling For stochastic gradient descent we average over samples zifrom
qφ(z)t og e t :
∇φL=1
kk/summationdisplay
i=1[(logp(x,zi)−logqφ(zi))∇φlogqφ(zi)] (10.28)
where∇φlogqφ(zi) is the score function The score gradient works for both
discrete and continuous models and a large family of variational distributions
and is therefore widely applicable (Ranganath et al., 2014) The problem with
the score function gradient is that the noisy gradients have a large variance

============================================================

=== CHUNK 118 ===
Palavras: 386
Caracteres: 3114
--------------------------------------------------
For
example, if we use Monte Carlo sampling for estimating a mean and there is high
variance, we would require many samples for a good estimate of the mean 10.2.3 Reparameterization Gradient
Distributions can be represented by transformations of other distributions We
therefore express the variational distribution z∼qφ(z)=N(μ,σ) by a transfor-
mation:
z=g(/epsilon1,φ) (10.29)
where/epsilon1∼s(/epsilon1) and get an equivalent way of describing the same distribution:
z∼qφ(z) (10.30)
For example, instead of z∼qφ(z)=N(μ,σ) we use:
z=μ+σ⊙/epsilon1 (10.31)
where/epsilon1∼N(0,1) to get the same distribution:
z∼N(μ,σ) (10.32)
Although these are two diﬀerent ways of describing the same distribution, the
advantages of this transformation are that we can (1) express the gradient of the
180 10 Variational Autoencoders
expectation; (2) achieve a lower variance than the score function estimator; and
(3) diﬀerentiate through the latent variable zto optimize by backpropagation We reparameterize ∇φEqφ(z)[fφ(z)], and by a change of variables Equation
10.20 becomes:
∇φEqφ(z)[fφ(z)] =∇φ/integraldisplay
s(/epsilon1)d/epsilon1
dzf(g(/epsilon1,φ))g/prime(/epsilon1,φ)d/epsilon1 (10.33)
and:
∇φ/integraldisplay
s(/epsilon1)d/epsilon1
dzf(g(/epsilon1,φ))g/prime(/epsilon1,φ)d/epsilon1=∇φEs(/epsilon1)[f(g(φ,/epsilon1))] = Es(/epsilon1)[∇φf(g(φ,/epsilon1))]
(10.34)
wheres(/epsilon1) is a ﬁxed distribution independent of φ, passing the gradient through
the expectation:
∇φEqφ(z)[fφ(z)] = Es(/epsilon1)[∇φf(g(φ,/epsilon1))] (10.35)
Since the gradient is inside the expectation, we can use Monte Carlo sampling to
estimate Es(/epsilon1)[∇φf(g(φ,/epsilon1))] The reparameterization method given by Equation
10.35 has a lower variance compared with the score function estimator given in
Equation 10.26 InthecaseoftheELBO L,thereparameterizedgradient(KingmaandWelling,
2014; Rezende et al., 2014) is given by:
∇φL=Es(/epsilon1)[∇φ[logp(x,z)−logqφ(z)]∇φg(/epsilon1,φ)] (10.36)
and rewriting the expectation:
∇φL=1
kk/summationdisplay
i=1∇φ[logp(x,g(/epsilon1i,φ))−logqφ(g(/epsilon1i,φ))] (10.37)
provided the entropy term has an analytic derivation and log p(x,z) and log q(z)
are diﬀerentiable with respect to z Similarly, the reparameterization gradient in
Equation 10.36 has a lower variance than the score gradient in Equation 10.27 In addition, we can use auto-diﬀerentiation for computing the gradient and reuse
diﬀerent transformations (Kucukelbir et al., 2017) The gradient variance is fur-
ther reduced by changing the computation graph in automatic diﬀerentiation
(Roeder et al., 2017) However, a limitation of the reparameterization gradi-
ent is that it requires a diﬀerentiable model, works only for continuous models
(Figurnov et al., 2018) and is computationally more expensive 10.2.4 Forward KL
The FKL divergence minimizes the KL between p(z|x)a n dqφ(z):
minimize
φDKL(p(z|x)||qφ(z)) = minimize
φ/integraldisplay
p(z|x)logp(z|x)
q(z)dz(10.38)
10.3 Variational Autoencoder 181
Ifp(z|x) is close to zero then logp(z|x)
qφ(z)does not contribute to the integral and
therefore there is no penalty for a large qφ(z)

============================================================

=== CHUNK 119 ===
Palavras: 353
Caracteres: 2605
--------------------------------------------------
We ﬁnd:
qφ⋆(z) = argmin
qφ(z)DKL(p(z|x)||qφ(z)) (10.39)
as illustrated on the right side of Figure 10.1, where:
DKL(p(z|x)||qφ(z)) = Ep(z|x)[logp(z|x)]−Ep(z|x)[logqφ(z)] (10.40)
Since the left term is independent of the parameter φit may be dropped when
minimizing for qφ, and turning the objective into maximization results in:
qφ⋆(z) = argmin
qφ(z)/parenleftbig
−Ep(z|x)[logqφ(z)]/parenrightbig
= argmax
qφ(z)Ep(z|x)[logqφ(z)] (10.41)
whichpromotesthatwherever p(z|x)hashighprobability qφalsohashighproba-
bility, also known as mass-covering or mean-seeking, which results in qφcovering
p(z|x) 10.3 Variational Autoencoder
Instead of optimizing a separate parameter for each example, amortized varia-
tional inference (AVI) approximates the posterior across all examples together
(Kingma and Welling, 2014; Rezende et al., 2014) Meta-AVI goes a step further
and approximates the posterior across models (Choi et al., 2019) Next, we give
a formulation of autoencoders, which motivates the AVI algorithm of VAEs 10.3.1 Autoencoder
As shown in Figure 10.2, an autoencoder is a neural network that performs non-
linear principle component analysis (PCA) (Hinton and Salakhutdinov, 2006;
Efron and Hastie, 2016) Non-linear PCA extracts useful features from unlabeled
data by minimizing:
minimize
W1,W2m/summationdisplay
i=1/bardblxi−(W2)Tf((W1)Txi)/bardbl2
2 (10.42)
where for single-layer networks W1andW2are matrices that are the network’s
parametersand fisapointwisenon-linearfunction.Anautoencoderiscomposed
of two neural networks The ﬁrst maps an input xby matrix multiplication
(W1)Tand a non-linearity to a low-dimensional variable z, which is a bottleneck,
and the second reconstructs the input as ˜ xusing (W2)T.W h e nfis the identity
this is equivalent to PCA The goal of variational inference is to ﬁnd a distribution qwhich approxi-
mates the posterior p(z|x), and a distribution p(x) which represents the data
well Motivated by autoencoders, we represent qandpusing back-to-back neu-
ral networks An encoder network represents qand a decoder network represents
182 10 Variational Autoencoders
Figure 10.2 Autoencoder Input xis passed through a low-dimensional bottleneck z
and reconstructed to form ˜ x, minimizing a loss between the input and output The
parameters W1of the encoder and W2of the decoder are optimized end-to-end These neural networks are non-linear functions Fwhich are a composition of
functions F(x)=f(f(...f(x))), where each individual function fhas a linear
and non-linear component, and the function Fis optimized given a large dataset
by stochastic gradient descent (SGD)

============================================================

=== CHUNK 120 ===
Palavras: 360
Caracteres: 2193
--------------------------------------------------
10.3.2 Variational Autoencoder
The ELBO is a lower bound on the log-likelihood of the data xgiven the latent
variable z It is a lower bound because it is not possible to compute the exact
log-likelihood of the data xgiven the latent variable z We will ﬁnd the optimal
parameters θ∗of the encoder and decoder by maximizing the ELBO The ELBO
can be used to train a generative model and is maximized by SGD This means
that the parameters of the encoder and decoder are updated in each iteration by
taking a step in the direction of the gradient of the ELBO with a small learning
rate The ELBO may also be used to train a discriminative model The ELBO as deﬁned in Equation 10.17 can be rewritten as:
L=/integraldisplay
q(z)logp(x|z)dz−/integraldisplay
q(z)logp(z)
q(z)dz (10.43)
which is the lower bound consisting of two terms:
L=Eq(z)[logp(x|z)]−DKL(q(z)||p(z)) (10.44)
The term log p(x|z), on the left, is the log-likelihood of the observed data x
10.3 Variational Autoencoder 183
Figure 10.3 Variational autoencoder The input xis passed through a low-dimensional
bottleneck zand reconstructed to form ˜ x, minimizing a loss between the input and
output The parameters φandθof the encoder qφand decoder pθdeep neural
networks are optimized end-to-end by backpropagation given the sampled latent variable z This term measures how well the samples
fromq(z) explain the data x The goal of this term is to reconstruct xfromz
and therefore is called the reconstruction error, representing a decoder which is
implemented by a deep neural network This term measures the likelihood of
beginning with data x, encoded by a latent variable z, and decoding it back to
the original data x The second term, on the right, consists of sampling z∼q(z|x), representing an
encoder which is also implemented by a deep neural network This term ensures
that the explanation of the data does not deviate from the prior beliefs p(z)a n d
is called the regularization term, deﬁned by the KL divergence between qand
the prior p(z) This term measures the closeness between the encoder and prior The objective function in Equation 10.44 is analogous to the formulation of
autoencoders, and therefore gives rise to the VAE

============================================================

=== CHUNK 121 ===
Palavras: 353
Caracteres: 2684
--------------------------------------------------
The VAE is a deep learning
algorithm, rather than a model, which is used for learning latent representations The learned representations can be used for applications such as synthesizing ex-
amples or interpolation between samples, of diﬀerent modalities such as images,
video, audio, geometry and text The VAE algorithm is deﬁned by two back-to-back neural networks as illus-
tratedinFigure10.3.Theﬁrstisanencoderneuralnetworkwhichinfersahidden
variablezfrom an observation x The second is a decoder neural network which
reconstructs an observation ˜ xfrom a hidden variable z The encoder qφand de-
coderpθare trained end-to-end, optimizing for both the encoder parameters φ
and decoder parameters θby backpropagation If we assume q(z|x)a n dp(x|z) are normally distributed then qis represented
by:
q(z|x)=N(μ(x),σ(x)⊙I) (10.45)
for deterministic functions μ(x)a n dσ(x), andpis represented by:
p(x|z)=N(μ(z),σ(z)⊙I) (10.46)
and
p(z)=N(0,I) (10.47)
Thevariationalpredictivenaturalgradient(TangandRanganath,2019)rescales
184 10 Variational Autoencoders
Figure 10.4 Variational encoder Rather than sampling directly z∼N(μ,σ) in the
latent space, reparameterization allows for backpropagation through the latent
variable z=μ+σ⊙/epsilon1, which is a sum of the mean μand covariance The covariance
σis multiplied by noise /epsilon1∼N(0,I) sampled from a normal distribution the gradient to capture the curvature of variational inference The correlated
VAE (Tang, Liang, Jebara and Ruozzi, 2019) extends the VAE to learn pairwise
variational distribution estimations which capture the correlation between data
points In practice, very good synthesis results for diﬀerent modalities are achieved
usingavectorquantizedvariationalautoencoder(VQ-VAE;(vandenOordetal.,
2017)) which learns a discrete latent representation Using an autoregressive de-
coder or prior with VQ-VAE (De Fauw et al., 2019; Razavi et al., 2019) generates
photorealistic high-resolution images (Ravuri and Vinyals, n.d.) 10.4 Generative Flows
This section describes transformations of simple posterior distribution approxi-
mations to complex distributions by normalizing ﬂows (Rezende and Mohamed,
2015) We would like to improve our variational approximation qφ(z)t ot h ep o s -
teriorp(z|x).Anapproachforachievingthisgoalistotransformasimpledensity,
such as a Gaussian, to a complex density using a sequence of invertible transfor-
mations, also known as normalizing ﬂows (Rezende and Mohamed, 2015; Dinh
et al., 2017; Kingma and Dhariwal, 2018) Instead of parameterizing a simple
distribution directly, a change of variables allows us to deﬁne a complex distri-
bution by warping q(z) using an invertible function f

============================================================

=== CHUNK 122 ===
Palavras: 363
Caracteres: 2939
--------------------------------------------------
Given a random variable
10.4 Generative Flows 185
z∼qφ(z) the log density of x=f(z)i s :
logp(x)=l o gp(z)−logdet/vextendsingle/vextendsingle/vextendsingle∂f(z)
∂z/vextendsingle/vextendsingle/vextendsingle (10.48)
A composition of multiple invertible functions results in a sequence of transfor-
mations,callednormalizingﬂows.Thesetransformationsmaybeimplementedby
neural networks, performing end-to-end optimization of the network parameters For example, for a planar ﬂow family of transformations:
f(z)=z+uh(wTz+b) (10.49)
wherehis a smooth diﬀerentiable non-linear function, and the log-det Jacobian
is computed by:
ψ(z)=h/prime(wTz+b)w (10.50)
and
/vextendsingle/vextendsingle∂f
∂z/vextendsingle/vextendsingle=/vextendsingle/vextendsingleI+uTψ(z)/vextendsingle/vextendsingle (10.51)
Ifzis a continuous random variable z(t) depending on time twith distribu-
tionp(z(t)) then for the diﬀerential equationdz
dt=f(z(t),t) the change in log
probability is:
∂logp(z(t))
∂t=−tr/parenleftBig
∂f
z(t)/parenrightBig
(10.52)
and the change in log density is:
logp(z(t1)) = logp(z(t0))−/integraldisplayt1
t0tr/parenleftBig
∂f
z(t)/parenrightBig
(10.53)
also known as continuous normal ﬂows (Chen, Rubanova, Bettencourt and Du-
venaud, 2018; Grathwohl et al., 2019) For the planar ﬂow family of transformations:
dz(t)
dt=uh(wTz(y)+b) (10.54)
and
logp(z(t))
∂t=−uT∂h
∂z(t)(10.55)
such that given p(z(0)),p(z(t)) is sampled and the density evaluated by solving
an ordinary diﬀerential equation (Chen, Rubanova, Bettencourt and Duvenaud,
2018) Generative ﬂows have been extended to equivariant normalizing ﬂows (Garcia
et al., n.d.), which are normalizing ﬂows that are equivariant to Euclidean sym-
metries and therefore perform well on particle systems and molecules Smooth
normalizing ﬂows (K¨ ohler et al., n.d.) incorporate forces into normalizing ﬂows
and yield smooth functions These are useful properties for modeling molecu-
lar simulations such as simulations of protein backbones represented by torsion
angles 186 10 Variational Autoencoders
10.5 Denoising Diﬀusion Probabilistic Model
A denoising diﬀusion probabilistic model (DDPM) (Sohl-Dickstein et al., 2015;
Ho et al., 2020; Dhariwal and Nichol, 2021; Nichol and Dhariwal, 2021), itera-
tively adds noise to a signal and then reverses the noising process by denoising
to generate signals from noise A DDPM forms a parameterized Markov chain
andistrainedusingvariationalinference.DDPMssynthesizehigh-qualityimages
and outperform other generative models (Dhariwal and Nichol, 2021) 10.5.1 Forward Noising Process
Starting with points from a distribution x0∼q(x0) we iteratively add Gaussian
noise to generate a sequence ( x1,...,x T) consisting of xtfort=1,...,T.T h e
last element in the sequence, xT, is approximately isotropic Gaussian noise The
sequence forms a Markov process such that:
q(xt|xt−1)=N(/radicalbig
1−βtxt−1,βtI) (10.56)
whereβt∈(0,1) is the variance of Gaussian noise

============================================================

=== CHUNK 123 ===
Palavras: 367
Caracteres: 2749
--------------------------------------------------
An element in this Markov
process may be generated directly from the ﬁrst element x0by:
q(xt|x0)=N(/radicalbig
ˆαtx0,(1−ˆαt)I) (10.57)
whereαt=1−βtand ˆαt=/producttextt
j=0αtsuch that:
xt=/radicalbig
ˆαtx0+/radicalbig
1−ˆαtε (10.58)
forε∼N(0,I) 10.5.2 Reverse Generation by Sampling
Reversing the noising process requires sampling the posteriors q(xt−1|xt) The
posteriors are Gaussian distributions, however they are unknown since they de-
pend on q(x0) Therefore we use a neural network to approximate the mean and
covariance of the posteriors normal distribution by:
pθ(xt−1|xt)=N(μθ(xt,t),σθ(xt,t)) (10.59)
AlternativelythemeanofthedistributionmaybederiveddirectlyusingBayes’
rule by predicting the noise εθ(xt,t) (Ho et al., 2020):
μθ(xt,t)=1√xt/parenleftbigg
xt−βt√1−ˆαεθ(xt,t)/parenrightbigg
(10.60)
10.6 Geometric Variational Inference 187
Figure 10.5 Manifold and tangent plane: exponential and logarithm maps between the
tangent plane and the manifold A line in the tangent plane corresponds to a geodesic
in the manifold 10.6 Geometric Variational Inference
This section generalizes variational inference and normalizing ﬂows from Eu-
clidean to Riemannian spaces (Gemici et al., 2016), describing families of distri-
butionsthatarecompatiblewithaRiemanniangeometryandmetric(Arvanitidis
et al., 2018; Davidson et al., 2018; Holbrook, 2018; Saha et al., 2019) Finally,
we consider the geometry of the latent space in variational autoencoders (Chen
et al., 2019; Shukla et al., 2018; Wang and Wang, 2019) We brieﬂy deﬁne a Riemannian manifold and metric, geodesic, tangent space,
exponential, and logarithm maps (Carmo, 1992; Spivak, 1999; Rahman et al.,
2005;O’Neill,2006;DoCarmo,2016).Amanifoldofdimension dhasateach p0∈
Ma tangent space Tp0(M) of dimension dconsisting of vectors θcorresponding
to derivatives of smooth paths p(t)∈M,t∈[0,1], withp(0) =p0 ARiemannian
manifold has a metric on the tangent space If for tangent vectors θwe adopt
a speciﬁc coordinate representation θi, this quadratic form can be written as/summationtext
ijgij(p)θiθj Between any two points p0andp1in the manifold, there is at
least one shortest path, having arc length /lscript(p0,p1) Such a geodesic has an initial
position p0, an initial directionθ
/bardblθ/bardbl2, and an initial speed /bardblθ/bardbl2 The procedure of
ﬁxing a vector in θ∈Tp(M) as an initial velocity for a constant-speed geodesic
establishes an association between Tp0(M) and a neighborhood of p∈M This
association is one-to-one over a ball of suﬃciently small size The association is
formally deﬁned by the exponential map p1= expp0(θ) Within an appropriate
neighborhood p0, the inverse mapping is called the logarithm map and is deﬁned
byθ=l o gp0(p1), as illustrated in Figure 10.5

============================================================

=== CHUNK 124 ===
Palavras: 362
Caracteres: 2547
--------------------------------------------------
188 10 Variational Autoencoders
Normalizing ﬂows have been extended from Euclidean space to Riemannian
space (Gemici et al., 2016) A simple density on a manifold Mis mapped to the
tangent space TpM Normalizing ﬂow transformations are then applied to the
mapped density in the tangent space, and the resulting density is mapped back
to the manifold InVI,severaltransformationchoicesofafamilyofdistributionsarecompatible
with a Riemannian geometry (Davidson et al., 2018; Holbrook, 2018; Falorsi
et al., 2019; Saha et al., 2019) For example, transforming a distribution by the
squareroot tothepositiveorthantof thesphereresultsinthesquarerootdensity
of probability distributions Probability distributions are then represented by
square root densities, and the geodesic distance is deﬁned by the shortest arc
length Again, p1= expp0(θ) maps the tangent space to the sphere, and θ=
logp0(p1) maps the sphere to the tangent space Densities are represented in the
tangent space, and in a similar fashion to normalizing ﬂows, parallel transport
is used to map one tangent space to another The decoder in the VAE is used for both reconstruction and synthesis, gen-
erating new samples xfrom latent variables z In the past decade, generating a
sequence of samples which smoothly morph or warp graphical objects required
meticulously specifying correspondence between landmarks on the objects In
contrast, using the decoder as a generator and interpolating between hidden vari-
ables in latent space allows us to perform this transformation without specifying
correspondence.Aquestionthatarisesiswhetherperforminglinearinterpolation
is suitable in the latent space Interpolation may be performed by walking along
a manifold rather than linear interpolation in the latent space Speciﬁcally, the
latent space of a VAE can be considered as a Riemannian space (Chen et al.,
2019) Using a Riemannian metric rather than a Euclidean metric in the latent
space provides better distance estimates (Arvanitidis et al., 2019; Mallasto et al.,
2019), which improve interpolation and synthesis results (Shukla et al., 2018), as
well as text-generation results (Wang and Wang, 2019), increasing the mutual
information between the latent and observed variables 10.6.1 Moser Flow
Moser Flow (Rozen et al., n.d.) is a continuous normalizing ﬂow on a manifold
in which the model density is parameterized by the diﬀerence between the prior
densityandthedivergenceofaneuralnetwork.Thedivergenceoperatorissimple
and local, and this approach avoids solving an ODE during training

============================================================

=== CHUNK 125 ===
Palavras: 368
Caracteres: 2426
--------------------------------------------------
10.6.2 Riemannian Score-Based Generative Models
Riemannian score-based generative models (De Bortoli et al., 2022) extend score-
based gradient models to Reimannian manifolds by using the time-reversal of
Brownian motion This approach scales to high dimensions and is applied to a
broad range of manifolds 10.7 Software Libraries 189
10.7 Software Libraries
Scalable implementations of VI and VAEs are available as part of Google’s Ten-
sorFlow Probability library (Dillon et al., 2017) and Uber’s Pyro library (Bing-
ham et al., 2019) and for Facebook’s PyTorch deep learning platform (Paszke
et al., 2017) 10.8 Summary
In this chapter we introduced VI using both RKL and FKL The extension
to BBVI is used in practice for inference on large datasets Key advantages
of Bayesian inference in the deep learning setting are that it generalizes deep
learning algorithms by computing posterior approximations and that it enables
sequential updates by iteratively setting the prior to be the previous posterior
and recomputing the posterior based on new data The chapter then covers the VAE algorithm, which consists of an encoder neu-
ral network for inference and decoder network for generation, trained end-to-end
by backpropagation We described a way in which the variational approximation
of the posterior is improved using a series of invertible transformations, known as
normalizing ﬂows, in both discrete and continuous domains Finally, we explore
the latent space manifold and extend variational inference and normalizing ﬂows
to manifolds Part IV
Reinforcement Learning

11 Reinforcement Learning
11.1 Introduction
Machinelearningcanbecategorizedintosupervisedlearning,unsupervisedlearn-
ing and reinforcement learning In supervised learning we are given input–output
pairs; in unsupervised learning we are given only input examples In reinforce-
ment learning we learn from interaction with an environment to achieve a goal We have an agent, a learner, that makes decisions under uncertainty In this set-
ting there is an environment, which is what the agent interacts with The agent
selects actions and the environment responds to those actions with a new state
and reward The agent’s goal is to maximize the reward over time, as shown in
Figure 11.1 The agent shown on the left of Figure 11.1 which is in a certain
state, interacts with the environment, performs an action, receives a reward, and
moves to another state

============================================================

=== CHUNK 126 ===
Palavras: 374
Caracteres: 2192
--------------------------------------------------
The goal is to learn the value of a state, or the prob-
ability of performing an action given a state, or the policy that maps a state
to an action There are many applications of reinforcement learning, including
autonomous vehicles, robot control, game playing, portfolio management, and
dialogue synthesis Consider a simple example of the video game Pong The
state is the image of the screen, the actions are the movements up, down, or no
movement, and the reward is the game score In chess, the state is represented
by the board conﬁguration; the actions are the possible movements of the game
pieces; and the reward is the game outcome of win, lose, or draw 11.2 Multi-Armed Bandit
Before considering reinforcement learning, we will consider the stateless setting
of a multi-armed bandit Given kslot machines, an action is to pull an arm
of one of the machines Pulling an arm results in a reward, which is a sample
drawn from that machine At each time step tthe agent chooses an action at
among the kactions, and receives a reward rt Taking action ais pulling arm
i, which gives a reward r(a) with probability pi Behind each machine there is
a probability distribution, and by pulling an arm we get a sample from that
distribution Our goal is to maximize the total expected return The value of
actionais the expected reward Q(a)=E[r|a=a]; however, we don’t know the
194 11 Reinforcement Learning
Figure 11.1 Reinforcement learning setting An agent interacts with an environment
by taking actions The environment transitions the agent to a new state and the
agent receives a reward Next, the agent takes another action and so on In
reinforcement learning the transition function and reward function are unknown to
the agent that samples the environment We can therefore estimate the value Qt(a) of action aat timet;f o r
example, by keeping the current mean reward for each action A greedy action
takes the best estimate at time t, exploiting knowledge at= argmax
aQt(a), for
example by choosing the action with the largest mean reward 11.2.1 Greedy Approach
Consider the example shown in Figure 11.2, with two possible actions: red or
blue (for example, to open a red door or a blue door)

============================================================

=== CHUNK 127 ===
Palavras: 359
Caracteres: 1701
--------------------------------------------------
If we choose the red door
and get a reward of 0, then the value of red is 0 If we then choose the blue door
a n dg e tar e w a r do f1 ,t h e nt h ev a l u eo fb l u ei s1 .I fw ef o l l o wag r e e d ys t r a t e g y ,
then since the value of the blue door of 1 is greater than the value of the red
door of 0, we will choose blue again Say we choose blue again and get 3; then
if we update our mean for the blue door then the value of the blue door is now
2, which is also greater than the value of the red door which is 0 So we choose
the blue door again, and so long as the mean is greater than 0 we will keep on
choosing the blue door However, it could have been the case that the value we
received for the red door of 0 was simply bad luck, and that value was sampled
from the tail of the distribution behind the red door, whereas the red distribution
may yield other high values However, if we act greedily then a sampled value is
deterministically used, and in this case we may continue choosing the blue door
indeﬁnitely, without going back to the red door 11.2.2 ε-greedy Approach
A non-greedy action is exploring If instead of taking a greedy action we behave
greedily most of the time, for example with a small probability εwe choose a
random action and with probability 1 −εwe take the greedy action, then we
are acting ε-greedy The ε-greedy approach ensures that once in a while we will
11.2 Multi-Armed Bandit 195
Figure 11.2 Greedy action selection In the ﬁrst step the agent chooses red and
observes a value of 0 Next, the agent chooses blue and observes a value of 1 Since 1
is greater than 0, the agent chooses blue again and this time observes the value 3, for
an average value of 2

============================================================

=== CHUNK 128 ===
Palavras: 366
Caracteres: 2343
--------------------------------------------------
The agent will continue selecting blue so long as the mean is
greater than 0, even though this result may be due to an unlucky value of 0 observed
for red There is a trade-oﬀ between exploiting known knowledge, namely the average
values, and exploring take a random action; this promotes exploration, and may avoid getting stuck
continuouslyexploitingtheknownactions.Pseudocodeforthe ε-greedyapproach
is shown in Algorithm 11.1 Algorithm 11.1 ε-greedy foreach action ado:
Q(a)=0
N(a) = 0 number of times action is chosen
foreach time step do:
a=⎧
⎨
⎩argmax
aQ(a) with probability 1 −ε
random action with probability ε
N(a)=N(a)+1
Q(a)=Q(a)+(r(a)−Q(a))/N(a)
196 11 Reinforcement Learning
11.2.3 Upper Conﬁdence Bound
We can choose to be optimistic under uncertainty by using both the mean and
variance of the reward, taking the action using the upper conﬁdence bound
(UCB) criteria (Auer et al., 2002):
argmax
a(μ(r(a))+εσ(r(a))) (11.1)
This criteria also appears in Monte Carlo tree search, which is used in Expert
Iteration and AlphaZero 11.3 State Machines
A state machine is deﬁned by a set Sof possible states, an initial state s0,as e t
of possible inputs X, a transition function f:S×X/mapsto →S mapping from states
and inputs to a state, a set of possible outputs Yand a mapping g:S /mapsto → Yfrom
a state to an output For example, Figure 11.3, shows a state machine with two
states denoted by circles S= standing ,moving The start state in this example
s0= standing is denoted by two concentric circles The set of possible inputs
X= slow,fast, and a transition function fis denoted by orange or purple edges
from source to target states The transition function f(s,x)=s/primemaps each state
sand input xto a new state s/prime For example s1=f(s0,fast) = moving The states may not be observed directly; for example, they may be sensor
measurements or, as shown in the example in Figure 11.4, the state is that there
isalionessinthegrass,whereasanobservationisonlyoftheoccludinggrass.The
state and observation in this example are diﬀerent and may result in diﬀerent
outcomes Formally, deﬁne Yto be the set of possible outputs or observations,
andg:S /mapsto → Yamappingfromastate stoanoutputorobservation y.Ifthestate
and observation are the same then gis the identity, and in the example shown
in Figure 11.3 we get y1=g(s1)=s1= moving

============================================================

=== CHUNK 129 ===
Palavras: 353
Caracteres: 2108
--------------------------------------------------
The tuple ( S,X,f,Y,g,s0)
deﬁnes the state machine The state machine is applied for each time step t,i n
which we iteratively compute:
st=f(st−1,xt)
yt=g(st)(11.2)
fort≥1 Notice that Equation 11.2 deﬁning a state machine is the same as
our earlier deﬁnition of a recurrent neural network, where the hidden states are
replaced with states st 11.4 Markov Processes
In the previous section we considered only actions in a stateless setting We now
consider the state of the agent In a Markov model, as illustrated in Figure 11.5,
11.4 Markov Processes 197
Figure 11.3 State machine with two states S={standing ,moving}, a starting state
s0=standing , two inputs X= slow,fast, and a transition function f:S×X/mapsto →S
denoted by orange and purple arcs In this example f(s0,slow) = standing ,
f(s0,fast) = moving,f(moving,slow) = standing , andf(moving,fast) = moving Figure 11.4 State (right) of a lioness in the grass, compared with an observation (left)
of only the grass occluding the lioness we make the assumption that state s2is only dependent on the previous state
s1, and generally that state st+1depends only on the previous state st In a Markov process, as illustrated in Figure 11.6, the probability of a state
st+1is dependent only on the previous state stand an action at,n a m e l yt h e
probability is p(st+1|st,at) Formally, a Markov process is deﬁned by a set of possible states S, a set of
possible actions A, and a transition model T:S×A×S/mapsto → R An example of
a Markov process is illustrated in Figure 11.7 In this example, the set of three
possible states of a robot are S={fallen,standing,moving} For each state,
there are two possible actions the robot may take A={slow,fast}denoted by
orange and purple arcs The transition model deﬁnes the probability distribution
over the next state given the previous state and action In this example gis the
identity and therefore the output is the state For example, if the robot is in
state fallen and takes a slow action, then with probability3
5the robot will stay
fallen and with probability2
5the robot will stand up and be in state standing

============================================================

=== CHUNK 130 ===
Palavras: 373
Caracteres: 2056
--------------------------------------------------
If
Figure 11.5 In a Markov model state st+1depends only on the previous state st 198 11 Reinforcement Learning
Figure 11.6 In a Markov process the probability of a state st+1depends only on the
previous state stand action at the robot is in state fallen and takes fast action it will stay fallen; therefore the
only way for a fallen robot to stand up is by taking a slow action If the robot is
standing and takes a slow action, then it will always, with probability 1, begin
to move, transitioning to state moving If the robot is moving and takes a slow
action, it will keep on moving If the robot is standing and takes a fast action,
then with probability3
5it will move, and with probability2
5it will fall If the
robot is moving and takes a fast action, then with probability4
5it will keep on
moving and with probability1
5it will fall The 3×3 transition matrices P(s,a,s/prime) for slow and fast actions are shown in
Equations 11.3 and are completely known The rows denote states s, the columns
denote states s/prime, and the values of the matrix are the transition probabilities For
example, taking a slow action as illustrated by orange arcs, the probability from
state fallen to fallen is3
5, from fallen to standing is2
5and from fallen to moving
there is no arc, which is 0 probability, such that a row of probabilities sums
to 1 The entire transition matrices are known, and there is no need to explore
in order to ﬁnd the transition probabilities In a similar fashion, the transition
matrix for taking a fast action is given and known to the robot:
P(s,slow,s/prime)=⎡
⎣3
52
50
001
001⎤
⎦P(s,fast,s/prime)=⎡
⎣100
2
503
5
1
504
5⎤
⎦ (11.3)
In a Markov model the probability of a state is conditioned on the previous
state, as shown in Figure 11.5 In a Markov process the probability of a state
is conditioned both on the previous state and on the action taken, as shown
in Figure 11.6 A policy π(a|s) maps state to action, and following the policy
allows the agent to decide which action to take given the state it is in, as shown
in Figure 11.8

============================================================

=== CHUNK 131 ===
Palavras: 369
Caracteres: 2123
--------------------------------------------------
11.5 Markov Decision Processes 199
Figure 11.7 A Markov process deﬁned by a set of states
S={Fallen,Standing ,Moving}and a set of actions A={slow,fast}, with a known
transition function T(s,a,s/prime) Figure 11.8 In a Markov process the probability of a state is conditioned both on the
previous state and on the action taken, and an action may be taken based on a policy
π 11.5 Markov Decision Processes
A Markov decision process (MDP) is deﬁned by a set of possible states S, a set
of possible actions A, a transition model T:S×A×S/mapsto → R, a reward function
R:S×A/mapsto → Rmapping a state and an action to a real value, and a discount fac-
torγ Together the tuple ( S,A,T,R,γ) deﬁnes an MDP At every time step tthe
agent ﬁnds itself in state s∈Sand selects an action a∈A The agent transitions
to the next state s/primeand receives a reward Next, the agent selects a new action,
and so on The reward R(s,a) is based on state and action For example, we
may deﬁne the rewards of our robot to be R(fallen,slow) = 1, R(fallen,fast) =
0,R(standing ,slow) = 1, R(standing ,fast) = 2, R(moving,slow) = 1, and
200 11 Reinforcement Learning
R(moving,fast) =−1, regardless of which of the possible arcs happens The
rewards may not necessarily be deﬁned in a deterministic fashion We may de-
ﬁne the rewards to be probabilistic based upon the transition function prob-
abilities as shown in Figure 11.9 For example, instead of having the reward
R(s,a)=R(fallen,slow) = 1 we may deﬁne the reward to be dependent on which
of the two arcs is taken such that for T(s,a,s/prime)=p(fallen,slow,standing) =2
5
the reward is 1 and for T(s,a,s/prime)=p(fallen,slow,fallen) =3
5the reward is −1 The 3×2 matrix Rof expected rewards given a state sand action afor the
robot example is given by Equation 11.4 The expected reward for state fallen
and slow action is3
5×(−1)+2
5×1=−1
5and the expected reward of state fallen
and fast action is 0 In a similar fashion, the expected reward of state standing
for slow action is 1 and for fast action is4
5, and the expected reward for state
moving and slow action is 1 and for a fast action is7
5

============================================================

=== CHUNK 132 ===
Palavras: 357
Caracteres: 2455
--------------------------------------------------
Considering each row of
the reward matrix, we can take the action that maximizes the reward from that
state Therefore, an optimal policy that chooses an action, for a single (myopic)
time step, with the maximum reward for each state will choose a fast action from
state fallen, receiving an expected reward of 0, a slow action from state standing,
receiving an expected reward of 1, and a fast action from state moving, receiving
an expected reward of7
5 In an MDP both the transition matrices T(s,a,s/prime)a n d
the reward function R(s,a) are known In contrast, in reinforcement learning the
agent does not know TandRand learns them by sampling the environment R(s,a)=⎡
⎣−1
50
14
5
17
5⎤
⎦ (11.4)
In summary, in an MDP the transitions are well deﬁned:
P(s/prime,r|s,a)=P(st+1=s/prime,rt+1=r|st=s,at=a) (11.5)
where/summationtext
s/prime/summationtext
rP(s/prime,r|s,a) = 1forall( s,a).Theexpectedrewardforstate–action
pairs are:
R(s,a)=E[rt+1|st=s,at=a]=/summationdisplay
rr/summationdisplay
s/primeP(s/prime,r|s,a) (11.6)
and the state–transition probabilities are:
P(s/prime|s,a)=P(st+1=s/prime|st=s,at=a)=/summationdisplay
rP(s/prime,r|s,a) (11.7)
and the expected rewards for state–action–next-state are:
R(s,a,s/prime)=E[rt+1|st=s,at=a,st+1=s/prime]=/summationtext
rrP(s/prime,r|s,a)
P(s/prime|s,a)(11.8)
11.5.1 State of Environment and Agent
In the real world the state is more complex since the state and what the agent
observes are often not the same For example, the agent may observe the grass,
11.5 Markov Decision Processes 201
Figure 11.9 Markov decision process deﬁned by a set of states
S={Fallen,Standing ,Moving}and a set of actions A={slow,fast}, with known
transition function T(s,a,s/prime) and reward function R(s,a) Figure 11.10 The agent action atis based on an observation otwhich may be diﬀerent
from the state st whereas the true state of the environment is that there is a lion hidden in the
grass that the agent does not observe and therefore cannot act upon In the real
world the state of the environment sand the observation oare often diﬀerent The state of the environment yields an observation and the agent’s action is
based on the observation rather than the environment state, as shown in Figure
11.10 For example, in the game Breakout the screen is the observation, whereas
the environment is the game console and the state of the environment are the
instructions and RAM of the game console, as shown in Figure 11.11

============================================================

=== CHUNK 133 ===
Palavras: 356
Caracteres: 2079
--------------------------------------------------
Given
suﬃcient data examples of observations and environment states we may consider
performing reverse engineering and infer the environment state from observation 202 11 Reinforcement Learning
Figure 11.11 In the video game Breakout the agent observes the screen pixels o The
game console is the environment e, and the environment state seare the instructions
and RAM of the console 11.6 Deﬁnitions
11.6.1 Policy
Next, we deﬁne a policy π:S /mapsto → Awhich is a mapping from state or observation
to action Consider a policy as being similar to a rule book which tells the
agent which action to take with a certain probability from each state For each
stateswe have a set of possible actions a, and for each of these actions we
have a probability of the action given that state In our robot example, shown
in Figure 11.7, we may deﬁne four policies: πAalways take a slow action; πB
always take a fast action; πCif fallen take slow action and otherwise take fast
action; and πDif moving take fast action otherwise take slow action These four
policies may represent four diﬀerent rule books A policy does not necessarily
need to be deterministic A policy may be stochastic by adding randomness to
the agent actions For example, the stochastic policy πEwhich for all states takes
a slow action with probability 0 .3 and a fast action with probability 0 .7 At each
time step tthe agent implements the mapping πfrom states to probabilities of
11.6 Deﬁnitions 203
Figure 11.12 Example of the state of an agent in a maze illustrated by a position in
the maze denoted by the black dot selecting each action:
πt(a|s)=P(at=a|st=s) (11.9)
as shown in Figure 11.8 Asasecondexampleofapolicy,consideramazewherethestateisanyposition
in the maze as shown in Figure 11.12 The agent begins at a state, for example
the start state shown on the bottom right of the maze, and has a goal state –
shown on the top center of the maze A policy is a rule book that tells the agent
what action to take, with what probability, from each state, as illustrated by the
arrows in the maze shown in Figure 11.13

============================================================

=== CHUNK 134 ===
Palavras: 355
Caracteres: 2178
--------------------------------------------------
This rule book may be a deterministic
policy deﬁned by a=πt(s), as illustrated by a single arrow in each square of the
maze, or a stochastic policy with four arrows, one in each direction, in each state
of the maze whose lengths denote the probability of moving in each direction
given that state πt(a|s)=P(at=a|st=s) Following the policy shown in
Figure 11.13 from any state results in the goal state 11.6.2 State Action Diagram
Figure 11.14 shows a state–action diagram as a tree The agent starts from a root
node representing state sand takes an action a The action is selected by the
agentbasedonapolicy πmappingstatetoaction.Basedonthestate–actionpair
(s,a) represented by a black node, the environment provides the agent with a
204 11 Reinforcement Learning
Figure 11.13 Example of a deterministic policy deﬁning movements, from each white
square, represented by green arrows The states are the white squares and the
possible actions are A={up,down,left,right}arrows A stochastic policy may deﬁne
a probability over the actions for each state rewardrrepresented by an edge from the black node, and the transition function
moves the agent to a new state s/primedenoted by a leaf node Nodes representing
states are shown in yellow, and nodes representing states and actions are shown
in black The state–action diagram tree represents an episode ( s,a,r,s/prime)o ft h e
agent In the ﬁrst part the agent takes an action, whereas in the second part the
transition function or environment provides a reward and moves the agent This
process is repeated from s/primefor another episode, and so on 11.6.3 State Value Function
Next we would like to know: What is the value of a policy π:S /mapsto → A This
dependsonthenumberofstepswetakefollowingthepolicy.Inourrobotexample
shown in Figure 11.9, we may rent the robot for hsteps; afterward we do not
have access to the robot – we can say the robot will be destroyed after hsteps We callhthe horizon, the number of time steps left for the policy to be applied DeﬁneVh
π(s) as the state value function with respect to a policy πwith horizon
hstarting at state s We can compute Vh
π(s) by induction on the number of steps
remaining, h

============================================================

=== CHUNK 135 ===
Palavras: 352
Caracteres: 2068
--------------------------------------------------
In the base case, there are no steps remaining, h= 0; therefore no
matter what state the agent is in, the value V0
π(s) = 0 Next, the value of a policy
πat stateswith horizon his the reward in splus the next state’s expected value
11.6 Deﬁnitions 205
Figure 11.14 State–action diagram tree The root of the tree represents a state s.T h e
agent takes action aleading to node ( s,a) The transition function or environment
then gives the agent a reward rand moves the agent to state s/primerepresented by a leaf
node Forh=1 :
V1
π(s)=R(s,π(s))+V0
π(s)=R(s,a)+0 (11.10)
Forh=2 :
V2
π(s)=R(s,π(s))+/summationdisplay
s/primeT(s,π(s),s/prime)R(s/prime,π(s/prime)) (11.11)
and for any h:
Vh
π(s)=R(s,π(s))+/summationdisplay
s/primeT(s,π(s),s/prime)Vh−1
π(s/prime) (11.12)
which deﬁne Vh
π(s) recursively as a function of Vh−1
π(s/prime) Consider the value of a state Vπ(s) with respect to a policy πfor the maze
example shown in Figure 11.15 The goal is to reach the center top state from
the start state at the bottom right, and in each step we lose a point When we
are one step away from the goal and we follow the policy shown in Figure 11.13,
which says to go up if you are in the state below the goal, then the value of that
state is−1 The value Vof a state sis always with respect to a policy π Given
a policy, or rule book, as shown in Figure 11.13, we can infer the value of states,
as shown in Figure 11.15 On the other hand, given the values of states we can
infer a policy The state value function Vπ(s) for a policy πmeasures how good
it is for the agent to be in a given state in terms of expected future rewards for
an inﬁnite horizon The value function deﬁned with respect to an agent’s policy
206 11 Reinforcement Learning
Figure 11.15 Example of a state value function deﬁned on a maze πis the expectation over the return:
Vπ(s)=Eπ[gt|st=s]=Eπ/bracketleftBigg/summationdisplay
kγkrt+k+1|st=s/bracketrightBigg
(11.13)
and is illustrated in Figure 11.14 This computation involves two steps In the
ﬁrst step, given a state we consider the set of possible actions

============================================================

=== CHUNK 136 ===
Palavras: 378
Caracteres: 2758
--------------------------------------------------
Once we take
an action, the second step is that the environment blows us to the next state We compute the expectation of the return since the policy may be stochastic We consider the return since we take into account the long-term rewards rather
than just the immediate reward The return is the reward over time discounted
by a factor γ.I fγ= 0 then the agent is myopic and takes into account only the
immediate reward If γ= 1 then the agent is farsighted, taking into account the
long-term reward In the case of an inﬁnite horizon h=∞we don’t know when the game or
robot episodes will be over and may potentially play an inﬁnite number of steps A problem is that Q∞may be inﬁnite, and therefore we cannot select one action
over another One solution is to ﬁnd a policy that maximizes an inﬁnite horizon
discounted value:
E/bracketleftBigg∞/summationdisplay
t=0γtRt|π,s0/bracketrightBigg
=E/bracketleftbig
R0+γR1+γ2R2+···|π,s0/bracketrightbig
(11.14)
wheretdenotesthenumberofstepsfromthestartingstate.Theexpectedinﬁnite
11.6 Deﬁnitions 207
horizon value of state sunder policy πis:
Vπ(s)=E/bracketleftbig
R0+γR1+γ2R2+···|π,s0=s/bracketrightbig
=E[R0+γ(R1+γ(R2+···))]|π,s0=s)
=R(s,π(s))+γ/summationdisplay
s/primeT(s,π(s),s/prime)Vπ(s/prime)
wheretdenotes the number of step from the start, yielding n=|S|linear
equations which can be solved 11.6.4 Action Value Function
Similar to the state value function we can consider the action value function,
which extends the mapping to each of the possible actions We can compute
Qh
π(s,a) with respect to a policy πwith horizon hfor state sand action ain a
similar fashion to our iterative computation of Vh
π(s) Forh=1 ,Q1
π(s,a)=R(s,a)+0 For h=2 :
Q2
π(s,a)=R(s,a)+/summationdisplay
s/primeT(s,a,s/prime)max
a/primeR(s/prime,a/prime) (11.15)
For anyhwe can use Qh−1
π(s/prime,a/prime) to compute Qh
π(s,a):
Qh
π(s,a)=R(s,a)+/summationdisplay
s/primeT(s,a,s/prime)max
a/primeQh−1
π(s/prime,a/prime) (11.16)
Fornstates|S|=n,mactions|A|=m, and horizon h, computation time of
Qh
π(s,a)i sO(nmh) In the maze example shown in Figure 11.16 we have four possible actions: A=
{up,down,left,right}so the action value function Qπ(s,a) takes into account
both the state sand the action awith respect to a policy π The action value
function Qπ(s,a) for policy πis the expected return for sandaunder policy π
with discount γ:
Qπ(s,a)=Eπ[gt|st=s,at=a]=Eπ/bracketleftBigg/summationdisplay
kγkrt+k+1|st=s,at=a/bracketrightBigg
(11.17)
The value of taking action ain state sunder policy πis the expected return This expectation is computed by summing the products of the probabilities of
each action by their returns, as illustrated in Figure 11.17 in which black nodes
represent state–action pairs and yellow nodes represent states

============================================================

=== CHUNK 137 ===
Palavras: 355
Caracteres: 2188
--------------------------------------------------
An example of state value and action value functions for the game of Breakout
is shown in Figure 11.18 As the ball moves up toward the bricks the value of
the state increases; as the ball moves down toward the paddle the value of the
state decreases The action value function shows the value of the state for each
possible action Given the action value function, we can select the action for
which the action value function is maximized 208 11 Reinforcement Learning
Figure 11.16 Example of an action value function Figure 11.17 Example of an action value state diagram The relationship between the state value function Vπ(s) and the action value
function Qπ(s,a)i s :
Vπ(s)=/summationdisplay
aπ(a|s)Qπ(s,a) (11.18)
11.6 Deﬁnitions 209
Figure 11.18 State value function Vπ(s) and action value functions Qπ(s,a) for actions
A={left,right,no-op}for the video game Breakout As the ball gets closer to the
brick wall the state value function increases due to the expected reward to be received
by hitting the wall, whereas as the ball goes down the state value function decreases
due to the possibility of missing the ball 11.6.5 Reward
In our maze example, the reward shown in Figure 11.19 is −1 for each time step
spent in the maze The return is the sum of rewards gt=rt+1+rt+2+···+rT If the agent plays in the maze for a very long time, for many time steps, the
agent will accumulate a very large negative reward Therefore, the reward can
be discounted by a discount factor γ∈[0,1] such that:
gt=rt+1+γrt+2+γ2rt+3+···=T−t−1/summationdisplay
k=0γkrt+k+1 (11.19)
Ifγ= 0 then the agent is myopic, maximizing only immediate rewards, and as γ
approaches 1 the agent becomes farsighted, considering the long-term horizon 210 11 Reinforcement Learning
Figure 11.19 Reward for each time step spent in the maze is −1 The returns at successive time steps are dependent upon on each other The
return at time step tis the next reward plus γtimes the return at the next time
stept+1, such that for an inﬁnite horizon:
gt=rt+1+γrt+2+γ2rt+3+···=rt+1+γ(rt+2+γrt+3+···)=rt+1+γgt+1
(11.20)
which deﬁnes a recursive relationship between the return gtat time step tand
the return gt+1at the next time step t+1

============================================================

=== CHUNK 138 ===
Palavras: 377
Caracteres: 2524
--------------------------------------------------
11.6.6 Model
We can build a model for the environment which will help us predict what the
environment will do next If the environment is deterministic then we can form
a transition matrix Tto predict the next state and a reward matrix Rto predict
the next reward Reinforcement learning methods can be
classiﬁed into model-free methods and model-based methods 11.6.7 Agent Types
Reinforcementlearningmethodsmaybecategorizedintomodel-basedandmodel-
free methods Model-based methods learn a model of the environment which
is used to predict the value of a given action in a given state For example,
11.6 Deﬁnitions 211
model-based methods may model the transition function and the reward func-
tion Model-based methods may further be divided into methods that are given
the model and methods that learn the world model Model-based reinforcement
learning methods that learn the world model begin with a policy and interact
with the environment using that policy to yield observations Next, given the
observations, we may build a world model from the known observations, and
ﬁnally use the world model to train the agent, resulting in an improved policy Incontrasttomodel-basedapproaches,model-freemethodseitherﬁndapolicy
directly or estimate a value function, for example by Q-learning Policy-based
methods learn a policy that maximizes the expected reward and do not require
a model of the environment Value-based methods learn a value function that
estimates the expected reward of taking a given action in a given state Model-
free agents may be based on optimizing only a value function, only a policy, or
both Actor–critic algorithms optimize for both the value function and policy Model-free methods are simpler to implement than model-based methods, and
are more suitable for real-time applications However, they are less likely to
succeed in complex environments Model-based methods may be more suitable
for complex environments, but require more computational resources 11.6.8 Problem Types
The planning problem is the case in which the environment is known, such that
when we take an action in each state we get a reward The reinforcement learning
problemhandlestherealworldinwhichtheenvironmentisunknownandchanges
since the agent and others interact with the environment There is a classical trade-oﬀ between the types of behaviors of an agent: specif-
ically, between exploration in which the agent ﬁnds out more about the environ-
ment, and exploitation in which the agent uses known information to maximize
returns

============================================================

=== CHUNK 139 ===
Palavras: 351
Caracteres: 2024
--------------------------------------------------
For example, consider the trade-oﬀ between showing a new ad com-
pared with showing the best ad based on previous performance for targeting an
audience 11.6.9 Agent Representation of State
As the agent moves between states by taking an action and receiving a reward,
it generates a set of action, state, and reward tuples ( at,st,rt), called episodes,
which together form a history: ht=a1,s1,r1,a2,s2,r2,...,a t,st,rt Our rein-
forcement learning algorithm maps the history htto the next action at+1.I f
we assume a Markovian property then we may consider the previous state or
consider the last episode, otherwise we may consider the entire agent history Our assumptions about agent state may vary Consider the example shown in
Figure 11.20 In the ﬁrst interaction with the environment the agent sees green,
green, blue, red and receives a reward of 100 In the second interaction with
the environment the agent sees red, green, blue, blue and loses 100 In the third
212 11 Reinforcement Learning
Figure 11.20 Diﬀerent representations of agent state lead to diﬀerent predicted
rewards The top row consists of the sequence of the colors green, blue, and red,
followed by a reward of 100 The second row consists of 2 blue nodes, 1 green, and 1
red, followed by a reward of −100 In the bottom row, if our representation of state is
the sequence of the last three colors we may expect a reward of 100, whereas if our
representation of state is the number of appearances of each color regardless of order
then we may expect a negative reward of −100 The representation of state may also
be diﬀerent from these two examples, and yield a diﬀerent reward altogether interaction with the environment the agent sees blue, green, blue, red If we as-
sume a Markov property then we may predict that after the sequence of green,
blue, red we may expect a reward of 100 Whereas if we assume that state is
modeled by number of reds, greens, and blues, then we may predict that having
seen two blues, one green, and one red we will lose 100

============================================================

=== CHUNK 140 ===
Palavras: 385
Caracteres: 3294
--------------------------------------------------
This example illustrates
that our representation of state results in diﬀerent predictions 11.6.10 Bellman Expectation Equation for State Value Function
The expected return starting from sand following policy πsatisﬁes the recursive
relationship:
Vπ(s)=Eπ[gt|st=s] (11.21)
=Eπ/bracketleftBigg/summationdisplay
kγkrt+k+1|st=s/bracketrightBigg
(11.22)
=Eπ/bracketleftBigg
rt+1+γ/summationdisplay
kγkrt+k+2|st=s/bracketrightBigg
(11.23)
=Eπ[rt+1+γgt+1|st=s] (11.24)
=/summationdisplay
aπ(a|s)/summationdisplay
s/prime/summationdisplay
rP(s/prime,r|s,a)(r+γEπ[gt+1|st+1=s/prime]) (11.25)
=/summationdisplay
aπ(a|s)/summationdisplay
s/prime,rp(s/prime,r|s,a)(r+γVπ(s/prime)) (11.26)
11.6 Deﬁnitions 213
Figure 11.21 Backup diagram corresponding to the Bellman expectation equation for
evaluating a state value function Vπ The equation
Vπ(s)=/summationtext
aπ(a|s)/summationtext
s/prime,rp(s/prime,r|s,a)(r+γVπ(s/prime)) is linear, and deﬁnes a recursive
relationship between Vπ(s) andVπ(s/prime) The equation is used for evaluating Vπ, and
there exists a unique solution The value of a state swith respect to a policy πis the
discounted value of the expected next state with respect to πplus the expected
reward The equation averages over all possibilities, weighing each by its probability
to occur for alls, called the Bellman equation for Vπwhich establishes the relationship
between the value of a state and values of successor states The Bellman ex-
pectation equation can be used to evaluate Vπ(s), and deﬁnes the relationship
between Vπ(s)a n dVπ(s/prime):
Vπ(s)=/summationdisplay
aπ(a|s)/summationdisplay
s/prime,rP(s/prime,r|s,a)(r+γVπ(s/prime)) (11.27)
which means that the value of a state equals the discounted value of the expected
next state with respect to πplus the expected reward The Bellman expectation
equation averages over all possibilities, weighting each by its probability of oc-
curring The Bellman equation is a linear equation and may be written in vector
notation as:
Vh+1
π=r+TVh
π (11.28)
whereVπis the vector of values for each state, ris the reward vector for each
state, and Tis the transition matrix 214 11 Reinforcement Learning
Figure 11.22 Backup diagram corresponding to the Bellman expectation equation for
evaluating an action value function Qπwith respect to a given policy π The equation
Qπ(s,a)=/summationtext
s/prime,rp(s/prime,r|s,a)/parenleftbig
r+γ/summationtext
a/primeπ(a/prime|s/prime)Qπ(s/prime,a/prime)/parenrightbig
is linear, and deﬁnes a
recursive relationship between Qπ(s,a) andQπ(s/prime,a/prime) Starting at state sand taking
actiona, the environment moves the agent to state s/primewhere we compute the average
over the available actions, and reach the state–action pair ( s/prime,a/prime) 11.6.11 Bellman Expectation Equation for Action Value Function
We deﬁne Qπ(s,a) recursively as a function of Qπ(s/prime,a/prime):
Qπ(s,a)=Eπ[gt|st=s,at=a] (11.29)
=Eπ/bracketleftbig
rt+1+γrt+2+γ2rt+3+···|s,a/bracketrightbig
(11.30)
=Es/prime,a/prime[r+γQπ(s/prime,a/prime)|s,a] (11.31)
=/summationdisplay
s/prime/summationdisplay
rP(s/prime,r|s,a)/parenleftBigg
r+γ/summationdisplay
a/primeπ(a/prime|s/prime)Qπ(s/prime,a/prime)/parenrightBigg
(11.32)
where the ﬁrst sum denotes where the wind will blow us and the second sum
what action we will take

============================================================

=== CHUNK 141 ===
Palavras: 362
Caracteres: 2438
--------------------------------------------------
The Bellman expectation equation for action value
function is also a linear equation 11.7 Optimal Policy
Solving a task requires ﬁnding the policy that achieves high reward over the long
run We deﬁne the optimal policy for MDPs by deﬁning ordering over policies A policy πis better than or equal to a policy π/primeif its expected return is greater
than or equal to that of π/primefor all states:
π≥π/primeiﬀVπ(s)≥Vπ(s) for alls (11.33)
11.7 Optimal Policy 215
There always exists at least one policy better than or equal to all other policies,
which is the optimal policy π⋆ 11.7.1 Optimal Value Function
The goal of ﬁnding an optimal policy is to maximize the expected return, and
optimal policies share the same optimal state value function:
V⋆(s)=m a x
πVπ(s)=m a x
πEπ[gt|st=s] (11.34)
for alls Similarly, optimal policies share the same optimal action value function:
Q⋆(s,a)=m a x
πQπ(s,a)=m a x
πEπ[gt|st=s,at=a] (11.35)
for allsanda GivenQh(s,a) for all states and actions we can compute the optimal ﬁnite
horizon policy by:
πh
⋆(s) = argmax
aQh(s,a) (11.36)
11.7.2 Bellman Optimality Equation for V⋆
The Bellman optimality equation for V⋆is that the value of a state under an
optimal policy is equal to the expected return for the best action from that
state:
V⋆(s)=m a x
aQπ(s,a) (11.37)
=m a x
aEπ⋆[gt|st=s,at=a] (11.38)
=m a x
aEπ⋆/bracketleftBigg/summationdisplay
kγkrt+k+1|st=s,at=a/bracketrightBigg
(11.39)
=m a x
aEπ⋆[rt+1+γgt+1|st=s,at=a] (11.40)
=m a x
aEπ⋆[rt+1+γV⋆(st+1)|st=s,at=a] (11.41)
=m a x
a/summationdisplay
s/prime,rP(s/prime,r|s,a)(r+γV⋆(s/prime)) (11.42)
which due to the maximum is a non-linear equation:
V⋆(s)=m a x
a/summationdisplay
s/prime,rP(s/prime,r|s,a)(r+γV⋆(s/prime)) (11.43)
withauniquesolutionindependentof π.ThecomputationisillustratedinFigure
11.23 Starting from a state s, we ﬁrst take the maximum over the actions,
maxa; then we are at a state–action pair ( s,a) and we take the expectation over
where the wind will blow us/summationtext
s/prime,r Compare the non-linear Bellman optimality
equation 11.43 that begins with a maximum operation with the linear Bellman
216 11 Reinforcement Learning
Figure 11.23 Backup diagram corresponding to the Bellman optimality equation for
ﬁnding the optimal state value function V⋆ Starting at state sthe agent maximizes
over the available actions From the state–action pair ( s,a) we compute the
expectation over where the environment takes the agent

============================================================

=== CHUNK 142 ===
Palavras: 424
Caracteres: 3000
--------------------------------------------------
The equation
V⋆(s) = max a/summationtext
s/prime,rp(s/prime,r|s,a)(r+γV⋆(s/prime)) is non-linear due to the maximum
operation, and deﬁnes a recursive relationship between V⋆(s) andV⋆(s/prime) It has a
unique solution that is independent of a policy π expectation Equation 11.27 that begins with a summation; the second terms of
both equations are the same 11.7.3 Bellman Optimality Equation for Q⋆
Connecting the optimal state value function V⋆to the optimal action value func-
tionQ⋆:
V⋆(s)=m a x
aQ⋆(s,a) (11.44)
therefore working with Qis convenient In a similar fashion, the Bellman optimality equation for Q⋆is:
Q⋆(s,a)=E/bracketleftBig
rt+1+γmax
aQ⋆(s/prime,a/prime)|s,a/bracketrightBig
(11.45)
=/summationdisplay
s/prime,rP(s/prime,r|s,a)(r+γmax
aQ⋆(s/prime,a/prime)) (11.46)
which is a non-linear equation whose computation is illustrated in Figure 11.24 Starting from a state–action pair ( s,a) the environment may take us to a new
states/prime Once in the new state s/primewe maximize over the next actions we can take
to reach ( s/prime,a/prime) 11.7 Optimal Policy 217
Figure 11.24 Backup diagram corresponding to the Bellman optimality equation for
ﬁnding the optimal action value function The equation is non-linear and used for
ﬁndingQ⋆by deﬁning a recursive relationship between Q⋆(s,a) andQ⋆(s/prime,a/prime) Starting from state and action ( s,a) the equation computes the expectation of where
the environment will take the agent, and then once in state s/primemaximizes over the
actions the agent can take Once we compute
Q⋆(s,a)=/summationtext
s/prime,rp(s/prime,r|s,a)(r+γmaxaQ⋆(s/prime,a/prime)), the agent can act according to the
optimal policy π⋆= argmax
aQ⋆(s,a) Once we know Q⋆(s,a) we can ﬁnd the best policy:
π⋆= argmax
aQ⋆(s,a) (11.47)
Overall, we’ve seen four Bellman equations: two linear expectation equations
(the Bellman expectation equation for state value function Vπ(s) deﬁned in
Equation 11.27 and the Bellman expectation equation for action value function
Qπ(s,a)deﬁnedinEquation11.29);andtwonon-linearoptimalityequations(the
Bellman optimality equation for state value function V⋆(s) deﬁned in Equation
11.43 and the Bellman optimality equation for action value function Q⋆(s,a)
deﬁned in Equation 11.45) Next, we can use the Bellman optimality equation to solve the MDP Consider
theexampleillustratedinFigure11.9.ApplyingtheBellmanoptimalityequation
we get:
V1
⋆(fallen) = 0 do nothing or fast action
V1
⋆(standing) = 1 slow action
V1
⋆(moving) =7
5fast action
218 11 Reinforcement Learning
V2
⋆(fallen) = max {−1
5+2
5×1,0+0}=1
5slow action
V2
⋆(standing) = max {1+7
5,4
5+3
5×7
5+2
5×0}=12
5slow action
V2
⋆(moving) = max {1+7
5,7
5+4
5×7
5+1
5×0}=2.52 fast action
V3
⋆(fallen) = max {−1
5+2
5×12
5+3
5×1
5,0+1×1
5}=0.88 slow action
V3
⋆(standing) = max {1+2.52,4
5+3
5×2.52+2
5×1
5}=3.52 slow action
V3
⋆(moving) = max {1+2.52,7
5+4
5×2.52+1
5×1
5}=3.52 slow action
computing the optimal policy given a perfect model by dynamic programming,
called planning

============================================================

=== CHUNK 143 ===
Palavras: 357
Caracteres: 2319
--------------------------------------------------
Our assumptions are that the environment is an MDP that is
known, namely that the state, action, and reward sets are known and ﬁnite,
and that the dynamics are given by known probability p(s/prime,r|s,a) for all states,
actions, and rewards Unfortunately, in the real world this is rarely useful since
we do not know the dynamics or a perfect model of the environment, we do not
have suﬃcient resource to store the entire MDP, and the Markov property may
not hold As a compromise, we will approximately solve the Bellman equation,
focusing our eﬀorts on learning to make good decisions at frequent states and
putting less eﬀort into learning rare states Next, we will use dynamic programming both for the prediction problem of
policy evaluation and the control problem of ﬁnding the best policy 11.8 Planning by Dynamic Programming with a Known MDP
11.8.1 Iterative Policy Evaluation
Next, we turn the Bellman expectation equation for state value function into
an algorithm for evaluating a policy π, called iterative policy evaluation We
will iteratively approximate Vand use Equation 11.27 to update the value of
each state Algorithm 11.2 describes the pseudocode The inner loop applies the
Bellman expectation equation repeatedly until the value of Vconverges to Vπ Figure 11.25 illustrates the updating of the array storing the state values at each
iteration The algorithm converges both when using two arrays for storing the
state values and when updating the state values array in-place 11.8.2 Policy Iteration
Next, we use the Bellman expectation equation and greedy policy improvement
to converge to the optimal policy π⋆ The policy iteration algorithm has an inner
loop of iterative policy evaluation followed by policy improvement Algorithm
11.3 describes the pseudocode 11.9 Reinforcement Learning 219
Algorithm 11.2 Iterative policy evaluation initialize V(s) = 0 for each state s
repeat:
Δ=0
foreach state sdo:
v=V(s)
V(s)=/summationtext
aπ(a|s)/summationtext
s/prime,rP(s/prime,r|s,a)(r+γV(s/prime))
Δ=m a x {Δ,/bardblv−V(s)/bardbl}
untilΔ<ε
Figure 11.25 Storing iterative updates of the state value function in an array of values
for each of the n=|S|states 11.8.3 Inﬁnite Horizon Value Iteration
Instead of policy iteration we can directly use the Bellman optimality equation
to eﬃciently converge to Q⋆

============================================================

=== CHUNK 144 ===
Palavras: 367
Caracteres: 2687
--------------------------------------------------
Our update rule is then:
Q(s,a)=R(s,a)+γ/summationdisplay
s/primeT(s,a,s/prime)max
a/primeQ(s/prime,a/prime) (11.48)
which turns into the value iteration algorithm shown in Algorithm 11.4 11.9 Reinforcement Learning
In the previous section we introduced algorithms for evaluating a policy and
ﬁnding the optimal policy for a known MDP, given the transition function and
reward function, which is also called planning In this section we introduce al-
gorithms for evaluating a policy and ﬁnding the optimal policy for an unknown
220 11 Reinforcement Learning
Algorithm 11.3 Policy iteration initialize V(s)a n dπ(s)
repeat:
policy evaluation
repeat:
Δ=0
foreach state sdo:
v=V(s)
V(s)=/summationtext
aπ(a|s)/summationtext
s/prime,rP(s/prime,r|s,a)(r+γV(s/prime))
Δ=m a x {Δ,/bardblv−V(s)/bardbl}
untilΔ<ε
policy improvement
convergence = True
foreach state sdo:
a=π(s)
π(s) = argmax
a/summationtext
s/prime,rP(s/prime,r|s,a)(r+γV(s/prime))
ifa/negationslash=π(s)then:
convergence = False
untilconvergence
Algorithm 11.4 Value iteration initialize Q(s,a) = 0 for each state sand action a
repeat:
foreach state sand action ado:
q=Q(s,a)
Q(s,a)=R(s,a)+γ/summationtext
s/primeT(s,a,s/prime)maxa/primeQ(s/prime,a/prime)
Δ=m a x {Δ,/bardblq−Q(s,a)/bardbl}
untilΔ<ε
MDP, without knowing the transition function or reward function in advance,
called reinforcement learning In the real world the MDP is unknown, and yet
we would still like to choose the best actions We do not assume a complete
known environment and therefore sample sequences of state, action and reward
from actual or simulated interaction with the environment to gain experience We generate sample transitions not knowing the complete probability distribu-
tion of transitions We will ﬁrst discuss model-based reinforcement learning and
then review two major sampling methods, namely Monte Carlo (MC) sampling
and temporal diﬀerence (TD) sampling Reinforcement learning methods may be divided into model-free and model-
basedapproaches.Inturn,model-freeapproachesmaybefurtherdividedinto(1)
value-based or Q-learning methods such as DQN (Mnih et al., 2015); (2) policy-
11.9 Reinforcement Learning 221
Figure 11.26 Storing iterative updates of the action value function in a 2D array of
values for each state and action based or policy optimization methods such as A3C (Mnih et al., 2016) and PPO
(Schulman et al., 2017); and (3) actor–critic methods such as DDPG (Lillicrap
et al., 2016), which are a combination of (1) and (2) Model-based approaches
may be divided into methods that are given the model, such as AlphaZero (Silver
et al., 2018), and methods that learn the model, such as world models (Ha and
Schmidhuber, 2018)

============================================================

=== CHUNK 145 ===
Palavras: 373
Caracteres: 2504
--------------------------------------------------
11.9.1 Model-Based Reinforcement Learning
One of the simplest approaches to reinforcement learning is to model the transi-
tionandrewardbasedonstates,actions,andrewardsexperiencedsofar( s,a,r,s/prime)
and to use these to model an MDP A simple model for a transition function is:
T(s,a,s/prime)=N(s,a,s/prime)+1
N(s,a)+|S|(11.49)
whereN(s,a,s/prime) counts the number of times the agent was in state s, took action
aandmovedtostate s/prime,andN(s,a)=/summationtext
s/primeN(s,a,s/prime)countsthenumberoftimes
the agent was is state sand took action a The correction by adding 1 to the
numerator makes sure we don’t estimate the probability to be 0, and adding |S|
to the denominator makes sure we don’t divide by zero This correction is only
required and signiﬁcant in the ﬁrst few samples, and then its eﬀect is diminished A simple model for the reward function is recording the rewards R(s,a) for state
and actions:
R(s,a)=/summationtext
(s,a)r(s,a)
N(s,a)(11.50)
Next, we can use these empirical estimates of the transition function and reward
function to solve the MDP as if TandRwere known A problem with this
222 11 Reinforcement Learning
approach is that it may be infeasible to estimate TorRif the space of states or
actions is very large or continuous 11.9.2 Policy Search
Instead of estimating the transition and reward functions, we can search for a
policy directly We can deﬁne a function f(s,θ)=p(a|s) by a machine learning
model with parameters θto approximate the probability of an action given a
state directly This can be trained by gradient descent for ﬁnding the optimal
parameters θ⋆ Policy-based methods work well in continuous spaces and for
learning stochastic policies, and are described in detail in Chapter 12 11.9.3 Monte Carlo Sampling
Monte Carlo sampling methods average sample returns and assume the experi-
ence is divided into episodes that terminate Only when an episode completes are
the value and policy updated Sampling is incremental, episode by episode, not
step by step As motivation for incremental updates, consider the incremental
computation of the mean The next mean at step tis the current mean at step
t−1 plus the update given a new sample xt The update is the normalized error
between the new sample xtand the previous mean at step t−1:
μt=1
tt/summationdisplay
i=1xi=1
t/parenleftBiggt−1/summationdisplay
i=1xi+xt/parenrightBigg
=1
t((t−1)μt−1+xt)
=1
t(xt+tμt−1−μt−1)=μt−1+1
t(xt−μt−1)
Given a policy πour ﬁrst goal is to learn the state value function Vπ

============================================================

=== CHUNK 146 ===
Palavras: 397
Caracteres: 2520
--------------------------------------------------
Dynamic
programming (DP) computes the value function from the MDP, whereas MC
learns the value function from sample returns:
V(st)=V(st)+α(gt−V(st)) (11.51)
wheregtis the actual return following time step t,(gt−V(st)) is the MC error,
andαis a constant step size In each sample we wait until the end of the episode
to determine the increment of V(st), as shown in Figure 11.27 Algorithm 11.5
describes the MC prediction pseudocode 11.9.4 Temporal Diﬀerence Sampling
Monte Carlo (MC) TD learning methods use experience to solve the prediction
problem Given experience following policy π, they update estimates of Vπforst
occurringinthatexperience.MonteCarlomethodswaituntilthereturnisknown
and use that return as a target for V(st) In contrast, TD learning (Sutton, 1988)
11.9 Reinforcement Learning 223
Figure 11.27 Monte Carlo sampling for reinforcement learning The return is evaluated
once an entire path, or rollout, terminates in a leaf node waits only until the next time step At time t+1 TD methods form a target and
make an update using the observed reward rt+1and the estimate V(st+1):
V(st)=V(st)+α(rt+1+γV(st+1)−V(st)) (11.52)
Monte Carlo methods update V(st) toward the actual return gt,a ss h o w ni n
Figure 11.27, whereas TD methods update V(st) toward the TD target rt+1+
γV(st+1), as shown in Figure 11.28 The diﬀerence between the TD target and
V(st) is called the TD error Algorithm 11.6 describes the TD prediction pseu-
docode As an example of the diﬀerence between MC and TD methods, consider the
episodes illustrated in Figure 11.29 Following these samples, the value of red is
3
4, but what is the value of blue Here is where MC sampling and TD sampling
diﬀer MC does not exploit the Markov property and therefore according to the
MC update rule in Equation 11.51 the value of blue is 0, whereas TD exploits
224 11 Reinforcement Learning
Algorithm 11.5 Monte Carlo prediction initialize
policyπto be evaluated
V(s) = 0 for all s
returns(s)=∅for all states s
repeat:
Generate episode of π
foreach state sin episode do:
g= return following ﬁrst occurrence of s
returns(s) = returns( s)∪g
V(s)=μ(returns( s))
untilconvergence
Algorithm 11.6 Temporal diﬀerence prediction initialize
policyπto be evaluated
V(s) = 0 for all s
repeat:
Generate episode of π
foreach state sin episode do:
a= action given by πfors
take action a, observe r,s/prime
V(s)=V(s)+α(r+γV(s/prime)−V(s))
s=s/prime
untilsis terminal
the Markov property and according to the TD update rule in Equation 11.52 the
value of blue is3
4

============================================================

=== CHUNK 147 ===
Palavras: 361
Caracteres: 2592
--------------------------------------------------
Monte Carlo methods use deep sampling until termination, whereas TD meth-
ods use shallow sampling Temporal diﬀerence methods with one step look-ahead
are called TD(0), and in between TD(0) and MC there exist multiple methods
TD(n), depending on the number nof look-ahead steps We can combine all the
n-step returns gn
tusing weights (1 −λ)λn−1to form TD( λ) following the update
rule:
V(st)=V(st)+α(gλ
t−V(st)) (11.53)
wheregλ
t=( 1−λ)/summationtext
nλn−1gn
t 11.9.5 Q-Learning
Q-learning is a model-free reinforcement learning method that does not model
the transitions or reward, and instead directly estimates a value function In
11.9 Reinforcement Learning 225
Figure 11.28 Temporal diﬀerence sampling for reinforcement learning A biased
estimate of the state value is computed by looking ahead one or more steps, rather
than following an episode all the way to termination model-based reinforcement learning we estimate TandR, and using value itera-
tion, given TandRwe can compute QbyQ(s,a)=R(s,a)+γ/summationtext
s/primeT(s,a,s/prime)
maxa/primeQ(s/prime,a/primeNext, instead of estimating TandR, we will learn the Qfunction
TorR, known as Q-learning Q-
Q(s,a)=Q(s,a)−α/parenleftBig
Q(s,a)−(r+γmax
a/primeQ(s/prime,a/prime)/parenrightBig
(11.54)
whereαis the learning rate, and γis a discount factor Instead of just taking
the action given by the value function, we use ε-greedy exploration Therefore,
we select a random action with probability εto promote exploration, and take
the action given by the value function otherwise This results in the Q-learning algorithm described in the pseudocode in Algo-
rithm 11.7: Q-learning is an oﬀ-policy reinforcement learning method that ﬁnds
the value function of an optimal policy while using another exploration policy.)
directly from experience without knowing
learning incrementally updates the value function by: 226 11 Reinforcement Learning
Figure 11.29 Example showing the diﬀerence between MC and TD sampling Algorithm 11.7 Q-learning initialize Q(s,a) = 0 for all states and actions
select start state s=s0
repeat:
a=/braceleftBigg
select action given Qandswith probability 1 −ε
select random action with probability ε
q=Q(s,a)
take action ato get reward rand next state s/prime
Q(s,a)=Q(s,a)−α(Q(s,a)−(r+γmaxa/primeQ(s/prime,a/prime))
Δ=m a x {Δ,/bardblq−Q(s,a)/bardbl}
s=s/prime
untilΔ</epsilon1
11.9.6 Sarsa
Similar to Q-learning, Sarsa is a model-free reinforcement learning method In
contrast with Q-learning, which is an oﬀ-policy method, Sarsa is an on-policy
method that estimates a value of a policy while using the policy

============================================================

=== CHUNK 148 ===
Palavras: 353
Caracteres: 2609
--------------------------------------------------
Sarsa, as the
acronym implies, uses ( s,a,r,s/prime,a/prime) tuples to incrementally update the value
function by:
Q(s,a)=Q(s,a)−α(Q(s,a)−(r+γQ(s/prime,a/prime))) (11.55)
11.10 Maximum Entropy Reinforcement Learning 227
11.9.7 On-Policy vs Oﬀ-Policy Methods
On-policy methods, such as Sarsa, evaluate or improve the policy that is used
to make decisions They estimate the value of a policy while using it for control In contrast, oﬀ-policy methods, such as Q-learning, evaluate or improve a policy
diﬀerent from that used to generate the data Oﬀ-policy methods separate these
two functions into (1) a behavior policy, which is the policy used to generate be-
havior, and (2) a target policy, which is the policy that is imitated and improved Oﬀ-policy methods follow the behavior policy while improving the target policy,
often reusing experience generated from old policies 11.9.8 Sarsa( λ)
We may use TD updates in Sarsa, and Q-learning, by propagating rewards from
states with high rewards to the states that lead to them more eﬃciently, also
known as eligibility traces The Sarsa( λ) update is deﬁned by:
Q(s,a)=Q(s,a)−αN(s,a)(Q(s,a)−(r+γQ(s/prime,a/prime)) (11.56)
whereN(s,a)=λγN(s,a) counts the number of times action ais taken in state
s,γis a discount factor, and the parameter λ∈(0,1) controls the rate of decay 11.10 Maximum Entropy Reinforcement Learning
The objective of reinforcement learning is to ﬁnd an optimal policy that maxi-
mizes return:
π⋆= argmax
πEr∼π/bracketleftBigg/summationdisplay
tR(st,at)/bracketrightBigg
(11.57)
The objective of maximum entropy reinforcement learning is to ﬁnd an optimal
policy that maximizes return and conditional action entropy:
π⋆= argmax
πEr∼π/bracketleftBigg/summationdisplay
tR(st,at)+Hπ(at|st)/bracketrightBigg
(11.58)
whereHπistheentropyofthepolicyconditionaldistributionoveractionsdeﬁned
by:
Hπ(at|st)=Eπ[−logπ(at|st)] (11.59)
Optimizing this objective promotes both high return and exploration, leading to
actions with higher reward that allow taking random actions in the future Max-
imum entropy reinforcement learning is more robust to disturbances to the dy-
namics and rewards (Eysenbach and Levine, 2022) and partial observations Im-
proving upon this, a maximum–minimum entropy reinforcement learning frame-
work (Han and Sung, 2021) ﬁnds an optimal policy that maximizes return while
228 11 Reinforcement Learning
visiting states with low entropy and maximizing their entropy for improving
exploration 11.11 Summary
This chapter begins by deﬁning a stateless multi-armed bandit, presenting the
trade-oﬀ between exploration and exploitation

============================================================

=== CHUNK 149 ===
Palavras: 364
Caracteres: 2502
--------------------------------------------------
Next, we deﬁne a state machine,
and then deﬁne an MDP with known transition and reward functions Finally,
we present reinforcement learning in which the transition and reward functions
are unknown and therefore the agent interacts with the environment by sampling
the world 12 Deep Reinforcement Learning
12.1 Introduction
Deep reinforcement learning uses deep neural networks for estimating value func-
tions and policies in reinforcement learning Deep reinforcement learning has
achieved excellent performance on challenging control problems This success
includes virtual environments with large state and action spaces, such as mas-
tering chess, Shogi, and Go (Silver et al., 2018), achieving Grandmaster level
in StarCraft II (Vinyals et al., 2019), outracing champion Gran Turismo drivers
(Wurman et al., 2022), and real-world changing environments, such as learning
quadrupedal locomotion over challenging terrain (Lee et al., 2020), autonomous
navigation of stratospheric balloons (Bellemare et al., 2020), and magnetic con-
trol of a Tokamak plasma fusion reactor (Degrave et al., 2022) Chapter 11 presented MDPs and reinforcement learning A key diﬀerence be-
tween the two is that when solving MDPs we know the transition function T
and reward function R, whereas in reinforcement learning we do not know the
transitionorrewardfunctions.Inreinforcementlearninganagentsamplestheen-
vironment; Chapter 11 ends with the Q-learning algorithm, which learns Q(s,a)
from experience In many cases, storing the Qvalues in a table may be infeasible
when the state or action spaces are very large or when they are continuous For
example, the game of Go consists of 10170states A solution is to approximate
the value function or approximate the policy A deep neural network provides a
fast function approximation, allowing eﬃcient interpolation between predicted
state values, state–action values, and action probabilities In deep reinforcement
learning we use deep neural networks as fast function approximations for repre-
senting the state value function V(s), state–action value function Q(s,a), policy
π(a|s), or model Chapter 5 introduced convolutional neural networks, which classify images,
while Chapter 8 described Transformers used in natural language processing
and vision In deep reinforcement learning, instead of predicting image classes
we may predict the values of a state or the probabilities of actions π(a|s) using a
neural network, and based on these probabilities we may take action

============================================================

=== CHUNK 150 ===
Palavras: 365
Caracteres: 2624
--------------------------------------------------
The neural
network serves as a function approximation One choice for a non-linear function
approximation is a CNN, as shown in Figure 12.1 Given a state sas input,
such as an image of pixels, the neural network outputs an approximated value
230 12 Deep Reinforcement Learning
Figure 12.1 Deep reinforcement learning mapping an image state directly to an action
using a CNN by evaluating a policy π(at|st) of the state or an approximated vector of probabilities for each action given the
state, and based on these takes an action a The action ataken by the agent
results in the environment responding and sending the agent to state s/primewith a
reward The CNN may generalize to predict values or action probabilities given
an unseen image, or state 12.2 Function Approximation
Real-world problems often consist of large or continuous state and action spaces Adeepneuralnetworkmaybeusedtoapproximateastatevaluefunction Vθ(s)≈
Vπ(s), a state–action value function Qθ(s,a)≈Qπ(s,a), or to approximate a
policypθ(a|s), where θdenotes the network parameters The neural networks are
then optimized by stochastic gradient descent (SGD) as described in Chapter 3 12.2.1 State Value Function Approximation
Our goal is to ﬁnd the neural network parameters θthat minimize the mean
squared error (MSE) between a value function Vπ(s) with respect to a policy π
such as the optimal policy, and the neural network approximation Vθ(s):
J(θ)=1
2Es/bracketleftbig
(Vπ(s)−Vθ(s))2/bracketrightbig
(12.1)
Computing the expectation over states s∈Sresults in:
J(θ)=1
2|S|/summationdisplay
s∈S/parenleftbig
(Vπ(s)−Vθ(s))2/parenrightbig
(12.2)
However, since there may be too many states, we may wish to focus on learning
states which are visited multiple times Therefore, under policy πwe may spend
timeμ(s) in state sand therefore compute the expectation as:
J(θ)=/summationdisplay
s∈Sμ(s)/parenleftbig
(Vπ(s)−Vθ(s))2/parenrightbig
(12.3)
12.2 Function Approximation 231
where/summationtext
s∈Sμ(s)=1 The optimization objective J(θ) is a diﬀerentiable function of the network
parameters θ The gradient of J(θ) with respect to θis the vector ∇θJ(θ)=
(∂θ
θ1,...,∂θ
θn)Tdeﬁned by:
∇θJ(θ)=−Es[(Vπ(s)−Vθ(s))∇θVθ(s)] (12.4)
Optimization by gradient descent involves updating θin the direction of the
negative gradient by:
Δθ=α∇θJ(θ)=αEs[(Vπ(s)−Vθ(s))∇θVθ(s)] (12.5)
whereαis the learning rate, and the optimization starts from an initial guess
θ0 The sequence of parameter values {θi}:
θi+1=θi−α∇θJ(θi) (12.6)
wherei=1,...,nare a sequence of monotonically non-increasing values of the
objective J(θ0)≥J(θ1)≥···≥ J(θn) that converge toward a local minimum

============================================================

=== CHUNK 151 ===
Palavras: 353
Caracteres: 2442
--------------------------------------------------
Using SGD we approximate the gradient using a single random sample at a time
such that:
Δθ=α∇θJ(θ)=α(Vπ(s)−Vθ(s))∇θVθ(s) (12.7)
Since we do not know the ground-truth value function Vπ(s)w em a yu s ea n
estimate instead, such as the return g≈Vπ(s) in Monte Carlo (MC) learning to
approximate the value function with respect to π, updating the parameters by:
Δθ=α(g−Vθ(s))∇θVθ(s) (12.8)
or use the temporal diﬀerence (TD) target r+γVθ(s/prime)≈Vπ(s)i nT Dl e a r n i n g ,
updating the parameters by:
Δθ=α(r+γVθ(s/prime)−Vθ(s))∇θVθ(s) (12.9)
Since the TD target is a biased sample of the ground-truth value Vπ(s), we may
use the states and targets to form a training set of {(s,r+γVθ(s))}pairs and
then proceed by supervised learning 12.2.2 Action Value Function Approximation
In action value function approximation the neural network inputs are the states
sand actions aand the network parameterized by θoutputs a value Qθ(s,a) In a similar fashion to state value function approximation, our objective may
be minimizing the MSE between the approximate action value function Qθ(s,a)
and the action value function Qπ(s,a) with respect to a policy π, such as the
optimal policy:
J(θ)=1
2E(s,a)∼π/bracketleftbig
(Qπ(s,a)−Qθ(s,a))2/bracketrightbig
(12.10)
232 12 Deep Reinforcement Learning
where the expectation is over ( s,a) pairs from the policy π The optimization
objective J(θ) is a diﬀerentiable function of the network parameters θ The gra-
dient of J(θ) with respect to θis the vector ∇θJ(θ)=(∂J
θ1,...,∂J
θn)Tdeﬁned
by:
∇θJ(θ)=−E(s,a)∼π[(Qπ(s,a)−Qθ(s,a))∇θQθ(s,a)] (12.11)
Optimization by gradient descent involves updating θin the direction of the
negative gradient by:
Δθ=−α∇θJ(θ)=α(Qπ(s,a)−Qθ(s,a))∇θQθ(s,a) (12.12)
Since again we don’t know the true action value function Qπw em a yu s ea n
estimate instead based on the return g≈Qπ(st,at)i nM Cl e a r n i n g :
Δθ=α(g−Qθ(s,a))∇θQθ(s,a) (12.13)
orusetheTDtarget r+γQθ(s/prime,a/prime)−Qθ(s,a)≈Qπ(s,a)inTDlearning,updating
the parameters by:
Δθ=α(r+γQθ(s/prime,a/prime)−Qθ(s,a))∇θQθ(s,a) (12.14)
Challenges of function approximation in the reinforcement learning setting
include that (1) the agent’s experience is not independent and identically dis-
tributed (IID); (2) the agent’s policy aﬀects the future data it will sample; and
(3) the environment may change In addition, methods that use function ap-
proximation along with bootstrapping and oﬀ-policy learning may not converge

============================================================

=== CHUNK 152 ===
Palavras: 353
Caracteres: 2662
--------------------------------------------------
Next,wedescribedeepreinforcementlearningmethodsthatattempttoovercome
these challenges We will describe model-free approaches which may be divided into (1) value-
based or Q-learning methods such as NFQ (Riedmiller, 2005) and DQN (Mnih
et al., 2015); (2) policy-based or policy optimization methods such as PPO
(Schulman et al., 2017); and (3) actor–critic methods such as DDPG (Lillicrap
et al., 2016), which are a combination of (1) and (2) 12.3 Value-Based Methods
Value-basedmethodsfordeepreinforcementlearningapproximatethestatevalue
function or the action value function using a neural network 12.3.1 Experience Replay
Insupervisedlearningthetrainingexamplesmaybesampledindependentlyfrom
an underlying distribution In contrast, in reinforcement learning the states, ac-
tions, and rewards that an agent learns from experience in successive time steps
are correlated in time A solution to this problem, known as experience replay,
12.3 Value-Based Methods 233
is to use a replay buﬀer that stores previous states, actions, and rewards, specif-
ically storing tuples of ( s,a,r,s/prime), and then sample from the replay buﬀer when
updating the Qvalues Using a replay buﬀer may avoid catastrophic forgetting
of the state and action spaces Each experience tuple may be used for updating
the network weights multiple times, which is an eﬃcient use of the data Ran-
dom uniform sampling from the replay buﬀer reduces variance and the temporal
correlations between episodes 12.3.2 Neural Fitted Q-Iteration
In action value function approximation we minimized the MSE loss between the
approximateactionvaluefunction Qθ(s,a)andtheactionvaluefunction Qπ(s,a)
with respect to a policy π.Q-learning converges to Q⋆using a table lookup Therefore we will minimize the MSE loss between the approximate action value
function Qθ(s,a) and the optimal action value function Q⋆(s,a) by minimizing
the loss:
J(θ)=1
2E(s,a)∼π/bracketleftbig
(Q⋆(s,a)−Qθ(s,a))2/bracketrightbig
(12.15)
Again, optimization by gradient descent involves updating θin the direction of
the negative gradient by:
Δθ=α(Q⋆(s,a)−Qθ(s,a))∇θQθ(s,a) (12.16)
Since we don’t know the optimal action value function we may approximate
Q⋆(s,a)b y :
r+γmax
a/primeQθ(s/prime,a/prime)≈Q⋆(s,a) (12.17)
updating the network parameters by:
Δθ=α/parenleftBig
r+γmax
a/primeQθ(s/prime,a/prime)−Qθ(s,a)/parenrightBig
∇θQθ(s,a) (12.18)
Just using a neural network to approximate the action value function in Q-
learning may diverge since there are correlations between the samples and the
target is non-stationary Therefore, to remove the correlations between sam-
ples we may generate a dataset from the agent’s experience

============================================================

=== CHUNK 153 ===
Palavras: 353
Caracteres: 3113
--------------------------------------------------
Neural ﬁtted Q-
iteration (NFQ; (Riedmiller, 2005)) and Batch-Q (Ernst et al., 2005) methods
store batches of data in a buﬀer Dand use supervised learning with a neural
network to learn the action value function Thisresultsintheneuralﬁtted Q-iterationpseudocodedescribedinAlgorithm
12.1 12.3.3 Deep Q-Network
DeepQ-Network (DQN) (Mnih et al., 2015) builds upon ﬁtted Q-learning by
incorporating a replay buﬀer and a second target neural network, as described
next 234 12 Deep Reinforcement Learning
Algorithm 12.1 Neural ﬁtted Q-iteration (NFQ) initialize D=∅empty replay buﬀer
Qθnetwork parameters θwith random values
select start state s=s0
repeat:
forksteps do:
runε-greedy policy based on Qθnetwork
collect transitions ( s,a,r,s/prime) intoD+
D=D∪D+
createsupervised training set S={(x(i),y(i))}
foreach (s,a,r,s/prime)∈Ddo:
x(i)=(s,a)
y(i)=r+γmaxa/primeQθ(s/prime,a/prime)
retrain Q network by supervised learning on S
12.3.4 Target Network
In NFQ we set y(i)=r+γmaxa/primeQθ(s/prime,a/prime), whereas in DQN we set y(i)=
r+γmaxa/primeQθ−(s/prime,a/prime), where θ−are parameters of a target network At each
iteration, DQN minimizes the MSE loss:
L(θi)=E(s,a,r,s/prime)∼Di/bracketleftBig
(y(i)−Qθi(s,a))2/bracketrightBig
(12.19)
=E(s,a,r,s/prime)∼Di/bracketleftBig
(r+γmax
a/primeQθ−(s/prime,a/prime)−Qθi(s,a))2/bracketrightBig
(12.20)
The parameters θ−of the target network Qθ−(s/prime,a/prime) are frozen for multiple steps
while the parameters θiof the online network Qθi(s,a) are updated by SGD:
∇θiL(θi)=E(s,a,r,s/prime)∼Di/bracketleftBig
(y(i)−Qθi(s,a))∇θiQθi(s,a)/bracketrightBig
(12.21)
=E(s,a,r,s/prime)∼Di/bracketleftBig
(r+γmax
a/primeQθ−(s/prime,a/prime)−Qθi(s,a))∇θiQθi(s,a)/bracketrightBig
(12.22)
12.3.5 Algorithm
DeepQ-Networkusesexperiencereplayandatargetnetwork.Statesandrewards
are generated by the environment and therefore the algorithm is model-free The
states and rewards are generated by an ε-greedy behavior policy that is diﬀerent
fromtheonlinepolicylearnedandthereforethealgorithmisoﬀ-policy.TheDQN
algorithm is described in pseudocode in Algorithm 12.2 12.3.6 Prioritized Replay
Instead of sampling from the replay buﬀer uniformly, prioritized experience re-
play (Schaul et al., 2016) samples important transitions more frequently, which
12.3 Value-Based Methods 235
Algorithm 12.2 DeepQ-network (DQN) initialize D=∅empty replay buﬀer
onlineQθnetwork with parameters θwith random values
targetQθ−network with parameters θ−=θ
start state s=s0
repeat:
foreach episode do:
runε-greedy policy based on Qθnetwork
collect transitions ( s,a,r,s/prime) intoD
q=Qθ(s,a)
take action ato get reward rand next state s/prime
Qθ(s,a)=Qθ(s,a)+α(r+γmaxa/primeQθ−(s/prime,a/prime)−Qθ(s,a))
Δ=m a x {Δ,/bardblq−Qθ(s,a)/bardbl}
s=s/prime
updateθ−=θevery number of episodes
results in more eﬃcient learning We store the experiences in a priority queue
by their DQN error |r+γmaxa/primeQθ−(s/prime,a/prime)−Qθ(s,a)|and prioritize samples by
pα
i/summationtext
jpα
jwherepiis proportional to the DQN error, and αcontrols the amount of
prioritization such that setting α= 0 results in no prioritization

============================================================

=== CHUNK 154 ===
Palavras: 397
Caracteres: 3024
--------------------------------------------------
12.3.7 Double DQN
A problem with DQN is that the maximum operator uses the same values for
both selecting and evaluating an action, which may result in a higher value For example, given a state with ground truth Q⋆(s,a) = 0 the estimates of
Q(s,a) may be positive or negative such that Q/parenleftbigg
s,argmax
aQ(s,a)/parenrightbigg
>0 whereas
Q⋆/parenleftbigg
s,argmax
aQ⋆(s,a)/parenrightbigg
= 0 A solution, called double DQN (Van Hasselt et al.,
n.d.), replaces the DQN target:
y(i)=r+γmax
a/primeQθ−(s/prime,a/prime) (12.23)
with:
y(i)=r+γQθ−/parenleftbigg
s/prime,argmax
a/primeQθi(s/prime,a/prime)/parenrightbigg
(12.24)
such that the current Q-network with parameters θiare used to select actions
whereastheprevious Q-networkwithparameters θ−areusedtoevaluateactions 12.3.8 Dueling Networks
Duelingnetworkarchitecturesfordeepreinforcementlearning(Wangetal.,2016)
use two separate neural networks One network approximates the state value
236 12 Deep Reinforcement Learning
function V(s), and a second network approximates the state–action advantage
function Aπ(s,a) The advantage function is the diﬀerence between the state–
action value function and state value function:
Aπ(s,a)=Qπ(s,a)−Vπ(s), (12.25)
The advantage function is a relative measure of the importance of each action,
comparing each action to the average action of the policy The expectation of
the advantage function over all actions is zero Ea∼π(s)[Aπ(s,a) ]=0 12.4 Policy-Based Methods
Stochastic policy functions may output a distribution over a discrete set of ac-
tions,ormaybecontinuoussuchthat a∼N(μθ(s),σ2
θ(s)).Policy-basedmethods
work well in continuous spaces for learning stochastic policies An agent that interacts with the environment generates a trajectory τof state,
action and reward episodes τ=s0,a0,r0,...,s t,at,rt The return g(τ)o fat r a -
jectoryτis the discounted sum of rewards g(τ)=/summationtext
tγtrt The goal or objective
of policy-based methods J(πθ) is to ﬁnd a policy πθparameterized by θthat
maximizes the expected return over all trajectories τ∼πθsampled from the
policy The objective J(πθ) is deﬁned by:
J(πθ)=Eτ∼πθ[g(τ)] = Eτ∼πθ/bracketleftBigg/summationdisplay
tγtrt/bracketrightBigg
(12.26)
and taking the maximum over θresults in:
max
θJ(πθ)=m a x
θEτ∼πθ/bracketleftBigg/summationdisplay
tγtrt/bracketrightBigg
(12.27)
We maximize J(πθ) by gradient ascent on the policy parameters θ, updating the
parameters by:
θ=θ+α∇θJ(πθ) (12.28)
whereαis a learning rate and ∇θJ(πθ) is the policy gradient The expectation with respect to the trajectory τof the return g(τ)i s :
J(πθ)=Eτ∼πθ[g(τ)] =/summationdisplay
τp(τ|θ)g(τ) (12.29)
wherep(τ|θ) is the probability of a trajectory when following policy πparame-
terized by θ:
p(τ|θ)=/productdisplay
tp(st+1|st,at)πθ(at|st) (12.30)
and our goal is to compute the gradient of the expectation with respect to the
parameters θ:
∇θJ(πθ)=∇θEτ[g(τ)] (12.31)
12.4 Policy-Based Methods 237
We assume that p(τ|θ) is a diﬀerentiable probability density function that we
may sample from

============================================================

=== CHUNK 155 ===
Palavras: 463
Caracteres: 3751
--------------------------------------------------
12.4.1 Policy Gradient
By deﬁnition of expectation, taking the gradient of Equation 12.26, and then
bringing in the gradient, the policy gradient is:
∇θJ(πθ)=∇θEτ∼πθ[g(τ)] =∇θ/integraldisplay
τg(τ)p(τ|θ)dτ=/integraldisplay
τ∇θg(τ)p(τ|θ)dτ(12.32)
Using the chain rule we get:
∇θJ(πθ)=/integraldisplay
τ∇θg(τ)p(τ|θ)dτ=/integraldisplay
τ(g(τ)∇θp(τ|θ)+p(τ|θ)∇θg(τ))dτ,
(12.33)
and setting ∇θg(τ) = 0, and then multiplying byp(τ|θ)
p(τ|θ)results in:
∇θJ(πθ)=/integraldisplay
τg(τ)∇θp(τ|θ)dτ=/integraldisplay
τg(τ)p(τ|θ)∇θp(τ|θ)
p(τ|θ)dτ (12.34)
Since log( x)/prime=1
x, we replace ∇θlogp(τ|θ)=∇θp(τ|θ)
p(τ|θ), and then by the deﬁnition
of expectation we get:
∇θJ(πθ)=/integraldisplay
τg(τ)p(τ|θ)∇θlogp(τ|θ)dτ=Eτ∼πθ[g(τ)∇θlogp(τ|θ)] (12.35)
The probability p(τ|θ) of a trajectory τgiven parameters θmay be repre-
sented using the policy πθ(a|s) and the transition probabilities of the environ-
mentp(s/prime|s,a) In state sthe agent takes an action awith probability based on
the policy, and then the environment transitions the agent to state s/primebased on
the state sand the agent’s action a, and this process continues over all time
steps; therefore:
p(τ|θ)=/productdisplay
tp(st+1|st,at)πθ(at|st), (12.36)
where the product is over time steps Taking the logarithm allows us to turn the
product into a sum:
logp(τ|θ)=/summationdisplay
t(logp(st+1|st,at)+logπθ(at|st)) (12.37)
Taking the gradient with respect to θand noticing that the environment transi-
tion probabilities are independent of θresults in:
∇θlogp(τ|θ)=∇θ/summationdisplay
(logp(s/prime|s,a)+logπθ(a|s)) =∇θ/summationdisplay
logπθ(a|s) (12.38)
Now, plugging Equation 12.38 into Equation 12.35 and then bringing the return
g(τ) into the sum results in the expectation:
∇θJ(πθ)=Eτ∼πθ[g(τ)∇θlogp(τ|θ)] = Eτ∼πθ/bracketleftBigg/summationdisplay
tgt(τ)∇θlogπθ(at|st)/bracketrightBigg
(12.39)
238 12 Deep Reinforcement Learning
which is a diﬀerentiable function and may be estimated by a sample mean 12.4.2 REINFORCE
The REINFORCE algorithm estimates the policy gradient numerically by MC
sampling, using random samples to approximate the policy gradient For each
episode we sample a new trajectory τ Next, for each time step twe compute the
returngt, and sum the policy gradients over all time steps to get ∇θJ(πθ) The
contribution of each time step to the policy gradient is the return gttimes the
score∇θlogπθ(at|st) which both depend on the current policy πθ, and there-
fore REINFORCE is an on-policy algorithm Finally, we update the network
parameters θusing the policy gradient ∇θJ(πθ) The REINFORCE pseudocode
is described in Algorithm 12.3 Algorithm 12.3 REINFORCE initialize learning rate α, parameters θof policy network πθ
repeat for each episode:
sample trajectory τ=s0,a0,r0,...,s T,aT,rTfollowing πθ
∇θJ(πθ)=0
foreach time step t=0,...,T do:
gt(τ)=/summationtextT
i=tγi−trt
update policy gradient ∇θJ(πθ)=∇θJ(πθ)+gt(τ)∇θlogπθ(at|st)
update policy parameters θ=θ+α∇θJ(πθ)
12.4.3 Subtracting a Baseline
The estimate of the gradient, given by:
ˆg=1
nn/summationdisplay
i=1∇θlogpθ(τ(i))g(τ) (12.40)
accurately approximates the true gradient for many samples i=1,...,n.T o
reduce the variance we may subtract a baseline bfrom the return such that:
ˆg=1
nn/summationdisplay
i=1∇θlogpθ(τ(i))(g(τ)−b) (12.41)
=1
nn/summationdisplay
i=1∇θlogpθ/parenleftBig
τ(i)/parenrightBig
g(τ)−1
nn/summationdisplay
i=1∇θlogpθ(τ(i))b (12.42)
12.5 Actor–Critic Methods 239
where the expectation of the term on the right is zero:
/summationdisplay
τpθ(τ)∇θlogpθ(τ)b=/summationdisplay
τpθ(τ)∇θpθ(τ)
pθ(τ)b (12.43)
=b/summationdisplay
τ∇θpθ(τ)=b∇θ/summationdisplay
τpθ(τ) = 0 (12.44)
The baseline may be a constant b=E[g(τ)], dependent on time bt=/summationtextn
i=1gi
t,o r
a function of state b(s)=Vπ(s)

============================================================

=== CHUNK 156 ===
Palavras: 375
Caracteres: 2940
--------------------------------------------------
12.5 Actor–Critic Methods
Actor–critic methods combine policy-based methods with value-based methods
by using both the policy gradient and value function The actor is a policy
network πθwith parameters θmapping states to action probabilities The critic
is avalue network Vφ(s)o rQφ(s,a)o rAφ(s,a) withparameters φapproximating
a state value function or action value function or advantage function Putting
these two networks back-to-back, the critic provides a loss function for the actor
and the gradients backpropagate from the critic to the actor In the policy-based REINFORCE algorithm we estimated the policy gradient
∇θJ(πθ) by randomly sampling one trajectory at a time This trajectory results
in a return that may be signiﬁcantly diﬀerent from returns of other trajectories
and therefore the policy gradient has high variance We may use value function
approximation to reduce this variance Speciﬁcally, in REINFORCE we estimate
the policy gradient by:
∇θJ(πθ)=Eτ∼πθ/bracketleftBigg/summationdisplay
tgt(τ)∇θlogπθ(at|st)/bracketrightBigg
(12.45)
Toreducevariancewemay replacethereturn g(τ)timesthescore ∇θlogπθ(at|st),
with a value function approximation Qφ(s,a) times the score, which results in
theQ-value actor–critic algorithm Alternatively, we may use the estimate Vφ(s)
as a baseline computing the action advantage function A(s,a)=g(τ)−Vφ(s) The actor–critic pseudocode is shown in Algorithm 12.4 We ﬁrst initialize the
policy parameters θand critic parameters φ Next, we repeatedly perform the
following steps: (1) sample trajectory τ={st,at}tusing the current policy πθ;
(2) ﬁt a value function Vφ(s) using MC or TD learning and update critic param-
etersφ; (3) compute the action advantage function Aφ(st,at)=gt−Vφ(st); (4)
approximate the policy gradient ∇θJ(πθ); and (5) update the policy parameters
θ 240 12 Deep Reinforcement Learning
Algorithm 12.4 Actor–critic initialize learning rate α, actor policy parameters θand critic parameters φ
foreach episode do:
sample trajectory τ=s0,a0,r0,...,s T,aT,rTfollowing πθ
ﬁt value function Vφ(s) using MC or TD learning
update critic parameters φ
compute action advantage function Aφ(st,at)
approximate policy gradient ∇θJ(πθ)
update policy parameters θ=θ+α∇θJ(πθ)
=0
12.5.1 Advantage Actor–Critic
Advantage actor–critic (A2C) methods (Mnih et al., 2016) estimate the policy
gradient based on an approximation of the advantage function:
∇θJ(πθ)=Eτ∼πθ/bracketleftBigg/summationdisplay
t∇θlogπθ(at|st)γt−1Aφ(st,at)/bracketrightBigg
(12.46)
where the advantage function is deﬁned by:
Aφ(s,a)=Er,s/prime[r+γVπθ(s/prime)−Vπθ(s)] (12.47)
The critic estimates ( r+γVπθ(s/prime)−Vπθ(s)) by the TD error ( r+γVφ(s/prime)−Vφ(s))
whereVφis an estimate of the value function Vπθ, and the gradient of the actor
(Schulman et al., 2016) is estimated by:
∇θJ(πθ)=Eτ∼πθ/bracketleftBigg/summationdisplay
t∇θlogπθ(at|st)γt−1(r+γVφ(st+1)−Vφ(st))/bracketrightBigg
(12.48)
by rolling out trajectories

============================================================

=== CHUNK 157 ===
Palavras: 361
Caracteres: 2900
--------------------------------------------------
Generalized advantage estimation (GAE) approxi-
mates the advantage by:
Aθ(s,a)=E/bracketleftBigg/summationdisplay
t(λγ)t−1(rt+γVφ(st+1)−Vφ(st))/bracketrightBigg
(12.49)
whereλtrades-oﬀ bias and variance 12.5.2 Asynchronous Advantage Actor–Critic
In order for neural network training to be stable, the gradient updates should
not be correlated, which is why experience replay is used in DQN An alternative
that does not use a replay buﬀer is to parallelize the experiences using multiple
threads, and therefore not be limited to oﬀ-policy methods and able to use
data from the current policy to improve the policy Asynchronous advantage
actor–critic (A3C) (Mnih et al., 2016) explores diﬀerent parts of the environment
using multiple agents that contribute experiences in parallel The agents may be
12.5 Actor–Critic Methods 241
trained using diverse policy gradient methods and may use diverse exploration
valuesof ε.InA3C,eachagentisresettoaglobalnetworkwhichmayhavediverse
policies or critics Next, the agents interact with the environment, computing the
value, policy loss, and gradients Finally, the agents update the global network
with the gradients, and the process is repeated The gradient updates may be
performed asynchronously, or applied synchronously by averaging the gradients
from all agents and updating the global network parameters 12.5.3 Importance Sampling
Given a function f(x), computing the expectation Ep(x)[f(x)] from a distribution
Pmay be diﬃcult, and therefore importance sampling allows sampling from a
diﬀerent distribution Q:
EP(x)[f(x)] = EQ(x)/bracketleftbiggP(x)
Q(x)f(x)/bracketrightbigg
(12.50)
and reweighting the samples Importancesamplingmaybeusedtoestimatetheexpectedreturnofastochas-
tic policy by turning:
J(πθ)=Eτ∼πθ[P(τ|θ)r(τ)] (12.51)
into a surrogate loss:
J(πθ)=Eτ∼πθ/prime/bracketleftbiggP(τ|θ/prime)
P(τ|θ)r(τ)/bracketrightbigg
(12.52)
such that the gradient is:
∇θJ(πθ)=Eτ∼πθ/prime/bracketleftbigg∇θ/primeP(τ|θ/prime)
P(τ|θ)r(τ)/bracketrightbigg
(12.53)
which allows collecting data from an old policy parameterized by θand com-
puting the direction in which the new policy parameterized by θ/primeshould be
improved For θ/prime=θthis reduces to policy gradient 12.5.4 Surrogate Loss
Policy gradient reinforcement learning algorithms rely on updating a policy by
modifying its parameters If this modiﬁcation results in a poor policy then this
will result in poor samples from that policy so that altogether the reinforcement
learningalgorithmmaybecomestuckwithpoorpoliciesandsubsequentsamples To overcome this problem we may add a constraint to the reinforcement learn-
ing objective that encourages the policy to improve while avoiding deteriorating
performance Trust region policy optimization (TRPO) and proximal policy op-
timization (PPO) add a constraint to the optimization objective, encouraging
consecutive policies to improve monotonically

============================================================

=== CHUNK 158 ===
Palavras: 383
Caracteres: 2763
--------------------------------------------------
These algorithms diﬀer in the im-
plementation of this constraint: TRPO implements a second-order constraint,
whereas PPO implements a simpler ﬁrst-order constraint 242 12 Deep Reinforcement Learning
12.5.5 Natural Policy Gradient
In reinforcement learning the dataset collected depends on the policy and when
using neural networks depends on the network parameters Therefore, when op-
timizing policy-based methods, choosing a step size for updating the policy pa-
rameters is key If the step size is too large then that will result in a bad policy,
which in turn will result in collecting bad data under that policy from which the
agent may not recover If the step size is too small then that will result in not
using the experience eﬃciently Takinggradientstepsintheparameterspace θofapolicynetwork πθisdeﬁned
by:
Δθ=θ/prime−θ=α∇θJ(πθ) (12.54)
Using the ﬁrst-order Taylor expansion of the objective J(πθ/prime)≈J(πθ)+
∇θJ(πθ)TΔθwe may constrain the gradient step using the term dependent on
θ/primeby a threshold εon the/lscript2norm of Δ θ:
maximize
θ/prime∇θJ(πθ)TΔθs.t.1
2ΔθTIΔθ=/bardblΔθ/bardbl2
2≤ε (12.55)
which has an analytic solution:
Δθ=√
2ε∇θJ(πθ)
/bardbl∇θJ(πθ)/bardbl(12.56)
Directly constraining Δ θdoes not consider the corresponding distance in the
policy space between πθ/primeandπθ Therefore, we may constrain the distribution
over policy trajectories based on the Kullback–Leibler (KL) divergence between
the old distribution πθand new distribution πθ/primesuch that DKL(πθ||πθ/prime)≤ε.T h e
natural policy gradient (NPG) constrains the objective function to be subject to
E[DKL(πθ(·|st)||πθ/prime(·|st)]≤ε Computing the second-order Taylor expansion of
the KL results in the objective:
maximize
θ/prime∇θJ(πθ)TΔθs.t.1
2ΔθTFθΔθ≤ε (12.57)
whereFθis the Fisher information matrix (Kakade, 2001), deﬁned as:
Fθ=Eτ∼πθ/bracketleftbig
∇logp(τ|θ)∇logp(τ|θ)T/bracketrightbig
(12.58)
and has an analytic solution:
Δθ=F−1
θ∇θJ(πθ)/radicalBigg
2ε
∇θJ(πθ)TF−1
θ∇θJ(πθ)(12.59)
which results in the natural gradient (Amari, 1998) gNsuch that Δ θ=αgNand
∇θJ(πθ)a n dFθmay be approximated by sampling trajectories using conjugate
gradient descent (Kakade, 2001) 12.5 Actor–Critic Methods 243
12.5.6 Trust Region Policy Optimization
The policy gradient approach uses a step size and gradient to update the policy
parameters, which is a ﬁrst-order approximation In contrast, TRPO (Schulman
et al., 2015) is a second-order method that uses the conjugate gradient to avoid
computing the inverse of the Hessian In supervised learning, using a step size
which is too large may be corrected for in following iterations; however, in re-
inforcement learning it may result in a bad policy that will result in poor data
collection and will be diﬃcult to recover from

============================================================

=== CHUNK 159 ===
Palavras: 351
Caracteres: 3122
--------------------------------------------------
Therefore, selecting a good step
size is important in policy gradient approaches Using line search for selecting
an optimal step size would require performing multiple rollouts for diﬀerent step
sizes, which is computationally expensive Instead, we use the NPG approach
to constrain the surrogate loss by the KL divergence between the new and old
policy, which results in a second-order method:
θ⋆= argmax
θL(πθ,πθ/prime) s.t.DKL(P(τ;θ)||P(τ;θ/prime))≤ε (12.60)
where:
L(πθ,πθ/prime)=E/bracketleftbiggπθ/prime(a|s)
πθ(a|s)Aθ(s,a)/bracketrightbigg
(12.61)
Plugging in
P(τ;θ)=P(s0)/productdisplay
tπθ(at|st)P(st+1|st,at) (12.62)
to the KL divergence:
DKL(P(τ;θ)||P(τ;θ/prime)) =/summationdisplay
τP(τ;θ)logP(τ;θ/prime)
P(τ;θ)(12.63)
we get:
DKL(P(τ;θ)||P(τ;θt)) =/summationdisplay
τP(τ;θ)logP(s0)/producttext
tπθ/prime(at|st)P(st+1|st,at)
P(s0)/producttext
tπθ(at|st)P(st+1|st,at)
(12.64)
and canceling out the dynamics yields:
DKL(P(τ;θ)||P(τ;θ/prime)) =/summationdisplay
τP(τ;θ)log/producttext
tπθ/prime(at|st)/producttext
tπθ(at|st)(12.65)
and sampling from the new policy:
DKL(P(τ;θ)||P(τ;θ/prime))≈1
n/summationdisplay
(a,s)∼πθ/primelogπθ/prime(a|s)
πθ(a|s)(12.66)
resulting in the constrained optimization or surrogate objective:
maximize
θE/bracketleftbiggπθ/prime(a|s)
πθ(a|s)Aθ(a,s)/bracketrightbigg
s.t E[DKL(πθ(·|s)||πθ/prime(·|s))]≤ε(12.67)
A high-level TRPO pseudocode is described in Algorithm 12.5 244 12 Deep Reinforcement Learning
Algorithm 12.5 Trust region policy optimization initialize learning rate α, parameters θof policy network πθ
foreach episode do:
sample trajectory τ=s0,a0,r0,...,s T,aT,rTfollowing πθ
estimate advantage function at all time steps
compute policy gradient g
use conjugate gradient to compute F−1g
Fis the Fisher information matrix
perform line search on the surrogate loss and KL constraint
12.5.7 Proximal Policy Optimization
Trust region policy optimization requires solving a second-order optimization
problem.Proximalpolicyoptimization(Schulmanetal.,2017)isbasedonTRPO;
however, it is a ﬁrst-order method that avoids computing the Hessian matrix or
line search by clipping the surrogate objective It clips the TRPO surrogate ob-
jective in Equation 12.67 around 1 ±δand takes the minimum of the original
and clipped objectives resulting in the PPO surrogate objective:
maximize
θE/bracketleftbigg
min/parenleftbiggπθ/prime(a|s)
πθ(a|s)Aθ(s,a),clip/parenleftbiggπθ/prime(a|s)
πθ(a|s),1−δ,1+δ/parenrightbigg
Aθ(s,a)/parenrightbigg/bracketrightbigg
(12.68)
s.t E[DKL(πθ(·|s)||πθ/prime(·|s))]≤ε (12.69)
A high-level PPO pseudocode is described in Algorithm 12.6 Algorithm 12.6 Proximal policy optimization initialize policyπθparameters θ
foreach episode do:
run old policy πθ
compute advantage estimates Aθ
optimize surrogate objective in Eq 12.68 with respect to θ
update policy parameters
12.5.8 Deep Deterministic Policy Gradient
Deep deterministic policy gradient (DDPG) Lillicrap et al (2016) may be used
in continuous action spaces and combines DQN with REINFORCE It uses an
action value critic Qφ(s,a) parameterized by φand a deterministic policy πθ(s)
parameterized by θ

============================================================

=== CHUNK 160 ===
Palavras: 374
Caracteres: 2542
--------------------------------------------------
In a similar fashion to actor–critic methods, we perform
gradient descent to minimize the loss function with respect to the parameters
φof the critic and gradient ascent to ﬁnd the parameters θthat maximize the
12.6 Model-Based Reinforcement Learning 245
actor objective The critic loss is deﬁned by:
L(φ)=1
2E(s,a,r,s/prime)/bracketleftbig
(r+γQφ(s/prime,πθ(s/prime))−Qφ(s,a))2/bracketrightbig
(12.70)
and the gradient as:
∇φL(φ)=E(s,a,r,s/prime)[(r+γQφ(s/prime,πθ(s/prime))−Qφ(s,a))
(γ∇φQφ(s/prime,πθ(s/prime))−∇φQφ(s,a))(12.71)
The actor loss is deﬁned by:
J(θ)=Es[Qφ(s,πθ(s))] (12.72)
In practice DDPG uses experience replay to improve stability and adding Gaus-
sian noise to the actions of the policy πθimproves exploration The DDPG
pseudocode is described in Algorithm 12.7 Algorithm 12.7 Deep deterministic policy gradient initialize policy parameters θof an actor πθand action value parameters φ
of a critic network Qφ
foreach episode do:
given initial state s
foreach time step do:
select action aaccording to policy network πθ(a|s)
execute action aand observe reward rand next state s/prime
store tuple ( s,a,r,s/prime) in buﬀer D=D∪(s,a,r,s/prime)
sample mini-batch of tuples from buﬀer ( si,ai,ri,s/primei)∈D
update critic by minimizing loss in Eq 12.70 over sampled tuples
update actor policy using sampled policy gradients
update actor and critic network parameters
12.6 Model-Based Reinforcement Learning
Model-based reinforcement learning approaches may be divided into methods
that are given the model, such as AlphaZero (Silver et al., 2018), and methods
that learn the model, such as world models (Ha and Schmidhuber, 2018) 12.6.1 Monte Carlo Tree Search
Search trees have been used in board games such as chess (Arenz, 2012) A
key problem with these search algorithms is that their branching factor grows
exponentially with the number of units or pieces in the game A simple forward
search has exponential time complexity of O((|S||A|)d) for a state set S, action
setAand tree depth d The set of states may be reduced by sampling a subset of
246 12 Deep Reinforcement Learning
states, though still has an exponential time complexity Branch-and-bound uses
a lower bound on the value function and an upper bound on the action value
function to prune branches of the search tree, though still has an exponential
time complexity in the worst case In contrast, Monte Carlo tree search (MCTS)
runs simulations from a given state and therefore has time complexity of O(nd),
wherenis the number of simulations and dthe tree depth

============================================================

=== CHUNK 161 ===
Palavras: 358
Caracteres: 2401
--------------------------------------------------
The MCTS algorithm selects actions based on the upper conﬁdence bound
(UCB):
Q(s,a)+c/radicalBigg
logN(s)
N(s,a)(12.73)
whereQ(s,a) is the action value function, cis an exploration constant, N(s,a)
is the number of action–state pairs, and N(s)=/summationtext
aN(s,a)i st h en u m b e ro f
state visits Algorithm 12.8 Monte Carlo tree search initialize start state s, action value function Q(s,a), number of state visits
N(s), number of state–action pairs N(s,a)
foreach simulation do:
sample trajectory τfollowing π
update policy parameters θ=θ+α∇θJ(πθ)
12.6.2 Expert Iteration and AlphaZero
Model-based reinforcement learning (Feinberg et al., 2018) has given rise to ex-
pert iteration (Anthony et al., 2017), which iterates between dual policies of a
deep neural network and MCTS, applied to the game of Hex, followed by Alp-
haZero (Silver et al., 2017, 2018), which adds self-play, applied to chess, Shogi,
and Go The MCTS hyperparameters are tuned using Bayesian optimization
(Chen, Huang, Wang, Antonoglou, Schrittwieser, Silver and de Freitas, 2018) Initially devised for two-player competitive board games such as Hex, Go, chess,
and Shogi, expert iteration and AlphaZero have been extended to single-player
games using a sequence model for automatic machine learning (Drori, Krishna-
murthy, Rampin, Lourenco, One, Cho, Silva and Freire, 2018) in a system called
AlphaD3M, which automatically synthesizes solution machine learning pipelines
for a given dataset and task The single-player extension has been used for solv-
ing the Rubik’s Cube (McAleer et al., 2018) using a training set generated by
scrambling the solution These methods have also been generalized to continu-
ous domains (Moerland et al., 2018) for control with applications in robotics and
self-driving cars for good sequential decision-making Expert iteration, or AlphaZero, uses a neural network to output a policy ap-
proximation πθ(a|s) and state value function V(s) approximation for guiding
12.6 Model-Based Reinforcement Learning 247
MCTS Originally, two separate networks were used, which were merged into a
single network fθ(s) that receives a state representation as input sand computes
a vector of probabilities pθ=P(a|s) over all valid actions aand state values Vθ(s)
over states s AlphaZero learns these action probabilities and estimated values
from games of self-play, which guide the search in future games

============================================================

=== CHUNK 162 ===
Palavras: 353
Caracteres: 2263
--------------------------------------------------
The parameters
θare updated by SGD on the following loss function:
L(θ)=−πlogp+(V−e)2+α/bardblθ/bardbl2(12.74)
maximizing cross entropy between policy vector pand search probabilities π,
minimizing the MSE between predicted performance vand actual evaluation e,
and regularizing the network parameters θto avoid overﬁtting AlphaZero uses
MCTS which is a stochastic search using a UCB update rule of the action value
function:
U(s,a)=Q(s,a)+cP(a|s)/radicalbig
N(s)
1+N(s,a)(12.75)
whereQ(s,a) is the expected reward for action afrom state s,N(s,a) is the
number of times action awas taken from state s,P(a|s)i st h ee s t i m a t eo f
the neural network for the probability of taking action afrom state sandc
is a constant that determines the amount of exploration At each step of the
simulation, we ﬁnd the action aand state swhich maximize U(s,a) and add
the new state to the tree, if it does not exist, with the neural network estimates
P(a|s),V(s), or call the search recursively Finally, the search terminates and
action is taken 12.6.3 World Models
World models (Ha and Schmidhuber, 2018) are an example of model-based re-
inforcement learning in which the model is not given A world model is a neural
game simulator that uses a variational autoencoder (VAE) and recurrent neural
network (RNN) to take action in an environment The VAE is trained on images
from the environment, learning a low-dimension latent representation zof state
s The RNN is trained on the VAE latent vectors ztthrough time, predicting
p(zt+1|at,zt,ht) The latent vector ztand RNN hidden vector htare fed into a
neural network controller that outputs an action that aﬀects the environment,
resulting in a new image or state stthat is fed back to the VAE Since the world
model also predicts the next latent space vector it may be used to synthesize
images of the environment, creating a neural simulation of the environment The
world model may then be trained within that simulation; however, the agent
needs to sample new data from the environment by exploration in order to learn
new regions of the state and action spaces 248 12 Deep Reinforcement Learning
12.7 Imitation Learning
Ratherthanlearningfromrewards,imitationlearninglearnsfromexampledemon-
strations provided by an expert

============================================================

=== CHUNK 163 ===
Palavras: 362
Caracteres: 2729
--------------------------------------------------
Behavioral cloning uses supervised learning to
ﬁnd parameters θof a policy πθby computing the maximum log-likelihood:
θ⋆= argmax
θ/summationdisplay
(s,a)∈Dlogπθ(a|s) (12.76)
whereDare expert demonstrations and the policy πθmay be a neural network Alimitationofbehavioralcloningisthatitperformspoorlynearboundarystates
that are not well represented by the demonstrations, and once encountered may
not recover from cascading errors Dataset aggregation (DAgger) (Ross et al., 2011) aims to solve the problem
of cascading errors by augmenting the data with expert action labels of pol-
icy rollouts DAgger iteratively aggregates additional correctly labeled data and
retrains the policy Stochastic mixing iterative learning (SMILe) (Ross and Bag-
nell, 2010) trains a new policy only on the augmented data and then mixes the
new policy with the previous policies by having the agent act according to each
new policy πiwith probability p(p−1)i Generative adversarial imitation learning (GAIL) (Ho and Ermon, 2016) uses
state and action examples ( s,a)∼Prealfrom expert demonstrations as real
samples for a discriminator in a GAN setting, as described in Chapter 9 The
generator learns a policy πθ(a|s) by generating actions from states, and these
(a,πθ(s)) pairs are input to a discriminator The discriminator’s Dφgoal is to
distinguish between expert demonstration pairs ( s,a)∼Prealand pairs syn-
thesized by the generator ( s,πθ(s)) The generator may learn a policy, such as
TRPO, using the discriminator’s feedback as a reward Inversereinforcementlearningexplicitlyderivesarewardfunctionfromasetof
expertdemonstrationsandusesthatrewardtolearnanoptimalpolicy.Maximum
entropyinversereinforcementlearning(Ziebartetal.,2008)prefersadistribution
over policy trajectories τof the form:
Pθ(τ)=exp(Rθ(τ))/summationtext
τexp(Rθ(τ))(12.77)
whereθare the parameters of the reward function Rθ(τ)a n dPθ(τ) is the prob-
ability of a trajectory τ In a similar fashion to Equation 12.76 we may ﬁnd the
best parameter θ⋆by computing the maximum log-likelihood:
θ⋆= argmax
θ/parenleftBigg/summationdisplay
τ∈DlogPθ(τ)/parenrightBigg
(12.78)
whereDis a set of expert demonstrations 12.8 Exploration 249
12.8 Exploration
Chapter 11 described ε-greedy, which is a simple approach that promotes explo-
ration by taking a random action with probability εand the greedy action with
probability 1 −ε We may also promote exploration using only greedy actions by
modifying the transition function and reward instead In the model-based rein-
forcement learning method presented in Chapter 11 the transition function and
reward are modeled based on the number of visited state–action pairs N(s,a)
and the number of visited ( s,a,s/prime) tuplesN(s,a,/primes)

============================================================

=== CHUNK 164 ===
Palavras: 366
Caracteres: 2574
--------------------------------------------------
To promote exploration of
unknown parts of state space we may modify the transition function and reward
by preferring states and actions that have not been highly explored (Brafman
and Tennenholtz, 2002) The modiﬁed transition function T(s,a,s/prime) sets the next
states/primeto be the current state sifN(s,a)<k,a n dN(s,a,s/prime)+1
N(s,a)+|S|otherwise Simi-
larly, the modiﬁed reward R(s,a) is set to a maximum value RmaxifN(s,a)<k,
and/summationtext
(s,a)r(s,a)
N(s,a)otherwise 12.8.1 Sparse Rewards
Environments with sparse rewards, such as the video games Montezuma’s Re-
venge and Pitfall, posed a challenge to reinforcement learning Early deep re-
inforcement learning methods such as DQN perform no better than random
on these games Go-Explore (Ecoﬀet et al., 2019) and First Return, then Ex-
plore (Ecoﬀet et al., 2021) are reinforcement learning algorithms that perform
at super-human level on these sparse reward game environments as well as real-
world pick-and-place tasks Rather than adding randomness to a fraction of the
actions using ε-greedy or by sampling from a stochastic policy, Go-Explore (1)
stores promising states in a buﬀer, (2) ﬁrst returns to these states and then (3)
explores the environment 12.9 Summary
This chapter covers deep reinforcement learning starting from function approxi-
mation Deep model-free and policy-based methods are described in detail, fol-
lowed by their combination resulting in actor–critic methods Deep model-based
methods that are given the model, including MCTS and AlphaZero, as well as
world models that learn the model, are presented Imitation learning learns a
policy from expert demonstrations rather than from an explicit reward Finally,
we discuss methods that promote exploration and recent methods that work well
in environments with sparse rewards Part V
Applications

13 Applications
13.1 Introduction
This chapter covers a dozen state-of-the-art applications of deep learning in a
broad range of domains: autonomous vehicles, climate change and climate moni-
toring, computer vision, audio processing, voice swapping, music synthesis, natu-
ral language processing, automated machine learning, learning-to-learn courses,
protein structure prediction and docking, combinatorial optimization, compu-
tational ﬂuid dynamics, and plasma physics Each deep learning application is
brieﬂy described, along with a visualization or system architecture 13.2 Autonomous Vehicles
With the rise in self-driving cars, building systems that translate to high on-road
performance is key to achieving deployable systems

============================================================

=== CHUNK 165 ===
Palavras: 365
Caracteres: 2442
--------------------------------------------------
End-to-end models have
been used to predict steering commands using raw pixels from a front camera
alone (Bojarski et al., 2016) The authors argue that such a system optimizes
overall performance instead of optimizing human-selected intermediate criteria
like lane detection, which does not necessarily guarantee overall performance Other systems try to use a 360-degree view and a route planner as part of the
inputs These incorporate more information than simply a front camera view This is closely related to the broader ﬁeld of perception and the mental mapping
of a route that a human inherently perceives These are especially useful in
complex driving scenarios like intersections and city environments (Hecker et al.,
2018) Map information along with passenger comfort measures have also been
shown to improve accuracy (Hecker et al., 2020) Related works have shown the
power of neural memory networks to capture temporal information (Fernando
et al., 2017), moving away from the paradigm of mapping a single frame to
action and instead incorporating long-term dependencies, which are crucial in
self-driving Rather than predicting a vehicle’s trajectory directly, many systems ﬁrst con-
struct mid-level representations of the environment These systems may rely on
LiDAR (light detection and ranging) and ultrasonic sensors in addition to im-
age data Typical tasks include object detection for objects relevant to driving,
254 13 Applications
such as pedestrians, traﬃc lights, and other vehicles, semantic segmentation to
delineate the boundaries of the road as opposed to the sidewalk and other areas,
and scene reconstruction, to generate 3D scenes given the input data (Shaﬁee
et al., 2020) These representations are then used as input for predicting driving
actions A multi-modal multi-task approach (Yang et al., 2018) was introduced to
address the inherent relationship between a speed and steering angle prediction They note that a human driver does not independently make decisions for each
of these tasks For example, to navigate an imminent obstacle, a human driver
wouldchooseadiﬀerentsteeringangledependingontheircurrentspeed.Amulti-
task system addresses the inherent interconnectedness of predicting both of these
actions Another approach (Luo et al., 2018), tried to jointly reason about 3D
detection, tracking, and forecasting, given data captured by a 3D sensor from a
bird’s eye view representation of the 3D world

============================================================

=== CHUNK 166 ===
Palavras: 362
Caracteres: 2488
--------------------------------------------------
Using this joint representation
makes the model more robust to problems such as occlusion and sparse data ChauﬀeurNet (Bansal et al., 2018) uses imitation learning to learn driving
patterns from 30 million examples To augment the dataset, they introduce per-
turbations that may result in undesirable events like collisions, and they incorpo-
rate additional losses into their training loss to penalize these undesirable events This leads to a more robust model Rather than predicting speed and steering
wheel angle directly, ChauﬀeurNet predicts trajectories, then uses a mid-level
controller to translate the trajectories to vehicle-speciﬁc actions This provides a
system that can be used more generally in autonomous vehicles of various makes Several works study the trajectory prediction of all agents within an envir-
onment Multiple futures prediction (Tang and Salakhutdinov, 2019) performs
planning via computing a conditional probability density over the trajectories
of other agents Multiple future predictions for the agent under consideration
and other agents are essential in considering the various possibilities at a given
instant in time Multi-head attention-based probabilistic vehicle trajectory pre-
diction (Kim et al., 2020) also goes about multiple future predictions, using
multi-head attention to attend to particular futures of speciﬁc agents more than
the rest Predicting the trajectory of a vehicle in a multi-agent environment is a chal-
lengingandcriticaltaskfordevelopingsafeautonomousvehicles.State-of-the-art
models rely on a representation of the environment from direct, low-level input
from sensors on the vehicle or a mid-level representation of the scene, which is
commonly a map annotated with agent positions These approaches rely on a
model to encode either camera data in the low-level case or annotated maps
in the mid-level case We show an example of both types of representations in
Figure 13.1 As depicted in the top-left, mid-level representations are used to
predict candidate trajectories, as shown in the top-right Low-level representa-
tions such as camera data shown in the bottom-left can be used end-to-end to
predict steering angles, as illustrated in the bottom right To encode these in-
put representations, rather than training a model from scratch, state-of-the-art
13.3 Climate Change and Climate Monitoring 255
Figure 13.1 An example of input and output representations for mid-level (top) and
low-level (bottom) representations

============================================================

=== CHUNK 167 ===
Palavras: 353
Caracteres: 2489
--------------------------------------------------
The mid-level input representation is an
annotated map of the scene (top-left) in the top row, with boxes representing agent
positions and colors representing semantic categories The output (top-right) is a
probability distribution over candidate trajectories In the bottom row, a low-level
representation uses the vehicle’s front-facing camera image as input (bottom-left) It
predicts the future steering wheel angle (bottom-right) and vehicle speed models rely on transfer learning with a model pre-trained on a supervised task
(Messaoud et al., 2021; Phan-Minh et al., 2020), such as ImageNet classiﬁcation We perform an ablation study comparing transfer learning of supervised and
semi-supervised models while keeping all other factors equal and showing that
semi-supervised models perform better than supervised models for low-level and
mid-level representations 13.3 Climate Change and Climate Monitoring
13.3.1 Predicting Ocean Biogeochemistry
Ship-based ocean measurements, like those collected by the Global Ocean Ship-
Based Hydrographic Investigations Program (GO-SHIP), as shown in Figure
13.2, provide valuable insight into ocean carbon uptake, biological processes, cir-
culation, and climate variability However, research cruises are expensive, sparse,
and often seasonally biased due to weather conditions The Biogeochemical-Argo
(BGC-Argo) program aims to become the ﬁrst globally comprehensive sensing
array for ocean ecosystems and biogeochemistry However, proﬁling ﬂoats are
limited in the number of sensors they can support (Chai et al., 2020) Develop-
ing models that accurately predict additional features, such as nutrient ratios,
256 13 Applications
Figure 13.2 Transect locations of GO-SHIP oceanographic cruises in the Southern
Ocean, between 03/08/2001-05/02/2019 Latitude 45 −90◦S, Longitude: −180−180◦
E, with surface ( P<10 dbar) values of phosphate (left) and silicate (right) in μmol
kg−1 from limited sensor data will broaden the applicability of BGC-Argo ﬂoats and
allow us to better monitor and understand changes to the Earth’s climate Previous work demonstrates the utility of applying machine learning to cruise
and ﬂoat data to estimate values of global N 2ﬁxation (Tang, Li and Cassar,
2019), particulate organic carbon (Sauz` ede et al., 2020), alkalinity, pH, and ni-
trate (Carter et al., 2018) Using Bayesian neural networks (Bittig et al., 2018)
allows accounting for uncertainties around predicted values to estimate nutri-
ent concentrations

============================================================

=== CHUNK 168 ===
Palavras: 371
Caracteres: 2530
--------------------------------------------------
Regression methods have also been applied for examining
interannual variability in primary productivity (D’Alelio et al., 2020) We draw on these methods to develop neural networks trained on cruise data
to predict phosphate and silicate, essential nutrients controlling ocean produc-
tivity and biodiversity (Weber and Deutsch, 2010) This is important because
these nutrients regulate biological processes that remove carbon from the surface
ocean at an annual rate roughly equivalent to anthropogenic carbon emissions The Southern Ocean is selected for developing and testing these models as it
is an important global carbon sink and has the most extensive BGC-Argo ﬂoat
coverage at this time (Gruber et al., 2019) We use GO-SHIP data (Carbon Hydrographic Data Oﬃce, 2021) in our train-
ing set to train our models The dataset includes 42 ,412 data points from South-
ern Ocean cruises for 2001–2019 We use GO-SHIP data for latitude, longitude,
pressure, temperature, salinity, oxygen, and nitrate to predict phosphate and
silicate We restrict our data to latitudes below 45◦S, remove rows with missing
dataandfollowtheWorldOceanCirculationExperimentHydrographicProgram
standards, and use quality control ﬂags to down-select our data We standard-
ize the pressure, temperature, salinity, oxygen, and nitrate features The posi-
13.3 Climate Change and Climate Monitoring 257
tion latitude and longitude data are projected to the WGS 84/Antarctic Polar
Stereographic coordinate reference system We do not include time dependency
(month) because the initial evaluation of our linear regression indicates the low
importance of seasonal variability in predicting silicate and phosphate variation We randomly shuﬄe the feature-encoded data into a 9:1 ratio of training to test
size and train our model using 10-fold cross-validation with mean squared er-
ror (MSE) loss We select the model with the lowest validation loss to evaluate
the testing error for both phosphate and silicate To evaluate uncertainty when
predicting silicate and phosphate from our data, we train (1) a one-layer feed-
forward, fully connected neural network with linear activation (equivalent com-
putation to linear regression); and (2) a two-layer feed-forward, fully connected
neural network with 64 hidden units, ReLU activation, and p=0.2 dropout
probability We estimate uncertainty by sampling using dropout (Kendall and
Gal, 2017), training the network using dropout, and then testing each example
by running multiple forward passes with dropout weights

============================================================

=== CHUNK 169 ===
Palavras: 374
Caracteres: 2420
--------------------------------------------------
We evaluate our network’s performance by comparing our model’s results of
phosphate and silicate to the values predicted from an Earth system model
(ESM) We use the Institut Pierre Simon Laplace Climate Model 5 (IPSL-CM5)
(Climate Modeling Center, 2021) model results from a 10-year historical model
run initialized in 2000 and a 30-year projection initialized in 2005 We take the
monthly averaged surface values (59,088) of temperature, salinity, oxygen, ni-
trate, phosphate, and silicate at each location over the historical and predicted
span of 35 years (2000–2035), apply our network model to these surface values
(assuming surface pressure = 5 dbar), and compare our model results to the
IPSL-CM5 values of phosphate and silicate Next, we apply our network to test
data from BGC-Argo ﬂoat proﬁles in the Southern Ocean equipped with dis-
solved oxygen and nitrate sensors There are 175 ﬂoats in the period 2000–2020,
measuring 16,710 proﬁles that meet these criteria, and we only use data points
where all input features are measured We apply our network to 181,329 data
points and run 100 dropout iterations to generate standard deviations for our
estimates The results from our linear regression analysis revealed a more signiﬁcant un-
certaintyinourestimatedphosphatevaluesthanoursilicatevalues.Additionally,
the uncertainty of our silicate results is more uniform over our test data range In
contrast, the phosphate results have more signiﬁcant uncertainty at lower values
and lower uncertainty at higher ones The uncertainties in our phosphate and
silicate estimates are reduced with our two-layer neural network The MSE also
decreases substantially for phosphate (MSE linear: 0.019, MSE NN: 0.0031) and
silicate (MSE linear: 240, MSE NN: 50) The most signiﬁcant uncertainties for
phosphate are at lower values, and for silicate, the most signiﬁcant uncertain-
ties are at higher silicate values This could result from the diﬀerences in the
distribution of these compounds in the water column Phosphate has a more sig-
niﬁcant variance in the upper water column (where phosphate values are lowest)
258 13 Applications
and lower variance at depth In contrast, the variance of silicate is more uniform
throughout the water column Neural networks for ESM data: We compared the ESM output values of phos-
phate and silicate to our neural network predicted values of phosphate and sili-
cate from the ESM features

============================================================

=== CHUNK 170 ===
Palavras: 382
Caracteres: 2855
--------------------------------------------------
Our neural network under-predicts phosphate val-
ues across the Southern Ocean and under-predicts silicate values away from the
Antarctic continent compared to the ESM values However, our neural network
is able to capture the spatial variations for both surface phosphate and silicate These results suggest that our neural network model is able to capture processes
modeled by the ESM However, there are still discrepancies between these two
model types Based on these results, we believe our neural network has a high
enough performance to apply to BGC-Argo data to estimate phosphate and
silicate values from actual observations NeuralnetworksforBGC-Argodata:OurneuralnetworkappliedtoBGC-Argo
data predicts similar spatial patterns of phosphate and silicate to those measured
by GO-SHIP and modeled by the ESM However, a few ﬂoat trajectories have
noticeablydiﬀerentvaluesfromotherﬂoatsintheregion.Whilethiscouldbedue
to local biogeochemical processes, it is likely due to sensor noise or drifts missed
during quality control The uncertainties estimated for phosphate are generally
lowanduniformthroughouttheregion.Incontrast,theuncertaintyestimatesfor
silicate present similar spatial patterns as the mean value estimates, with high
uncertainties near the continent This suggests a systematic error close to the
continent, which could be attributed to ice dynamics causing higher variability
in our features These results suggest a relationship between latitude and silicate
distributions Our neural network models are generally successful, demonstrating high po-
tential for progress in this application However, our proof-of-concept implemen-
tation leaves areas for improvement We plan to improve our models by: (1)
including a temporal component and using a spatial-temporal graph neural net-
work (GNN) representation; (2) preserving the spatial relationships within the
training data using a GNN; and (3) training the models on a subset of shallower
GO-SHIP data to better compare our model output to the surface model results
from the ESM 13.3.2 Predicting Atlantic Multidecadal Variability
The Atlantic Multidecadal Variability (AMV, also known as the Atlantic Multi-
decadalOscillation)isabasin-wideﬂuctuationofsea-surfacetemperatures(SST)
in the North Atlantic with a periodicity of approximately 60–70 years The AMV
hasbroadsocietalimpacts.ThepositivephaseofAMV,forexample,isassociated
with anomalously warm summers in northern Europe and hot, dry summers in
southern Europe (Gao et al., 2019), and increased hurricane activity (Zhang and
Delworth, 2006) These impacts highlight the importance of predicting extreme
AMV states 13.3 Climate Change and Climate Monitoring 259
The AMV Index measures the state of AMV (Figure 13.3, bottom-right panel,
solid black line), calculated by averaging SST anomalies over the entire North
Atlantic basin

============================================================

=== CHUNK 171 ===
Palavras: 358
Caracteres: 2367
--------------------------------------------------
The maximum warming characterizes the spatial pattern of SST
associated with a positive AMV phase in the subpolar North Atlantic and a
secondary maximum in the tropical Atlantic with minimum warming (or slightly
cooling) in between Notwithstanding the value of reliable prediction of AMV, progress in predict-
ing AMV at decadal and longer timescales has been limited Previous eﬀorts
have used computationally expensive numerical climate models to perform sea-
sonal to multi-year predictions with lead times of up to 10 years The subpolar
region in the North Atlantic is one of the most predictable regions globally It
has been associated with the predictability of weather and climate in Europe
and North America for up to 10 years An outstanding question is whether such
predictability can be extended to prediction lead times longer than ten years,
particularly in a changing climate Our objective is to predict these extreme states of the AMV using various
oceanicandatmosphericﬁeldsaspredictors.Thisisformulatedasaclassiﬁcation
problem, where years above and below one standard deviation of the AMV index
correspond to extremely warm and cold states In this work, we use multiple
machine learning models to explore the predictability of AMV up to 25 years in
advance Machine learning techniques have been successfully applied to predict climate
variability, especially the El Ni˜ no-Southern Oscillation (ENSO), an interannual
mode of variability (each cycle is about 3–7 years) in the tropical Paciﬁc Ocean Severalstudieshaveusedconvolutionalneuralnetworks(CNNs)topredictENSO
12–16 months ahead using various features (e.g SST, ocean heat content, sea
surface height; (Ham et al., 2019; Pal et al., 2020; Yan et al., 2020) This out-
performed the typical 10-month lead time ENSO forecast with state-of-the-art,
fully coupled dynamic models (Ham et al., 2019) However, little work has been done to predict decadal and longer modes of
variability, such as the AMV using ML The biggest challenge is the lack of data Widespread observational records for many variables are only available after the
1980s, limiting both the temporal extent and pool of predictors that may be used
for training For interannual modes such as ENSO, current observations can be
easily partitioned into ample training and testing datasets with multiple ENSO
cycles in each subset of data

============================================================

=== CHUNK 172 ===
Palavras: 373
Caracteres: 2349
--------------------------------------------------
However, a single AMV cycle requires 60–70 years,
making it nearly impossible to train and test a neural network on observational
data alone To remedy the lack of observational data for the AMV, we used the Commu-
nity Earth System Model version 1.1 Large Ensemble Simulations.1This is a
fully coupled global climate model that includes the best of current knowledge
of physics and has shown good performance in simulating the periodicity and
1Seehttps://ncar.github.io/cesm-lens-aws
260 13 Applications
Figure 13.3 Variability of input predictors, which include SST, sea surface salinity,
and sea-level pressure The prediction objectives (lower right) are strongly positive
(red) and negative (blue) AMV states outside one standard deviation of the AMV
index (dashed black line) The AMV spatial pattern from the CESM simulation
reasonably captures the enhanced warming at subpolar and tropical latitudes large-scale patterns of the AMV, comparable with observations (Wang et al.,
2015) There are 40 ensemble runs, each between 1920 and 2005 The individ-
ual runs are slightly perturbed in their initial conditions and thus treated as 40
parallel worlds The variability of the ocean and atmospheric dynamics in each
run represents intrinsic natural variability in the climate system that we aim to
predict and provides a diverse subsampling of AMV behavior Our objective is to train machine learning models to predict the AMV state
(AMV+, AMV −, neutral) Each model is given two-dimensional maps of SST,
sea surface salinity (SSS), and sea-level pressure (SLP) and is trained to predict
the AMV state at a given lead time ahead, from 0-year (AMV at the current
year) to 25-year lead (AMV 25 years into the future) We train models to make
predictions every three years This results in nine models for each architecture,
each specialized in predicting AMV at a particular lead time The procedure is
repeated ten times for each lead time to account for sensitivity to the initializa-
tion of model weights and randomness during the training and testing process To quantify the success of each model, we deﬁne prediction skill as the accu-
racy of correct predictions for each AMV state We compare the performance
of the models against a persistence forecast, which is a standard baseline in the
13.3 Climate Change and Climate Monitoring 261
discipline

============================================================

=== CHUNK 173 ===
Palavras: 352
Caracteres: 2396
--------------------------------------------------
The persistence forecast is formulated so that the current state is used
to predict the target state The accuracy of this prediction method is evaluated
for each lead time in the dataset This study used a CNN residual neural network
(speciﬁcally ResNet50), AutoML, and FractalDB 13.3.3 Predicting Wildﬁre Growth
According to projections, the warmer, drier conditions caused in part by climate
change will result in longer, more severe ﬁre seasons as time goes on (Halofsky
et al., 2020) When taken together, the direct and indirect costs of wildﬁres in
the United States account for hundreds of billions of dollars in losses each year,
with the state of California alone suﬀering some $100 billion in costs after the
2017ﬁre season (Roman et al., 2020) Since the early 2000s, machine learning has
been applied in a variety of wildﬁre applications (Jain et al., 2020) Of particular
interest is predicting how quickly and in which direction wildﬁres grow within
the ignition With early detection of wildﬁres pivotal to ﬁre response eﬀorts and
the inherent unpredictability of wildﬁre movements, predicting the behavior of
a wildﬁre within the ﬁrst few hours of ignition provides ﬁrst responders with
invaluable information (Sahin and Ince, 2009) To this end, we compare the performance of baseline models in predicting the
growth of wildﬁre fronts up to 30 hours after ignition using a dataset of simu-
lated wildﬁres Leveraging OpenAI’s Codex model, we synthesize model variants
from the baseline models, tune hyperparameters, and ensemble the human ex-
pert model variants and the Codex model variants Among the human baseline
models, a many-to-many convolutional long short-term memory (LSTM) model
performs best Our results demonstrate the power of leveraging program synthe-
sis to generate variations of wildﬁre behavior prediction models automatically Within the subﬁeld of ﬁre behavior, researchers have predicted ﬁre growth on
variousscales.Atahighlevel,ﬁrebehaviorhasbeenformulatedasaclassiﬁcation
problem (Markuzon and Kolitz, 2009) using Bayesian networks, k-nearest neigh-
bors,andrandomforestsonsatellitedatatopredictthefuturesizeofanincipient
ﬁre as a binary value Several works have attempted to predict ﬁre spread at the
pixel level on a more granular scale Convolutional LSTMs (ConvLSTMs; (Burge
et al., 2020)) yield more accurate predictions than CNNs (Hodges and Lattimer,
2019)

============================================================

=== CHUNK 174 ===
Palavras: 378
Caracteres: 2362
--------------------------------------------------
Convolutional LSTMs model the transient dynamics in the wildﬁre data Reinforcement learning has been used for modeling forest wildﬁre dynamics from
satellite images (Ganapathi Subramanian and Crowley, 2018) The relationships
between forest ﬁres and weather conditions from long-term observations have
also been explored (Koutsias et al., 2013) In this work, we demonstrate a de-
crease in performance due to distribution shift when training on simulated data
and testing on real-world data Next, we compare a CNN, CNN-LSTM, and a
ConvLSTM on a more complex dataset of simulated ﬁres Among these baseline
models, ConvLSTM performs best Finally, we demonstrate that a synthesized
model outperforms these baselines 262 13 Applications
Figure 13.4 Examples of input channels used in the FARSITE wildﬁre simulator The
domain in this ﬁgure is the Eel River area in California These environmental features
and FARSITE burn maps are used as input for our models The data used in this work is the output of the FARSITE wildﬁre simulator
(Hodges and Lattimer, 2019), which is a burn map simulation of the ﬁre growth The FARSITE simulator uses images of topography, vegetation, precipitation,
and wind as inputs Sample simulator inputs are shown in Figure 13.4 The sim-
ulator uses Finney’s method (Finney, 1998) of crown ﬁre calculation to simulate
the ﬁre growth Our training and testing set consists of 2,500 FARSITE simu-
lations of randomly selected 50 ×50 km regions in the state of California with
resolutions of 0 .03 km of realistic landscape and vegetation and varying moisture
content and wind Each ﬁre is simulated up to 48 hours with output burn maps
at 1 km resolution (50 ×50 arrays) extracted every 6 hours In addition to these
burn maps, the input to our models also included 12 relevant down-sampled en-
vironmental variables, also represented by 50 ×50 arrays: a fuel model; 1-, 10-,
and 100-hour moistures; live herbaceous and woody moistures; canopy cover,
top height, and base height; east–west and north–south winds; and elevation We note that we split the dataset such that we train on 1,804 images and test
on 290 images for all models 13.4 Computer Vision
13.4.1 Kinship Veriﬁcation
The ability to recognize kinship between faces based only on images contributes
to applications such as social media, forensics, reuniting families, and geneal-
ogy

============================================================

=== CHUNK 175 ===
Palavras: 369
Caracteres: 2313
--------------------------------------------------
However, these ﬁelds each possess unique datasets that are highly varied
in terms of image quality, lighting conditions, pose, facial expression, and view-
ing angle, making creating an image-processing algorithm that works in general
quite challenging To address these issues, an annual automatic kinship recog-
nition challenge Recognizing Families in the Wild (RFIW) releases a sizeable
13.4 Computer Vision 263
Figure 13.5 Kinship veriﬁcation deep learning architecture: Multiple deep Siamese
networks are used A pair of images for veriﬁcation are fed through a pre-trained
convolutional backbone (He et al., 2016 a; Hu et al., 2018) The backbones project the
images into a latent feature space which are ﬂattened and then combined by feature
fusion (Yu et al., 2020) The result of the feature fusion is fed through a fully
connected network in which the ﬁnal layer is a single binary classiﬁcation predicting
kin or non-kin Multiple Siamese networks written by both human experts and
OpenAI Codex are ensembled multi-task dataset to aid the development of modern data-driven approaches for
solving these critical visual kin-based problems (Robinson et al., 2016, 2021) Deep learning models have been developed for kinship veriﬁcation, which en-
tails the binary classiﬁcation of two pictures’ relationships as kin or non-kin The architecture shown in Figure 13.11 uses a variety of models written by both
human experts and automatically by OpenAI Codex (Chen et al., 2021) The
models are then ensembled to predict the conﬁdence that a pair of face images
are kin Each model utilizes a Siamese convolutional backbone with pre-trained
weights to encode one-dimensional embeddings of each image Embeddings are
combined by feature fusion (He et al., 2016 a; Hu et al., 2018; Yu et al., 2020),
and the fused encoding is fed through a series of fully connected layers in order
to make a prediction The network predictions of many models are ensembled
before applying a threshold to obtain a binary classiﬁcation 13.4.2 Image-to-3D
Three-dimensional model construction from 2D images of objects is an active
research area (Fu et al., 2021; Kniaz et al., 2020; Yu and Oh, 2022) Algorithms
exist (Lim et al., 2013) for modeling the ﬁne-pose of objects within captured 2D
images and matching them to a set of 3D models

============================================================

=== CHUNK 176 ===
Palavras: 379
Caracteres: 2492
--------------------------------------------------
Generative adversarial network
(GAN)-based approaches (Pan et al., 2021; Hu et al., 2021) for 3D reconstruc-
tion demonstrate high-quality outputs and have recently been extended to allow
control over the output Other approaches (Girdhar et al., 2016) develop vector
representations of 3D objects that are predictable from 2D images and methods
264 13 Applications
Figure 13.6 Our method takes an image as input and produces a voxelized 3D model,
w h i c hi st h e nc o n v e r t e dt oaL E G O ®brick set From the provided pieces and
instructions, the LEGO ®model can then be built in the real world; example shown
at the right for automatic generation of 3D models through octree-based pruning (Stigeborn,
2018) 13.4.3 Image2LEGO ®
Fordecades,LEGO ®brickshavebeen astapleof entertainment for children and
adults alike, oﬀering the ability to construct anything one can imagine from sim-
plebuildingblocks.However,forallbutthemostexceptionalLEGO ®engineers,
dreamsquicklyoutgrowskills,andconstructingthecompleximagesaroundthem
becomes too great a challenge LEGO ®bricks have been assembled into intri-
cate and fantastical structures in many cases, and simplifying constructing the
more complex designs is essential to maintaining appeal for amateur builders
and attracting a new generation of LEGO ®enthusiasts To make these creative
possibilities accessible to all, we developed an end-to-end approach for producing
LEGO ®-type brick 3D models directly from 2D images Our work has three se-
quentialcomponents:it(1)convertsa2Dimagetoalatentrepresentation;(2)de-
codes the latent representation to a 3D voxel model; and (3) applies an algorithm
to transform the voxelized model to 3D LEGO ®bricks Our work represents the
ﬁrst complete approach that allows users to generate real LEGO ®sets from 2D
images in a single pipeline A basic high-level demonstration of the entire Im-
age2LEGO ®pipeline is presented in Figure 13.6 A photograph of an airplane is
converted to a 3D LEGO ®model, and the corresponding instructions and brick
parts list are used to construct a physical LEGO ®airplane build We tackle the
issues speciﬁc to constructing high-resolution real 3D LEGO ®models, such as
color and hollow structures We present a pipeline that combines creating a 3D
model from a 2D image with an algorithm for mapping this 3D model to a set of
LEGO ®-compatible bricks to provide this new Image2LEGO ®application and
evaluate by examples and analysis to show how and when this pipeline works

============================================================

=== CHUNK 177 ===
Palavras: 356
Caracteres: 2446
--------------------------------------------------
We focus on our novel approach for multi-class object-image-to-Lego construc-
tion However, the same approach is extended to other creative applications by
13.4 Computer Vision 265
leveragingpreviousimage-to-modelwork.Forinstance,generatingLEGO ®models
from pictures of one’s face is already an application of interest However, current
work is limited to the generation of 2D LEGO ®mosaics from images, gener-
ated by the commercial product called LEGO ®Mosaic Maker (Lego, 2020) However, we extend the Image2LEGO ®pipeline to include the pre-trained Vol-
umetric Regression Network (VRN) for single-image 3D reconstruction of faces
(Jackson et al., 2017) In contrast to the 2D mosaic, our approach generates a
3D LEGO ®face from a single 2D image Moreover, other learned tools may be
appended to the base pipeline to develop more innovative tools For instance, by
prepending the VRN with a sketch-to-face model (Chen et al., 2020), we develop
a tool that directly converts an imagined drawing into a LEGO ®model, of-
fering nearly limitless creative possibilities We demonstrate another extension,
where we apply the Image2LEGO ®pipeline with DALL-E (Ramesh, Pavlov,
Goh, Gray, Voss, Radford, Chen and Sutskever, 2021; Ramesh et al., 2022) out-
puts to create a tool that automatically converts captions to LEGO ®models The challenge of converting voxelized 3D models into LEGO ®designs has
beenpreviouslyexplored.Real-timeconversionofsurfacemeshestovoxelstoLE-
GOs ®(Silva et al., 2009) and methods for high-detail LEGO ®representations
of triangle mesh boundaries (Lambrecht, 2006) have been demonstrated A gap
hasremainedbetween3DmodelgenerationfromimagesandLEGO ®generation
from 3D models Our work aims to bridge this gap by developing a complete Im-
age2LEGO ®pipeline, allowing anyone to create custom LEGO ®models from
2D images The problem of LEGO ®generation from images adds a goal to 3D model
generation, namely that it is essential to have ﬂexibility in the output reso-
lution Additionally, the latent space should have some ﬂexibility to generate
unseen structures from new input images The former helps provide users with
LEGO ®designs of diﬀerent scales and resolutions, to achieve better varying
levels of diﬃculty, availability of material resources, and cognitive eﬀort For
instance, small renditions of an object may be helpful as ﬁne elements in a
greater scene, while larger renditions may serve as independent LEGO ®models

============================================================

=== CHUNK 178 ===
Palavras: 355
Caracteres: 2494
--------------------------------------------------
The latter feature of a generalizable latent space allows users to generate new
LEGO ®sets associated with newly captured images This work represents the
ﬁrst eﬀort to combine these approaches, using an octree-structured autoencoder
in the image-to-model pipeline We evaluate its ability to perform this task on
new images in several examples 13.4.4 Imaging through Scattering Media
In biological imaging, tissues act as scattering media that induce aberration
and background noise in the captured image, where the true object is faded
out Retrieving the hidden object from the image thus becomes a challenging
inverse problem in computational optics Normally, the random scattering me-
dia properties are unknown and are diﬃcult to characterize fully Traditional
266 13 Applications
techniques formulate this problem as an optimization based on a transmission
matrix or forward operator, with a regularization term derived from the object
prior knowledge: ˆ x= argmin
x/bardbly−Ax/bardbl2+λΦ(x), where xis the unknown object
with ˆxbeing its estimation, and yis the observed image, Athe forward matrix,
andaregularizationfunctionΦ( x)withaweightingparameter λ.However,many
practical imaging instances arise when such formulations and methods fail The
nonlinearity in the forward imaging process, especially under heavy light scat-
tering conditions, means that learning from examples is an ideal solution due to
the ability to handle nonlinearities Real-world applications of imaging through scattering media include (1) imag-
ing through tissue with visible light, which allows for non-invasive sensing inside
the body without exposure to excess radiation while potentially allowing for bet-
ter functional imaging than standards today such as MRI; (2) privacy-preserving
use cases, such as human–computer interaction systems, where the agent must
observe the characteristics of the human, but the image of them is obscured to
preserve their privacy Thus, the agent is able to capture essential information
without capturing identifying information; (3) sensing through dense fog for au-
tonomous navigation (driving, ﬂight, etc.) allows for safe movement in inclement
weather; and (4) underwater imaging, where turbulence and particulate matter
obscure the line of sight Instead of solving for data ﬁdelity and regularizer by optimization, learning-
based methods alternatively model the forward operator and regularizer simulta-
neously through known objects and images through random media

============================================================

=== CHUNK 179 ===
Palavras: 354
Caracteres: 2291
--------------------------------------------------
The ﬁrst im-
plementation of this approach (Horisaki et al., 2016) used support vector regres-
sion learning and successfully learned to reconstruct face objects However, the
fully connected two-layer architecture fails to eﬀectively generalize from trained
face objects to other non-facial object classes A better network architecture
is necessary for more generalizable learning and accurate performance A U-
Net was ﬁrst proposed for biomedical image segmentation (Ronneberger et al.,
2015) The skip-connection in the U-Net architecture enables its superiority in
extracting image features over other CNN architectures Such a U-Net model has been applied to this problem (Li, Deng, Lee, Sinha
and Barbastathis, 2018), taking a speckle pattern as input and using an encoder–
decoder structure to generate high-resolution images In order to account for
the data sparsity that often accompanies computational imaging, the negative
Pearson correlation coeﬃcient (NPCC) is used (Li, Deng, Lee, Sinha and Bar-
bastathis, 2018) rather than cross entropy as the neural network loss function The resulting network, called IDiﬀNet, adapts to diﬀerent scattering media for
sparse inputs, with the NPCC used to learn sparsity as a strong prior in the
ground-truth values While IDifNet works well on training and testing data from
the same database and distribution, it does not generalize well among diﬀerent
databases and suﬀers from overﬁtting (Li, Deng, Lee, Sinha and Barbastathis,
2018) 13.4 Computer Vision 267
Similarly, a U-Net is used (Li, Xue and Tian, 2018) to map speckle patterns to
two output images – the predicted object and background – for a set of diﬀerent
diﬀusers Instead of implementing computational imagining as an inverse prob-
lem, recent work learns the statistical properties of speckle intensity patterns in
a way that generalizes to various scattering media Data augmentation may be
used to increase the training set size, for example, by simulation (Wang, Wang,
Wang, Li and Situ, 2019) In this work, we use a new experimental setup, using a
digital micromirror device (DMD) instead of a spatial light modulator (SLM) as
the pixelwise intensity object, resulting in speckles of 10 micrometers instead of
16 micrometers, as seen in previous work (Li, Xue and Tian, 2018)

============================================================

=== CHUNK 180 ===
Palavras: 359
Caracteres: 2415
--------------------------------------------------
When test-
ing on speckles from previously unseen objects through unseen diﬀusers (types of
scattering media), neural networks trained on image sets with multiple diﬀusers
perform better than ones trained on a single diﬀuser (Li, Xue and Tian, 2018) An experimental setup is illustrated in Figure 13.7 Light from a laser source is
ﬁrst collimated and then illuminates onto a DMD (DLP LightCrafter 6500, pixel
size 7.6 micrometers) The DMD, placed at a certain angle relative to the illumi-
nation beam, acts as a pixelwise intensity object After modulation by the DMD,
the beam passes through a thin glass diﬀuser (Thorlabs, 220 grits, DG10-220)
and is scattered A two-lens telescope imaging system then relays the resulting
image onto a camera (FLIR, Grasshopper 3, pixel size 3.45 micrometers) A two-channel network splits each input image into two tensors, one for the
object and another for the background A U-Net considers a single channel out-
putted through a sigmoid activation layer This produces a clear reconstruction Each convolutional layer is replaced with a dense block The U-Net model is sep-
arated into an encoder and decoder The encoder uses ﬁve layers, each consisting
of 2D Convolution-ReLU-Dense Blocks followed by max-pooling, to reduce the
lateral size of the image while increasing the number of tensors in the channel
dimension The convolutional kernel is size 3 ×3, and the dense kernel is 5 ×5 The decoder uses a similar series of operations joined by upsampling and con-
catenation in the channel dimension with the corresponding encoder layer This
re-expands the lateral image size and results in the number of channel-dimension
tensors being one output image Eachdenseblockconsistsofseveralsubsequentconvolutionalblocks.Thiscon-
volutional block series is repeated four times during encoding, while decoding
has this series repeated only three times The basic structure of a convolutional
blockconsistsofbatchnormalization,ReLU,convolutionallayer,andconditional
dropout with a probability of 0.5 The resulting feature maps from these sub-
sequent convolutional blocks are concatenated in the channel dimension The
upsampling function consists of three layers: nearest-neighbor up-sampling, 2D
convolution, and ReLU The upsampling is used in the decoding part of the
network, which is iteratively followed by concatenation with the previous dense
block outputs in the channel dimension

============================================================

=== CHUNK 181 ===
Palavras: 350
Caracteres: 2599
--------------------------------------------------
The network is trained using stochastic gradient descent (SGD) with momen-
tum During training, the batch is forward-propagated through the model, the
268 13 Applications
Figure 13.7 Experimental setup of the scattering media imaging system Top:
schematic of the optical conﬁguration, with an example of a speckle pattern (right)
that is mapped to the corresponding ground truth object (left) Bottom: the physical
conﬁguration corresponding to the schematic diagram loss is computed and backpropagated, the tracked gradients for the modules are
zeroed, and the step function is applied to the optimizer During evaluation, the
model is validated using previously unseen validation data The training loss
and validation loss are computed for each epoch Commonly used loss functions
including MSE and mean absolute error (MAE) do not promote sparsity since
they assume the underlying signals follow Gaussian and Laplace statistics, re-
spectively Considering the high sparsity in the MNIST database, we consider
two more appropriate candidates for the loss function: the negative Pearson cor-
relation coeﬃcient (NPCC) and average binary cross entropy (BCE):
LNPCC=−/summationtext
i(x−˜x)(p−˜p)/radicalBig/summationtext
i(x−˜x)2/radicalBig/summationtext
i(p−˜p)2(13.1)
LBCE=−1
2N/summationdisplay
i(xlog(p)+(1−x)log(1−p)) (13.2)
where ˜xand ˜pare the average ground truth xand network output p,a n di
indexes each of the Npixels of the image 13.5 Speech and Audio Processing 269
13.4.5 Contrastive Language-Image Pre-training
Contrastive language-image pre-training (CLIP) (Radford et al., 2021) uses 400
milliontext–imagepairscollectedfromthewebtotrainapairofencoders:onefor
textandanotherforimages.CLIPistrainedusingacontrastiveloss,encouraging
the model to map similar images to similar text and diﬀerent images to diﬀerent
text A new image is ﬁrst embedded, and then the model is used to ﬁnd the most
similar embedded text, performing zero-shot classiﬁcation The CLIP model is
used in downstream tasks such as image captioning, image retrieval, and zero-
shot classiﬁcation 13.5 Speech and Audio Processing
13.5.1 Audio Reverb Impulse Response Synthesis
Artiﬁcial Reverberation
Historically, recording studios built reverberant chambers with speakers and mi-
crophones to apply reverb to prerecorded audio directly within a physical space
(Rettinger, 1957) Reverberation circuits, ﬁrst proposed in the 1960s, use a net-
work of ﬁlters and delay lines to mimic a reverberant space (Schroeder and
Logan, 1961) Later, digital algorithmic approaches applied numerical methods
to simulate similar eﬀects

============================================================

=== CHUNK 182 ===
Palavras: 355
Caracteres: 2466
--------------------------------------------------
Conversely, convolution reverb relies on audio record-
ings of a space’s response to a broadband stimulus, typically a noise burst or sine
sweep This results in a digital replica of a space’s reverberant characteristics,
which can then be applied to any audio signal (Anderegg et al., 2004) Convolutionalneuralnetworkshavebeenusedforestimatinglate-reverberation
statistics from images (Kon and Koike, 2019, 2020), though not to model the
complete audio impulse response (IR) from an image This work is based on the
ﬁnding that experienced acoustic engineers readily estimate a space’s IR or re-
verberant characteristics from an image (Kon and Koike, 2018) Room geometry
has also been estimated from 360-degree images of four speciﬁc rooms (Remaggi
et al., 2019) and used to create virtual acoustic environments that are compared
with ground-truth recordings, though again, IRs are not directly synthesized
from the images A related line of work synthesizes spatial audio based on vi-
sual information (Li, Langlois and Zheng, 2018; Gao and Grauman, 2019; Kim
et al., 2019) Prior work exists on the synthesis of IRs using RNNs (Sali and
Lerch, 2020), autoencoders (Steinmetz, 2018), and GANs: IR-GAN (Ratnarajah
et al., 2021) uses parameters from real-world IRs to generate new synthetic IRs,
whereas our work synthesizes an audio IR directly from an image Recent work has shown that GANs are amenable to audio generation and
can result in more globally coherent outputs (Donahue et al., 2018) GANSynth
(Engeletal.,2019)generatesanaudiosequenceinparallelviaaprogressiveGAN
architecture, allowing faster than real-time synthesis and higher eﬃciency than
the autoregressive WaveNet (Oord et al., 2016) architecture Unlike WaveNet,
270 13 Applications
whichusestime-distributedlatentcoding,GANSynthsynthesizesanentireaudio
segment from a single latent vector Given our need for a global structure, we
create a ﬁxed-length representation of our input and adapt our generator model
from this approach Measured IRs have been approximated with shaped noise (Lee et al., 2010;
Bryan, 2020) While room IRs exhibit statistical regularities (Traer and McDer-
mott, 2016) that can be modeled stochastically, the domain of this modeling is
time and frequency limited (Badeau, 2019) and may not reﬂect all characteris-
tics of real-world recorded IRs Simulating reverb with ray tracing is possible but
prohibitively expensive for typical applications (Schissler and Manocha, 2016)

============================================================

=== CHUNK 183 ===
Palavras: 357
Caracteres: 2377
--------------------------------------------------
By directly approximating measured audio IRs at the spectrogram level, our
outputs are immediately applicable to tasks such as convolution reverb, which
applies the reverberant characteristics of the IR to another audio signal Between visual and auditory domains, conditional GANs have been used for
translating between images and audio samples of people playing instruments
(Chen et al., 2017) The model employs a conditional GAN with an image en-
coder that takes images as input and produces spectrograms A similar over-
all design, with an encoder, generator, and conditional discriminator (Mentzer
et al., n.d.) has been applied to obtain state-of-the-art results on image com-
pression, among many other applications The generator and discriminator are
deep convolutional networks based on the GANSynth (Engel et al., 2019) model
(non-progressive variant), with modiﬁcations to suit our dataset, dimensions,
and training procedure The encoder module combines image feature extraction with depth estima-
tion to produce latent vectors from two-dimensional images of scenes For depth
estimation, we use the pre-trained Monodepth2 network (Godard et al., 2019),
a monocular depth-estimation encoder–decoder network that produces a one-
channel depth map corresponding to our input image The main feature extrac-
tor is a ResNet50 (He et al., 2016 a) pre-trained on Places365 (Zhou et al., n.d.),
which takes a four-channel representation of our scene including the depth chan-
nel (4×224×224) We add randomly initialized weights to accommodate the
additional input channel for the depth map Since we are ﬁne-tuning the entire
network, albeit, at a low learning rate, we expect it will learn the relevant fea-
tures during optimization The architecture’s components are shown in Figure
13.8 13.5.2 Voice Swapping
Deep learning systems allow two speakers to swap their voices from any two
unpaired sentences such that the result is indistinguishable from authentic voices
and is performed in real-time on a laptop Each of the two speakers pronounces
any unpaired single short sentences into a microphone The system plays the
original voice recordings, then swaps the speakers’ voices, playing the words
pronounced by the ﬁrst speaker with the second speaker’s voice and vice-versa 13.5 Speech and Audio Processing 271
Figure 13.8 Image2Reverb deep learning system architecture

============================================================

=== CHUNK 184 ===
Palavras: 375
Caracteres: 2580
--------------------------------------------------
The system consists of
an autoencoder and GAN networks Left: An input image is converted into four
channels: red, green, blue, and depth The depth map is estimated by Monodepth2, a
pre-trained encoder–decoder network Right: The model employs a conditional GAN An image feature encoder is given the RGB and depth images and produces part of
the generator’s latent vector, which is then concatenated with noise The
discriminator applies the image latent vector label at an intermediate stage via
concatenation to make a conditional real/fake prediction, calculating loss and
optimizing the encoder, generator, and discriminator The two input voices are processed in two distinct ways; one to extract the text
of each speech and one to learn each speaker’s unique voice proﬁle The text
from speaker A’s speech is extracted using state-of-the-art pre-trained voice-
to-text models Next, the audio from speaker B is passed through an encoder,
whichderivesanembeddingthatdescribesspeakerB’sdistinctivefeatures.Next,
we use the text extracted from speaker A and the embeddings of speaker B to
synthesize the Mel spectrogram, which is fed into a vocoder to generate the
ﬁnal audio of speaker A’s sentence with speaker B’s voice The exact process is
mirroredwithspeakers’rolesswapped.Ourimplementationleveragespre-trained
neural networks – an encoder, synthesizer, and vocoder models – for a realistic
real-time performance 13.5.3 Explainable Musical Phrase Completion
Music is a multi-modal medium, having both rich spectro-temporal and sym-
bolic representations and tactile and motor experiences Thanks to this multi-
modality, neuroscientists have observed that learning a new musical instrument
has a profound impact on our cognitive ability (Zatorre et al., 2007) Music can
be synthesized and completed using multiple modalities, most naturally using
the audio spectrogram (Drori et al., 2004) While music consists of multiple note
streams (Huang, Cooijmans, Roberts, Courville and Eck, 2017), this work uses a
language model We demonstrate the completion of partial musical sequences by
deep neural networks, conditioned on the surrounding context, using explainable
edit operations of insertion, deletion, and replacement of musical notes and shift-
ing attention between notes Related work, such as MidiNet (Yang, Chou and
272 13 Applications
Figure 13.9 Sample of musical phrases: (a) Spectrogram of original musical phrase
with corresponding notes below; (b) musical phrase with missing time segment; (c)
result of MaskGAN completion; and, (d) result of Neural Editor completion

============================================================

=== CHUNK 185 ===
Palavras: 358
Caracteres: 2278
--------------------------------------------------
Yang, 2017), demonstrates a compelling ability to generate music using a condi-
tional GAN DeepBach uses a graphical model which successfully produces poly-
phonic rhythmic outputs using pseudo-Gibbs sampling (Hadjeres et al., 2017) Our approach of using Neural Editor (Guu et al., 2018) for music is unique in
that it is explainable by design We collected 3,428 classical music compositions by eight of the top classical
composersfromalargedigitalmusicrepository(Smythe,2018).Wetokenizedthe
main instrument of each song to generate simple monophonic musical phrases We split the dataset into 95% training and 5% test sets, using the same sets for
the Neural Editor and MaskGAN models We masked out the middle notes of
equal length from the held-out test data, which we completed and synthesized
by our models Figure 13.9 shows a sample of results of musical phrase completion using the
MaskGAN and Neural Editor The odd rows show spectrograms, and the even
rows show their corresponding notes Column (a) shows the input spectrogram,
(b) shows the spectrogram of the music with missing notes, (c) shows the spec-
trogram completed by MaskGAN, and (d) shows the spectrogram completed by
Neural Editor The Neural Editor model generates vector representations for discrete musical
note tokens The middle phrase of the note sequence is masked and is com-
pleted by our model These masked vectors serve as inputs to a bi-directional
LSTM model, where edit vectors apply various operations to musical notes: in-
sert, delete, replace, move left, move right The output is a novel synthesized
musical sequence, and we train the model by maximizing the marginal likeli-
hood MaskGAN (Fedus et al., 2018) takes a unique approach to conditional
sequence generation When using MaskGAN to generate new notes to complete
the masked out portion of a musical sequence, rather than being only autore-
gressive, MaskGAN conditions its output on the entire context 13.6 Natural Language Processing 273
Figure 13.10 Our experimental setup For a given pair of datasets ( Aand B), we
perform three sets of train/test combinations We train and test within the same
distribution ( A/A and B/B), between distributions ( A/B and B/A), and between
distributions with target ﬁne-tuning ( AftB/B and BftA/A)

============================================================

=== CHUNK 186 ===
Palavras: 360
Caracteres: 2291
--------------------------------------------------
13.6 Natural Language Processing
13.6.1 Quantifying and Alleviating Distribution Shifts in Foundation Models on
Review Classiﬁcation
The impact of distribution shifts on the accuracy of review classiﬁcation when
using Transformer models is signiﬁcant Consider the task of classifying cus-
tomer reviews as fake or real based only on the review text The extent of the
drop in accuracy when the model tries to predict labels for distributions other
than the one it was trained on is signiﬁcant, not only because of the dearth of
labeled datasets but also to gain insight into the information encoded by the
Transformer embeddings and what steps may be taken to make their decisions
more robust to possible shifts The extent of the degradation in accuracy de-
pends primarily on the independent variable across which the shift is created We use the available metadata to narrow down four independent variables that
give us balanced training and testing dataset splits while diﬀering with the cho-
sen variable We train and test across all four permutations of splits for each
of these The distribution shifts investigated are: (1) Industry Type – hotel and
restaurant reviews; (2) Time– old (pre-2014) and new (post-2014) reviews; (3)
Product Type – Japanese and Italian restaurant reviews; and (4) Sentiment –
positive and negative reviews Since one of our goals is to gain insights into Transformer model selection
for tasks that require robustness across distribution shifts, we use three popular
constructs for Transformers: encoder-only BERT (bidirectional encoder repre-
sentations from Transformers) models, an encoder and decoder T5 model, and
the Jurassic-I model with few-shot training Subsequently, to address the prob-
lem of accuracy degradation due to distribution shifts, we suggest and report
results from our solution of ﬁrst training on the known distribution, then freez-
ing weights for all but the ﬁnal layer in the model, and ﬁne-tuning weights for
this ﬁnal layer with a much smaller subset of the new distribution (100–300 re-
view text samples compared to the previous 10,000 samples) to allow the model
a chance at using the generalizable patterns it saw in the ﬁrst distribution, while
also enabling it to create distribution-speciﬁc insights for the new distribution

============================================================

=== CHUNK 187 ===
Palavras: 368
Caracteres: 2390
--------------------------------------------------
274 13 Applications
Detectingfakereviewsisawell-knowntask,theeconomicimplicationsofwhich
have been analyzed thoroughly in previous work (He et al., 2020), but with
the growth of the industry for hiring and selling fake reviews, detecting fake
reviews at scale has become a trade of its own and one particularly suited for
the use of natural language processing (Ren and Ji, 2017) We build on the
same motivation by combining this natural language processing task of review
classiﬁcation with methodology partly based on existing work outside of natural
language processing (Koh et al., 2021) that sets up structures for analyzing
implications of distribution shifts and creating insights for model selection and
red ﬂags in model training Moreover, the architecture for our BERT instances
is inspired by previous work (Kennedy et al., 2019) that created BERT models
for review datasets We build on previous work by using a richer dataset, testing
three sizes of BERT, a T5 model, and then, most importantly, investigating and
interpreting the performance of these models on distribution shifts We also take
inspiration from two notable works (Sun et al., 2017; Arjovsky et al., 2019) to
suggest and report results from a solution of ﬁne-tuning the model based on a
small subset of the distribution-shifted data WeusethemethodologyshowninFigure13.10,whichispartlybasedonprevi-
ous work (Koh et al., 2021) on distribution shifts (1) We begin by standardizing
the review texts to make them compliant with the pre-trained Transformer mod-
els’ expected input, ensuring all steps are applied to any other source’s review
texts (2) We ﬁne-tune our pre-trained Transformer models, evaluating the per-
formance of the models on an out-of-sample test set in the same distribution to
ascertain how well the model does when it sees reviews similar to the ones it
was trained on This gives us baseline benchmarks (upper bounds) to assess our
distribution shift metrics We make sure to achieve state-of-the-art performance
in this problem space by employing Transformer models that were previously
shown to be most successful with the task (3) For each of the distribution shifts
above, we train and test within the same distribution (e.g., train and test both
on pre-2014 reviews), as well as train and test across the distribution shifts (e.g.,
train on pre-2014 reviews and test on post-2014 reviews)

============================================================

=== CHUNK 188 ===
Palavras: 381
Caracteres: 2588
--------------------------------------------------
We do so for all the
diﬀerent permutations for these shifts – employing BERT (three size instances),
T5, and Jurassic-I (with few-shot learning) (4) Lastly, we use the created mod-
els that were trained on one distribution, freeze the weights for all but the last
layer, and ﬁne-tune this layer based on a small subset of 100–300 review text
samples from the new distribution We do this for each split that was explored
in the previous step, employing only the BERT and T5 instances to report this
method as a solution to the degradation Weusetwolabeleddatasets:theﬁrstisforrestaurantreviewsfromYelp(Rayana
and Akoglu, 2015) and the second is for hotel reviews (Ott et al., 2013), which
combines internet sources like Expedia, Hotels.com, Orbitz, Priceline, and Tri-
pAdvisor Both datasets have the review text, fake/actual labels, and metadata The metadata was used to ﬁnd the independent variables along which we could
splitthedatatocreatedistributionshifts.Sinceourgoalistolookatthegeneral-
13.7 Automated Machine Learning 275
Figure 13.11 Overview of our method We leverage dataset descriptions and other
AutoML methods to provide zero-shot ML pipeline selection izability of the models, we create and their translations to a diﬀerent distribution
(e.g., from various sources), we decided to limit our input features to standard-
ized review text only We chose these datasets to work in conjunction because
they are both collections of consumer reviews but are diﬀerent in that the cus-
tomers are restaurant clients in one and hotel clients in the other We found
these datasets to be common enough to cross validate transfer learning and, at
the same time, diﬀerent enough to create an interesting distribution shift 13.7 Automated Machine Learning
A data scientist facing a challenging new supervised learning task does not gen-
erally invent a new algorithm Instead, they consider what they know about
the dataset and which algorithms have worked well for similar datasets Auto-
mated machine learning (AutoML) seeks to automate such tasks, enabling the
widespread and accessible use of machine learning by non-experts A signiﬁcant
challenge in the ﬁeld is to develop fast, eﬃcient algorithms to accelerate machine
learning applications (Kokiopoulou et al., 2019) This work develops automated solutions that exploit human expertise to learn
whichdatasetsaresimilarandwhichalgorithmsperformbest.Weuseatransformer-
based language model (Vaswani et al., 2017) to process text descriptions of
datasets and algorithms and a feature extractor (BYU-DML, 2019) to represent
the data itself

============================================================

=== CHUNK 189 ===
Palavras: 356
Caracteres: 2295
--------------------------------------------------
Our approach fuses each of these representations, representing
each dataset as a node in a graph of datasets We train our model on other ex-
isting AutoML system solutions, speciﬁcally: AutoSklearn (Feurer et al., 2015)
and OBOE (Yang, Akimoto, Kim and Udell, 2019) By leveraging these existing
276 13 Applications
systems and openly accessible datasets, we achieve state-of-the-art results using
multiple approaches across various classiﬁcation problems To predict a machine learning pipeline, a simple idea is to use a pipeline that
performed well on the same task and similar datasets; however, what consti-
tutes a similar dataset The success of an AutoML system often hinges on this
question Diﬀerent frameworks have diﬀerent answers: for example, AutoSklearn
(Feurer et al., 2015) computes a set of meta-features, that is, features describing
thedatafeatures,foreachdataset,whileOBOE(Yang,Akimoto,KimandUdell,
2019) uses the performance of a few fast, informative models to compute latent
features More generally, for any supervised learning task, one can view the rec-
ommended algorithms generated by any AutoML system as a vector describing
that task This work is the ﬁrst to use the information that a human would check
ﬁrst: a summary description of the dataset and algorithms, written in free text These dataset features induce a metric structure on the space of datasets Under
an ideal metric, a model that performs well on one dataset would also perform
well on nearby datasets The methods we develop in this work show how to learn
such a metric using the recommendations of an AutoML framework together
with the dataset description We provide a new zero-shot AutoML method that
predicts accurate machine learning pipelines for an unseen dataset and classiﬁ-
cation task in real-time Bringing techniques from natural language processing to AutoML, we specif-
ically use a large-scale Transformer model to extract information from the de-
scription of both the datasets and algorithms This allows us to access large
amounts of relevant information that existing AutoML systems are typically not
privy to These embeddings of dataset and pipeline descriptions are fused with
data meta-features to build a graph where each dataset is a single node This
graph is then the input to a GNN

============================================================

=== CHUNK 190 ===
Palavras: 359
Caracteres: 2414
--------------------------------------------------
Our real-time AutoML method predicts a pipeline with good performance
within milliseconds given a new dataset The running time of this predicted
pipeline is typically up to one second, mainly for hyperparameter tuning The
accuracy of our method is competitive with state-of-the-art AutoML methods
that are given minutes, thus, reducing computation time by orders of magnitude
while improving performance Generally, GNNs are used for three main tasks: (1) node prediction, (2) link
prediction, and (3) sub-graph or entire graph property prediction In this work,
we use a GNN for node prediction, predicting the best machine learning pipeline
for an unseen dataset Speciﬁcally, we use a graph attention network (GAT)
(Veliˇ ckovi´ c et al., 2018) with neighborhood aggregation, in which an attention
function adaptively controls the contribution of neighbors An advantage of us-
ing a GNN in our use case is that data, metadata, and algorithm information are
shared between datasets (graph nodes) by messages passed between the graph
nodes.Inaddition,GNNsgeneralizewelltonewunknowndatasetsusingtheirag-
gregated weights learned during training, which are shared with the test dataset
13.7 Automated Machine Learning 277
during testing Beyond just a single new dataset, GNNs can generalize further
to an entirely new set of datasets Solutions from existing AutoML systems are used to train a new AutoML
model Our ﬂexible architecture can be extended to use pipeline recommenda-
tionsfromotherAutoMLsystemstoimproveperformancefurther.AutoMLisan
emerging ﬁeld of machine learning with the potential to transform the practice of
data science by automatically choosing a model to ﬁt the data best Several com-
prehensive surveys of the ﬁeld are available (He et al., 2021; Z¨ oller and Huber,
2021) The most straightforward approach to AutoML considers each dataset in
isolation and asks how to choose the best hyperparameter settings for a given
algorithm While the most popular method is still grid search, other more eﬃ-
cient approaches include Bayesian optimization (Snoek et al., 2012) and random
search (Solis and Wets, 1981) Recommender systems learn, often exhaustively,
which algorithms and hyperparameter settings perform best for a training set
of datasets and use this information to select better algorithms on a test set
without exhaustive search This approach reduces the time required to ﬁnd a
good model

============================================================

=== CHUNK 191 ===
Palavras: 362
Caracteres: 2524
--------------------------------------------------
An example is OBOE (Yang, Akimoto, Kim and Udell, 2019) and
TensorOBOE (Yang et al., 2020), which ﬁt a low-rank model to learn the low-
dimensional representations for the models or pipelines and datasets that best
predict the cross-validated errors, among all bilinear models To ﬁnd promising
models for a new dataset, OBOE runs a set of fast but informative algorithms on
the new dataset It uses their cross-validated errors to infer the feature vector for
the new dataset A related approach (Fusi et al., 2018) using probabilistic ma-
trix factorization powers Microsoft Azure’s AutoML service (Mukunthu, 2019) Auto-tuned models (Swearingen et al., 2017) represent the search space as a
tree with nodes being algorithms or hyperparameters and searches for the best
branch using a multi-armed bandit AlphaD3M (Drori, Krishnamurthy, Rampin,
Lourenco, One, Cho, Silva and Freire, 2018; Drori et al., 2019) formulates Au-
toML as a single-player game The system uses reinforcement learning with self-
play and a pre-trained model, which generalizes from many diﬀerent datasets
and similar tasks TPOT (Olson and Moore, 2016) and Autostacker (Chen, Wu,
Mo, Chattopadhyay and Lipson, 2018) use genetic programming to choose both
hyperparameter settings and the topology of a machine learning pipeline TPOT
represents pipelines as trees, whereas Autostacker represents them as layers AutoSklearn (Feurer et al., 2015) chooses a model for a new dataset by ﬁrst
computingdatameta-featurestoﬁndnearest-neighbordatasets.Thebest-performing
methods on the neighbors are reﬁned by Bayesian optimization and used to form
anensemble.End-to-endlearningofmachinelearningpipelinescanbeperformed
usingdiﬀerentiableprimitives(Milutinovicetal.,2017)formingadirectedacyclic
graph One major factor in the performance of an AutoML system is the base set
of algorithms it can use to compose more complex pipelines For a fair compar-
ison, in our numerical experiments, we compare our proposed methods only to
other AutoML systems that use Scikit-learn (Pedregosa et al., 2011) primitives 278 13 Applications
13.8 Education
13.8.1 Learning-to-Learn STEM Courses
Can a machine learn university-level STEM courses The answer is a resounding
yes (Drori et al., n.d.; Tang et al., 2022) There is a common misconception that
neural networks cannot solve STEM courses at a university level (Choi, 2021) Recent progress in solving machine learning problems (Tran et al., 2021) uses
Transformers pre-trained on code, and GNNs achieve super-human performance

============================================================

=== CHUNK 192 ===
Palavras: 370
Caracteres: 2600
--------------------------------------------------
However, those systems handle only numeric outputs, take a week of curation
and training for one speciﬁc course, overﬁt the course, and do not scale up to
multiple courses We automatically solve, explain, and generate university-level course prob-
lems from multiple STEM courses (at MIT, Harvard, and Columbia) for the ﬁrst
time We curate a new dataset of course questions and answers across a dozen
departments: Aeronautics and Astronautics, Chemical Engineering, Chemistry,
Computer Science, Economics, Electrical Engineering, Materials Science, Math-
ematics, Mechanical Engineering, Nuclear Science, Physics, and Statistics The
courses, their departments, and their universities are shown in Table 13.1 In order to test the quality of our machine-generated questions, we generate
new questions and use them in a Columbia University course, and perform A/B
testsdemonstratingthatthesemachine-generatedquestionsareindistinguishable
from human-written questions and that machine-generated explanations are as
useful as human-written explanations, again for the ﬁrst time Our approach
consists of the following steps: (1) given course questions, we automatically gen-
erate programs by program synthesis and few-shot learning using a Transformer
model, OpenAI Codex (Chen et al., 2021), pre-trained on text and ﬁne-tuned on
code; (2) execute the programs to obtain and evaluate the answers; (3) automat-
ically explain the correct solutions using Codex; (4) automatically generate new
questions that are qualitatively indistinguishable from human-written questions Our approach handles multiple output modalities, including numbers, text, and
visual outputs We verify that we do not overﬁt by solving an entirely new course
not available online This work is a signiﬁcant step forward in applying machine
learning to education, automating a considerable part of the work involved in
teaching Our approach allows the personalization of questions based on diﬃ-
culty level and student backgrounds It is the ﬁrst scalable solution, scaling up
to a broad range of courses across the schools of engineering and science The generative aspect of OpenAI’s Codex also gives us the ability to generate
newquestionsappropriatefordevelopingnewcoursecontent.Weintroducethese
newly generated questions into a Columbia University course and demonstrate
by an A/B test that the quality of these questions is on par with human-written
questions, again for the ﬁrst time This work is a signiﬁcant step forward in applying machine learning to ed-
ucation, automating a considerable part of the work involved in teaching

============================================================

=== CHUNK 193 ===
Palavras: 367
Caracteres: 2506
--------------------------------------------------
Our
13.8 Education 279
Table 13.1 University STEM courses: we curate, solve, explain, and generate new
questions for each course Department Course Number
1 MIT Aeronautics and Astronautics Unified Engineering 1-4 16.01-4
2 MIT Aeronautics and Astronautics Estimation & Control of Aerospace Systems 16.30
3 MIT Aeronautics and Astronautics Intro to Propulsion Systems 16.50
4 MIT Materials Science & Eng Fundamentals of Materials Science 3.012
5 MIT Materials Science & Eng Math for Materials Scientists & Engineers 3.016
6 MIT Materials Science & Eng Introduction to Solid-State Chemistry 3.091
7 MIT Chemical Engineering Chemical and Biological Reaction Eng 10.37
8 MIT Chemistry Principles of Chemical Science 5.111
9 MIT IDSS Statistical Thinking & Data Analysis IDS.013(J)
10 MIT EECS Signal Processing 6.003
11 MIT EECS Introduction to Machine Learning 6.036
12 MIT EECS Mathematics for Computer Science 6.042
13 MIT Physics Introduction to Astronomy 8.282
14 MIT Nuclear Science & Engineering Intro to Nuclear Eng & Ionizing Radiation 22.01
15 MIT Economics Principles of Microeconomics 14.01
16 MIT Mechanical Engineering Hydrodynamics 2.016
17 MIT Mechanical Engineering Nonlinear Dynamics I: Chaos 2.050J
18 MIT Mechanical Engineering Information & Entropy 2.110J
19 MIT Mechanical Engineering Marine Power and Propulsion 2.611
20 MIT Mathematics Single Variable Calculus 18.01
21 MIT Mathematics Multi-variable Calculus 18.02
22 MIT Mathematics Differential Equations 18.03
23 MIT Mathematics Introduction to Probability and Statistics 18.05
24 MIT Mathematics Linear Algebra 18.06
25 MIT Mathematics Theory of Numbers 18.781
26 Harvard Statistics Probability STATS110
27 Columbia Computer Science Computational Linear Algebra COMS3251
approach allows the personalization of questions based on diﬃculty level and
student backgrounds and scales up to multiple courses across a broad range of
STEM topics As a ﬁrst example, we solve MIT’s Linear Algebra 18.06 and Columbia Univer-
sity’s Computational Linear Algebra COMS3251 courses with perfect accuracy
by interactive program synthesis This surprisingly strong result is achieved by
turning the course questions into programming tasks and then running the pro-
grams to produce the correct answers We use OpenAI Codex with zero-shot
learning to synthesize code from questions without providing any examples in
the prompts We quantify the diﬀerence between the original question text and
the transformed question text that yields a correct answer

============================================================

=== CHUNK 194 ===
Palavras: 350
Caracteres: 2237
--------------------------------------------------
Since none of the
COMS3251 questions are available online, the model is not overﬁtting We inter-
actively work with Codex to produce both the correct result and visually good
plots, as shown in Figure 13.12 We place the question in context by augmenting
the question with deﬁnitions and information required for solving the question,
then rephrase and simplify Finally, we automatically generate new questions given a few sample questions
that may be used as new course content As a second example, we solve university-level probability and statistics ques-
280 13 Applications
Figure 13.12 Interactive workﬂow: (a) We begin with the original question Codex
generates a program, which is executed The result is missing the projection (b) We
transform the question, and Codex generates a program again to get the correct
answer, though the zero projection vector does not appear on the plot (c) An
additional task to plot the projection vector with a marker so that it is visible results
in Codex generating modiﬁed code which is executed to yield a correct answer and
visually pleasing result tions by program synthesis using OpenAI’s Codex We transform course prob-
lems from MIT’s 18.05 Introduction to Probability and Statistics and Harvard’s
STAT110 Probability into programming tasks We then execute the generated
code to get a solution Since these course questions are grounded in probabil-
ity, we often aim to have Codex generate probabilistic programs that simulate
many probabilistic dependencies to compute its solution Our approach requires
prompt engineering to transform the question from its original form to an ex-
plicit, tractable form that results in a correct program and solution To estimate
the amount of work needed to translate an original question into its tractable
form, we measure the similarity between original and transformed questions Our work is the ﬁrst to introduce a new dataset of university-level probability
and statistics problems and solve these problems in a scalable fashion using the
program synthesis capabilities of large language models This work is a signiﬁcant step forward in solving quantitative math problems
and opens the door for solving many university-level STEM courses by machine

============================================================

=== CHUNK 195 ===
Palavras: 361
Caracteres: 2598
--------------------------------------------------
13.9 Proteomics
13.9.1 Protein Structure Prediction
Proteins are necessary for various functions within cells, including transport, an-
tibodies, enzymes, and catalysis They are polymer chains of amino acid residues
whose sequences dictate stable spatial conformations, with particular torsion an-
glesbetweensuccessivemonomers.Theaminoacidresiduesmustfoldintoproper
13.9 Proteomics 281
conﬁgurations to perform their functions The sequence space of proteins is vast,
with 20 possible residues per position, and evolution has been sampling it over
billions of years Thus, current proteins are highly diverse in sequences, struc-
tures, and functions The high-throughput acquisition of DNA sequences, and
therefore the ubiquity of known protein sequences, stands in contrast to the lim-
ited availability of 3D structures, which are more functionally relevant From a
physics standpoint, the process of protein folding is a search for the minimum
energyconformationthathappensinnatureparadoxicallyfast(Levinthal,1969) Unfortunately, explicitly computing the energy of a protein conformation and its
surrounding water molecules is highly complex Inferring local secondary structure (Drori, Dwivedi, Shrestha, Wan, Wang, He,
Mazza, Krogh-Freeman, Leggas, Sandridge et al., 2018) consists of linear anno-
tation of structural elements along the sequence (Kabsch and Sander, 1983) Inferring tertiary structure consists of resolving the 3D atom coordinates of pro-
teins When highly similar sequences are available with known structures, this
homology can be used for modeling PSP was recently tackled by ﬁrst predict-
ing contact points between amino acids and then leveraging the contact map
to infer structure A primary contact indicator between a pair of amino acids is
their tendency to have correlated and compensatory mutations during evolution The availability of large-scale data on DNA, and therefore protein sequences, al-
lows detection of such co-evolutionary constraints from sets of sequences that
are homologous to a protein of interest Registering such contacting pairs in a
matrix facilitates a framework for their probabilistic prediction This contact
map matrix can be generalized to register distances between amino acids (Xu,
2019) Machine learning approaches garnered recent success in PSP (Anand and
Huang, 2018; AlQuraishi, 2019) and its sub-problems (Wang, Cao, Zhang and
Qi, 2018) These leverage available repositories of tertiary structure (Berman
et al., 2000) and its curated compilations (Orengo et al., 1997) as training data
for models that predict structure from sequence

============================================================

=== CHUNK 196 ===
Palavras: 355
Caracteres: 2516
--------------------------------------------------
Speciﬁcally, the recent bian-
nual critical assessment of PSP methods (Moult et al., 2018) featured multiple
such methods Most prominently, a ResNet-based architecture (Jumper et al.,
2021) has achieved impressive results in the CASP evaluation settings, based on
representing protein structures both by their distance matrices as well as their
torsion angles In this work, we design a novel representation of biologically rel-
evant input data and construct a processing ﬂow for PSP, as shown in Figure
13.13 Our method leverages advances in deep sequence models and proposes a
method to learn transformations of amino acids and their auxiliary information The method operates in three stages by (1) predicting backbone atom distance
matrices and torsion angles; (2) recovering backbone atom 3D coordinates; and
(3) reconstructing the full atom protein by optimization We demonstrate state-of-art protein structure prediction results using deep
learning models to predict backbone atom distance matrices and torsion angles,
recover backbone atom 3D coordinates and reconstruct the full atom protein
282 13 Applications
Figure 13.13 Our method operates by (1) predicting backbone atom distance matrices
and torsion angles; (2) recovering backbone atom 3D coordinates; and (3)
reconstructing the full atom protein by optimization
by optimization We present a gold standard dataset of around 75,000 proteins,
which we call the CUProtein dataset, which is easy to use in developing deep
learning models for PSP Next, we demonstrate competitive results with the
winning teams on CASP13 and a comparison with AlphaFold (A7D) (Jumper
et al., 2021) with results mostly superseding the winning teams on CASP12 Thisworkexploresencodedrepresentationforsequencesofaminoacidsalongside
their auxiliary information We oﬀer full access to data, models, and code, which
removesentrybarriersforinvestigatorsandmakespubliclyavailablethemethods
for this critical application domain We address two problems: (1) predicting backbone distance matrices and tor-
sion angles of backbone atoms from amino acid sequences, Q8 secondary struc-
ture, PSSMs, and co-evolutionary multiple sequence alignment; and (2) recon-
struction of all-atom coordinates from the predicted distance matrices and tor-
sion angles We begin with a one-hot representation of each amino acid and secondary
structure sequence, and real-valued PSSMs and MSA covariance matrices These
are passed through embedding layers and then onto an encoder–decoder archi-
tecture

============================================================

=== CHUNK 197 ===
Palavras: 371
Caracteres: 2914
--------------------------------------------------
To leverage sequence homology, we compute the covariance matrix of
theMSAfeaturesbyembeddingthehomologyinformationalonga k-dimensional
vectortoforma3-tensorandcontractthetensor Aijkalongwiththe k-dimensional
embedding, which is then passed as input to the encoder: Σ = Ak
jiAijk We use encoder–decoder models with a bottleneck to train prediction models The encoder freceives as input the aggregation A(by concatenation) of the em-
beddings eiof each input xi, and two separate decoders g1andg2that output
distance matrices and torsion angles for i∈{1,...,4}:gj/parenleftbigg
f/parenleftbigg
Agg
xi∈X(ei(xi))/parenrightbigg/parenrightbigg Inaddition,wealsouseamodelthatconsistsofseparateencoders fiforeachem-
bedded input ei(xi), which are aggregated by concatenation after encoding, and
separate decoders g1andg2for torsion angles and backbone distance matrices:
gj/parenleftbigg
Agg
xi∈X(fi(ei(x1)))/parenrightbigg Using a separate encoder model involves a more signiﬁ-
13.9 Proteomics 283
cant number of trainable parameters Our models diﬀer in the use of embeddings
for the input, their models, and loss functions Building on techniques commonly used in natural language processing, our
models use embeddings and a sequence of bidirectional gated recurrent units
(GRUs) and LSTMs with skip connections They include batch normalization,
dropout, and dense layers We experimented with various loss functions, includ-
ing MAE, MSE, Frobenius norm, and distance logarithm, to handle the dynamic
range of distances We have also implemented distance matrix prediction using
conditional GANs and variational autoencoders (VAEs) for protein subsequences
to learn the loss function Once backbone distance matrices and torsion angle are predicted, we address
two reconstruction sub-problems: (1) reconstructing the protein backbone coor-
dinates from their distance matrices, and (2) reconstructing the full atom protein
coordinates from the CαorCβcoordinates and torsion angles We employ three diﬀerent techniques for reconstructing the 3D coordinates X
between backbone atoms of a protein from the predicted matrix of their pairwise
distances (Dokmanic et al., 2015) Given a predicted distance matrix ˆD, our goal
is to recover 3D coordinates ˆXofnpoints We notice that D(X) depends only
on the Gram matrix XTX:
D(X)=1diag(XTX)T−2XTX+diag(XTX)1T(13.3)
Multi-dimensional scaling (MDS):
minimize
ˆXD(ˆX)−ˆDF2(13.4)
Semi-deﬁnite programming (SDP) and relaxation (SDR):
minimize
GK(G)−ˆDF2s.t.G∈C (13.5)
whereK(G)=1diag(G)T−2G+diag(G)1Toperates on the Gram matrix G Alternating direction method of multipliers (ADMM) (Anand and Huang, 2018):
minimize
G,Z,ηλη1+1
2⎛
⎝n/summationdisplay
i,j=1(Gii+Gjj−2Gij+ηij−ˆD2
ij)⎞
⎠2
+1{Z∈Sn
+}s.t.G−Z=0
We have found multi-dimensional scaling to be the fastest and most robust
method of the three, without depending on algorithm hyperparameters, which
is most suitable for our purposes

============================================================

=== CHUNK 198 ===
Palavras: 397
Caracteres: 2791
--------------------------------------------------
We assign plausible coordinates to the rest of the protein’s atoms given back-
bonecoordinates.Webeginwithaninitialguessorpredictionfor φandψtorsion
angles We maintain a look-up table of mean φ,ψvalues for each combination of
two consecutive αtorsions and three α-angles (the angles deﬁned by three con-
secutive atoms) Using these values and the Cαpositions we generate an initial
model A series of energy minimization simulations then relax this model under
284 13 Applications
an energy function that includes: standard bonded terms (bond, angle, plane
and out-of-plane), knowledge-based Ramachandran and pairwise terms, torsion
constraints on the φandψangles, and tether constraints on the Cαposition The latter term reduces the perturbations of the initial high forces Finally, we
add side-chains using a rotamer library, and remove clashes by a series of energy
minimization simulations We develop a very similar method for reconstructing
the full-atom protein from Cβatom distance matrices We have compared our predictions on CASP12 and CASP13 (Abriata et al.,
2019) test targets Deep learning methods were widely used only starting from
CASP13 AlphaFold was introduced starting from CASP13 The use of deep
learning methods in CASP13, due to the availability of DL programming frame-
works, signiﬁcantly improved performance over CASP12 A representative com-
parison between the winning CASP12 and CASP13 competition models, Al-
phaFold models for CASP13 for which A7D submitted predictions to CASP,
and our models shows results of RMSD around 2 Angstrom on test targets,
which is considered accurate in CASP Our results supersede the winning teams
of CASP12 compared with each best team for each protein, highlighting the im-
provementusingdeeplearningmethods.Ourapproachisonparwiththewinning
teams in CASP13, compared with the winning team for each protein, highlight-
ing that our method is state-of-the-art We measure the sequence-independent
RMSD, consistent with the CASP evaluation reports, and match the deposited
structures and our predictions CASP competitors such as AlphaFold provide
predictions for selected proteins Overall, our performance on CASP is highly
competitive.TrainingourmodelsonacloudinstancetakestwodaysusingGPUs Prediction of backbone distance matrices and torsion angles takes a few seconds
per protein, and reconstruction of full-atom proteins from distance matrices and
torsion angles takes a few minutes per protein, depending on protein length Limitations of this work are: (1) we only handle single-domain proteins and not
complexes with multiple chains; (2) PSSM and MSA data for several of the
CASP targets are limited to a subsequence of the full length protein; and (3) we
do not use available methods for detecting and reconstructing beta-sheets

============================================================

=== CHUNK 199 ===
Palavras: 356
Caracteres: 2646
--------------------------------------------------
13.9.2 Protein Docking
Modeling protein–protein interactions is a primary challenge for elucidating the
mechanisms behind biology’s most fundamental processes Recent advances in
machine learning for protein folding have established the foundation for pre-
dicting protein–protein interactions through co-folding A generalized folding
pipeline operates directly on structures for end-to-end protein docking, elimi-
nating the need for costly sequence alignments Euclidean-equivariant networks
for inferring pairwise three-dimensional matching between pairs of proteins, and
geometric models for iterative construction and reﬁnement of complexes signif-
icantly reduce the computational cost and inference time for protein docking,
reaching metrics on par with state-of-the-art classical methods 13.10 Combinatorial Optimization 285
13.10 Combinatorial Optimization
A core and essential area in computer science and operations research is the
domain of graph algorithms and combinatorial optimization The literature is
rich in both exact (slow) and heuristic (fast) algorithms (Golden et al., 1980);
however, each algorithm is designed afresh for each new problem with careful
attention by an expert to the problem structure Approximation algorithms for
NP-hard problems provide only worst-case guarantees (Williamson and Shmoys,
2011), and are not usually linear time, and hence not scalable Our motivation is
to learn new heuristic algorithms for these problems that require an evaluation
oracle for the problem as input and return a good solution in a pre-speciﬁed time
budget Concretely, we target combinatorial and graph problems in increasing
order of complexity, from polynomial problems such as minimum spanning tree
(MST), and shortest paths (SSP), to NP-hard problems such as the traveling
salesman problem (TSP) and the vehicle routing problem (VRP) The aptitude of deep learning systems for solving combinatorial optimization
problems has been demonstrated across a wide range of applications in the past
several years (Dai et al., 2017; Bengio et al., 2021) Two recent surveys of re-
inforcement learning methods (Mazyavkina et al., 2021) and machine learning
methods (Vesselinova et al., 2020) for combinatorial optimization over graphs
with applications have become available during the time of this writing The
power of GNNs (Xu et al., 2019) and the algorithmic alignment between GNNs
andcombinatorialalgorithmshaverecentlybeenstudied(Xuetal.,2020).Graph
neural networks trained using speciﬁc aggregation functions emulate speciﬁc al-
gorithms:forexample,aGNNalignswell(Xuetal.,2020)withtheBellman–Ford
algorithm for shortest paths

============================================================

=== CHUNK 200 ===
Palavras: 364
Caracteres: 2551
--------------------------------------------------
Our work is motivated by recent theoretical and empirical results in rein-
forcement learning and GNNs Graph neural network training is equivalent to a
dynamic programming algorithm (Xu et al., 2020), hence GNNs by themselves
can be used to mimic algorithms with polynomial time complexity Reinforce-
ment learning methods with GNNs can be used to ﬁnd approximate solutions to
NP-hard combinatorial optimization problems (Dai et al., 2017; Bengio et al.,
2021; Kool et al., 2019) Combinatorial optimization problems may be solved by exact methods, ap-
proximation algorithms, or heuristics Machine learning approaches for combi-
natorialoptimizationhavemainlyusedsupervisedorreinforcementlearning.Our
approach is unsupervised and is based on reinforcement learning We require nei-
ther output labels nor knowing the optimal solutions, and our method improves
by self-play Reinforcement learning methods can be divided into model-free
and model-based methods In turn, model-free methods can be divided into Q-
learning and policy optimization methods (OpenAI, 2020) Model-based meth-
ods have two ﬂavors: methods in which the model is given, such as expert it-
eration (Anthony et al., 2017) or AlphaZero (Silver et al., 2017), and methods
that learn the model, such as World Models (Ha and Schmidhuber, 2018) or
286 13 Applications
MuZero (Schrittwieser et al., 2019) AlphaZero has been generalized to many
games (Cazenave et al., 2020), both multiplayer and single-player (Drori, Kr-
ishnamurthy, Rampin, Lourenco, One, Cho, Silva and Freire, 2018) This work
views algorithms on graphs as single-player games and learns graph algorithms The supplementary material includes a comprehensive list of supervised and re-
inforcement learning methods used for combinatorial optimization of NP-hard
problems and classiﬁcation of all previous work by problem, method, and type This work provides a general framework for model-free reinforcement learning
using a GNN representation that elegantly adapts to diﬀerent problem classes
by changing an objective or reward and using the line graph Our approach generalizes well from examples on small graphs, where even
exhaustivesearchiseasy,tolargergraphs;andthearchitectureworksequallywell
when trained on polynomial problems such as MST and on NP-hard problems
such as TSP, though training time is signiﬁcantly larger for hard problems We
explore the limits of these algorithms as well: For what kinds of problem classes,
problem instances, and time budgets do they outperform classical approximation
algorithms

============================================================

=== CHUNK 201 ===
Palavras: 379
Caracteres: 2356
--------------------------------------------------
For all graph problems, our approximation running time is linear O(n+m)i n
thenumber of nodes nand edges m, both in theory and in practice For MSTand
SSP our running time is linear O(m) in the number of edges For TSP and VRP
our running time is linear O(n) in the number of nodes The TSP approximation
algorithms and heuristics have runtimes that grow at least quadratically in the
graph size OnrandomEuclideangraphswith100nodes,ourmethodis1–3ordersofmag-
nitude faster and delivers a comparable optimality gap, Moreover, this speedup
improves as the graph size increases S2V-DQN (Dai et al., 2017), another re-
inforcement learning method, builds a 10-nearest-neighbor graph, and also has
quadratic runtime complexity; on these graphs, our method runs 52 times faster
and obtains a lower (better) optimality gap, the ratio between a method’s re-
ward and the optimal reward GPN (Ma et al., 2020) has runtime complexity
O(nlogn) with a more signiﬁcant optimality gap and does not generalize as well
nor easily extend to other problems The running time for solving MST using Prim’s algorithm is O(mlogm)a n d
the running time for solving SSP using Dijkstra’s algorithm is O(nlogn+m) For MST, running our method on larger graphs (for longer times) results in
optimality gaps close to 1, converging to an optimal solution Generalization on graphs (1) From small to large random graphs: For MST,
we generalize from small to large graphs accurately For TSP, we generalize from
small to larger graphs with median tour lengths (and optimality gaps) better
than other methods (2) Between diﬀerent types of random graphs: For MST,
we generalize accurately between diﬀerent types of random graphs (3) From
random to real-world graphs: For TSP, we generalize from random graphs to
real-world graphs better than other methods A uniﬁed framework for solving any combinatorial optimization problem over
13.10 Combinatorial Optimization 287
Figure 13.14 Our uniﬁed framework (a) The primal graph (white nodes and solid
edges) and its edge-to-vertex line graph (gray nodes and dashed edges) Two nodes in
the line graph are connected if the corresponding edges in the primal graph share a
node Notice that while the number of primal edges (7) is equal to the number of dual
nodes (7), the number of dual edges (10) is not necessarily equal to the number of
primal nodes (6)

============================================================

=== CHUNK 202 ===
Palavras: 378
Caracteres: 2260
--------------------------------------------------
(b) Combinatorial optimization as a single-player game deﬁned by
states, actions, and rewards Traversing a path (green) from the root to a leaf node
(pink square) corresponds to a solution for a problem White nodes represent states
and black nodes represent actions From each state, there may be many possible
actions (more than the two illustrated here) representing the possible nodes or edges
in the problem graph The leaf nodes represent rewards or costs, such as the sum of
weights in MST or length of the tour in TSP (c) Graph algorithms for polynomial
problems MST and SSP, and NP-hard problems TSP and VRP formulated as
single-player games by reinforcement learning using states, actions, and rewards For
MST, the state includes the graph, line graph, weights, and selected edges T(red) graphs: (a) We model problems that involve both actions on nodes and edges by
using the edge-to-vertex line graph Figure 13.14a shows an example of a primal
graph and its line graph (b) We model graph algorithms as a single-player game
as shown in Figure 13.14b (c) We learn diﬀerent problems by changing the
objective or reward function, as shown in Figure 13.14c Given a graph G=(V,E,W),V={1,...,n}is the set of vertices (nodes), E
is the set of edges and Wis the set of edge weights For edges eijbetween nodes
iandjin an undirected graph, wij=wji.|V|and|E|represent the number
of vertices and edges in the graph Given a node i,N(i) denotes the set of its
neighboring nodes Given a primal graph G=(V,E,W), the edge-to-vertex dual
orline graph ,G∗=(V∗,E∗,W∗), is deﬁned so each edge in the primal graph
corresponds to a node in the line graph: V∗=E Two nodes in the line graph
are connected if the corresponding edges in the primal graph share a node Edge
288 13 Applications
weights in the primal graph become node weights W∗in the line graph Figure
13.14a illustrates the relationship between the primal and line graphs We learn MST and SSP by training and running on ﬁve diﬀerent types of ran-
dom graphs: Erd˝ os–R´ enyi (ER) (Erd¨ os and R´ enyi, 2011), Barab´ asi–Albert (Al-
bert and Barab´ asi, 2002), stochastic block model (Holland et al., 1983), Watts–
Strogatz (Watts and Strogatz, 1998), and random regular (Steger and Wormald,
1999; Kim and Vu, 2003)

============================================================

=== CHUNK 203 ===
Palavras: 357
Caracteres: 2223
--------------------------------------------------
We learn TSP and VRP by training and running on
complete graphs with diﬀerent numbers of random nodes drawn uniformly from
[0,1]2 For MST and SSP, edge weights are chosen uniformly between 0 and
1 for pairs of nodes that are connected For TSP and VRP, these weights are
the distances between the nodes We also test our models on real-world graphs
(Reinelt, 2020) 13.10.1 Problems over Graphs
Given a connected and undirected graph G=(V,E,W), the MST problem is to
ﬁnd a tree T=(VT,ET) withVT=V,ET⊂Eminimizing the sum of the edge
weightsWT⊂W Algorithms for MST problems include Boruvka’s (Neˇ setˇ ril
et al., 2001), Prim’s (Prim, 1957) and Kruskal’s (Kruskal, 1956) algorithms; all
are greedy algorithms with time complexity O(|E|log|V|) We consider the SSP problem with non-negative edge weights Given a con-
nected and directed graph G=(V,E,W) and a source vertex, the SSP problem
is to ﬁnd the shortest paths from the source to all other vertices For the SSP
problem with non-negative weights, Dijkstra’s algorithm (Dijkstra, 1959) com-
plexity is O(|V|log|V|+|E|) using a heap For the general single-source shortest
paths problem, Bellman–Ford (Bang-Jensen and Gutin, 2000) runs in O(|V||E|) In addition, the Floyd–Warshall algorithm (Cormen et al., 1990) solves the SSP
problem between all pairs of nodes with cubic time complexity O(|V|3) Given a graph G=(V,E,W), letVrepresent a list of cities and Wrepresent
the distances between each pair of cities The goal of the TSP is to ﬁnd the
shortest tour that visits each city once and returns to the starting city The TSP
is an NP-hard problem Approximation algorithms and heuristics include LKH
(Lin and Kernighan, 1973), Christoﬁdes (Christoﬁdes, 1976), 2-opt (Lin, 1965;
Aarts and Lenstra, 2003), farthest insertion and nearest neighbor (Rosenkrantz
et al., 1977) Concorde (Applegate et al., 2006) is an exact TSP solver Gurobi
(Achterberg, 2019) is a general integer programming solver that can also be used
to ﬁnd an exact TSP solution GivenMvehicles and a graph G=(V,E) with|V|cities, the goal of the
VRP is to ﬁnd optimal routes for the vehicles Each vehicle m∈{1,...,M}
starts from the same depot node, visits a subset V(m) of cities and returns to
the depot node

============================================================

=== CHUNK 204 ===
Palavras: 386
Caracteres: 2484
--------------------------------------------------
The routes of diﬀerent vehicles do not intersect except at the
depot; together, the vehicles visit all cities The optimal routes minimize the
longest tour length of any single route The TSP is a special case of VRP for one
vehicle 13.11 Physics 289
13.10.2 Learning Graph Algorithms as Single-Player Games
We represent the problem space as a search tree The leaves of the search tree
represent all (possibly exponentially many) possible solutions to the problem A
search traverses this tree, choosing a path guided by a neural network as shown
in Figure 13.14b The initial state, represented by the root node, may be the
empty set, random, or other initial states Each path from the root to a leaf
consists of moving between nodes (states) along edges (taking actions), reaching
a leaf node (reward) Actions may include adding or removing a node or edge The reward (or cost) may be the solution’s value; for example, a sum of weights
or length of tour For each problem, Figure 13.14c deﬁnes the states, actions,
and rewards within our framework We show that the single-player formulation
extends our framework to other combinatorial optimization problems on graphs When the predictions of our neural network capture the global structure of the
problem, this mechanism is very eﬃcient On the other hand, even if the network
makes poor predictions for a particular problem, the search will still ﬁnd the
solution if run for a suﬃciently long (possibly exponential) time The network
is retrained using the results of the evaluation oracle on the leaf nodes reached
by the search to improve its predictions In the context of perfect information
games, a similar mechanism converges asymptotically to the optimal policy (Sun
et al., 2018) 13.11 Physics
13.11.1 Pedestrian Wind Estimation in Urban Environments
The ﬁeld of ﬂuid dynamics deals with enormous amounts of data from ﬁeld mea-
surements and experiments to more extensive full-ﬂow ﬁeld data generated from
computational ﬂuid dynamics (CFD) simulations (Brunton et al., 2020) This
wealth of data, coupled with advances in computing architectures and progress
in machine learning in the last decade, has led to an interest in applying deep
neuralnetworksforrapidlyapproximatingCFD.Applicationsofdeepneuralnet-
works to ﬂuid dynamics include physics model augmentation with uncertainty
quantiﬁcation, accuracy prediction improvements, and surrogate modeling for
enabling design exploration (Nathan Kutz, 2017; Duraisamy et al., 2019)

============================================================

=== CHUNK 205 ===
Palavras: 380
Caracteres: 2502
--------------------------------------------------
Con-
volutional neural networks have been particularly explored for the latter due to
their capacity to represent non-linear input and output functions while extract-
ing spatial relationships, and GANs as well due to their additional ability to
learn without explicitly deﬁning a loss function A number of implementations
have been successful at reducing the computational expense of velocity ﬂuid ﬂow
approximations with a minor error compromise (Guo et al., 2016; Farimani et al.,
2017) In contrast to other deep neural network applications such as image and
speech recognition, a major challenge in ﬂuid dynamics is the strict requirement
for ﬂuid ﬂow ﬁelds quantiﬁcation to be precise, generalizable and interpretable
290 13 Applications
Figure 13.15 Testing set sample generator predictions, uncertainties, and absolute
errors A sample of model predictions for select urban patches is shown and their
associated uncertainties and the absolute error Visual inspection of the results shows
the model’s capacity to identify zones of impact created by wind obstructions in an
urban scene It also shows its limited capacity to capture the scale of impact for high
wind factor zones Other artifacts include inconsistent color patches in portions of the
image and grainy noise (Brunton et al., 2020) The computational expense of CFD simulations addi-
tionally makes it largely unfeasible to repeat experiments and expand datasets Thus, the ﬁnite amount of training data, coupled with distinct feature represen-
tation and accuracy requirements across ﬂuid domain disciplines, motivates the
development of application-speciﬁc deep learning models Figure 13.15 shows a
sample of model predictions, uncertainties, and absolute errors 13.11.2 Fusion Plasma
The analysis of turbulent ﬂows is a signiﬁcant area in fusion plasma physics Cur-
rent theoretical models quantify the degree of turbulence based on the evolution
of speciﬁc plasma density structures, called blobs In this work, we track these
blobs’ shape and position in high-frequency video data obtained from gas puﬀ
imaging (GPI) diagnostics We compare various tracking approaches and ﬁnd
that an optical ﬂow method is appropriate for these applications We train on
synthetic data and test on both synthetic and real data As a result, our model
eﬀectively tracks blob structures on both synthetic and real experimental GPI
data, showing its prospect as a powerful tool to estimate blob statistics linked
with edge turbulence of the tokamak plasma

============================================================

=== CHUNK 206 ===
Palavras: 353
Caracteres: 2317
--------------------------------------------------
In tokamak fusion reactors, plasmas are magnetically conﬁned to produce en-
ergy from nuclear fusion In order to maximize the rate of fusion, it is vital
to maintain conﬁnement as long as possible The quality of this conﬁnement is
13.12 Summary 291
Figure 13.16 (left) Cross-section of a plasma in tokamak reactor, TCV, with the
locations GPI views on the last closed ﬂux surface (LCFS) (center) Snapshot of real
GPI data capturing a blob passing by the LCFS Here, empty spots correspond to
dead GPI views The brightness level is color-coded, low as blue and high as yellow (right) Snapshot of synthetic data capturing a blob passing by the LCFS The blob is
represented with a Gaussian ellipse with a major and minor axis marked by
perpendicular black lines closely related to the turbulence at the edge region of the plasma core (Figure
13.16a) Current theoretical models can quantify the degree of turbulence from
theevolutionofspeciﬁcstructures(“blobs”)withintheplasmadensityﬁeld.This
is an evolving area of research Diﬀerent models require the analysis of diﬀerent
“blobstatistics”thatcanbederivedfromimagedata(e.g.blobvelocity,size,and
intermittency) For example, the ﬂuctuations in the plasma can be described by
a stochastic model as a superposition of uncorrelated Lorentzian pulses, which
is parameterized by the intermittency of blobs (Garcia et al., 2016; Garcia and
Theodorsen, 2017) Furthermore, the radial velocity and the size of blobs can
be used to determine the theoretical regime, predicting dependencies for the
radial velocity of blobs on plasma parameters (Myra et al., 2006) Comparing
various approaches for tracking, we ﬁnd that optical ﬂow based on deep learning
accurately tracks the position of blobs in low-resolution (12 ×10 pixel), high-
frequency (2 MHz) video data obtained from GPI diagnostics (Zweben et al.,
2017) Gass puﬀ imaging is an edge diagnostic tool that measures the spatially
resolved ﬂuctuations of brightness which can be used as a proxy for plasma
density measurements Figure 13.16b shows a snapshot of the GPI data that
captures a blob passing by the plasma edge (i.e., the last closed ﬂux surface, or
LCFS) and moving radially out 13.12 Summary
We have covered a dozen novel applications of deep learning, demonstrating
system architectures and representative results

============================================================

=== CHUNK 207 ===
Palavras: 354
Caracteres: 2243
--------------------------------------------------
These include breakthrough ap-
plications in deep learning for protein structure prediction, climate science, au-
292 13 Applications
tonomous driving, combinatorial optimization, vision, audio and language, and
education While humans are generalists, many deep learning applications are
specialists However, deep learning systems are not limited to specialized do-
mains, as demonstrated by the application of learning-to-learn in many STEM
courses using a single foundation model trained on both text and code Appendix A: Matrix Calculus
Matrix calculus deﬁnes the partial derivatives of a function with respect to vari-
ables and is used in gradient computations for backpropagation and optimiza-
tion We will write the derivative of a scalar with respect to a column vector as
a column vector, adopting the denominator layout commonly used in machine
learning In contrast, in numerator layout the dimensions are transposed A.1 Gradient Computations for Backpropagation
We deﬁne the gradients of a scalar with respect to a vector or a matrix This is
useful for computing the gradient of a loss function with respect to activations,
pre-activations, or weights The dimension of the gradient in these cases is the
dimension of the denominator Next, we deﬁne the gradient of a vector with
respect to another vector which results in the Hessian matrix Finally, we deﬁne
the gradient of a matrix with respect to a scalar A.1.1 Scalar by Vector
The gradient of a scalar ywith respect to an n×1-dimensional column vector x
is deﬁned by the n×1-dimensional column vector:
∂y
∂x=⎡
⎢⎣∂y
∂x1 ∂y
∂xn⎤
⎥⎦ (A.1)
For example, the gradient of the loss Lwith respect to the n×1 weight vector
wis then×1-dimensional gradient∂L
∂w A.1.2 Scalar by Matrix
The gradient of a scalar ywith respect to the m×n-dimensional matrix Xis
deﬁned by the m×n-dimensional matrix:
∂y
∂X=⎡
⎢⎣∂y
∂x11...∂y
∂x1n ∂y
∂xm1...∂y⎤
⎥⎦ (A.2)
294 A Matrix Calculus
For example, the gradient of the loss function L, which is a scalar, with respect
to anm×nweight matrix Wis them×n-dimensional gradient∂L
∂W A.1.3 Vector by Vector
The gradient of the m×1-dimensional vector ywith respect to the n×1-
dimensional vector xis deﬁned by the m×n-dimensional matrix:
∂y
∂x=⎡
⎢⎣∂y1
∂x1...∂y1
∂xn

============================================================

=== CHUNK 208 ===
Palavras: 361
Caracteres: 2460
--------------------------------------------------
∂ym
∂x1...∂ym
∂xn⎤
⎥⎦ (A.3)
For example, the gradient of the n×1 activation vector awith respect to the
n×1 pre-activation vector zis ann×n-dimensional gradient∂a
∂z A.1.4 Matrix by Scalar
The derivative of an m×n-dimensional matrix Ywith respect to a scalar xis
them×n-dimensional matrix:
∂Y
∂x=⎡
⎢⎣∂y11
∂x...∂y1n
∂x ∂ym1
∂x...∂ymn
∂x⎤
⎥⎦ (A.4)
A.2 Gradient Computations for Optimization
We deﬁne the gradient of a dot product of vectors with respect to a vector used
in optimization and the gradient of a quadratic form with respect to a vector,
which is useful for quasi-Newton method computations A.2.1 Dot Product by Vector
The gradient of the dot product aTxof the 1×nvectoraTwith the n×1 vector
xwith respect to the vector xis then×1 vector:
∂aTx
∂x=a (A.5)
since∂aTx
∂xi=aifor alli=1,...,n The gradient of the dot product of vectors
aTbwith respect to another vector xis:
∂aTb
∂x=∂a
∂xb+∂ba (A.6)
A.2 Gradient Computations for Optimization 295
A.2.2 Quadratic Form by Vector
The gradient of the quadratic form xTAxwith respect to an n×1-dimensional
vectorxis then×1-dimensional vector:
∂xTAx
∂x=(A+AT)x (A.7)
since∂xTAx
∂xi=/summationtextn
j=1xj(aij+aji) for alli=1,...,n The second-order derivative
with respect to xis therefore A+AT.I fAis a symmetric matrix then A+AT=
2A These equations are used for deriving quasi-Newton optimization methods Appendix B: Scientiﬁc Writing and
Reviewing Best Practices
Communicating deep learning methods and results is essential for successful re-
search in academia and industry This Appendix describes writing and reviewing
best practices B.1 Writing Best Practices
Good writing requires rewriting, and therefore, it often helps to start writing
early, write a draft, take breaks, and return to the manuscript while iterat-
ing the process Once we have a draft version of the text, we may improve it
by omitting needless words (Strunk Jr and White, 2007), speciﬁcally: Omit-
ting subjective words that are often unnecessary and may even be misleading,
omitting unnecessary phrases, simplifying the text, using active voice, and using
parallel constructions B.1.1 Introduction
A research paper usually begins with an abstract followed by three sections:
introduction,methods,andresults,andendswithadiscussionorconclusions.An
abstract may consist of the opening sentences from paragraphs of each text part It is essential to place the key contributions upfront in an abstract, explaining
them clearly to the reader

============================================================

=== CHUNK 209 ===
Palavras: 375
Caracteres: 2392
--------------------------------------------------
Introductory paragraphs may begin with the main
point or an example and then expand The introduction usually moves from a
general description to speciﬁc details, whereas the discussion moves from the
speciﬁc to the general big picture The introduction may describe a research
problem, explaining why it is essential to the reader A related-work section may
be part of the introduction and describes previous work, other solutions to the
same problem, or similar approaches previously applied to other problems The
related work may explain the limitations of previous work and then describe the
contribution of the work presented B.1.2 Methods
Methods sections describe the proposed solution, the dataset, and the evaluation
metrics The proposed solution should be described in detail, including the archi-
B.2 Reviewing Best Practices 297
tecture, the training process, and the hyperparameters The dataset should be
described in detail, including the number of samples, the number of classes, the
number of features, and the distribution of the classes The evaluation metrics
should be described in detail, for example, the number of folds, the number of
repetitions, and the number of samples per fold B.1.3 Figures and Tables
Figures should display information, and following a guiding principle of “less is
more” may produce good graphics (Tufte, 1985) Readers ﬁrst skim the ﬁgures
of a manuscript and the ﬁrst sentence of each paragraph Therefore, the ﬁgures
and their captions should be self-contained Captions may be lengthy, explaining
what to pay attention to Tables may compare diﬀerent approaches and the
present method B.1.4 Results
In the results section, it is essential not to over-sell the work and deliver a correct
message regarding the performance of the methods while clearly explaining the
scope and limitations of the work B.1.5 Abbreviations and Notation
Writing a book chapter or book requires consistent notation and text style
throughout the manuscript Once a version of the text is ready, copy editing and
proofreading best practice is to prepare a style sheet of abbreviations, spelling,
hyphenation, capitalization, and text style so that the manuscript is consistent An example of parts of the style sheet prepared for this book is:
•Spelling: US (not UK) spelling, spell out and capitalize Equation (not Eq.)
when referring to a numbered equation, okay to use Eq

============================================================

=== CHUNK 210 ===
Palavras: 356
Caracteres: 2597
--------------------------------------------------
•Hyphens: Hyphenated as an adjective: long-term, long-range, high-quality,
multi-XX, pre-XX; hyphenated as an adjective and noun: image-to-image,
video-to-video, etc., trade-oﬀ, mini-batch, saddle-point(s), non-linear; not hy-
phenated: pseudocode, overﬁt, overﬁtting, underﬁt, underﬁtting, hyperparam-
eter, pointwise, piecewise, stepwise, elementwise, cross validation (two words) •Capitalization and text style: the internet is lowercased unless starting a sen-
tence.AlwayscapitalizeTransformer(s),Swish,TensorFlow,andPyTorch.Use
“quasi-Newton” not “Quasi-Newton” or “Quasi Newton.”
B.2 Reviewing Best Practices
Reviewing scientiﬁc work begins by reading the paper or work and listing the
strengths and weaknesses, optionally classifying them into minor and major
298 B Scientiﬁc Writing and Reviewing Best Practices
strengths and weaknesses The reviewer may mark everything they would like
to comment on, including typos, missing references, observations, etc A typical
conference paper review takes around 2–4 hours The reviewer should brieﬂy describe the report The reviewer should address
whether the exposition and presentation are clear and suggest how they could be
improved Next, the reviewer should check if the references are adequate and list
anyadditionalreferencesthataremissing.The reviewmay involvegoingthrough
the implementation code or evaluating whether the work may be reproduced
based on the paper The reviewer should verify that the paper discusses all the
essential details and clearly states the work’s scope and limitations B.2.1 Ranking
After reading the paper and optionally going through the supplementary mate-
rial, the reviewer scores the report This includes explaining the score by dis-
cussing strengths and weaknesses The ranking should be based on scientiﬁc
merit rather than personal opinion The review may include suggestions for im-
provement B.2.2 Rebuttal
A rebuttal is part of the review process The authors’ goal in the rebuttal is
to clarify and improve the evaluation The goal of both the authors and the
reviewers is to help understand what can be improved, have a discussion, and
clear up any misunderstandings Insummary,agoodreviewprocessisnotonlyfairandrigorous.Italsorespects
the time and eﬀort the authors put into the work and, therefore, should be kind (2003), Local Search in Combinatorial Optimization ,
Princeton University Press Abadi, M., Barham, P., Chen, J., Chen, Z et al.(2016), “Tensorﬂow: A system
for large-scale machine learning”, in Proceedings of the 12th USENIX Sympo-
sium on Operating Systems Design and Implementation , pp

============================================================

=== CHUNK 211 ===
Palavras: 356
Caracteres: 2703
--------------------------------------------------
E., and Dal Peraro, M (2019), “A further leap of im-
provement in tertiary structure prediction in CASP13 prompts new routes
for future assessments”, Proteins: Structure, Function, and Bioinformatics
87(12), 1100–1112 (2019), “Gurobi solver”, www.gurobi.com/pdfs/benchmarks.pdf (2021), “HistoGAN: Controlling
colors of GAN-generated and real images via color histograms”, in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp Akbari, H., Yuan, L., Qian, R., Chuang, W.-H et al.(2021), “VATT: Trans-
formers for multimodal self-supervised learning from raw video, audio and
text”, in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) (2002), “Statistical mechanics of complex net-
works”,Reviews of Modern Physics 74(1), 47 (2019), “ProteinNet: A standardized data set for machine learn-
ing of protein structure”, BMC Bioinformatics 20(1), 311 Amari,S.-I.(1998),“Naturalgradientworkseﬃcientlyinlearning”, Neural Com-
putation 10(2), 251–276 (2018), “Generative modeling for protein struc-
tures”, in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) , pp Anderegg, R., Felber, N., Fichtner, W., and Franke, U (2004), “Implementation
of high-order convolution algorithms with low latency on silicon chips”, in
Audio Engineering Society Convention , number 117 Anthony, T., Tian, Z., and Barber, D (2017), “Thinking fast and slow with deep
learning and tree search”, in Proceedings of Advances in Neural Information
Processing Systems (NeurIPS) , pp 300 References
Antipov, G., Baccouche, M., and Dugelay, J.-L (2017), “Face aging with condi-
tional generative adversarial networks”, in Proceedings of the IEEE Interna-
tional Conference on Image Processing (ICIP) , pp Applegate, D., Bixby, R., Chvatal, V., and Cook, W (2006), “Concorde TSP
Solver”, Computer program (2012), Monte Carlo Chess, Master’s thesis, Technische Universitaet
Darmstadt Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D (2019), “Invariant
risk minimization”, arXiv preprint arXiv:1907.02893 Arjovsky,M.,Chintala,S.,andBottou,L.(2017),“Wassersteingenerativeadver-
sarial networks”, in Proceedings of the International Conference on Machine
Learning (ICML) , pp Arvanitidis, G.,Hansen,L K.,and Hauberg, S.(2018),“Latentspace oddity: On
the curvature of deep generative models”, in Proceedings of the International
Conference on Learning Representations (ICLR) Arvanitidis, G., Hauberg, S., Hennig, P., and Schober, M (2019), “Fast and
robust shortest paths on manifolds learned from data”, in Proceedings of the
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) Auer, P., Cesa-Bianchi, N., and Fischer, P

============================================================

=== CHUNK 212 ===
Palavras: 354
Caracteres: 2631
--------------------------------------------------
(2002), “Finite-time analysis of the
multiarmed bandit problem”, Machine Learning 47(2–3), 235–256 (2019), “Common mathematical framework for stochastic reverbera-
tion models”, Journal of the Acoustical Society of America 145(4), 2733–2745 Bahdanau, D., Cho, K., and Bengio, Y (2015), “Neural machine translation by
jointly learning to align and translate”, in Proceedings of the International
Conference on Learning Representations (ICLR) Bang-Jensen,J.andGutin,G.(2000),“Section2.3.4:TheBellman–Ford–Moore
algorithm”, in Digraphs: Theory, Algorithms and Applications , Springer Bansal, M., Krizhevsky, A., and Ogale, A (2018), “ChauﬀeurNet: Learning
to drive by imitating the best and synthesizing the worst”, arXiv preprint
arXiv:1812.03079 Battaglia, P., Pascanu, R., Lai, M., Rezende, D (2016), “Interaction
networks for learning about objects, relations and physics”, in Proceedings
of Advances in Neural Information Processing Systems (NeurIPS) , pp G., Cornish, R., Rubio, D M., Schmidt, M., and Wood, F (2018),
“Online learning rate adaptation with hypergradient descent”, in Proceedings
of the International Conference on Learning Representations (ICLR) Behrmann, J., Grathwohl, W., Chen, R T., Duvenaud, D., and Jacobsen, J.-
H (2019), “Invertible residual networks”, in Proceedings of the International
Conference on Machine Learning (ICML) , pp Belkin, M., Hsu, D., Ma, S., and Mandal, S (2019), “Reconciling modern
machine-learning practice and the classical bias–variance trade-oﬀ”, Proceed-
ings of the National Academy of Sciences 116(32), 15849–15854 References 301
Bell-Kligler, S., Shocher, A., and Irani, M (2019), “Blind super-resolution ker-
nel estimation using an internal-GAN”, in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) , pp B e l l e m a r e ,M .G et al.(2020), “Au-
tonomous navigation of stratospheric balloons using reinforcement learning”,
Nature 588(7836), 77–82 (1965), “Wengert’s numeri-
calmethodfor partialderivatives, orbitdeterminationandquasilinearization”,
Communications of the ACM 8(4), 231–232 Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C (2003), “A neural proba-
bilistic language model”, Journal of Machine Learning Research 3, 1137–1155 Bengio, Y., Lodi, A., and Prouvost, A (2021), “Machine learning for combina-
torial optimization: A methodological tour d’horizon”, European Journal of
Operational Research 290(2), 405–421 Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo, D (2009), “The Fifth
PASCAL Recognizing Textual Entailment Challenge”, in Proceedings of the
Text Analysis Conference M., Westbrook, J., Feng, Z., Gilliland, G

============================================================

=== CHUNK 213 ===
Palavras: 367
Caracteres: 2598
--------------------------------------------------
et al.(2000), “The protein
data bank”, Nucleic Acids Research 28(1), 235–242 P., Jankowiak, M., Obermeyer, F et al.(2019), “Pyro:
Deep universal probabilistic programming”, Journal of Machine Learning Re-
search 20, 1–6 (2006), Pattern Recognition and Machine Learning , Springer C., Steinhoﬀ, T., Claustre, H., Fiedler, B et al.(2018), “An alterna-
tive to static climatologies: Robust estimation of open ocean CO2 variables
and nutrient concentrations from T, S, and O2 data using Bayesian neural
networks”, Frontiers in Marine Science 5, 328 M., Kucukelbir, A., and McAuliﬀe, J (2017), “Variational inference:
A review for statisticians”, Journal of the American Statistical Association
112(518), 859–877 Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B et al.(2016), “End to
end learning for self-driving cars”, arXiv preprint arXiv:1604.07316 Bojchevski, A., Shchur, O., Z¨ ugner, D., and G¨ unnemann, S (2018), “NetGAN:
Generatinggraphsviarandomwalks”,in Proceedings of the International Con-
ference on Machine Learning (ICML) , pp Bommasani, R., Hudson, D A., Adeli, E., Altman, R et al.(2021), “On the op-
portunities and risks of foundation models”, arXiv preprint arXiv:2108.07258 (2005), Spectral Properties of Banded Toeplitz
Matrices, Vol (2010), “Large-scale machine learning with stochastic gradient de-
scent”, in Proceedings of the International Conference on Computational
Statistics (COMPSTAT) , pp 302 References
Brafman, R (2002), “R-MAX: A general polynomial
time algorithm for near-optimal reinforcement learning”, Journal of Machine
Learning Research 3, 213–231 Brock, A., Donahue, J., and Simonyan, K (2019), “Large scale GAN training
for high ﬁdelity natural image synthesis”, in Proceedings of the International
Conference on Learning Representations (ICLR) Brock, A., Lim, T., Ritchie, J (2017), “Neural photo editing
with introspective adversarial networks”, in Proceedings of the International
Conference on Learning Representations (ICLR) Brooks, S., Gelman, A., Jones, G., and Meng, X.-L (2011), Handbook of Markov
Chain Monte Carlo , CRC Press B., Mann, B., Ryder, N., Subbiah, M et al.(2020), “Language mod-
els are few-shot learners”, in Proceedings of Advances in Neural Information
Processing Systems (NeurIPS) R., and Koumoutsakos, P (2020), “Machine learning
for ﬂuid mechanics”, Annual Review of Fluid Mechanics 52, 477–508 (2020), “Impulse response data augmentation and deep neural net-
works for blind room acoustic parameter estimation”, in Proceedings of the
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp

============================================================

=== CHUNK 214 ===
Palavras: 352
Caracteres: 2652
--------------------------------------------------
Burge, J., Bonanni, M., Ihme, M., and Hu, L (2020), “Convolutional LSTM
neural networks for modeling wildland ﬁre dynamics”, arXiv preprint
arXiv:2012.06679 BYU-DML (2019), “BYU’s Python library of useable tools for metalearning”,
github.com/byu-dml/metalearn/tree/develop/metalearn/metafeatures Caccia, M., Caccia, L., Fedus, W., Larochelle, H et al.(2018), “Language GANs
fallingshort”,in Proceedings of the International Conference on Learning Rep-
resentations (ICLR) Carbon Hydrographic Data Oﬃce (2021), “GO-SHIP data”, cchdo.ucsd.edu Carion, N., Massa, F., Synnaeve, G., Usunier, N et al.(2020), “End-to-end ob-
ject detection with transformers”, in Proceedings of the European Conference
on Computer Vision (ECCV) , pp (1992), Riemannian Geometry , Birkh¨ auser Carter, B., Feely, R., Williams, N., Dickson, A et al.(2018), “Updated meth-
ods for global locally interpolated estimation of alkalinity, pH, and nitrate”,
Limnology and Oceanography: Methods 16(2), 119–131 Casanova, A., Careil, M., Verbeek, J., Drozdzal, M., and Romero, A (2021),
“Instance-conditioned GAN”, in Proceedings of Advances in Neural Informa-
tion Processing Systems (NeurIPS) Cazenave, T., Chen, Y.-C., Chen, G.-W., Chen, S.-Y et al.(2020), “Polygames:
Improved zero learning”, ICGA Journal 42(4), 244–256 Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L (2017), “SemEval-
2017 task 1: Semantic textual similarity multilingual and cross-lingual focused
evaluation”, in International Workshop on Semantic Evaluation References 303
Chai, F., Johnson, K S., Claustre, H., Xing, X et al.(2020), “Monitoring ocean
biogeochemistry with autonomous platforms”, Nature Reviews Earth & En-
vironment 1(6), 315–326 Chan, C., Ginosar, S., Zhou, T., and Efros, A (2019), “Everybody dance
now”,in Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) , pp Chang, H., Lu, J., Yu, F., and Finkelstein, A (2018), “PairedcycleGAN: Asym-
metric style transfer for applying and removing makeup”, in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
pp Chang,M.B.,Ullman,T.,Torralba,A.,andTenenbaum,J.B.(2017),“Acompo-
sitional object-based approach to learning physical dynamics”, in Proceedings
of the International Conference on Learning Representations (ICLR) Chen, B., Wu, H., Mo, W., Chattopadhyay, I., and Lipson, H (2018), “Au-
tostacker: A compositional evolutionary learning system”, The Genetic and
Evolutionary Computation Conference (GECCO) Chen, L., Lu, K., Rajeswaran, A., Lee, K et al.(2021), “Decision trans-
former: Reinforcement learning via sequence modeling”, arXiv preprint
arXiv:2106.01345

============================================================

=== CHUNK 215 ===
Palavras: 352
Caracteres: 2582
--------------------------------------------------
Chen, L., Srivastava, S., Duan, Z., and Xu, C (2017), “Deep cross-modal audio-
visual generation”, in Proceedings of the Thematic Workshops of ACM Multi-
media, pp (2021), “Evaluating large language models trained on code”,
arXiv preprint 2107.03374 Chen, N., Ferroni, F., Klushyn, A., Paraschos, A et al.(2019), “Fast approx-
imate geodesics for deep generative models”, in International Conference on
Artiﬁcial Neural Networks (ICANN) , pp Chen, S.-Y., Su, W., Gao, L., Xia, S., and Fu, H (2020), “DeepFaceDrawing:
Deepgenerationoffaceimagesfromsketches”, ACM Transactions on Graphics
(Proceedings of ACM SIGGRAPH 2020) 39(4), 72:1–72:16 Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D (2018), “Neu-
ral ordinary diﬀerential equations”, in Proceedings of Advances in Neural In-
formation Processing Systems (NeurIPS) , pp Chen, Y., Huang, A., Wang, Z., Antonoglou, I et al.(2018), “Bayesian opti-
mization in AlphaGo”, arXiv preprint arXiv:1812.06855 Chen, Z., Zhang, H., Zhang, X., and Zhao, L (2018), “Quora question pairs”,
www.kaggle.com/c/quora-question-pairs Cho, K., Van Merri¨ enboer, B., Gulcehre, C., Bahdanau, D et al.(2014), “Learn-
ing phrase representations using RNN encoder-decoder for statistical machine
translation”, in Proceedings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP) (2021), “7 revealing ways AIs fail: Neural networks can be dis-
astrously brittle, forgetful, and surprisingly bad at math”, IEEE Spectrum
58(10), 42–47 304 References
Choi, K., Wu, M., Goodman, N., and Ermon, S (2019), “Meta-amortized varia-
tional inference and learning”, in Proceedings of the International Conference
on Learning Representations (ICLR) Choi, Y., Choi, M., Kim, M., Ha, J.-W et al.(2018), “StarGAN: Uniﬁed gen-
erative adversarial networks for multi-domain image-to-image translation”, in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , pp (2015), “Keras”, github.com/fchollet/keras (1976), Worst-case analysis of a new heuristic for the travelling
salesman problem, Technical report, Carnegie-Mellon University Pittsburgh
PA Management Sciences Research Group M., Khosla, A., Pantazis, D., Torralba, A., and Oliva, A (2016), “Com-
parison of deep neural networks to spatio-temporal cortical dynamics of hu-
man visual object recognition reveals hierarchical correspondence”, Scientiﬁc
Reports 6(1), 1–13 Climate Modeling Center (2021), “Institut Pierre Simon Laplace Climate Model
5 (IPSL-CM5)”, cmc.ipsl.fr/international-projects/cmip5/ (1990), Introduction
to Algorithms , MIT Press

============================================================

=== CHUNK 216 ===
Palavras: 353
Caracteres: 2583
--------------------------------------------------
Dai, H., Khalil, E., Zhang, Y., Dilkina, B., and Song, L (2017), “Learning com-
binatorial optimization algorithms over graphs”, in Proceedings of Advances
in Neural Information Processing Systems (NeurIPS) , pp Dai, Z., Cai, B., Lin, Y., and Chen, J (2021), “Up-DETR: Unsupervised
pre-training for object detection with transformers”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp Dai, Z., Yang, Z., Yang, Y., Cohen, W et al.(2019), “Transformer-XL:
Attentive language models beyond a ﬁxed-length context” d’Apolito, S., Paudel, D P., Huang, Z., Romero, A., and Van Gool, L (2021),
“Ganmut: Learning interpretable conditional space for gamut of emotions”, in
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp d’Ascoli, S., Touvron, H., Leavitt, M., Morcos, A et al.(2021), “ConViT: Im-
proving vision transformers with soft convolutional inductive biases”, in Pro-
ceedings of the International Conference on Machine Learning (ICML) Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H (2017), “Training GANs
with optimism”, arXiv preprint arXiv:1711.00141 (1991), “Variable metric method for minimization”, SIAM Jour-
nal on Optimization 1(1), 1–17 R., Falorsi, L., De Cao, N., Kipf, T., and Tomczak, J (2018),
“Hyperspherical variational auto-encoders”, in Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence Conference (UAI) De Bortoli, V., Mathieu, E., Hutchinson, M., Thornton, J et al.(2022), “Rie-
mannian score-based generative modeling”, arXiv preprint arXiv:2202.02763 References 305
DeFauw,J.,Dieleman,S.,andSimonyan,K.(2019),“Hierarchicalautoregressive
image models with auxiliary decoders”, arXiv preprint arXiv:1903.04933 Degrave, J., Felici, F., Buchli, J., Neunert, M et al.(2022), “Magnetic
control of tokamak plasmas through deep reinforcement learning”, Nature
602(7897), 414–419 Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K (2019), “BERT: Pre-
training of deep bidirectional transformers for language understanding”, in
Proceedings of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies (NAACL) , pp (2021), “Diﬀusion models beat GANs on image syn-
thesis”, in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) (1959), “A note on two problems in connexion with graphs”,
Numerische Mathematik 1(1), 269–271 V., Langmore, I., Tran, D., Brevdo, E et al.(2017), “TensorFlow
distributions”, arXiv preprint arXiv:1711.10604 Dinh, L., Sohl-Dickstein, J., and Bengio, S

============================================================

=== CHUNK 217 ===
Palavras: 359
Caracteres: 2633
--------------------------------------------------
(2017), “Density estimation using
real NVP”, in Proceedings of the International Conference on Learning Rep-
resentations (ICLR) (2016), Diﬀerential Geometry of Curves and Surfaces , 2nd edn,
Courier Dover Publications Dokmanic, I., Parhizkar, R., Ranieri, J., and Vetterli, M (2015), “Euclidean dis-
tance matrices: Essential theory, algorithms, and applications”, IEEE Signal
Processing Magazine 32(6), 12–30 (2005), “Automatically constructing a corpus of
sentential paraphrases”, in International Workshop on Paraphrasing Donahue, C., McAuley, J., and Puckette, M (2018), “Adversarial audio synthe-
sis”, inInternational Conference on Learning Representations (ICLR) Dong, H.-W., Hsiao, W.-Y., Yang, L.-C., and Yang, Y.-H (2018), “MuseGAN:
Multi-track sequential generative adversarial networks for symbolic music gen-
eration and accompaniment”, in Proceedings of Thirty-Second AAAI Confer-
ence on Artiﬁcial Intelligence Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D et al.(2020), “An
image is worth 16x16 words: Transformers for image recognition at scale”,
inProceedings of the International Conference on Learning Representations
(ICLR) (2016), “Incorporating Nesterov momentum into Adam”, in Proceed-
ings of the International Conference on Learning Representations (ICLR) Drori, I., Cohen-Or, D., and Yeshurun, H (2003 a), “Example-based style syn-
thesis”, in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , Vol Drori, I., Cohen-Or, D., and Yeshurun, H (2003 b), “Fragment-based image com-
pletion”, in ACM Transactions on Graphics (TOG) , Vol 306 References
Drori, I., Dwivedi, I., Shrestha, P., Wan, J et al.(2018), “High quality prediction
of protein q8 secondary structure by diverse neural network architectures”,
NeurIPS Workshop on Machine Learning for Molecules and Materials Drori, I., Fishbach, A., and Yeshurun, Y (2004), “Spectral sound gap ﬁlling”, in
Proceedings of the International Conference on Pattern Recognition (ICPR) Drori, I., Krishnamurthy, Y., Rampin, R., Lourenco, R et al.(2018), “Al-
phaD3M: Machine learning pipeline synthesis”, in ICML International Work-
shop on Automated Machine Learning Drori, I., Krishnamurthy, Y., Rampin, R., Lourenco, R et al.(2019), “Auto-
matic machine learning by pipeline synthesis using model-based reinforcement
learning and a grammar”, in ICML International Workshop on Automated
Machine Learning Drori, I., Zhang, S., Shuttleworth, R., Tang, L et al.(n.d.), “A neural network
solves, explains, and generates university math problems by program synthesis
and few-shot learning at human level”, Submitted

============================================================

=== CHUNK 218 ===
Palavras: 351
Caracteres: 2599
--------------------------------------------------
Duchi, J., Hazan, E., and Singer, Y (2011), “Adaptive subgradient methods
for online learning and stochastic optimization”, Journal of Machine Learning
Research 12, 2121–2159 Duraisamy, K., Iaccarino, G., and Xiao, H (2019), “Turbulence modeling in the
age of data”, Annual Review of Fluid Mechanics 51, 357–377 K., Maclaurin, D., Iparraguirre, J., Bombarell, R et al.(2015),
“Convolutional networks on graphs for learning molecular ﬁngerprints”, in
Proceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp D’Alelio, D., Rampone, S., Cusano, L et al.(2020), “Machine
learning identiﬁes a strong association between warming and reduced primary
productivity in an oligotrophic ocean gyre”, Scientiﬁc Reports 10(1), 1–12 Ecoﬀet, A., Huizinga, J., Lehman, J., Stanley, K (2019),
“Go-Explore: A new approach for hard-exploration problems”, arXiv preprint
arXiv:1901.10995 Ecoﬀet, A., Huizinga, J., Lehman, J., Stanley, K (2021), “First
return, then explore”, Nature 590(7847), 580–586 (2016), Computer Age Statistical Inference , Cambridge
University Press Engel, J., Agrawal, K K., Chen, S., Gulrajani, I et al.(2019), “GANSynth:
Adversarial neural audio synthesis”, in Proceedings of the International Con-
ference on Learning Representations (ICLR) (2011), On the evolution of random graphs, inThe
structure and dynamics of networks, Princeton University Press, pp Ernst, D., Geurts, P., and Wehenkel, L (2005), “Tree-based batch mode rein-
forcement learning”, Journal of Machine Learning Research 6, 503–556 (2022), “Maximum entropy RL (provably) solves
some robust RL problems”, in Proceedings of the International Conference on
Learning Representations (ICLR) References 307
Falorsi,L.,deHaan,P.,Davidson,T.R.,andForr´ e,P.(2019),“Reparameterizing
distributions on Lie groups”, in Proceedings of Machine Learning Research
(PMLR), pp Fang, F., Yamagishi, J., Echizen, I., and Lorenzo-Trueba, J (2018), “High-
quality nonparallel voice conversion based on cycle-consistent adversarial net-
work”, in Proceedings of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pp B., Gomes, J., and Pande, V (2017), “Deep learning the physics
of transport phenomena”, arXiv preprint arXiv:1709.02432 Fedus, W., Goodfellow, I., and Dai, A (2018), “MaskGAN: Better text gen-
eration via ﬁlling in the ”, inProceedings of the International Conference on
Learning Representation (ICLR) Feinberg, V., Wan, A., Stoica, I., Jordan, M et al.(2018), “Model-based value
estimation for eﬃcient model-free reinforcement learning”, arXiv preprint
arXiv:1803.00101

============================================================

=== CHUNK 219 ===
Palavras: 362
Caracteres: 2759
--------------------------------------------------
Felleman,D.J.andVanEssen,D.C.(1991),“Distributedhierarchicalprocessing
in the primate cerebral cortex”, Cerebral Cortex 1(1), 1–47 Fernando, T., Denman, S., Sridharan, S., and Fookes, C (2017), “Going
deeper: Autonomous steering with neural memory networks”, in Proceedings
of the IEEE/CVF International Conference on Computer Vision Workshops ,
pp Feurer, M., Klein, A., Eggensperger, K., Springenberg, J et al.(2015), “Eﬃcient
androbustautomatedmachinelearning”,in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) , pp (2019), “Fast graph representation learning with Py-
Torch Geometric”, in Proceedings of the International Conference on Learning
Representations (ICLR) Figurnov, M., Mohamed, S., and Mnih, A (2018), “Implicit reparameterization
gradients”, in Proceedings of Advances in Neural Information Processing Sys-
tems (NeurIPS) , pp (1998), FARSITE: Fire Area Simulator–model development and
evaluation , number 4, US Department of Agriculture, Forest Service, Rocky
Mountain Research Station (1963), “A rapidly convergent descent method for
minimization”, The Computer Journal 6(2), 163–168 (2002), “Example-based super-
resolution”, IEEE Computer Graphics and Applications pp Fr¨ uhst¨ uck,A.,Alhashim,I.,andWonka,P.(2019),“TileGAN:Synthesisoflarge-
scale non-homogeneous textures”, ACM Transactions on Graphics (TOG)
38(4), 1–11 Fu, K., Peng, J., He, Q., and Zhang, H (2021), “Single image 3D object recon-
struction based on deep learning: A review”, Multimedia Tools and Applica-
tions 80, 463–498 308 References
Fukushima, K (1988), “Neocognitron: A hierarchical neural network capable of
visual pattern recognition”, Neural Networks 1(2), 119–130 Fusi, N., Sheth, R., and Elibol, M (2018), “Probabilistic matrix factorization
for automated machine learning”, in Proceedings of Advances in Neural Infor-
mation Processing Systems (NeurIPS) , pp Ganapathi Subramanian, S (2018), “Using spatial reinforce-
ment learning to build forest wildﬁre dynamics models from satellite images”,
Frontiers in ICT 5,6 Gao, M., Yang, J., Gong, D., Shi, P et al.(2019), “Footprints of Atlantic multi-
decadal oscillation in the low-frequency variation of extreme high temperature
in the northern hemisphere”, Journal of Climate 32(3), 791–802 (2019), “2.5D visual sound”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp E., Kube, R., Theodorsen, A., and P´ ecseli, H (2016), “Stochastic
modelling of intermittent ﬂuctuations in the scrape-oﬀ layer: Correlations,
distributions, level crossings, and moment estimation”, Physics of Plasmas
23(5), 052308 (2017), “Power law spectra and intermit-
tent ﬂuctuations due to uncorrelated Lorentzian pulses”, Physics of Plasmas
24(2), 020704

============================================================

=== CHUNK 220 ===
Palavras: 351
Caracteres: 2624
--------------------------------------------------
(2018), “Few-shot learning with graph neural net-
works”, in Proceedings of the International Conference on Learning Represen-
tations (ICLR) Garcia, V., Hoogeboom, E., Fuchs, F., Posner, I., and Welling, M (n.d.), “E(n)
equivariant normalizing ﬂows”, in Proceedings of Advances in Neural Informa-
tion Processing Systems (NeurIPS) Gemici,M.C.,Rezende,D.,andMohamed,S.(2016),“NormalizingﬂowsonRie-
mannian manifolds”, in Proceedings of the NeurIPS Bayesian Deep Learning
Workshop Gilmer, J., Schoenholz, S F., Vinyals, O., and Dahl, G (2017),
“Neural message passing for quantum chemistry”, in Proceedings of the Inter-
national Conference on Machine Learning (ICML) , pp Giordano, R., Broderick, T., and Jordan, M (2018), “Covariances, robustness,
and variational Bayes”, Journal of Machine Learning Research 19(1), 1981–
2029 Girdhar, R., Fouhey, D F., Rodriguez, M., and Gupta, A (2016), “Learning
a predictable and generative vector representation for objects”, in European
Conference on Computer Vision , pp (2010), “Understanding the diﬃculty of training deep
feedforward neural networks”, in Proceedings of the Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS) , pp References 309
Godard, C., Mac Aodha, O., Firman, M., and Brostow, G (2019), “Dig-
ging into self-supervised monocular depth prediction”, in Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV) Golden, B., Bodin, L., Doyle, T., and W, S (1980), “Approximate traveling
salesman algorithms”, Operations Research 28(3,part-II), 694–711 N., Huang, S., Zhang, I., Li, B et al.(2018), “Unsupervised cipher
cracking using discrete GANs”, in Proceedings of the International Conference
on Learning Representations (ICLR) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B et al.(2014), “Generative
adversarialnets”,in Proceedings of Advances in Neural Information Processing
Systems (NeurIPS) , pp Google (2020), “Compare GAN library”, github.com/google/compare gan Grathwohl, W., Chen, R T., Bettencourt, J., Sutskever, I., and Duvenaud, D (2019), “FFJORD: Free-form continuous dynamics for scalable reversible gen-
erative models”, in Proceedings of the International Conference on Learning
Representations (ICLR) Grover, A.,Chute, C.,Shu, R.,Cao, Z.,and Ermon,S.(2020), “AlignFlow:Cycle
consistent learning from multiple domains via normalizing ﬂows”, in Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence , Vol (2016), “Node2Vec: Scalable feature learning for net-
works”, in Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining , pp

============================================================

=== CHUNK 221 ===
Palavras: 356
Caracteres: 2538
--------------------------------------------------
Gruber,N.,Landsch¨ utzer,P.,andLovenduski,N.S.(2019),“ThevariableSouth-
ern Ocean carbon sink”, Annual Review of Marine Science 11, 159–186 Guo, J., Lu, S., Cai, H., Zhang, W et al.(2018), “Long text generation via
adversarial training with leaked information”, in Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence Guo, X., Li, W., and Iorio, F (2016), “Convolutional neural networks for steady
ﬂow approximation”, in Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining , pp Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., and Alahi, A (2018), “Social-
GAN: Socially acceptable trajectories with generative adversarial networks”,
inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp Guu, K., Hashimoto, T B., Oren, Y., and Liang, P (2018), “Generating sen-
tences by editing prototypes”, Transactions of the Association for Computa-
tional Linguistics 6, 437–450 (2018), “World models”, in Proceedings of Advances
in Neural Information Processing Systems (NeurIPS) Hadjeres, G., Pachet, F., and Nielsen, F (2017), “DeepBach: A steerable model
for Bach chorales generation”, in Proceedings of the International Conference
on Machine Learning (ICML) Halofsky, J., Peterson, D., and Harvey, B (2020), “Changing wildﬁre, changing
310 References
forests: The eﬀects of climate change on ﬁre regimes and vegetation in the
Paciﬁc Northwest, USA”, Fire Ecology 16(4) Ham, Y.-G., Kim, J.-H., and Luo, J.-J (2019), “Deep learning for multi-year
ENSO forecasts”, Nature 573(7775), 568–572 Hamilton,W.,Ying,Z.,andLeskovec,J.(2017),“Inductiverepresentationlearn-
ing on large graphs”, in Proceedings of Advances in Neural Information Pro-
cessing Systems (NeurIPS) , pp (2021), “A max–min entropy framework for reinforcement
learning”, in Proceedings of Advances in Neural Information Processing Sys-
tems (NeurIPS) Han, X., Wu, Z., Huang, W., Scott, M (2019), “FiNet: Com-
patibleanddiversefashionimageinpainting”,in Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pp (2006), The CMA evolution strategy: A comparing review , Springer,
pp He, K., Zhang, X., Ren, S., and Sun, J (2016 a), “Deep residual learning for
imagerecognition”,in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp He, K., Zhang, X., Ren, S., and Sun, J (2016 b), “Identity mappings in deep
residual networks”, in Proceedings of the European Conference on Computer
Vision (ECCV) , pp

============================================================

=== CHUNK 222 ===
Palavras: 352
Caracteres: 2642
--------------------------------------------------
He, S., Hollenbeck, B., and Proserpio, D (2020), “The market for fake reviews”,
Marketing Science He, X., Zhao, K., and Chu, X (2021), “AutoML: A survey of the state-of-the-
art”,Knowledge-Based Systems 212 Hecker, S., Dai, D., Liniger, A., Hahner, M., and Van Gool, L (2020), “Learn-
ing accurate and human-like driving using semantic maps and attention”,
inIEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp Hecker, S., Dai, D., and Van Gool, L (2018), “End-to-end learning of driving
models with surround-view cameras and route planners”, in Proceedings of the
European Conference on Computer Vision (ECCV) , pp Hertzmann, A., Jacobs, C E., Oliver, N., Curless, B., and Salesin, D (2001),
“Image analogies”, in Proceedings of ACM SIGGRAPH Conference on Com-
puter Graphics and Interactive Techniques , pp Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,andHochreiter,S.(2017),
“GANs trained by a two time-scale update rule converge to a local Nash
equilibrium”, in Proceedings of Advances in Neural Information Processing
Systems (NeurIPS) , pp (2006), “Reducing the dimensionality of
data with neural networks”, Science 313(5786), 504–507 (2016), “Generative adversarial imitation learning”, in
Proceedings of Advances in Neural Information Processing Systems (NeurIPS) References 311
Ho, J., Jain, A., and Abbeel, P (2020), “Denoising diﬀusion probabilistic mod-
els”, inProceedings of Advances in Neural Information Processing Systems
(NeurIPS) , pp (1997), “Long short-term memory”, Neural
Computation 9(8), 1735–1780 (2019), “Wildland ﬁre spread modeling using con-
volutional neural networks”, Springer Fire Technology 55, 2115–2142 M., Wang, C., and Paisley, J (2013), “Stochastic vari-
ational inference”, Journal of Machine Learning Research 14(1), 1303–1347 (2018), Geometric Bayes, PhD thesis, UC Irvine Holland,P.W.,Laskey,K.B.,andLeinhardt,S.(1983),“Stochasticblockmodels:
First steps”, Social Networks 5(2), 109–137 Horisaki,R.,Takagi,R.,andTanida,J.(2016),“Learning-basedimagingthrough
scattering media”, Optics Express 24(13), 13738–13743 Howard, A., Zhu, M., Chen, B., Kalenichenko, D et al.(2017), “MobileNets:
Eﬃcient convolutional neural networks for mobile vision applications”, arXiv
preprint arXiv:1704.04861 Hu, J., Shen, L., and Sun, G (2018), “Squeeze-and-excitation networks”, in Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp Hu, T., Wang, L., Xu, X., Liu, S., and Jia, J (2021), “Self-supervised 3D mesh
reconstruction from single images”, in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) , pp

============================================================

=== CHUNK 223 ===
Palavras: 366
Caracteres: 2573
--------------------------------------------------
A., Cooijmans, T., Roberts, A., Courville, A., and Eck, D (2017),
“Counterpoint by convolution”, in Proceedings of the 18th International Soci-
ety for Music Information Retrieval Conference (ISMIR) Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K (2017), “Densely
connected convolutional networks”, in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) , pp Huang, P.-Y., Patrick, M., Hu, J., Neubig, G et al.(2021), “Multilingual multi-
modal pre-training for zero-shot cross-lingual transfer of vision-language mod-
els”, inProceedings of the Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL) , pp (2017), “Arbitrary style transfer in real-time with
adaptive instance normalization”, in Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pp Huang, X., Liu, M.-Y., Belongie, S., and Kautz, J (2018), “Multimodal unsuper-
vised image-to-image translation”, in Proceedings of the European Conference
on Computer Vision (ECCV) , pp (1968), “Receptive ﬁelds and functional archi-
tecture of monkey striate cortex”, Journal of Physiology 195(1), 215–243 Hyun, S., Kim, J., and Heo, J.-P (2021), “Self-supervised video GANs: Learn-
ing for appearance consistency and motion coherency”, in Proceedings of
312 References
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp Iizuka, S., Simo-Serra, E., and Ishikawa, H (2017), “Globally and locally consis-
tent image completion”, ACM Transactions on Graphics (ToG) 36(4), 107 (2015), “Batch normalization: Accelerating deep net-
work training by reducing internal covariate shift”, in Proceedings of the In-
ternational Conference on Machine Learning (ICML) Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A (2017), “Image-to-image transla-
tion with conditional adversarial networks”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A (2018),
“Averaging weights leads to wider optima and better generalization”, in Pro-
ceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI) S., Bulat, A., Argyriou, V., and Tzimiropoulos, G (2017), “Large
pose 3D face reconstruction from a single image via direct volumetric CNN re-
gression”, in Proceedings of the IEEE/CVF International Conference on Com-
puter Vision et al.(2020), “A
review of machine learning applications in wildﬁre science and management”,
Environmental Reviews 28(4), 478–505

============================================================

=== CHUNK 224 ===
Palavras: 353
Caracteres: 2587
--------------------------------------------------
Jerfel, G., Wang, S., Wong-Fannjiang, C., Heller, K et al.(2021), “Varia-
tional reﬁnement for importance sampling using the forward Kullback–Leibler
divergence”, in Proceedings of the Conference on Uncertainty in Artiﬁcial In-
telligence (UAI) , pp Jin, W., Barzilay, R., and Jaakkola, T (2018), “Junction tree variational au-
toencoder for molecular graph generation”, arXiv preprint arXiv:1802.04364 I., Ghahramani, Z., Jaakkola, T (1999), “An
introduction to variational methods for graphical models”, Machine Learning
37(2), 183–233 Jozefowicz, R., Zaremba, W., and Sutskever, I (2015), “An empirical exploration
ofrecurrentnetworkarchitectures”,in Proceedings of International Conference
on Machine Learning , pp Jumper, J., Evans, R., Pritzel, A., Green, T et al.(2021), “Highly accurate
protein structure prediction with AlphaFold”, Nature 596(7873), 583–589 (1983), “Dictionary of protein secondary structure:
Pattern recognition of hydrogen-bonded and geometrical features”, Biopoly-
mers: Original Research on Biomolecules 22(12), 2577–2637 Kahng, M., Thorat, N., Chau, D B., and Wattenberg, M (2018), “GAN Lab: Understanding complex deep generative models using in-
teractive visual experimentation”, IEEE Transactions on Visualization and
Computer Graphics 25(1), 1–11 (2001), “A natural policy gradient”, in Proceedings of Advances
in Neural Information Processing Systems (NeurIPS) References 313
Kaneko, T., Kameoka, H., Tanaka, K., and Hojo, N (2019), “CycleGAN-VC2:
Improved CycleGAN-based non-parallel voice conversion”, in Proceedings of
the IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp Kant, Y., Batra, D., Anderson, P., Schwing, A et al.(2020), “Spatially aware
multimodal transformers for textVQA”, in Proceedings of the European Con-
ference on Computer Vision (ECCV) , pp Karras, T., Aila, T., Laine, S., and Lehtinen, J (2018), “Progressive growing
of GANs for improved quality, stability, and variation”, in Proceedings of the
International Conference on Learning Representations (ICLR) Karras, T., Laine, S., and Aila, T (2019), “A style-based generator architecture
for generative adversarial networks”, in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR) , pp Kazeminia, S., Baur, C., Kuijper, A., van Ginneken, B et al.(2020), “GANs for
medical image analysis”, Artiﬁcial Intelligence in Medicine 109 (2017), “What uncertainties do we need in Bayesian
deep learning for computer vision?”, in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) , Vol

============================================================

=== CHUNK 225 ===
Palavras: 355
Caracteres: 2592
--------------------------------------------------
Kennedy, S., Walsh, N., Sloka, K., Foster, J., and McCarren, A (2019), “Fact or
factitious Contextualized opinion spam detection”, Proceedings of the Annual
Meeting of the Association for Computational Linguistics: Student Research
Workshop pp S., McCann, B., Varshney, L R., Xiong, C., and Socher, R (2019),
“CTRL: A conditional transformer language model for controllable genera-
tion”,arXiv preprint arXiv:1909.05858 (2018), “Geometry score: A method for com-
paring generative adversarial networks”, in Proceedings of the International
Conference on Machine Learning (ICML) Kim, H., Carrido, P., Tewari, A., Xu, W et al.(2018), “Deep video portraits”,
ACM Transactions on Graphics (TOG) 37(4), 163 Kim, H., Kim, D., Kim, G., Cho, J., and Huh, K (2020), “Multi-head attention
based probabilistic vehicle trajectory prediction”, in IEEE Intelligent Vehicles
Symposium (IV) , pp Kim, H., Remaggi, L., Jackson, P (2019), “Immersive
spatial audio reproduction for VR/AR using room acoustic modelling from
360 °images”, in Proceedings of the IEEE Conference on Virtual Reality and
3D User Interfaces (VR) , pp (2003), “Generating random regular graphs”, in Pro-
ceedings of the Annual ACM Symposium on Theory of Computing (STOC) ,
pp W., Philion, J., Torralba, A., and Fidler, S (2021), “DriveGAN:
Towards a controllable high-quality neural simulation”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp 314 References
Kim, Y., Wiseman, S., and Rush, A (2018), “A tutorial on deep latent
variable models of natural language”, arXiv preprint arXiv:1812.06834 Kingma,D.P.andBa,J.(2014),“Adam:Amethodforstochasticoptimization”,
inProceedings of the International Conference on Learning Representations
(ICLR) (2018), “Glow: Generative ﬂow with invertible
1x1 convolutions”, in Proceedings of Advances in Neural Information Process-
ing Systems (NeurIPS) , pp (2014), “Auto-encoding variational Bayes”,
inProceedings of the International Conference on Learning Representations
(ICLR) (2017), “Semi-supervised classiﬁcation with graph
convolutional networks”, in Proceedings of the International Conference on
Learning Representations (ICLR) Kiros, R., Zhu, Y., Salakhutdinov, R et al.(2015), “Skip-thought
vectors”,in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) , pp A., Remondino, F., Bordodymov, A., and Moshkant-
sev, P (2020), “Image-to-voxel model translation for 3D scene reconstruction
and segmentation”, in Proceedings of the European Conference on Computer
Vision (ECCV) , pp W., Sagawa, S., Marklund, H., Xie, S

============================================================

=== CHUNK 226 ===
Palavras: 362
Caracteres: 2680
--------------------------------------------------
et al.(2021), “Wilds: A bench-
mark of in-the-wild distribution shifts”, in Proceedings of the International
Conference on Machine Learning (ICML) , pp K¨ ohler, J., Kr¨ amer, A., and No´ e, F (n.d.), “Smooth normalizing ﬂows”, in Pro-
ceedings of Advances in Neural Information Processing Systems (NeurIPS) Kokiopoulou, E., Hauth, A., Sbaiz, L., Gesmundo, A et al.(2019), “Fast task-
aware architecture inference”, arXiv preprint arXiv:1902.05781 (2018), “Deep neural networks for cross-modal estima-
tions of acoustic reverberation characteristics from two-dimensional images”,
inAudio Engineering Society , number 144 (2019), “Estimation of late reverberation characteristics
from a single two-dimensional environmental image using convolutional neural
networks”, Journal of the Audio Engineering Society 67, 540–548 (2020), “An auditory scaling method for reverb synthe-
sis from a single two-dimensional image”, Acoustical Science and Technology
41(4), 675–685 Kong, L., Lian, C., Huang, D., Li, Z et al.(n.d.), “Breaking the dilemma of
medical image-to-image translation”, in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) Kool, W., van Hoof, H., and Welling, M (2019), “Attention, learn to solve rout-
ing problems!”, in Proceedings of the International Conference on Learning
Representations (ICLR) References 315
Koutsias, N., Xanthopoulos, G., Founda, D., Xystrakis, F et al.(2013), “On the
relationships between forest ﬁres and weather conditions in Greece from long-
term national observations (1894–2010)”, International Journal of Wildland
Fire22, 493–507 Krizhevsky, A., Sutskever, I., and Hinton, G (2012), “ImageNet classiﬁca-
tion with deep convolutional neural networks”, in Proceedings of Advances in
Neural Information Processing Systems (NeurIPS) , pp (1956), “On the shortest spanning subtree of a graph and the trav-
eling salesman problem”, Proceedings of the American Mathematical Society
7(1), 48–50 Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D (2017),
“Automatic diﬀerentiation variational inference”, Journal of Machine Learn-
ing Research 18(1), 430–474 Kumar, A., Sattigeri, P., and Fletcher, T (2017), “Semi-supervised learning
with GANs: Manifold invariance with improved inference”, in Proceedings of
Advances in Neural Information Processing Systems (NeurIPS) , pp (2006), “Voxelization of boundary representations using oriented
LEGO ®plates”, code.google.com/archive/p/lsculpt/ Lan, Z., Chen, M., Goodman, S., Gimpel, K et al.(2020), “Albert: A lite BERT
for self-supervised learning of language representations”, in Proceedings of the
International Conference on Learning Representations (ICLR)

============================================================

=== CHUNK 227 ===
Palavras: 361
Caracteres: 2797
--------------------------------------------------
Le,Q.andMikolov,T.(2014),“Distributedrepresentationsofsentencesanddoc-
uments”, in Proceedings of the International Conference on Machine Learning
(ICML), pp LeCun, Y., Kavukcuoglu, K., and Farabet, C (2010), “Convolutional networks
and applications in vision”, in Proceedings of the IEEE International Sympo-
sium on Circuits and Systems , pp Ledig, C., Theis, L., Husz´ ar, F., Caballero, J et al.(2017), “Photo-realistic
single image super-resolution using a generative adversarial network”, in Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., and Hutter, M (2020), “Learn-
ing quadrupedal locomotion over challenging terrain”, Science Robotics 5(47) (2010), “Approximating measured rever-
beration using a hybrid ﬁxed/switched convolution structure”, in Proceedings
of the 13th International Conference on Digital Audio Eﬀects (DAFX) Lee, S., Yu, Y., Kim, G., Breuel, T et al.(2021), “Parameter eﬃcient multi-
modal transformers for video representation learning”, in Proceedings of the
International Conference on Learning Representations (ICLR) Lego (2020), “Lego Mosaic Maker”, www.lego.com/en-us/product/mosaic-
maker-40179 (1969), “How to fold graciously”, in Proceedings of a meeting held
at Allerton House , pp 316 References
Lewis,M.,Liu,Y.,Goyal,N.,Ghazvininejad,M et al.(2020),“BART:Denoising
sequence-to-sequencepre-trainingfornaturallanguagegeneration,translation,
and comprehension”, Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL) pp Li,D.,Langlois,T.R.,andZheng,C.(2018),“Scene-awareaudiofor360videos”,
ACM Transactions on Graphics (TOG) 37(4), 1–12 Li, L., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang, K.-W (2020), “Visual-
BERT: A simple and performant baseline for vision and language”, Proceed-
ings of the Annual Meeting of the Association for Computational Linguistics
(ACL) Li, S., Deng, M., Lee, J., Sinha, A., and Barbastathis, G (2018), “Imaging
through glass diﬀusers using densely connected convolutional networks”, Op-
tica5(7), 803–813 Li, Y., Choi, D., Chung, J., Kushman, N et al.(2022), “Competition-level code
generation with AlphaCode”, arXiv preprint arXiv:2203.07814 Li, Y., Liu, S., Yang, J., and Yang, M.-H (2017), “Generative face completion”,
inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp Li, Y., Song, J., and Ermon, S (2017), “Infogail: Interpretable imitation learning
from visual demonstrations”, in Proceedings of Advances in Neural Informa-
tion Processing Systems (NeurIPS) , pp Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R (2016), “Gated graph se-
quence neural networks”, in Proceedings of the International Conference on
Learning Representations (ICLR)

============================================================

=== CHUNK 228 ===
Palavras: 361
Caracteres: 2556
--------------------------------------------------
(2016), “R´ enyi divergence variational inference”, in
Proceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp Li, Y., Xue, Y., and Tian, L (2018), “Deep speckle correlation: A deep learn-
ing approach toward scalable imaging through scattering media”, Optica
5(10), 1181–1190 J., Pritzel, A., Heess, N et al.(2016), “Continuous
control with deep reinforcement learning”, in Proceedings of the International
Conference on Learning Representations (ICLR) J., Pirsiavash, H., and Torralba, A (2013), “Parsing IKEA objects: Fine
pose estimation”, in Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pp Lin, J., Zhang, R., Ganz, F., Han, S., and Zhu, J.-Y (2021), “Anycost GANs
for interactive image synthesis and editing”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Lin, K., Li, D., He, X., Zhang, Z., and Sun, M.-T (2017), “Adversarial ranking
for language generation”, in Proceedings of Advances in Neural Information
Processing Systems (NeurIPS) , pp References 317
Lin, S (1965), “Computer solutions of the traveling salesman problem”, Bell
System Technical Journal 44(10), 2245–2269 (1973), “An eﬀective heuristic algorithm for the
traveling-salesman problem”, Operations Research 21(2), 498–516 Lin, T., Jin, C., and Jordan, M (2020), “On gradient descent ascent for
nonconvex-concave minimax problems”, in Proceeding of the International
Conference on Machine Learning (ICML) , pp Lin, Z., Feng, M., Santos, C et al.(2017), “A structured self-
attentive sentence embedding”, in Proceedings of the International Conference
on Learning Representations (ICLR) Liu, B., Hu, W., Zitnik, M., and Leskovec, J (2020), “Open Graph Benchmark”,
ogb.stanford.edu (1989), “On the limited memory BFGS method for
large scale optimization”, Mathematical Programming 45(1-3), 503–528 Liu, H., Wan, Z., Huang, W., Song, Y et al.(2021), “PD-GAN: Probabilistic di-
verseGANforimageinpainting”,in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pp Liu, M.-Y., Huang, X., Mallya, A., Karras, T et al.(2019), “Few-shot unsuper-
vised image-to-image translation”, in Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) (2016), “Coupled generative adversarial networks”, in
Proceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp Liu, Y., Ott, M., Goyal, N., Du, J et al.(2019), “RoBERTa: A robustly opti-
mized BERT pretraining approach”, arXiv preprint arXiv:1907.11692

============================================================

=== CHUNK 229 ===
Palavras: 367
Caracteres: 2695
--------------------------------------------------
Liu, Z., Lin, Y., Cao, Y., Hu, H et al.(2021), “Swin Transformer: Hierarchi-
cal vision transformer using shifted windows”, Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) Lu, J., Batra, D., Parikh, D., and Lee, S (2019), “VilBERT: Pretraining task-
agnostic visiolinguistic representations for vision-and-language tasks”, in Pro-
ceedings of Advances in Neural Information Processing Systems (NeurIPS) Luo, H., Nagano, K., Kung, H.-W., Xu, Q et al.(2021), “Normalized avatar
synthesis using StyleGAN and perceptual reﬁnement”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp Luo, W., Yang, B., and Urtasun, R (2018), “Fast and furious: Real time end-to-
end 3d detection, tracking and motion forecasting with a single convolutional
net”, in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pp Luong, M.-T., Pham, H., and Manning, C (2015), “Eﬀective approaches to
attention-based neural machine translation”, in Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pp 318 References
Ma, L., Jia, X., Sun, Q., Schiele, B et al.(2017), “Pose guided person image
generation”, in Proceedings of Advances in Neural Information Processing Sys-
tems (NeurIPS) , pp Ma, N., Zhang, X., Zheng, H.-T., and Sun, J (2018), “Shuﬄenet v2: Practical
guidelines for eﬃcient CNN architecture design”, in Proceedings of the Euro-
pean Conference on Computer Vision (ECCV) , pp Ma, Q., Ge, S., He, D., Thaker, D., and Drori, I (2020), “Combinatorial opti-
mization by graph pointer networks and hierarchical reinforcement learning”,
AAAI Workshop on Deep Learning on Graphs: Methodologies and Applica-
tions Mallasto, A., Hauberg, S., and Feragen, A (2019), “Probabilistic Riemannian
submanifold learning with wrapped Gaussian process latent variable models”,
inProceedings of the International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS) Mao,X.,Li,Q.,Xie,H.,Lau,R.Y et al.(2017),“Leastsquaresgenerativeadver-
sarial networks”, in Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pp (2009), “Data driven approach to estimating ﬁre
danger from satellite images and weather information”, IEEE Applied Imagery
Pattern Recognition Workshop pp Mazyavkina,N.,Sviridov,S.,Ivanov,S.,andBurnaev,E.(2021),“Reinforcement
learning for combinatorial optimization: A survey”, Computers & Operations
Research 134 McAleer, S., Agostinelli, F., Shmakov, A., and Baldi, P (2018), “Solving the
Rubik’s Cube with approximate policy iteration”, Proceedings of the Interna-
tional Conference on Learning Representations (ICLR)

============================================================

=== CHUNK 230 ===
Palavras: 354
Caracteres: 2724
--------------------------------------------------
Mentzer, F., Toderici, G D., Tschannen, M., and Agustsson, E (n.d.), “High-
ﬁdelity generative image compression”, in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) Messaoud, K., Deo, N., Trivedi, M M., and Nashashibi, F (2021), “Trajectory
prediction for autonomous driving based on multi-head attention with joint
agent-map representation”, in IEEE Intelligent Vehicles Symposium (IV) ,
pp Metz, L., Poole, B., Pfau, D., and Sohl-Dickstein, J (2017), “Unrolled generative
adversarial networks (2016)”, in Proceedings of the International Conference
on Learning Representations (ICLR) Mikolov, T., Chen, K., Corrado, G., and Dean, J (2013), “Eﬃcient estimation
of word representations in vector space”, in Proceedings of the International
Conference on Learning Representations Workshop Mikolov, T., Sutskever, I., Chen, K., Corrado, G (2013), “Dis-
tributed representations of words and phrases and their compositionality”, in
Proceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp References 319
Milutinovic, M., Baydin, A G., Zinkov, R., Harvey, W et al.(2017), “End-to-
end training of diﬀerentiable pipelines across machine learning frameworks”,
inNIPS Workshop on Autodiﬀ (2014), “Conditional generative adversarial nets”,
NIPS Deep Learning and Representation Learning Workshop P., Mirza, M., Graves, A et al.(2016), “Asynchronous
methods for deep reinforcement learning”, in Proceedings of the International
Conference on Machine Learning (ICML) , pp Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A et al.(2015), “Human-level
control through deep reinforcement learning”, Nature 518(7540), 529–533 Moerland,T.M.,Broekens,J.,Plaat,A.,andJonker,C.M.(2018),“A0C:Alpha
Zero in continuous action space”, arXiv preprint arXiv:1805.09613 Monti, F., Boscaini, D., Masci, J., Rodola, E et al.(2017), “Geometric deep
learning on graphs and manifolds using mixture model CNNs”, in Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp Morgenstern,L.andOrtiz,C.(2015),“TheWinogradschemachallenge:Evaluat-
ing progress in commonsense reasoning”, in Proceedings of the Twenty-Seventh
IAAI Conference Moult, J., Fidelis, K., Kryshtafovych, A., Schwede, T., and Birkbeck,
M (2018), “13th Community Wide Experiment on the Critical As-
sessment of Techniques for Protein Structure Prediction”, predictioncen-
ter.org/casp13/index.cgi (2019), “Announcing automated ML capability in Azure Machine
Learning”, azure.microsoft.com et al.(2006), “Blob
birth and transport in the tokamak edge plasma: Analysis of imaging data”,
Physics of Plasmas 13(9), 092509 (1964), “On estimating regression”, Theory of Probability & Its
Applications 9(1), 141–142

============================================================

=== CHUNK 231 ===
Palavras: 353
Caracteres: 2458
--------------------------------------------------
(2017), “Deep learning in ﬂuid dynamics”, Fluid Mechanics
814, 1–4 Neˇ setˇ ril, J., Milkov´ a, E., and Neˇ setˇ rilov´ a, H (2001), “Otakar borvka on mini-
mum spanning tree problem translation of both the 1926 papers, comments,
history”, Discrete Mathematics 233(1-3), 3–36 (2021), “Improved denoising diﬀusion probabilis-
tic models”, in Proceedings of International Conference on Machine Learning
(ICML), pp (2021), “Giraﬀe: Representing scenes as compo-
sitional generative neural feature ﬁelds”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Nowozin, S., Cseke, B., and Tomioka, R (2016), “f-GAN: Training generative
320 References
neural samplers using variational divergence minimization”, in Proceedings of
Advances in Neural Information Processing Systems (NeurIPS) , pp (2016), “Semi-supervised learning with generative adversarial net-
works”,arXiv preprint arXiv:1606.01583 Odena, A., Olah, C., and Shlens, J (2017), “Conditional image synthesis with
auxiliary classiﬁer GANs”, in Proceedings of the International Conference on
Machine Learning (ICML) , Vol Oliver, A., Odena, A., Raﬀel, C D., and Goodfellow, I (2018),
“Realistic evaluation of deep semi-supervised learning algorithms”, in Pro-
ceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp (2016), “TPOT: A tree-based pipeline optimiza-
tion tool for automating machine learning”, in Proceeding of the Workshop on
Automatic Machine Learning , pp (2006), Elementary Diﬀerential Geometry , Elsevier d., Dieleman, S., Zen, H., Simonyan, K et al.(2016), “WaveNet:
A generative model for raw audio”, in Proceedings of the ISCA Workshop on
Speech Synthesis , p OpenAI (2020), “Spinning Up in Deep RL”, spinningup.openai.com OpenAI (2021), “GitHub Co-Pilot”, copilot.github.com (2001), Advanced Mean Field Methods: Theory and
Practice, MIT Press D., Jones, S., Jones, D et al.(1997), “CATH: A
hierarchic classiﬁcation of protein domain structures”, Structure 5(8), 1093–
1109 Ott, M., Cardie, C., and Hancock, J (2013), “Negative deceptive opinion
spam”, in Proceedings of the Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies
(NAACL) , pp Pal, M., Maity, R., Ratnam, J., Nonaka, M., and Behera, S (2020), “Long-
lead prediction of ENSO Modoki index using machine learning algorithms”,
Scientiﬁc Reports 10(1), 1–13 Pan, X., Dai, B., Liu, Z., Loy, C

============================================================

=== CHUNK 232 ===
Palavras: 352
Caracteres: 2499
--------------------------------------------------
(2021), “Do 2D GANs know
3D shape Unsupervised 3D shape reconstruction from 2D image GANs”,
inProceedings of the International Conference on Learning Representations
(ICLR) (1988), Statistical Field Theory , Addison-Wesley Park, T., Liu, M.-Y., Wang, T.-C., and Zhu, J.-Y (2019), “Semantic image
synthesis with spatially-adaptive normalization”, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L et al.(2018), “Image trans-
former”, in Proceedings of the International Conference on Machine Learning
(ICML), pp References 321
Paszke, A., Gross, S., Chintala, S., Chanan, G et al.(2017), “Automatic diﬀer-
entiation in PyTorch”, in Proceedings of NIPS Workshop on Autodiﬀ Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A (2016),
“Contextencoders:Featurelearningbyinpainting”,in Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition (CVPR) , pp Patrick, M., Campbell, D., Asano, Y et al.(2021), “Keeping
your eye on the ball: Trajectory attention in video transformers”, in Proceed-
ings of Advances in Neural Information Processing Systems (NeurIPS) Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V et al.(2011), “Scikit-
learn: Machine learning in Python”, Journal of Machine Learning Research
12, 2825–2830 Perozzi, B., Al-Rfou, R., and Skiena, S (2014), “Deepwalk: Online learning of so-
cial representations”, in Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining , pp E., Neumann, M., Iyyer, M., Gardner, M et al.(2018), “Deep con-
textualized word representations”, Annual Conference of the North American
Chapter of the Association for Computational Linguistics Phan-Minh, T., Grigore, E A., Beijbom, O., and Wolﬀ, E (2020), “CoverNet: Multimodal behavior prediction using trajectory sets”, in
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp Pizzati, F., Cerri, P., and de Charette, R (2021), “CoMoGAN: Continuous
model-guided image-to-image translation”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp S., Mi´ covi´ c, D., Diaz, H et al.(2021), “Hierarchical rein-
forcement learning for air-to-air combat”, in Proceedings of the International
Conference on Unmanned Aircraft Systems (ICUAS) , pp (1957), “Shortest connection networks and some generalizations”,
Bell System Technical Journal 36(6), 1389–1401

============================================================

=== CHUNK 233 ===
Palavras: 361
Caracteres: 2754
--------------------------------------------------
PyTorch (2021), “TorchGAN library”, github.com/torchgan/torchgan Qian, Y., Zhang, H., and Furukawa, Y (2021), “Roof-GAN: Learning to gen-
erate roof geometry and relations for residential houses”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp W., Hallacy, C., Ramesh, A et al.(2021), “Learning trans-
ferable visual models from natural language supervision”, in Proceedings of
International Conference on Machine Learning (ICML) , pp Radford, A., Metz, L., and Chintala, S (2015), “Unsupervised representa-
tion learning with deep convolutional generative adversarial networks”, arXiv
preprint arXiv:1511.06434 Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I (2018),
322 References
“Improving language understanding by generative pre-training”,
openai.com/blog/language-unsupervised Radford, A., Wu, J., Child, R., Luan, D et al.(2019), “Language models are
unsupervised multitask learners”, OpenAI Blog 1(8) Raﬀel, C., Shazeer, N., Roberts, A., Lee, K et al.(2020), “Exploring the limits of
transfer learning with a uniﬁed text-to-text transformer”, Journal of Machine
Learning Research 21, 1–67 U., Drori, I., Stodden, V L., and Schr¨ oder, P (2005),
“Multiscale representations for manifold-valued data”, Multiscale Modeling &
Simulation 4(4), 1201–1232 Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P (2016), “SQUaD: 100,000+
questionsformachinecomprehensionoftext”,in Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP) Ramachandran, P., Zoph, B., and Le, Q (2017), “Searching for activation
functions”, arXiv preprint arXiv:1710.05941 Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M (2022), “Hierar-
chical text-conditional image generation with CLIP latents”, arXiv preprint
arXiv:2204.06125 Ramesh, A., Pavlov, M., Goh, G., and Gray, S (2021), “DALL ·E: Creating
images from text”, openai.com/blog/dall-e Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S et al.(2021),“Zero-shottext-to-image
generation”, Proceedings of the International Conference on Machine Learning
(ICML) pp Ranganath, R., Gerrish, S., and Blei, D (2014), “Black box variational infer-
ence”, in Proceedings of the International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS) , pp Ratnarajah, A., Tang, Z., and Manocha, D (2021), “IR-GAN: Room impulse
response generator for speech augmentation”, in Proceedings of Interspeech ,
pp (n.d.), “Classiﬁcation accuracy score for conditional
generative models”, Proceedings of Advances in Neural Information Processing
Systems (2015), “Collective opinion spam detection: Bridging
review networks and metadata”, in Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining , pp

============================================================

=== CHUNK 234 ===
Palavras: 365
Caracteres: 2737
--------------------------------------------------
Razavi, A., van den Oord, A., and Vinyals, O (2019), “Generating diverse high-
resolution images with VQ-VAE”, in Proceedings of the International Confer-
ence on Learning Representations Workshop J., Kale, S., and Kumar, S (2018), “On the convergence of Adam and
beyond”, in Proceedings of the International Conference on Learning Repre-
sentations (ICLR) (2020), “TSPLIB: Library of sample instances for the TSP” References 323
Remaggi, L., Kim, H., Jackson, P (2019), “Reproducing real
world acoustics in virtual reality using spherical cameras”, in Proceedings of
the AES International Conference on Immersive and Interactive Audio (2017), “Neural networks for deceptive opinion spam detec-
tion: An empirical study”, Information Sciences 385, 213–224 (1957), “Reverberation chambers for broadcasting and recording
studios”, Journal of the Audio Engineering Society 5(1), 18–22 (2015), “Variational inference with normalizing
ﬂows”, in Proceedings of the International Conference on Machine Learning
(ICML) J., Mohamed, S., and Wierstra, D (2014), “Stochastic backpropa-
gation and approximate inference in deep generative models”, in Proceedings
of the International Conference on Machine Learning (ICML) (2005), “Neural ﬁtted Q iteration–ﬁrst experiences with a data
eﬃcientneuralreinforcementlearningmethod”,in Proceedings of the European
Conference on Machine Learning (ECML) , pp Robbins,H.andMonro,S.(1951),“Astochasticapproximationmethod”, Annals
of Mathematical Statistics 22(3), 400–407 P., Shao, M., and Fu, Y (2021), “Survey on the analysis and model-
ing of visual kinship: A decade in the making”, IEEE Transactions on Pattern
Analysis and Machine Intelligence Robinson, J., Shao, M., Wu, Y., and Fu, Y (2016), “Families in the wild (FIW):
Large-scale kinship image database and benchmarks”, in Proceedings of the
ACM on Multimedia Conference Roeder, G., Wu, Y., and Duvenaud, D (2017), “Sticking the landing: Simple,
lower-variance gradient estimators for variational inference”, in Proceedings
of Advances in Neural Information Processing Systems (NeurIPS) , pp Roman, J., Verzoni, A., and Sutherland, S (2020), “Greetings from
the 2020 wildﬁre season: Five undeniable truths from a pivotal year
in the world’s growing struggle with wildﬁre”, www.nfpa.org/News-
and-Research/Publications-and-media/NFPA-Journal/2020/November-
December-2020/Features/Wildﬁre Ronneberger, O., Fischer, P., and Brox, T (2015), “U-Net: Convolutional net-
works for biomedical image segmentation”, in Proceedings of the International
Conference on Medical Image Computing and Computer-Assisted Intervention
(MICCAI) , pp (1977), “An analysis
of several heuristics for the traveling salesman problem”, SIAM Journal on
Computing 6(3), 563–581

============================================================

=== CHUNK 235 ===
Palavras: 363
Caracteres: 2721
--------------------------------------------------
(2010), “Eﬃcient reductions for imitation learning”, in
Proceedings of the thirteenth International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS) , pp 324 References
Ross, S., Gordon, G., and Bagnell, D (2011), “A reduction of imitation learning
and structured prediction to no-regret online learning”, in Proceedings of the
fourteenth International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS) , pp Rozen, N., Grover, A., Nickel, M., and Lipman, Y (n.d.), “Moser Flow:
Divergence-based generative modeling on manifolds”, Proceedings of Advances
in Neural Information Processing Systems (NeurIPS) (1986), “Learning repre-
sentations by back-propagating errors”, Nature 323(6088), 533 Saha, A., Bharath, K., and Kurtek, S (2019), “A geometric variational ap-
proach to Bayesian inference”, Journal of the American Statistical Association
115, 822–835 (2009), “Early forest ﬁre detection using radio-acoustic
sounding system”, Sensors 9(3), 1485–1498 (2020), “Generating impulse responses
using recurrent neural networks”, 109ecc9c-0e76-482f-90c5-
fe6cd93cf581.ﬁlesusr.com/ugd/4a27c6 fa8281568425494 e8ca16133fe724c6e.pdf Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V et al.(2016), “Improved
techniques for training GANs”, in Proceedings of Advances in Neural Infor-
mation Processing Systems (NeurIPS) , pp Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I (2017), “Evolution
strategies as a scalable alternative to reinforcement learning”, arXiv preprint
arXiv:1703.03864 Sanchez-Gonzalez, A., Heess, N., Springenberg, J et al.(2018),
“Graph networks as learnable physics engines for inference and control”, in
Proceedings of the International Conference on Machine Learning (ICML) (2003), “Introduction to the CoNLL-2003 shared
task: Language-independent named entity recognition”, in Proceedings of the
Conference on Natural Language Learning at HLT-NAACL Sanh, V., Debut, L., Chaumond, J., and Wolf, T (2019), “DistilBERT, a dis-
tilled version of BERT: Smaller, faster, cheaper and lighter”, arXiv preprint
arXiv:1910.01108 Santoro, A., Raposo, D., Barrett, D et al.(2017), “A simple
neural network module for relational reasoning”, in Proceedings of Advances
in Neural Information Processing Systems (NeurIPS) , pp Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A (2018), “How does batch
normalization help optimization?”, in Proceedings of Advances in Neural In-
formation Processing Systems (NeurIPS) , pp Sauz` ede, R., Johnson, J E., Claustre, H., Camps-Valls, G., and Ruescas, A (2020), “Estimation of oceanic particulate organic carbon with machine learn-
ing”,ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial In-
formation Sciences 2, 949–956

============================================================

=== CHUNK 236 ===
Palavras: 375
Caracteres: 2692
--------------------------------------------------
References 325
Schaul, T., Quan, J., Antonoglou, I., and Silver, D (2016), “Prioritized expe-
rience replay”, in Proceedings of the International Conference on Learning
Representations (ICLR) (2016), “Interactive sound propagation and ren-
dering for large multi-source scenes”, ACM Transactions on Graphics (TOG)
36(4), 1 Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K et al.(2019), “Mas-
tering Atari, Go, chess and Shogi by planning with a learned model”, arXiv
preprint arXiv:1911.08265 (1961), “”Colorless” artiﬁcial reverberation”,
IRE Transactions on Audio (6), 209–214 Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P (2015), “Trust
region policy optimization”, in Proceedings of the International Conference on
Machine Learning (ICML) , pp Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P (2016), “High-
dimensional continuous control using generalized advantage estimation”, in
Proceedings of the International Conference on Learning Representations
(ICLR) Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O (2017),
“Proximal policy optimization algorithms”, arXiv preprint arXiv:1707.06347 J., Jeddi, A., Nazemi, A., Fieguth, P., and Wong, A (2020), “Deep
neural network perception models and robust autonomous driving systems:
Practical solutions for mitigation and improvement”, IEEE Signal Processing
Magazine 38(1), 22–30 R., Dekel, T., and Michaeli, T (2019), “SinGAN: Learning a gen-
erative model from a single natural image”, in Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pp Shoeybi, M., Patwary, M., Puri, R., LeGresley, P et al.(2019), “Megatron-LM:
Training multi-billion parameter language models using GPU model paral-
lelism”,arXiv preprint arXiv:1909.08053 Shor, J., Sarna, A., Drori, Y., and Westbrook, D (2020), “TensorFlow GAN
library”, github.com/tensorﬂow/gan Shporer, A., Tran, S., Singh, N., Kates, B et al.(2022), “Learning methods for
solving Astronomy course problems” Shrivastava, A., Pﬁster, T., Tuzel, O., Susskind, J et al.(2017), “Learning from
simulated and unsupervised images through adversarial training”, in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pp Shukla, A., Uppal, S., Bhagat, S., Anand, S., and Turaga, P (2018), “Geometry
of deep generative models for disentangled representations”, in Proceedings of
the Indian Conference on Computer Vision, Graphics and Image Processing ,
pp (2009), “Legolizer: A real-time
system for modeling and rendering lego representations of boundary models”,
326 References
inProceedings of the Brazilian Symposium on Computer Graphics and Image
Processing , pp

============================================================

=== CHUNK 237 ===
Palavras: 360
Caracteres: 2818
--------------------------------------------------
Silver, D., Huang, A., Maddison, C et al.(2016), “Master-
ing the game of Go with deep neural networks and tree search”, Nature
529(7587), 484–489 Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I et al.(2017), “Mastering
chess and Shogi by self-play with a general reinforcement learning algorithm”,
arXiv preprint arXiv:1712.01815 Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I et al.(2018), “A general
reinforcement learning algorithm that masters chess, Shogi, and Go through
self-play”, Science 362(6419), 1140–1144 (2014), “Very deep convolutional networks for
large-scale image recognition”, arXiv preprint arXiv:1409.1556 (n.d.), “Tensorﬂow playground”, play-
ground.tensorﬂow.org (2018), “Kunst der Fuge site: Classical music in MIDI ﬁles”,
www.kunstderfuge.com Snoek,J.,Larochelle,H.,andAdams,R.P.(2012),“PracticalBayesianoptimiza-
tion of machine learning algorithms”, in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) , pp Socher, R., Perelygin, A., Wu, J., Chuang, J et al.(2013), “Recursive deep
models for semantic compositionality over a sentiment treebank”, in Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP) , pp Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S (2015),
“Deep unsupervised learning using nonequilibrium thermodynamics”, in Pro-
ceedings of International Conference on Machine Learning (ICML) , pp (1981), “Minimization by random search tech-
niques”, Mathematics of Operations Research 6(1), 19–30 (1999), A Comprehensive Introduction to Diﬀerential Geometry ,
3rd edn, Publish or Perish Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J et al.(2021), “Bottleneck trans-
formers for visual recognition”, in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pp Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R (2014), “Dropout: A simple way to prevent neural networks from overﬁtting”,
Journal of Machine Learning Research 15(1), 1929–1958 (1999), “Generating random regular graphs
quickly”, Combinatorics, Probability and Computing 8(4), 377–396 Steinmetz,C.(2018),“NeuralReverberator”, www.christiansteinmetz.com/projects-
blog/neuralreverberator (2018), “Generating 3D-objects using neural networks” References 327
Strunk Jr., W (2007), The Elements of Style Illustrated , Pen-
guin Su, W., Zhu, X., Cao, Y., Li, B et al.(2020), “Vl-BERT: Pre-training of generic
visual-linguistic representations”, in Proceedings of the International Confer-
ence on Learning Representations (ICLR) Sun, B., Feng, J., and Saenko, K (2017), Correlation alignment for unsupervised
domainadaptation, inG.Csurka,ed.,DomainAdaptationinComputerVision
Applications, pp Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C

============================================================

=== CHUNK 238 ===
Palavras: 354
Caracteres: 2487
--------------------------------------------------
(2019),
“VideoBERT: A joint model for video and language representation learning”,
inProceedings of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pp J., Boots, B., and Bagnell, J (2018), “Dual policy itera-
tion”, in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) , pp Sutskever, I., Martens, J., Dahl, G., and Hinton, G (2013), “On the impor-
tance of initialization and momentum in deep learning”, in Proceedings of the
International Conference on Machine Learning (ICML) , pp Sutskever, I., Vinyals, O., and Le, Q (2014), “Sequence to sequence learn-
ing with neural networks”, in Proceedings of Advances in Neural Information
Processing Systems (NeurIPS) , pp (1988), “Learning to predict by the methods of temporal diﬀer-
ences”,Machine Learning 3(1), 9–44 Swearingen, T., Drevo, W., Cyphers, B., Cuesta-Infante, A et al.(2017), “ATM:
A distributed, collaborative, scalable system for automated machine learning”,
inProceedings of the IEEE International Conference on Big Data Szegedy, C., Liu, W., Jia, Y., Sermanet, P et al.(2015), “Going deeper with con-
volutions”, in Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., and Wojna, Z (2016), “Re-
thinking the inception architecture for computer vision”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp (2019), “LXMERT: Learning cross-modality encoder
representations from transformers”, in Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing (EMNLP) (2020), “Vokenization: Improving language understand-
ing with contextualized, visual-grounded supervision”, in Proceedings of the
Conference on Empirical Methods in Natural Language Processing (EMNLP) (2019), “Multiple futures prediction”, in
Proceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp Tang, D., Liang, D., Jebara, T., and Ruozzi, N (2019), “Correlated variational
328 References
autoencoders”, in Proceedings of the International Conference on Machine
Learning (ICML) (2019), “The variational predictive natural gra-
dient”, in Proceedings of the International Conference on Machine Learning
(ICML) Tang, J., Qu, M., Wang, M., Zhang, M et al.(2015), “Line: Large-scale informa-
tion network embedding”, in Proceedings of the 24th International Conference
on World Wide Web (WWW) , pp Tang, L., Ke, E., Feng, B., Austin, D

============================================================

=== CHUNK 239 ===
Palavras: 359
Caracteres: 2704
--------------------------------------------------
et al.(2022), “Solving Probability and
Statistics problems by probabilistic program synthesis at human level and
predicting solvability”, in Proceedings of the International Conference on Ar-
tiﬁcial Intelligence in Education Tang, W., Li, Z., and Cassar, N (2019), “Machine learning estimates of global
marine nitrogen ﬁxation”, Journal of Geophysical Research: Biogeosciences
124(3), 717–730 (2000), “Separating style and content
with bilinear models”, Neural Computation 12(6), 1247–1283 (2012), “RMSProp”, Coursera lecture Touvron, H., Cord, M., Douze, M., Massa, F et al.(2021), “Training data-
eﬃcient image transformers & distillation through attention”, in Proceedings
of the International Conference on Machine Learning (ICML) , pp (2016), “Statistics of natural reverberation en-
able perceptual separation of sound and space”, Proceedings of the National
Academy of Sciences 113(48), E7856–E7865 Tran, S., Krishna, P., Pakuwal, I., Kaﬂe, P et al.(2021), “Solving machine
learning problems”, Proceedings of the Asian Conference on Machine Learning
(ACML) (1985), “The visual display of quantitative information”, Journal
for Healthcare Quality (JHQ) 7(3), 15 van den Oord, A., Vinyals, O., and Koray, K (2017), “Neural discrete represen-
tation learning”, in Proceedings of Advances in Neural Information Processing
Systems (NeurIPS) , pp Van Hasselt, H., Guez, A., and Silver, D (n.d.), “Deep reinforcement learning
with double Q-learning”, in Proceedings of the AAAI conference on artiﬁcial
intelligence Van Steenkiste, S., Chang, M., Greﬀ, K., and Schmidhuber, J (2018), “Rela-
tionalneuralexpectationmaximization:Unsuperviseddiscoveryofobjectsand
their interactions”, in Proceedings of the International Conference on Learning
Representations (ICLR) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J et al.(2017), “Attention is
all you need”, in Proceedings of Advances in Neural Information Processing
Systems (NeurIPS) , pp References 329
Veliˇ ckovi´ c, P., Cucurull, G., Casanova, A., Romero, A et al.(2018), “Graph at-
tention networks”, in Proceedings of the International Conference on Learning
Representations (ICLR) Vesselinova, N., Steinert, R., Perez-Ramirez, D (2020),
“Learning combinatorial optimization on graphs: A survey with applications
to networking”, IEEE Access 8, 120388–120416 Villegas, R., Yang, J., Ceylan, D., and Lee, H (2018), “Neural kinematic net-
works for unsupervised motion retargetting”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Vinyals,O.,Babuschkin,I.,Czarnecki,W.M.,Mathieu,M et al.(2019),“Grand-
master level in StarCraft II using multi-agent reinforcement learning”, Nature
575(7782), 350–354

============================================================

=== CHUNK 240 ===
Palavras: 354
Caracteres: 2601
--------------------------------------------------
Vondrick, C., Pirsiavash, H., and Torralba, A (2016), “Generating videos with
scene dynamics”, in Proceedings of Advances in Neural Information Processing
Systems (NeurIPS) , pp Wainwright,M.J.andJordan,M.I.(2008),“Graphicalmodels,exponentialfam-
ilies, and variational inference”, Foundations and Trends in Machine Learning
1(1–2), 1–305 Wang, A., Singh, A., Michael, J., Hill, F et al.(2019), “GLUE: A multi-task
benchmarkandanalysisplatformfornaturallanguageunderstanding”,in Pro-
ceedings of the International Conference on Learning Representations (ICLR) Wang, F., Wang, H., Wang, H., Li, G., and Situ, G (2019), “Learning from
simulation: An end-to-end deep-learning approach for computational ghost
imaging”, Optics Express 27(18), 25560–25572 Wang, J., Cao, H., Zhang, J (2018), “Computational protein
design with deep learning neural networks”, Scientiﬁc Reports 8(1), 6349 Wang, J., Zhang, Y., Tang, K., Wu, J., and Xiong, Z (2019), “AlphaStock: A
buying-winners-and-selling-losers investment strategy using interpretable deep
reinforcement attention networks”, in Proceedings of the ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data Mining , pp Wang, M., Yu, L., Gan, Q., Zhaoogle, J et al.(2020), “Deep Graph Library”,
www.dgl.ai (2019), “Imagined by a GAN”, www.thispersondoesnotexist.com (2019), “Riemannian normalizing ﬂow on varia-
tional Wasserstein autoencoder for text modeling”, in Proceedings of the Con-
ference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , pp Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., Liu, G et al.(2018), “Video-to-video syn-
thesis”, in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., Tao, A et al.(2018), “High-resolution
330 References
image synthesis and semantic manipulation with conditional GANs”, in Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp Wang, W., Xie, E., Li, X., Fan, D.-P et al.(2021), “Pyramid vision transformer:
A versatile backbone for dense prediction without convolutions”, Proceedings
of the IEEE/CVF International Conference on Computer Vision (ICCV) Wang, X., Li, Y., Zhang, H., and Shan, Y (2021), “Towards real-world blind
face restoration with generative facial prior”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Wang, Y., Sun, Y., Liu, Z., Sarma, S et al.(2019), “Dynamic graph CNN for
learningonpointclouds”, ACM Transactions on Graphics (TOG) 38(5),1–12 Wang, Y., Zhang, G., and Ba, J

============================================================

=== CHUNK 241 ===
Palavras: 362
Caracteres: 2602
--------------------------------------------------
(2020), “On solving minimax optimization lo-
cally: A follow-the-ridge approach”, in Proceedings of the International Con-
ference on Learning Representations (ICLR) Wang, Z., Li, Y., Liu, B., and Liu, J (2015), “Global climate internal vari-
ability in a 2000-year control simulation with community earth system model
(CESM)”, Chinese Geographical Science 25(3), 263–273 Wang, Z., Schaul, T., Hessel, M., Hasselt, H et al.(2016), “Dueling network
architectures for deep reinforcement learning”, in Proceedings of the Interna-
tional Conference on Machine Learning (ICML) , pp Warstadt, A., Singh, A., and Bowman, S (2019), “Neural network acceptabil-
ity judgments”, Transactions of the Association for Computational Linguistics
7, 625–641 (1964), “Smooth regression analysis”, Sankhy¯ a: The Indian Jour-
nal of Statistics, Series A 26(4), 359–372 Watters, N., Zoran, D., Weber, T., Battaglia, P et al.(2017), “Visual interaction
networks:Learningaphysicssimulatorfromvideo”,in Proceedings of Advances
in Neural Information Processing Systems (NeurIPS) , pp (1998), “Collective dynamics of small-world
networks”, Nature 393(6684), 440 (2010), “Ocean nutrient ratios governed by plank-
ton biogeography”, Nature 467(7315), 550–554 (1964), “A simple automatic derivative evaluation program”,
Communications of the ACM 7(8) Williams, A., Nangia, N., and Bowman, S (2018), “A broad-coverage chal-
lenge corpus for sentence understanding through inference”, Proceedings of
the North American Chapter of the Association for Computational Linguistics
(NAACL) (2011), The Design of Approximation
Algorithms , Cambridge University Press C., Roelofs, R., Stern, M., Srebro, N., and Recht, B (2017), “The
References 331
marginal value of adaptive gradient methods in machine learning”, in Pro-
ceedings of Advances in Neural Information Processing Systems (NeurIPS) ,
pp Wolf, T., Debut, L., Sanh, V., Chaumond, J et al.(2020), “Huggingface’s trans-
formers: State-of-the-art natural language processing”, in Proceedings of the
Conference on Empirical Methods in Natural Language Processing (EMNLP) et al.(2017),
“Deep MR to CT synthesis using unpaired data”, in International Workshop
on Simulation and Synthesis in Medical Imaging , pp Wu, B., Xu, C., Dai, X., Wan, A et al.(2021), “Visual Transformers: Where
do Transformers really belong in vision models?”, in Proceedings of the
IEEE/CVF International Conference on Computer Vision (CVPR) , pp Wu, F., Zhang, T., Souza Jr, A et al.(2019), “Simplifying graph
convolutional networks”, in Proceedings of the International Conference on
Machine Learning (ICML)

============================================================

=== CHUNK 242 ===
Palavras: 357
Caracteres: 2623
--------------------------------------------------
Wu, J., Zhang, C., Xue, T., Freeman, B., and Tenenbaum, J (2016), “Learning a
probabilistic latent space of object shapes via 3D generative-adversarial mod-
eling”, in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS) , pp R., Barrett, S., Kawamoto, K., MacGlashan, J et al.(2022), “Out-
racing champion Gran Turismo drivers with deep reinforcement learning”,
Nature 602(7896), 223–228 Xia, W., Yang, Y., Xue, J.-H., and Wu, B (2021), “TediGAN: Text-
guided diverse face image generation and manipulation”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp (2019), “Distance-based protein folding powered by deep learning”, Pro-
ceedings of the National Academy of Sciences 116(34), 16856–16865 Xu, K., Hu, W., Leskovec, J., and Jegelka, S (2019), “How powerful are graph
neural networks?”, in Proceedings of the International Conference on Learning
Representations (ICLR) Xu, K., Li, J., Zhang, M., Du, S et al.(2020), “What can neural networks
reason about?”, in Proceedings of the International Conference on Learning
Representations (ICLR) Xu, T., Zhang, P., Huang, Q., Zhang, H et al.(2018), “AttnGAN: Fine-grained
text to image generation with attentional generative adversarial networks”, in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , pp Yan, J., Mu, L., Wang, L., Ranjan, R., and Zomaya, A (2020), “Temporal
convolutionalnetworksfortheadvancepredictionofENSO”, Scientiﬁc Reports
10(1), 1–15 Yang,C.,Akimoto,Y.,Kim,D.W.,andUdell,M.(2019),“OBOE:Collaborative
332 References
ﬁltering for AutoML model selection”, in Proceedings of the ACM SIGKDD
Conference on Knowledge Discovery and Data Mining , pp Yang, C., Wu, Z., Fan, J., and Udell, M (2020), “AutoML pipeline selection:
Eﬃciently navigating the combinatorial space”, in Proceedings of the ACM
SIGKDD Conference on Knowledge Discovery and Data Mining Yang, L.-C., Chou, S.-Y., and Yang, Y.-H (2017), “MidiNet: A convolutional
generativeadversarialnetworkforsymbolic-domainmusicgeneration”,in Pro-
ceedings of the 18th International Society for Music Information Retrieval
Conference (ISMIR) Yang, S., Xie, L., Chen, X., Lou, X et al.(2017), “Statistical parametric speech
synthesis using generative adversarial networks under a multi-task learning
framework”,in IEEE Automatic Speech Recognition and Understanding Work-
shop (ASRU) , pp Yang, Z., Dai, Z., Yang, Y., Carbonell, J et al.(2019), “XLNet: Generalized au-
toregressive pretraining for language understanding”, Proceedings of Advances
in Neural Information Processing Systems (NeurIPS)

============================================================

=== CHUNK 243 ===
Palavras: 354
Caracteres: 2575
--------------------------------------------------
Yang, Z., Zhang, Y., Yu, J., Cai, J., and Luo, J (2018), “End-to-end multi-
modal multi-task vehicle control for self-driving cars with visual perceptions”,
inProceedings of the 24th International Conference on Pattern Recognition
(ICPR), pp A., Chen, C., Yian Lim, T., Schwing, A et al.(2017), “Semantic
image inpainting with deep generative models”, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , pp (2022), “Anytime 3D object reconstruction using multi-modal
variational autoencoder”, IEEE Robotics and Automation Letters Yu, J., Li, M., Hao, X., and Xie, G (2020), “Deep fusion siamese network for
automatickinshipveriﬁcation”, in Proceedings of the IEEE International Con-
ference on Automatic Face and Gesture Recognition (FG) , pp Yu, J., Lin, Z., Yang, J., Shen, X et al.(2019), “Free-form image inpainting with
gatedconvolution”,in Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pp Yu, L., Zhang, W., Wang, J., and Yu, Y (2017), “SeqGAN: Sequence generative
adversarialnetswithpolicygradient”,in Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S (2018), “Adaptive
methods for nonconvex optimization”, in Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) , pp (2007), “When the brain plays
music: Auditory–motor interactions in music perception and production”, Na-
ture Neuroscience 8(7), 547–558 (2012), “AdaDelta: An adaptive learning rate method”, arXiv
preprint arXiv:1212.5701 References 333
Zellers, R., Bisk, Y., Schwartz, R., and Choi, Y (2018), “SWAG: A large-scale
adversarialdatasetforgroundedcommonsenseinference”,in Proceedings of the
Conference on Empirical Methods in Natural Language Processing (EMNLP) Zhai, M., Chen, L., and Mori, G (2021), “Hyper-LifelongGAN: Scalable lifelong
learning for image conditioned generation”, in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp Zhang, C., Butepage, J., Kjellstrom, H., and Mandt, S (2018), “Advances in
variational inference”, IEEE Transactions on Pattern Analysis and Machine
Intelligence 41(8), 2008–2026 Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A (2019), “Self-attention
generative adversarial networks”, in Proceedings of the International Confer-
ence on Machine Learning (ICML) Zhang, H., Sindagi, V., and Patel, V (2019), “Image de-raining using a
conditional generative adversarial network”, IEEE Transactions on Circuits
and Systems for Video Technology

============================================================

=== CHUNK 244 ===
Palavras: 363
Caracteres: 2739
--------------------------------------------------
Zhang, H., Xu, T., Li, H., Zhang, S et al.(2017), “StackGAN: Text to photo-
realistic image synthesis with stacked generative adversarial networks”, in
Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV), pp Zhang,H.,Xu,T.,Li,H.,Zhang,S et al.(2018),“StackGAN++:Realisticimage
synthesis with stacked generative adversarial networks”, IEEE Transactions
on Pattern Analysis and Machine Intelligence 41(8), 1947–1962 Zhang, J., Chen, X., Cai, Z., Pan, L et al.(2021), “Unsupervised 3d shape com-
pletion through GAN inversion”, in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pp Zhang, L., Naesseth, C (2022), “Transport score climbing:
Variational inference using forward KL and adaptive neural transport”, arXiv
preprint arXiv:2202.01841 (2006), “Impact of Atlantic multidecadal oscilla-
tions on India/Sahel rainfall and Atlantic hurricanes”, Geophysical Research
Letters 33(17) Zhang, X., Zhou, X., Lin, M., and Sun, J (2018), “ShuﬄeNet: An extremely
eﬃcient convolutional neural network for mobile devices”, in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp Zhang, Z., Song, Y., and Qi, H (2017), “Age progression/regression by condi-
tional adversarial autoencoder”, in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pp Zhao, E., Yan, R., Li, J., Li, K., and Xing, J (2022), “AlphaHoldem: High-
performance artiﬁcial intelligence for Heads-Up No-Limit Texas Hold’em from
end-to-end reinforcement learning”, in Proceedings of Thirty-Six AAAI Con-
ference on Artiﬁcial Intelligence 334 References
Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva, A (n.d.), “Learning
deep features for scene recognition using places database”, in Proceedings of
Advances in Neural Information Processing Systems (NeurIPS) , pp Zhu,J.-Y.,Park,T.,Isola,P.,andEfros,A.A.(2017),“Unpairedimage-to-image
translation using cycle-consistent adversarial networks”, in Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV) , pp Zhu, X., Su, W., Lu, L., Li, B et al.(2021), “Deformable DETR: Deformable
transformers for end-to-end object detection”, in Proceedings of the Interna-
tional Conference on Learning Representations (ICLR) (2008), “Maximum
entropy inverse reinforcement learning”, in Proceedings of the Twenty-Third
AAAI Conference on Artiﬁcial Intelligence , Vol (2021), “Benchmark and survey of automated
machine learning frameworks”, Journal of Artiﬁcial Intelligence Research
(70), 409–472 (2017), “Invited
review article: Gas puﬀ imaging diagnostics of edge plasma turbulence in mag-
netic fusion devices”, Review of Scientiﬁc Instruments 88(4), 041101

============================================================

=== CHUNK 245 ===
Palavras: 350
Caracteres: 2760
--------------------------------------------------
Index /lscript1norm,59 /lscript1regularization, 62 /lscript2norm,60 /lscript2regularization, 62 /lscript∞norm,60 /lscriptpnorm,60 ε-greedy, 194 action,193 action value function, 207 activation units, 9 actor,239 actor–critic methods, 239 AdaDelta, 47 adaptive gradient descent, 45 adaptive moment estimation (Adam), 47 adaptive subgradient descent (Adagrad), 47 advantage actor–critic (A2C), 240 agent,193 agent action, 193 AlphaZero, 245 analytic gradient, 38 asynchronous advantage actor-critic (A3C), 240 attention, 118 auto-regressive Transformers, 147 autoencoder, 181 autoencoding Transformers, 146 backpropagation, 21 backpropagation through time, 99 backtrack line search, 43 bag of words, 91 batch normalization, 65 Bellman expectation equation, 213 Bellman optimality equation, 215 best step-size optimization, 43 BFGS correction, 54 bias,59 bidirectional encoder representations from Transformers (BERT), 143 bidirectional recurrent neural network, 98 bottleneck, 181 Bregman divergence, 157, 176 chain rule for diﬀerentiation, 26, 30co-evolution, 155 conditional GAN (CGAN), 164 contrastive language-image pre-training (CLIP), 269 convex optimization, 35 convolution, 70 convolution kernel, 70 convolution layer, 78 convolution padding, 70 convolution stride, 73 convolutional neural network (CNN), 80 covariance matrix adaptation, 55 critic,239 cross validation, 59 cycle consistent GAN (CycleGAN), 165 data augmentation, 56, 64 deepQ-network (DQN), 233 deep convolutional GAN (DCGAN), 164 deep deterministic policy gradient (DDPG), 244 denoising diﬀusion probabilistic model (DDPM), 186 DenseNet, 83 derivative, 37 derivative of hyperbolic tangent function, 27 derivative of log-likelihood function, 178 derivative of ReLU, 27 derivative of sigmoid function, 26 deterministic policy, 203 DFP correction, 54 discount factor, 199, 209 double DQN, 235 dropout, 56, 62 dueling network, 235 dynamic programming, 218 Earth mover’s distance (EMD), 162 eligibility traces, 227 environment, 193 environment model, 210 episode, 211 evidence lower bound (ELBO), 177 evolution strategies, 37, 54 expected log likelihood, 178 336 Index expected return, 212 expected reward, 193 experience replay, 232 expert iteration, 246 exploitation, 211 exploration, 211 exponential family of distributions, 175 exponentially weighted moving average, 46 f-divergence, 175 f-GAN, 158 farsighted agent, 206 feature vector sentence representation, 92 ﬁnite horizon, 204 ﬁrst-order methods, 37 Fisher information matrix, 242 forward propagation, 11 Frechet inception distance (FID), 168 fully connected neural network, 9 GAN discriminator, 156 GAN discriminator training, 160 GAN discriminator–generator training, 161 GAN generator, 156 GAN generator training, 160 GAN minimax

============================================================

=== CHUNK 246 ===
Palavras: 350
Caracteres: 2538
--------------------------------------------------
loss, 162 GAN training, 160 gated graph neural networks, 139 gated recurrent unit (GRU), 102 generalization gap, 56 generative adversarial network (GAN), 153 Go-Explore, 249 GPT-3, 148 gradient, 38 gradient descent, 32, 36, 39 gradient descent ascent (GDA), 158 gradient descent of loss function, 33 gradient descent update, 33 gradient descent with momentum, 46 gradient of ELBO, 178 graph,126 graph adjacency list, 126 graph adjacency matrix, 126 graph attention network (GAT), 140 graph connected components, 129 graph convolution network (GCN), 138 graph edge, 126 graph Laplacian matrix, 127 graph neighborhood aggregation, 138 graph neighborhood aggregation function, 139 graph neural network (GNN), 136 graph node, 126 graph node degree, 127 graph node embedding, 130 graph node feature vector, 131 graph node similarity, 131 graph random walk, 133graph random walk normalized Laplacian matrix,128 graph shallow node embedding, 131 graph symmetric normalized Laplacian matrix,128 graph walk, 129 graphSAGE, 139 greedy action, 194 Hessian, 38, 53 history, 211 huggingface Transformers platform, 150 hyperbolic tangent function, 16 hypergradient descent, 48 image-to-image translation (Pix2Pix), 165 imitation learning, 248 inception score (IS), 168 inﬁnite horizon, 206 initial state, 196 input normalization, 33 instance conditioned GAN (IC-GAN), 168 inverse document frequency, 91 inverse Hessian, 53 inverse reinforcement learning, 248 invertible residual neural network, 86 iterative policy evaluation, 218 Jacobian matrix, 31 Jensen–Shannon (JS) divergence, 157 joint density, 174 Kullback–Leibler (KL) divergence, 157, 176 L-BFGS, 54 Lasso,61 latent representation, 183 latent variable, 174 learning curves, 58 learning rate, 41 least-squares GAN (LS-GAN), 158 likelihood function, 174 local maximum, 38 local minimum, 38 logistic regression, 14 logistic regression loss, 21 long short-term memory (LSTM), 108 loss function, 19, 35 marginal density, 174 Markov decision process (MDP), 199 Markov model, 92 Markov process, 197 max pooling, 78 mean squared error, 20 message-passing graph neural network, 140 mini-batch gradient descent, 44 minimax optimization problem, 155 MobileNet, 85 model-based reinforcement learning, 210, 245 model-free reinforcement learning, 211, 232 Index 337 Monte Carlo sampling, 222 multi-armed bandit, 193 multi-head attention, 144 multi-modal Transformer, 149 myopic agent, 206 n-gram, 92 negative entropy, 178 Nesterov momentum, 46 neural ﬁtted Q-iteration, 233 neural network, 9 neural network

============================================================

=== CHUNK 247 ===
Palavras: 350
Caracteres: 2678
--------------------------------------------------
layers, 9 Newton’s method, 49 Newton–Raphson update, 52 non-convex optimization, 36 non-linear activation function, 16 normalizing ﬂows, 184 numerical gradient, 38 observation, 196 ODENet, 85 oﬀ-policy reinforcement learning, 227 on-policy reinforcement learning, 227 OpenAI Codex, 149 optimal action value function, 215 optimal inﬁnite horizon policy, 215 optimal policy, 214 optimal state value function, 215 optimistic gradient descent ascent (OGDA), 159 optimization problem, 35 overﬁtting, 56 planning, 218 policy,198, 202 policy evaluation, 218 policy gradient, 237 policy improvement, 218 policy iteration, 218 policy search, 222 policy-based reinforcement learning, 236 pooling, 78 posterior, 174 pre-activations, 9 proximal policy optimization (PPO), 244 PyTorch, 33 Q-learning, 224 quasi-Newton methods, 36, 53 rectiﬁed linear unit (ReLU), 17 recurrent hidden units, 94 recurrent loss function, 95 recurrent neural network (RNN), 93 registration GAN (RegGAN), 167 regularization, 56 regularization parameter, 61 regularized loss functions, 61 REINFORCE algorithm, 238 reinforcement learning, 193reparameterization gradient, 180 replay buﬀer, 233 residual neural network (ResNet), 81 return,210 reward, 193, 209 reward matrix, 200 ridge regression, 60 Riemannian manifold, 187 RMSProp, 47 saddle-point, 38 saddle-point problem, 155 score function, 179 secant condition, 53 second derivative, 37 second-order methods, 37, 49 self-attention, 144 self-attention GAN (SAGAN), 167 semi-supervised GAN (SGAN), 164 sequence-to-sequence, 117 sequence-to-sequence Transformers, 147 sigmoid function, 16 softmax function, 18 squeeze and excitation network (SENet), 84 SR1 update, 54 state,200 state machine, 92, 196 state of environment, 193, 201 state value function, 204 state–action diagram, 203 step size, 41 stochastic gradient descent, 28, 44 stochastic policy, 203 stochastic weight averaging, 45 sub-graph embedding, 131 Swish function, 18 target network, 234 temporal diﬀerence (TD) learning, 222 TensorFlow, 33 term frequency, 91 test accuracy, 58 TFIDF, 92 training data, 58 training neural networks, 45 Transformer, 142 Transformer decoder, 146 Transformer encoder, 145 Transformer ﬁne-tuning, 146 Transformer masked word prediction, 146 Transformer position encoding, 145 Transformer pre-training, 146 transition function, 196 transition matrix, 198 transition model, 199 trust region policy optimization (TRPO), 243 upper conﬁdence bound (UCB), 196 338 Index value function approximation, 230 value iteration, 219 variance, 59 variational autoencoder (VAE), 183 variational distribution, 175 variational inference (VI), 175 variational inference problem, 175 vector

============================================================

=== CHUNK 248 ===
Palavras: 26
Caracteres: 185
--------------------------------------------------
norms, 59 vision Transformer (ViT), 148 Wasserstein GAN (WGAN), 162 Wasserstein GAN loss, 162 Wasserstein-1 distance, 162 weight initialization, 33 word embedding, 121 world models, 245

============================================================


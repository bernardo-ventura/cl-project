=== CHUNK 001 ===
Palavras: 361
Caracteres: 2510
--------------------------------------------------
Foundations  
and Concepts Christopher M Bishop
with Hugh Bishop
   Deep Learning 
Deep Learning
Christopher M Bishop ‚Ä¢ Hugh Bishop
Deep Learning 
Foundations and Concepts
 
ISBN 978-3-031-45467-7 ISBN 978-3-031-45468-4 (eBook) 
https://doi.org/10.1007/978-3-031-45468-4 
 
¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
This work is subject to copyright All rights are solely and exclusively licensed by the Publisher, whether the whole or part of the 
material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on 
microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, 
or by similar or dissimilar methodology now known or hereafter developed The use of general descriptive names, registered names, trademarks, service marks, etc in this publication does not imply, even in the 
absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for 
general use The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and 
accurate at the date of publication Neither the publisher nor the authors or the editors give a warranty, express ed or implied, with respect 
to the material contained herein or for any errors or omissions that may have been made The publisher remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations Cover illustration: maksimee / Alamy Stock Photo 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
 
Paper in this product is recyclable Bishop 
Microsoft Research 
Cambridge, UK Hugh Bishop 
Wayve Technologies Ltd 
London, UK  
Preface
Deep learning uses multilayered neural networks trained with large data sets to
solve complex information processing tasks and has emerged as the most successful
paradigm in the Ô¨Åeld of machine learning Over the last decade, deep learning has
revolutionized many domains including computer vision, speech recognition, and
natural language processing, and it is being used in a growing multitude of applica-
tions across healthcare, manufacturing, commerce, Ô¨Ånance, scientiÔ¨Åc discovery, and
many other sectors

============================================================

=== CHUNK 002 ===
Palavras: 377
Caracteres: 2308
--------------------------------------------------
Recently, massive neural networks, known as large language
models and comprising of the order of a trillion learnable parameters, have been
found to exhibit the Ô¨Årst indications of general artiÔ¨Åcial intelligence and are now
driving one of the biggest disruptions in the history of technology Goals of the book
This expanding impact has been accompanied by an explosion in the number
and breadth of research publications in machine learning, and the pace of innova-
tion continues to accelerate For newcomers to the Ô¨Åeld, the challenge of getting
to grips with the key ideas, let alone catching up to the research frontier, can seem
daunting Against this backdrop, Deep Learning: Foundations and Concepts aims
to provide newcomers to machine learning, as well as those already experienced in
the Ô¨Åeld, with a thorough understanding of both the foundational ideas that underpin
deep learning as well as the key concepts of modern deep learning architectures and
techniques This material will equip the reader with a strong basis for future spe-
cialization Due to the breadth and pace of change in the Ô¨Åeld, we have deliberately
avoided trying to create a comprehensive survey of the latest research Instead, much
of the value of the book derives from a distillation of key ideas, and although the Ô¨Åeld
itself can be expected to continue its rapid advance, these foundations and concepts
are likely to stand the test of time For example, large language models have been
evolving very rapidly at the time of writing, yet the underlying transformer archi-
tecture and attention mechanism have remained largely unchanged for the last Ô¨Åve
years, while many core principles of machine learning have been known for decades v
vi PREFACE
Responsible use of technology
Deep learning is a powerful technology with broad applicability that has the po-
tential to create huge value for the world and address some of society‚Äôs most pressing
challenges However, these same attributes mean that deep learning also has poten-
tial both for deliberate misuse and to cause unintended harms We have chosen not
to discuss ethical or societal aspects of the use of deep learning, as these topics are of
such importance and complexity that they warrant a more thorough treatment than is
possible in a technical textbook such as this

============================================================

=== CHUNK 003 ===
Palavras: 350
Caracteres: 2244
--------------------------------------------------
Such considerations should, however,
be informed by a solid grounding in the underlying technology and how it works,
and so we hope that this book will make a valuable contribution towards these im-
portant discussions The reader is, nevertheless, strongly encouraged to be mindful
about the broader implications of their work and to learn about the responsible use
of deep learning and artiÔ¨Åcial intelligence alongside their studies of the technology
itself Structure of the book
The book is structured into a relatively large number of smaller bite-sized chap-
ters, each of which explores a speciÔ¨Åc topic The book has a linear structure in
the sense that each chapter depends only on material covered in earlier chapters It
is well suited to teaching a two-semester undergraduate or postgraduate course on
machine learning but is equally relevant to those engaged in active research or in
self-study A clear understanding of machine learning can be achieved only through the
use of some level of mathematics SpeciÔ¨Åcally, three areas of mathematics lie at the
heart of machine learning: probability theory, linear algebra, and multivariate cal-
culus The book provides a self-contained introduction to the required concepts in
probability theory and includes an appendix that summarizes some useful results in
linear algebra It is assumed that the reader already has some familiarity with the
basic concepts of multivariate calculus although there are appendices that provide
introductions to the calculus of variations and to Lagrange multipliers The focus
of the book, however, is on conveying a clear understanding of ideas, and the em-
phasis is on techniques that have real-world practical value rather than on abstract
theory Where possible we try to present more complex concepts from multiple com-
plementary perspectives including textual description, diagrams, and mathematical
formulae In addition, many of the key algorithms discussed in the text are summa-
rized in separate boxes These do not address issues of computational efÔ¨Åciency, but
are provided as a complement to the mathematical explanations given in the text We therefore hope that the material in this book will be accessible to readers from a
variety of backgrounds

============================================================

=== CHUNK 004 ===
Palavras: 351
Caracteres: 2189
--------------------------------------------------
Conceptually, this book is perhaps most naturally viewed as a successor to Neu-
ral Networks for Pattern Recognition (Bishop, 1995b), which provided the Ô¨Årst com-
prehensive treatment of neural networks from a statistical perspective It can also
be considered as a companion volume to Pattern Recognition and Machine Learn-
ing(Bishop, 2006), which covered a broader range of topics in machine learning
although it predated the deep learning revolution However, to ensure that this
PREF
ACE vii
new book is self-contained, appropriate material has been carried over from Bishop
(2006) and refactored to focus on those foundational ideas that are needed for deep
learning This means that there are many interesting topics in machine learning dis-
cussed in Bishop (2006) that remain of interest today but which have been omitted
from this new book For example, Bishop (2006) discusses Bayesian methods in
some depth, whereas this book is almost entirely non-Bayesian The book is accompanied by a web site that provides supporting material, in-
cluding a free-to-use digital version of the book as well as solutions to the exercises
and downloadable versions of the Ô¨Ågures in PDF and JPEG formats:
https://www.bishopbook.com
The book can be cited using the following BibTex entry:
@book{Bishop:DeepLearning24,
author = {Christopher M Bishop and Hugh Bishop},
title = {Deep Learning: Foundations and Concepts},
year = {2024},
publisher = {Springer}
}
If you have any feedback on the book or would like to report any errors, please
send these to feedback@bishopbook.com
References
In the spirit of focusing on core ideas, we make no attempt to provide a com-
prehensive literature review, which in any case would be impossible given the scale
and pace of change of the Ô¨Åeld We do, however, provide references to some of the
key research papers as well as review articles and other sources of further reading In many cases, these also provide important implementation details that we gloss
over in the text in order not to distract the reader from the central concepts being
discussed Many books have been written on the subject of machine learning in general and
on deep learning in particular

============================================================

=== CHUNK 005 ===
Palavras: 352
Caracteres: 2205
--------------------------------------------------
Those which are closest in level and style to this book
include Bishop (2006), Goodfellow, Bengio, and Courville (2016), Murphy (2022),
Murphy (2023), and Prince (2023) Over the last decade, the nature of machine learning scholarship has changed
signiÔ¨Åcantly, with many papers being posted online on archival sites ahead of, or
even instead of, submission to peer-reviewed conferences and journals The most
popular of these sites is arXiv, pronounced ‚Äòarchive‚Äô, and is available at
https://arXiv.org
The site allows papers to be updated, often leading to multiple versions associated
with different calendar years, which can result in some ambiguity as to which version
should be cited and for which year It also provides free access to a PDF of each pa-
per We have therefore adopted a simple approach of referencing the paper according
to the year of Ô¨Årst upload, although we recommend reading the most recent version viii PREFACE
Papers on arXiv are indexed using a notation arXiv:YYMM.XXXXX where YYand
MMdenote the year and month of Ô¨Årst upload, respectively Subsequent versions are
denoted by appending a version number Nin the form arXiv:YYMM.XXXXXvN Exercises
Each chapter concludes with a set of exercises designed to reinforce the key
ideas explained in the text or to develop and generalize them in signiÔ¨Åcant ways These exercises form an important part of the text and each is graded according to
difÔ¨Åculty ranging from (?), which denotes a simple exercise taking a few moments
to complete, through to (???), which denotes a signiÔ¨Åcantly more complex exercise The reader is strongly encouraged to attempt the exercises since active participation
with the material greatly increases the effectiveness of learning Worked solutions to
all of the exercises are available as a downloadable PDF Ô¨Åle from the book web site Mathematical notation
We follow the same notation as Bishop (2006) For an overview of mathematics
in the context of machine learning, see Deisenroth, Faisal, and Ong (2020) Vectors are denoted by lower case bold roman letters such as x, whereas matrices
are denoted by uppercase bold roman letters, such as M All vectors are assumed to
be column vectors unless otherwise stated

============================================================

=== CHUNK 006 ===
Palavras: 350
Caracteres: 2174
--------------------------------------------------
A superscript Tdenotes the transpose of a
matrix or vector, so that xTwill be a row vector The notation (w1;:::;wM)denotes
a row vector with Melements, and the corresponding column vector is written as
w= (w 1;:::;wM)T TheM√óMidentity matrix (also known as the unit matrix)
is denoted IM, which will be abbreviated to Iif there is no ambiguity about its
dimensionality It has elements Iijthat equal 1ifi=jand0ifi/negationslash=j The elements
of a unit matrix are sometimes denoted by ij The notation 1denotes a column
vector in which all elements have the value 1.a‚äïbdenotes the concatenation of
vectors aandb, so that if a= (a 1;:::;aN)andb= (b 1;:::;bM)thena‚äïb=
(a1;:::;aN;b1;:::;bM).|x|denotes the modulus (the positive part) of a scalar x,
also known as the absolute value We use detAto denote the determinant of a matrix
A The notation x‚àºp(x) signiÔ¨Åes that xis sampled from the distribution p(x) Where there is ambiguity, we will use subscripts as in px(¬∑)to denote which density
is referred to The expectation of a function f(x;y)with respect to a random variable
xis denoted by Ex[f(x;y)] In situations where there is no ambiguity as to which
variable is being averaged over, this will be simpliÔ¨Åed by omitting the sufÔ¨Åx, for
instance E[x] If the distribution of xis conditioned on another variable z, then
the corresponding conditional expectation will be written Ex[f(x)|z ] Similarly, the
variance of f(x)is denoted var[f (x)], and for vector variables, the covariance is
written cov[x; y] We will also use cov[x] as a shorthand notation for cov[x; x] The symbol‚àÄmeans ‚Äòfor all‚Äô, so that ‚àÄm‚ààM denotes all values of mwithin
the setM We use Rto denote the real numbers On a graph, the set of neighbours of
nodeiis denotedN(i), which should not be confused with the Gaussian or normal
distributionN(x|;2) A functional is denoted f[y]wherey(x)is some function The concept of a functional is discussed in Appendix B Curly braces {}denote a
PREFACE ix
set The notation g(x) =O(f(x))denotes that|f(x)=g(x)| is bounded as x‚Üí‚àû For instance, if g(x) = 3x2+ 2, theng(x) =O(x2) The notation‚åäx‚åãdenotes the
‚ÄòÔ¨Çoor‚Äô ofx, i.e., the largest integer that is less than or equal to x

============================================================

=== CHUNK 007 ===
Palavras: 353
Caracteres: 2267
--------------------------------------------------
If we haveNindependent and identically distributed (i.i.d.) values x1;:::;xN
of aD-dimensional vector x= (x 1;:::;xD)T, we can combine the observations
into a data matrix Xof dimension N√óDin which the nth row of Xcorresponds
to the row vector xT
n Thus, then;ielement of Xcorresponds to the ith element of
thenth observation xnand is written xni For one-dimensional variables, we denote
such a matrix by x, which is a column vector whose nth element is xn Note that
x(which has dimensionality N) uses a different typeface to distinguish it from x
(which has dimensionality D) Acknowledgements
We would like to express our sincere gratitude to the many people who re-
viewed draft chapters and provided valuable feedback In particular, we wish to
thank Samuel Albanie, Cristian Bodnar, John Bronskill, Wessel Bruinsma, Ignas
Budvytis, Chi Chen, Yaoyi Chen, Long Chen, Fergal Cotter, Sam Devlin, Alek-
sander Durumeric, Sebastian Ehlert, Katarina Elez, Andrew Foong, Hong Ge, Paul
Gladkov, Paula Gori Giorgi, John Gossman, Tengda Han, Juyeon Heo, Katja Hof-
mann, Chin-Wei Huang, Yongchaio Huang, Giulio Isacchini, Matthew Johnson,
Pragya Kale, Atharva Kelkar, Leon Klein, Pushmeet Kohli, Bonnie Kruft, Adrian
Li, Haiguang Liu, Ziheng Lu, Giulia Luise, Stratis Markou, Sergio Valcarcel Macua,
Krzysztof Maziarz, Mat Àáej Mezera, Laurence Midgley, Usman Munir, F ¬¥elix Musil,
Elise van der Pol, Tao Qin, Isaac Reid, David Rosenberger, Lloyd Russell, Maxi-
milian Schebek, Megan Stanley, Karin Strauss, Clark Templeton, Marlon Tobaben,
Aldo Sayeg Pasos-Trejo, Richard Turner, Max Welling, Furu Wei, Robert Weston,
Chris Williams, Yingce Xia, Shufang Xie, Iryna Zaporozhets, Claudio Zeni, Xieyuan
Zhang, and many other colleagues who contributed through valuable discussions We would also like to thank our editor Paul Drougas and many others at Springer, as
well as the copy editor Jonathan Webley, for their support during the production of
the book We would like to say a special thank-you to Markus Svens ¬¥en, who provided
immense help with the Ô¨Ågures and typesetting for Bishop (2006) including the L ATEX
style Ô¨Åles, which have also been used for this new book We are also grateful to the
many scientists who allowed us to reproduce diagrams from their published work

============================================================

=== CHUNK 008 ===
Palavras: 351
Caracteres: 2018
--------------------------------------------------
Acknowledgements for speciÔ¨Åc Ô¨Ågures appear in the associated Ô¨Ågure captions Chris would like to express sincere gratitude to Microsoft for creating a highly
stimulating research environment and for providing the opportunity to write this
book The views and opinions expressed in this book, however, are those of the
authors and are therefore not necessarily the same as those of Microsoft or its afÔ¨Ål-
iates It has been a huge privilege and pleasure to collaborate with my son Hugh in
preparing this book, which started as a joint project during the Ô¨Årst Covid lockdown x PREFACE
Hugh would like to thank Wayve Technologies Ltd for generously allowing him
to work part time so that he could collaborate in writing this book as well as for
providing an inspiring and supportive environment for him to work and learn in The views expressed in this book are not necessarily the same as those of Wayve or
its afÔ¨Åliates He would like to express his gratitude to his Ô¨Åanc ¬¥ee Jemima for her
constant support as well as her grammatical and stylistic consultations He would
also like to thank Chris, who has been an excellent colleague and an inspiration to
Hugh throughout his life Finally, we would both like to say a huge thank-you to our family members
Jenna and Mark for so many things far too numerous to list here It seems a very
long time ago that we all gathered on the beach in Antalya to watch a total eclipse
of the sun and to take a family photo for the dedication page of Pattern Recognition
and Machine Learning Chris Bishop and Hugh Bishop
Cambridge, UK
October, 2023
Contents
Pr
eface v
Contents xi
1 The
Deep Learning Revolution 1
1.1 The Impact of Deep Learning 2
1.1.1 Medical diagnosis 2
1.1.2 Protein structure 3
1.1.3 Image synthesis 4
1.1.4 Large language models 5
1.2 A Tutorial Example 6
1.2.1 Synthetic data 6
1.2.2 Linear models 8
1.2.3 Error function 8
1.2.4 Model complexity 9
1.2.5 Regularization 12
1.2.6 Model selection 14
1.3 A Brief History of Machine Learning 16
1.3.1 Single-layer networks

============================================================

=== CHUNK 009 ===
Palavras: 351
Caracteres: 2418
--------------------------------------------------
17
1.3.2 Backpropagation 18
1.3.3 Deep networks 20
2 Probabilities 23
2.1 The Rules of Probability 25
2.1.1 A medical screening example 25
2.1.2 The sum and product rules 26
2.1.3 Bayes‚Äô theorem 28
2.1.4 Medical screening revisited 30
2.1.5 Prior and posterior probabilities 31
xi
xii CONTENTS
2.1.6 Independent
variables 31
2.2 Probability Densities 32
2.2.1 Example distributions 33
2.2.2 Expectations and covariances 34
2.3 The Gaussian Distribution 36
2.3.1 Mean and variance 37
2.3.2 Likelihood function 37
2.3.3 Bias of maximum likelihood 39
2.3.4 Linear regression 40
2.4 Transformation of Densities 42
2.4.1 Multivariate distributions 44
2.5 Information Theory 46
2.5.2 Physics perspective 47
2.5.3 Differential entropy 49
2.5.4 Maximum entropy 50
2.5.5 Kullback‚ÄìLeibler divergence 51
2.5.6 Conditional entropy 53
2.5.7 Mutual information 54
2.6 Bayesian Probabilities 54
2.6.1 Model parameters 55
2.6.2 Regularization 56
2.6.3 Bayesian machine learning 58
3 Standard Distributions 65
3.1 Discrete Variables 66
3.1.1 Bernoulli distribution 66
3.1.2 Binomial distribution 67
3.1.3 Multinomial distribution 68
3.2 The Multivariate Gaussian 70
3.2.1 Geometry of the Gaussian 75
3.2.4 Conditional distribution 76
3.2.5 Marginal distribution 79
3.2.6 Bayes‚Äô theorem 81
3.2.7 Maximum likelihood 84
3.2.8 Sequential estimation 85
3.2.9 Mixtures of Gaussians 86
3.3 Periodic Variables 89
3.3.1 V on Mises distribution 89
3.4 The Exponential Family 94
3.4.1 SufÔ¨Åcient statistics 97
3.5 Nonparametric Methods 98
CONTENTS xiii
3.5.1
Histograms 98
3.5.2 Kernel densities 100
3.5.3 Nearest-neighbours 105
4 Single-layer Networks: Regression 111
4.1 Linear Regression 112
4.1.1 Basis functions 112
4.1.2 Likelihood function 114
4.1.3 Maximum likelihood 115
4.1.4 Geometry of least squares 117
4.1.5 Sequential learning 117
4.1.6 Regularized least squares 118
4.1.7 Multiple outputs 119
4.2 Decision theory 120
4.3 The Bias‚ÄìVariance Trade-off 128
5 Single-layer Networks: ClassiÔ¨Åcation 131
5.1 Discriminant Functions 132
5.1.1 Two classes 132
5.1.2 Multiple classes 134
5.1.3 1-of-K coding 135
5.1.4 Least squares for classiÔ¨Åcation 136
5.2 Decision Theory 138
5.2.1 MisclassiÔ¨Åcation rate 139
5.2.2 Expected loss 140
5.2.3 The reject option 142
5.2.4 Inference and decision 143
5.2.5 ClassiÔ¨Åer accuracy 148
5.3 Generative ClassiÔ¨Åers 150
5.3.1 Continuous inputs 152
5.3.2 Maximum likelihood solution 153
5.3.3 Discrete features

============================================================

=== CHUNK 010 ===
Palavras: 351
Caracteres: 2525
--------------------------------------------------
156
5.3.4 Exponential family 156
5.4 Discriminative ClassiÔ¨Åers 157
5.4.1 Activation functions 158
5.4.2 Fixed basis functions 158
5.4.3 Logistic regression 159
5.4.4 Multi-class logistic regression 161
5.4.5 Probit regression 163
5.4.6 Canonical link functions 166
xiv CONTENTS
6
Deep Neural Networks 171
6.1 Limitations of Fixed Basis Functions 172
6.1.1 The curse of dimensionality 172
6.1.2 High-dimensional spaces 175
6.1.3 Data manifolds 176
6.1.4 Data-dependent basis functions 178
6.2 Multilayer Networks 180
6.2.1 Parameter matrices 181
6.2.2 Universal approximation 181
6.2.3 Hidden unit activation functions 182
6.2.4 Weight-space symmetries 185
6.3 Deep Networks 186
6.3.1 Hierarchical representations 187
6.3.2 Distributed representations 187
6.3.3 Representation learning 188
6.3.4 Transfer learning 189
6.3.5 Contrastive learning 191
6.3.6 General network architectures 194
6.4 Error Functions 194
6.4.2 Binary classiÔ¨Åcation 196
6.4.3 multiclass classiÔ¨Åcation 197
6.5 Mixture Density Networks 198
6.5.1 Robot kinematics example 198
6.5.2 Conditional mixture distributions 199
6.5.3 Gradient optimization 201
6.5.4 Predictive distribution 204
7 Gradient Descent 209
7.1 Error Surfaces 210
7.1.1 Local quadratic approximation 211
7.2 Gradient Descent Optimization 213
7.2.1 Use of gradient information 214
7.2.2 Batch gradient descent 214
7.2.3 Stochastic gradient descent 214
7.2.4 Mini-batches 216
7.2.5 Parameter initialization 220
7.3.2 Learning rate schedule 222
7.3.3 RMSProp and Adam 223
7.4 Normalization 224
7.4.1 Data normalization 226
CONTENTS xv
7.4.2
Batch normalization 227
7.4.3 Layer
normalization 230
8 Backpropagation 233
8.1 Evaluation of Gradients 234
8.1.1 Single-layer networks 234
8.1.2 General feed-forward networks 235
8.1.3 A simple example 238
8.1.4 Numerical differentiation 239
8.1.5 The Jacobian matrix 240
8.1.6 The Hessian matrix 242
8.2 Automatic Differentiation 244
8.2.1 Forward-mode automatic differentiation 246
8.2.2 Reverse-mode automatic differentiation 250
9 Regularization 253
9.1 Inductive Bias 254
9.1.1 Inverse problems 254
9.1.2 No free lunch theorem 255
9.1.3 Symmetry and invariance 256
9.1.4 Equivariance 260
9.2.1 Consistent regularizers 262
9.2.2 Generalized weight decay 264
9.3 Learning Curves 266
9.3.1 Early stopping 266
9.3.2 Double descent 268
9.4 Parameter Sharing 270
9.4.1 Soft weight sharing 271
9.5 Residual Connections 274
9.6 Model Averaging 281
10 Convolutional Networks 287
10.1 Computer Vision 288
10.1.1 Image data 289
10.2 Convolutional Filters

============================================================

=== CHUNK 011 ===
Palavras: 353
Caracteres: 2619
--------------------------------------------------
290
10.2.1 Feature detectors 290
10.2.2 Translation equivariance 294
10.2.4 Strided convolutions 294
10.2.5 Multi-dimensional convolutions 296
xvi CONTENTS
10.2.7
Multilayer convolutions 298
10.2.8 Example network architectures 299
10.3 Visualizing Trained CNNs 302
10.3.1 Visual cortex 302
10.3.2 Visualizing trained Ô¨Ålters 303
10.3.3 Saliency maps 305
10.3.4 Adversarial attacks 306
10.3.5 Synthetic images 308
10.4 Object Detection 308
10.4.1 Bounding boxes 309
10.4.2 Intersection-over-union 310
10.4.3 Sliding windows 311
10.4.4 Detection across scales 313
10.4.5 Non-max suppression 314
10.4.6 Fast region CNNs 314
10.5 Image Segmentation 315
10.5.1 Convolutional segmentation 315
10.5.2 Up-sampling 316
10.5.3 Fully convolutional networks 318
10.5.4 The U-net architecture 319
10.6 Style Transfer 322
11 Structured Distributions 325
11.1 Graphical Models 326
11.1.1 Directed graphs 326
11.1.2 Factorization 327
11.1.3 Discrete variables 329
11.1.4 Gaussian variables 332
11.1.5 Binary classiÔ¨Åer 334
11.1.6 Parameters and observations 334
11.1.7 Bayes‚Äô theorem 336
11.2 Conditional Independence 337
11.2.1 Three example graphs 338
11.2.2 Explaining away 341
11.2.3 D-separation 343
11.2.4 Naive Bayes 344
11.2.5 Generative models 346
11.2.6 Markov blanket 347
11.2.7 Graphs as Ô¨Ålters 348
11.3 Sequence Models 349
11.3.1 Hidden variables 353
CONTENTS xvii
12
Transformers 357
12.1 Attention 358
12.1.1 Transformer processing 360
12.1.2 Attention coefÔ¨Åcients 361
12.1.3 Self-attention 362
12.1.4 Network parameters 363
12.1.5 Scaled self-attention 366
12.1.6 Multi-head attention 366
12.1.7 Transformer layers 368
12.1.8 Computational complexity 370
12.1.9 Positional encoding 371
12.2 Natural Language 374
12.2.1 Word embedding 375
12.2.2 Tokenization 377
12.2.3 Bag of words 378
12.2.4 Autoregressive models 379
12.2.5 Recurrent neural networks 380
12.2.6 Backpropagation through time 381
12.3 Transformer Language Models 382
12.3.1 Decoder transformers 383
12.3.2 Sampling strategies 386
12.3.3 Encoder transformers 388
12.3.4 Sequence-to-sequence transformers 390
12.3.5 Large language models 390
12.4 Multimodal Transformers 394
12.4.1 Vision transformers 395
12.4.2 Generative image transformers 396
12.4.3 Audio data 399
12.4.4 Text-to-speech 400
12.4.5 Vision and language transformers 403
13 Graph Neural Networks 407
13.1 Machine Learning on Graphs 409
13.1.1 Graph properties 410
13.1.2 Adjacency matrix 410
13.1.3 Permutation equivariance 411
13.2 Neural Message-Passing 412
13.2.1 Convolutional Ô¨Ålters 413
13.2.2 Graph convolutional networks 414
13.2.3 Aggregation operators 416
13.2.4 Update operators

============================================================

=== CHUNK 012 ===
Palavras: 358
Caracteres: 2592
--------------------------------------------------
418
13.2.5 Node classiÔ¨Åcation 419
13.2.6 Edge classiÔ¨Åcation 420
13.2.7 Graph classiÔ¨Åcation 420
xviii CONTENTS
13.3 General
Graph Networks 420
13.3.1 Graph attention networks 421
13.3.2 Edge embeddings 421
13.3.3 Graph embeddings 422
13.3.4 Over-smoothing 422
13.3.5 Regularization 423
13.3.6 Geometric deep learning 425
14 Sampling 429
14.1 Basic Sampling Algorithms 430
14.1.1 Expectations 430
14.1.2 Standard distributions 431
14.1.3 Rejection sampling 433
14.1.4 Adaptive rejection sampling 435
14.1.5 Importance sampling 437
14.1.6 Sampling-importance-resampling 439
14.2 Markov Chain Monte Carlo 440
14.2.1 The Metropolis algorithm 441
14.2.2 Markov chains 442
14.2.3 The Metropolis‚ÄìHastings algorithm 445
14.2.4 Gibbs sampling 446
14.2.5 Ancestral sampling 450
14.3 Langevin Sampling 451
14.3.1 Energy-based models 452
14.3.2 Maximizing the likelihood 453
14.3.3 Langevin dynamics 456
15 Discrete Latent Variables 459
15.1K-means Clustering 460
15.1.1 Image segmentation 464
15.2 Mixtures of Gaussians 466
15.2.1 Likelihood function 468
15.2.2 Maximum likelihood 470
15.3 Expectation‚ÄìMaximization Algorithm 474
15.3.1 Gaussian mixtures 478
15.3.2 Relation to K-means 480
15.3.3 Mixtures of Bernoulli distributions 481
15.4 Evidence Lower Bound 485
15.4.1 EM revisited 486
15.4.2 Independent and identically distributed data 488
15.4.3 Parameter priors 489
15.4.4 Generalized EM 489
15.4.5 Sequential EM 490
CONTENTS xix
16
Continuous Latent Variables 495
16.1 Principal Component Analysis 497
16.1.1 Maximum variance formulation 497
16.1.2 Minimum-error formulation 499
16.1.3 Data compression 501
16.1.4 Data whitening 502
16.1.5 High-dimensional data 504
16.2 Probabilistic Latent Variables 506
16.2.1 Generative model 506
16.2.2 Likelihood function 507
16.2.3 Maximum likelihood 509
16.2.4 Factor analysis 513
16.2.5 Independent component analysis 514
16.2.6 Kalman Ô¨Ålters 515
16.3 Evidence Lower Bound 516
16.3.1 Expectation maximization 518
16.3.2 EM for PCA 519
16.3.3 EM for factor analysis 520
16.4 Nonlinear Latent Variable Models 522
16.4.1 Nonlinear manifolds 522
16.4.2 Likelihood function 524
16.4.3 Discrete data 526
16.4.4 Four approaches to generative modelling 527
17 Generative Adversarial Networks 533
17.1 Adversarial Training 534
17.1.1 Loss function 535
17.1.2 GAN training in practice 544
18 Normalizing Flows 547
18.1 Coupling Flows 549
18.2 Autoregressive Flows 552
18.3 Continuous Flows 554
18.3.1 Neural differential equations 554
18.3.2 Neural ODE backpropagation 555
18.3.3 Neural ODE Ô¨Çows 559
xx CONTENTS
19
Autoencoders 563
19.1 Deterministic Autoencoders

============================================================

=== CHUNK 013 ===
Palavras: 372
Caracteres: 2502
--------------------------------------------------
564
19.1.1 Linear autoencoders 564
19.1.2 Deep autoencoders 565
19.1.3 Sparse autoencoders 566
19.1.4 Denoising autoencoders 567
19.1.5 Masked autoencoders 567
19.2 Variational Autoencoders 569
19.2.1 Amortized inference 572
19.2.2 The reparameterization trick 578
20 Diffusion Models 581
20.1 Forward Encoder 582
20.1.1 Diffusion kernel 583
20.1.2 Conditional distribution 584
20.2 Reverse Decoder 585
20.2.1 Training the decoder 587
20.2.2 Evidence lower bound 588
20.2.3 Rewriting the ELBO 589
20.2.4 Predicting the noise 591
20.2.5 Generating new samples 592
20.3 Score Matching 594
20.3.1 Score loss function 595
20.3.2 ModiÔ¨Åed score loss 596
20.3.3 Noise variance 597
20.3.4 Stochastic differential equations 598
20.4 Guided Diffusion 599
20.4.1 ClassiÔ¨Åer guidance 600
20.4.2 ClassiÔ¨Åer-free guidance 603
Appendix A Linear Algebra 609
A.1 Matrix Identities 609
A.2 Traces and Determinants 610
A.3 Matrix Derivatives 612
Appendix B Calculus of Variations 617
Appendix C Lagrange Multipliers 621
Bibliography 625
Index 641
1
The Deep
Learning
Revolution
Machine learning today is one of the most important, and fastest growing, Ô¨Åelds
of technology Applications of machine learning are becoming ubiquitous, and so-
lutions learned from data are increasingly displacing traditional hand-crafted algo-
rithms This has not only led to improved performance for existing technologies but
has opened the door to a vast range of new capabilities that would be inconceivable
if new algorithms had to be designed explicitly by hand One particular branch of machine learning, known as deep learning, has emerged
as an exceptionally powerful and general-purpose framework for learning from data Deep learning is based on computational models called neural networks which were
originally inspired by mechanisms of learning and information processing in the hu-
man brain The Ô¨Åeld of artiÔ¨Åcial intelligence, or AI, seeks to recreate the powerful
capabilities of the brain in machines, and today the terms machine learning and AI
are often used interchangeably Many of the AI systems in current use represent ap-
1 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_1    
2 1 THE DEEP LEARNING REVOLUTION
plications of machine learning which are designed to solve very speciÔ¨Åc and focused
problems, and while these are extremely useful they fall far short of the tremendous
breadth of capabilities of the human brain

============================================================

=== CHUNK 014 ===
Palavras: 363
Caracteres: 2267
--------------------------------------------------
This has led to the introduction of the
term artiÔ¨Åcial general intelligence, or AGI, to describe the aspiration of building
machines with this much greater Ô¨Çexibility After many decades of steady progress,
machine learning has now entered a phase of very rapid development Recently,
massive deep learning systems called large language models have started to exhibit Chapter 12
remarkable capabilities that have been described as the Ô¨Årst indications of artiÔ¨Åcial
general intelligence (Bubeck et al., 2023) The Impact of Deep Learning
We begin our discussion of machine learning by considering four examples drawn
from diverse Ô¨Åelds to illustrate the huge breadth of applicability of this technology
and to introduce some basic concepts and terminology What is particularly remark-
able about these and many other examples is that they have all been addressed using
variants of the same fundamental framework of deep learning This is in sharp con-
trast to conventional approaches in which different applications are tackled using
widely differing and specialist techniques It should be emphasized that the exam-
ples we have chosen represent only a tiny fraction of the breadth of applicability for
deep neural networks and that almost every domain where computation has a role is
amenable to the transformational impact of deep learning 1.1.1 Medical diagnosis
Consider Ô¨Årst the application of machine learning to the problem of diagnosing
skin cancer Melanoma is the most dangerous kind of skin cancer but is curable
if detected early Figure 1.1 shows example images of skin lesions, with malig-
nant melanomas on the top row and benign nevi on the bottom row Distinguishing
between these two classes of image is clearly very challenging, and it would be vir-
tually impossible to write an algorithm by hand that could successfully classify such
images with any reasonable level of accuracy This problem has been successfully addressed using deep learning (Esteva et
al., 2017) The solution was created using a large set of lesion images, known as
Figure 1.1 Examples of skin lesions cor-
responding to dangerous ma-
lignant melanomas on the top
row and benign nevi on the bot-
tom row It is difÔ¨Åcult for the
untrained eye to distinguish be-
tween these two classes

============================================================

=== CHUNK 015 ===
Palavras: 376
Caracteres: 2243
--------------------------------------------------
The Impact of Deep Learning 3
atraining set, each of which is labelled as either malignant or benign, where the
labels are obtained from a biopsy test that is considered to provide the true class
of the lesion The training set is used to determine the values of some 25 million
adjustable parameters, known as weights, in a deep neural network This process of
setting the parameter values from data is known as learning ortraining The goal
is for the trained network to predict the correct label for a new lesion just from the
image alone without needing the time-consuming step of taking a biopsy This is an
example of a supervised learning problem because, for each training example, the
network is told the correct label It is also an example of a classiÔ¨Åcation problem
because each input must be assigned to a discrete set of classes (benign or malignant
in this case) Applications in which the output consists of one or more continuous
variables are called regression problems An example of a regression problem would
be the prediction of the yield in a chemical manufacturing process in which the inputs
consist of the temperature, the pressure, and the concentrations of reactants An interesting aspect of this application is that the number of labelled training
images available, roughly 129,000, is considered relatively small, and so the deep
neural network was Ô¨Årst trained on a much larger data set of 1.28 million images of
everyday objects (such as dogs, buildings, and mushrooms) and then Ô¨Åne-tuned on
the data set of lesion images This is an example of transfer learning in which the
network learns the general properties of natural images from the large data set of
everyday objects and is then specialized to the speciÔ¨Åc problem of lesion classiÔ¨Åca-
tion Through the use of deep learning, the classiÔ¨Åcation of skin lesion images has
reached a level of accuracy that exceeds that of professional dermatologists (Brinker
et al., 2019) 1.1.2 Protein structure
Proteins are sometimes called the building blocks of living organisms They are
biological molecules that consist of one or more long chains of units called amino
acids, of which there are 22 different types, and the protein is speciÔ¨Åed by the se-
quence of amino acids

============================================================

=== CHUNK 016 ===
Palavras: 368
Caracteres: 2294
--------------------------------------------------
Once a protein has been synthesized inside a living cell, it
folds into a complex three-dimensional structure whose behaviour and interactions
are strongly determined by its shape Calculating this 3D structure, given the amino
acid sequence, has been a fundamental open problem in biology for half a century
that had seen relatively little progress until the advent of deep learning The 3D structure can be measured experimentally using techniques such as X-
ray crystallography, cryogenic electron microscopy, or nuclear magnetic resonance
spectroscopy However, this can be extremely time-consuming and for some pro-
teins can prove to be challenging, for example due to the difÔ¨Åculty of obtaining a
pure sample or because the structure is dependent on the context In contrast, the
amino acid sequence of a protein can be determined experimentally at lower cost
and higher throughput Consequently, there is considerable interest in being able
to predict the 3D structures of proteins directly from their amino acid sequences in
order to better understand biological processes or for practical applications such as
drug discovery A deep learning model can be trained to take an amino acid se-
quence as input and generate the 3D structure as output, in which the training data
4 1 THE DEEP LEARNING REVOLUTION
Figure 1.2 Illustration of the 3D shape of a pro-
tein called T1044/6VR4 The green
structure shows the ground truth
as determined by X-ray crystallog-
raphy, whereas the superimposed
blue structure shows the prediction
obtained by a deep learning model
called AlphaFold [From Jumper et
al.(2021) with permission.]
consist of a set of proteins for which the amino acid sequence and the 3D structure
are both known Protein structure prediction is therefore another example of super-
vised learning Once the system is trained it can take a new amino acid sequence as
input and can predict the associated 3D structure (Jumper et al., 2021) Figure 1.2
compares the predicted 3D structure of a protein and the ground truth obtained by
X-ray crystallography 1.1.3 Image synthesis
In the two applications discussed so far, a neural network learned to transform
an input (a skin image or an amino acid sequence) into an output (a lesion classiÔ¨Åca-
tion or a 3D protein structure, respectively)

============================================================

=== CHUNK 017 ===
Palavras: 373
Caracteres: 2206
--------------------------------------------------
We turn now to an example where the
training data consist simply of a set of sample images and the goal of the trained net-
work is to create new images of the same kind This is an example of unsupervised
learning because the images are unlabelled, in contrast to the lesion classiÔ¨Åcation
and protein structure examples Figure 1.3 shows examples of synthetic images gen-
erated by a deep neural network trained on a set of images of human faces taken in a
studio against a plain background Such synthetic images are of exceptionally high
quality and it can be difÔ¨Åcult tell them apart from photographs of real people This is an example of a generative model because it can generate new output
examples that differ from those used to train the model but which share the same
statistical properties A variant of this approach allows images to be generated that
depend on an input text string known, as a prompt, so that the image content reÔ¨Çects
the semantics of the text input The term generative AI is used to describe deep learn- Chapter 10
ing models that generate outputs in the form of images, video, audio, text, candidate
drug molecules, or other modalities The Impact of Deep Learning 5
Figure 1.3 Synthetic face images generated by a deep neural network trained using unsupervised learning [From https://generated.photos .]
1.1.4 Large language models
One of most important advances in machine learning in recent years has been
the development of powerful models for processing natural language and other forms
of sequential data such as source code A large language model, or LLM, uses deep
learning to build rich internal representations that capture the semantic properties
of language An important class of large language models, called autoregressive
language models, can generate language as output, and therefore, they are a form of
generative AI Such models take a sequence of words as the input and for the output,
generate a single word that represents the next word in the sequence The augmented
sequence, with the new word appended at the end, can then be fed through the model
again to generate the subsequent word, and this process can be repeated to generate
a long sequence of words

============================================================

=== CHUNK 018 ===
Palavras: 388
Caracteres: 2232
--------------------------------------------------
Such models can also output a special ‚Äòstop‚Äô word that
signals the end of text generation, thereby allowing them to output text of Ô¨Ånite
length and then halt At that point, a user could append their own series of words
to the sequence before feeding the complete sequence back through the model to
trigger further word generation In this way, it is possible for a human to have a
conversation with the neural network Such models can be trained on large data sets of text by extracting training pairs
each consisting of a randomly selected sequence of words as input with the known
next word as the target output This is an example of self-supervised learning in
which a function from inputs to outputs is learned but where the labelled outputs are
obtained automatically from the input training data without needing separate human-
6 1 THE DEEP LEARNING REVOLUTION
Figure 1.4 Plot of a training data set of N=
10points, shown as blue circles,
each comprising an observation
of the input variable xalong with
the corresponding target variable
t The green curve shows the
function sin(2x) used to gener-
ate the data Our goal is to pre-
dict the value of tfor some new
value ofx, without knowledge of
the green curve 0 1x‚àí11
t
deri
ved labels Since large volumes of text are available from multiple sources, this
approach allows for scaling to very large training sets and associated very large neu-
ral networks Large language models can exhibit extraordinary capabilities that have been de-
scribed as the Ô¨Årst indications of emerging artiÔ¨Åcial general intelligence (Bubeck et
al., 2023), and we discuss such models at length later in the book On the next page, Chapter 12
we give an illustration of language generation, based on a model called GPT-4 (Ope-
nAI, 2023), in response to an input prompt ‚ÄòWrite a proof of the fact that there are
inÔ¨Ånitely many primes; do it in the style of a Shakespeare play through a dialogue
between two parties arguing over the proof.‚Äô A
Tutorial Example
F
or the newcomer to the Ô¨Åeld of machine learning, many of the basic concepts and
much of the terminology can be introduced in the context of a simple example in-
volving the Ô¨Åtting of a polynomial to a small synthetic data set (Bishop, 2006)

============================================================

=== CHUNK 019 ===
Palavras: 371
Caracteres: 2062
--------------------------------------------------
This
is a form of supervised learning problem in which we would like to make a prediction
for a target variable, given the value of an input variable 1.2.1 Synthetic data
We denote the input variable by xand the target variable by t, and we assume
that both variables take continuous values on the real axis Suppose that we are
given a training set comprising Nobservations of x, writtenx1;:::;xN, together
with corresponding observations of the values of t, denotedt1;:::;tN Our goal
is to predict the value of tfor some new value of x The ability to make accurate
predictions on previously unseen inputs is a key goal in machine learning and is
known as generalization We can illustrate this using a synthetic data set generated by sampling from a
sinusoidal function Figure 1.4 shows a plot of a training set comprising N= 10 data
points in which the input values were generated by choosing values of xn, forn=
1;:::;N , spaced uniformly in the range [0;1] The associated target data values were
obtained by Ô¨Årst computing the values of the function sin(2x )for each value of x
1.2 A Tutorial Example 7
GPT -4: Proof of inÔ¨Ånitely many primes
FIRST PARTY:
Thou dost not understand, my friend,
There simply is no upper end
To primes, those numbers indivisible,
No matter if it seems impossible SECOND PARTY:
But how dost thou so surely know To prove this truth, what canst thou show FIRST PARTY:
Consider this, my doubtful peer,
A clever proof that will make clear:
Assume that there‚Äôs a final prime,
The largest one we‚Äôll see in time Let‚Äôs call this number "P" for short,
And then, my friend, we will purport
To build a number "Q" that‚Äôs new,
One larger than "P" and prime too SECOND PARTY:
But how, my friend, will this be done The proof is not yet fairly won FIRST PARTY:
It‚Äôs simple, friend, just wait and see,
We‚Äôll multiply the primes, that‚Äôs key All primes from two to "P" combined,
Will make this new "Q" we will find But one thing more we must include,
To make the proof more watertight,
We‚Äôll add a one onto the end,
And now the proof is in our sight

============================================================

=== CHUNK 020 ===
Palavras: 358
Caracteres: 2190
--------------------------------------------------
SECOND PARTY:
I see it now, it‚Äôs crystal clear,
For "Q" must be a prime, my dear It can‚Äôt divide by any prime,
All smaller than itself in time And since we chose "P" as the last,
This proves that our assumption‚Äôs past There is no final prime, we see,
The numbers go on endlessly THE DEEP LEARNING REVOLUTION
and then adding a small level of random noise (governed by a Gaussian distribution) Section 2.3
to each such point to obtain the corresponding target value tn By generating data
in this way, we are capturing an important property of many real-world data sets,
namely that they possess an underlying regularity, which we wish to learn, but that
individual observations are corrupted by random noise This noise might arise from
intrinsically stochastic (i.e., random) processes such as radioactive decay but more
typically is due to there being sources of variability that are themselves unobserved In this tutorial example we know the true process that generated the data, namely
the sinusoidal function In a practical application of machine learning, our goal is to
discover the underlying trends in the data given the Ô¨Ånite training set Knowing the
process that generated the data, however, allows us to illustrate important concepts
in machine learning 1.2.2 Linear models
Our goal is to exploit this training set to predict the value /hatwidetof the target variable
for some new value /hatwidexof the input variable As we will see later, this involves im-
plicitly trying to discover the underlying function sin(2x ) This is intrinsically a
difÔ¨Åcult problem as we have to generalize from a Ô¨Ånite data set to an entire function Furthermore, the observed data is corrupted with noise, and so for a given /hatwidexthere
is uncertainty as to the appropriate value for /hatwidet.Probability theory provides a frame- Chapter 2
work for expressing such uncertainty in a precise and quantitative manner, whereas
decision theory allows us to exploit this probabilistic representation to make predic- Chapter 5
tions that are optimal according to appropriate criteria Learning probabilities from
data lies at the heart of machine learning and will be explored in great detail in this
book

============================================================

=== CHUNK 021 ===
Palavras: 368
Caracteres: 2255
--------------------------------------------------
To start with, however, we will proceed rather informally and consider a simple
approach based on curve Ô¨Åtting In particular, we will Ô¨Åt the data using a polynomial
function of the form
y(x;w) =w0+w1x+w2x2+:::+wMxM=M/summationdisplay
j=0wjxj(1.1)
whereMis the order of the polynomial, and xjdenotesxraised to the power of j The polynomial coefÔ¨Åcients w0;:::;wMare collectively denoted by the vector w Note that, although the polynomial function y(x;w)is a nonlinear function of x, it
is a linear function of the coefÔ¨Åcients w Functions, such as this polynomial, that are
linear in the unknown parameters have important properties, as well as signiÔ¨Åcant
limitations, and are called linear models Chapter 4
1.2.3 Error function
The values of the coefÔ¨Åcients will be determined by Ô¨Åtting the polynomial to the
training data This can be done by minimizing an error function that measures the
misÔ¨Åt between the function y(x;w), for any given value of w, and the training set
data points One simple choice of error function, which is widely used, is the sum of
1.2 A Tutorial Example 9
Figure 1.5 The error function (1.2) cor-
responds to (one half of)
the sum of the squares of
the displacements (shown
by the vertical green arrows)
of each data point from the
functiony(x;w) xn
tn
y(xn,w)
xt
the
squares of the differences between the predictions y(xn;w)for each data point
xnand the corresponding target value tn, given by
E(w) =1
2N/summationdisplay
n
=1{y(xn;w)‚àítn}2(1.2)
where the factor of 1=2is included for later convenience We will later derive this
error function starting from probability theory Here we simply note that it is a non- Section 2.3.4
negative quantity that would be zero if, and only if, the function y(x;w)were to
pass exactly through each training data point The geometrical interpretation of the
sum-of-squares error function is illustrated in Figure 1.5 We can solve the curve Ô¨Åtting problem by choosing the value of wfor which
E(w)is as small as possible Because the error function is a quadratic function of
the coefÔ¨Åcients w, its derivatives with respect to the coefÔ¨Åcients will be linear in the
elements of w, and so the minimization of the error function has a unique solution,
denoted by w?, which can be found in closed form

============================================================

=== CHUNK 022 ===
Palavras: 387
Caracteres: 2229
--------------------------------------------------
The resulting polynomial is Exercise 4.1
given by the function y(x;w?) 1.2.4 Model complexity
There remains the problem of choosing the order Mof the polynomial, and as
we will see this will turn out to be an example of an important concept called model
comparison ormodel selection In Figure 1.6, we show four examples of the results
of Ô¨Åtting polynomials having orders M= 0;1;3, and 9to the data set shown in
Figure 1.4 Notice that the constant (M = 0) and Ô¨Årst-order (M = 1) polynomials give poor
Ô¨Åts to the data and consequently poor representations of the function sin(2x ) The
third-order (M = 3) polynomial seems to give the best Ô¨Åt to the function sin(2x )of
the examples shown in Figure 1.6 When we go to a much higher order polynomial
(M= 9), we obtain an excellent Ô¨Åt to the training data In fact, the polynomial
10 1 THE DEEP LEARNING REVOLUTION
0 1x‚àí11
t
M= 0
0 1x‚àí11
t
M= 1
0 1x‚àí11
t
M= 3
0 1x‚àí11
t
M= 9
Figure
1.6 Plots of polynomials having various orders M, shown as red curves, Ô¨Åtted to the data set shown in
Figure 1.4 by minimizing the error function (1.2) passes exactly through each data point and E(w?) = 0 However, the Ô¨Åtted curve
oscillates wildly and gives a very poor representation of the function sin(2x ) This
latter behaviour is known as over-Ô¨Åtting Our goal is to achieve good generalization by making accurate predictions for
new data We can obtain some quantitative insight into the dependence of the gener-
alization performance on Mby considering a separate set of data known as a test set,
comprising 100data points generated using the same procedure as used to generate
the training set points For each value of M, we can evaluate the residual value of
E(w?)given by (1.2) for the training data, and we can also evaluate E(w?)for the
test data set Instead of evaluating the error function E(w), it is sometimes more
convenient to use the root-mean-square (RMS) error deÔ¨Åned by
ERMS=/radicaltp/radicalvertex/radicalvertex/radicalbt1
NN/summationdisplay
n
=1{y(xn;w)‚àítn}2(1.3)
in which the division by Nallows us to compare different sizes of data sets on an
equal footing, and the square root ensures that ERMS is measured on the same scale
(and in the same units) as the target variable t

============================================================

=== CHUNK 023 ===
Palavras: 358
Caracteres: 2074
--------------------------------------------------
Graphs of the training-set and test-set
1.2 A Tutorial Example 11
Figure 1.7 Graphs of the root-mean-
square error, deÔ¨Åned by (1.3), evaluated on
the training set, and on an independent test
set, for various values of M 0 3 6 9
M01ERMSTraining
Test
RMS
errors are shown, for various values of M, inFigure 1.7 The test set error
is a measure of how well we are doing in predicting the values of tfor new data
observations of x Note from Figure 1.7 that small values of Mgive relatively large
values of the test set error, and this can be attributed to the fact that the corresponding
polynomials are rather inÔ¨Çexible and are incapable of capturing the oscillations in
the function sin(2x ) Values of Min the range 36M68give small values
for the test set error, and these also give reasonable representations of the generating
function sin(2x ), as can be seen for M= 3inFigure 1.6 ForM= 9, the training set error goes to zero, as we might expect because
this polynomial contains 10degrees of freedom corresponding to the 10coefÔ¨Åcients
w0;:::;w 9, and so can be tuned exactly to the 10data points in the training set However, the test set error has become very large and, as we saw in Figure 1.6, the
corresponding function y(x;w?)exhibits wild oscillations This may seem paradoxical because a polynomial of a given order contains all
lower-order polynomials as special cases The M= 9 polynomial is therefore ca-
pable of generating results at least as good as the M= 3polynomial Furthermore,
we might suppose that the best predictor of new data would be the function sin(2x )
from which the data was generated (and we will see later that this is indeed the case) We know that a power series expansion of the function sin(2x )contains terms of all
orders, so we might expect that results should improve monotonically as we increase
M We can gain some insight into the problem by examining the values of the co-
efÔ¨Åcients w?obtained from polynomials of various orders, as shown in Table 1.1 We see that, as Mincreases, the magnitude of the coefÔ¨Åcients typically gets larger

============================================================

=== CHUNK 024 ===
Palavras: 388
Caracteres: 2273
--------------------------------------------------
In particular for the M= 9 polynomial, the coefÔ¨Åcients have become Ô¨Ånely tuned
to the data They have large positive and negative values so that the corresponding
polynomial function matches each of the data points exactly, but between data points
(particularly near the ends of the range) the function exhibits the large oscillations
observed in Figure 1.6 Intuitively, what is happening is that the more Ô¨Çexible poly-
nomials with larger values of Mare increasingly tuned to the random noise on the
target values Further insight into this phenomenon can be gained by examining the behaviour
of the learned model as the size of the data set is varied, as shown in Figure 1.8 We
see that, for a given model complexity, the over-Ô¨Åtting problem become less severe
12 1 THE DEEP LEARNING REVOLUTION
0 1x‚àí11
tN= 15
0 1x‚àí11
tN= 100
Figure 1.8 Plots of the solutions obtained by minimizing the sum-of-squares error function (1.2) using the
M= 9polynomial for N= 15 data points (left plot) and N= 100 data points (right plot) We see that increasing
the size of the data set reduces the over-Ô¨Åtting problem as the size of the data set increases Another way to say this is that with a larger
data set, we can afford to Ô¨Åt a more complex (in other words more Ô¨Çexible) model
to the data One rough heuristic that is sometimes advocated in classical statistics
is that the number of data points should be no less than some multiple (say 5 or
10) of the number of learnable parameters in the model However, when we discuss
deep learning later in this book, we will see that excellent results can be obtained
using models that have signiÔ¨Åcantly more parameters than the number of training
data points Section 9.3.2
1.2.5 Regularization
There is something rather unsatisfying about having to limit the number of pa-
rameters in a model according to the size of the available training set It would seem
more reasonable to choose the complexity of the model according to the complexity
of the problem being solved One technique that is often used to control the over-
Ô¨Åtting phenomenon, as an alternative to limiting the number of parameters, is that
ofregularization, which involves adding a penalty term to the error function (1.2) to
discourage the coefÔ¨Åcients from having large magnitudes

============================================================

=== CHUNK 025 ===
Palavras: 362
Caracteres: 2151
--------------------------------------------------
The simplest such penalty
Table 1.1 Table of the coefÔ¨Åcients w for polynomials of various or-
der Observe how the typ-
ical magnitude of the coefÔ¨Å-
cients increases dramatically
as the order of the polynomial
increases.M= 0M= 1M= 3 M= 9
w 0 0:11 0:90 0:12 0:26
w 1‚àí1:58 11:20 ‚àí66:13
w A Tutorial Example 13
0 1x‚àí11
tlnŒª=‚àí18
0 1x‚àí11
tlnŒª= 0
Figure
1.9 Plots ofM= 9polynomials Ô¨Åtted to the data set shown in Figure 1.4 using the regularized error
function (1.4) for two values of the regularization parameter corresponding to ln=‚àí18 andln= 0 The
case of no regularizer, i.e., = 0, corresponding to ln=‚àí‚àû, is shown at the bottom right of Figure 1.6 term takes the form of the sum of the squares of all of the coefÔ¨Åcients, leading to a
modiÔ¨Åed error function of the form
/tildewideE(w) =1
2N/summationdisplay
n
=1{y(xn;w)‚àítn}2+
2/bardbl
w/bardbl2(1.4)
where/bardblw/bardbl2‚â°wTw=w2
0+w2
1+:::+w2
M, and the coefÔ¨Åcient governs the rel-
ative importance of the regularization term compared with the sum-of-squares error
term Note that often the coefÔ¨Åcient w0is omitted from the regularizer because its
inclusion causes the results to depend on the choice of origin for the target variable
(Hastie, Tibshirani, and Friedman, 2009), or it may be included but with its own
regularization coefÔ¨Åcient Again, the error function in (1.4) can be minimized ex- Section 9.2.1
actly in closed form Techniques such as this are known in the statistics literature as Exercise 4.2
shrinkage methods because they reduce the value of the coefÔ¨Åcients In the context
of neural networks, this approach is known as weight decay because the parameters
in a neural network are called weights and this regularizer encourages them to decay
towards zero Figure 1.9 shows the results of Ô¨Åtting the polynomial of order M= 9 to the
same data set as before but now using the regularized error function given by (1.4) We see that, for a value of ln=‚àí18, the over-Ô¨Åtting has been suppressed and we
now obtain a much closer representation of the underlying function sin(2x ) If,
however, we use too large a value for then we again obtain a poor Ô¨Åt, as shown in
Figure 1.9 forln= 0

============================================================

=== CHUNK 026 ===
Palavras: 361
Caracteres: 2140
--------------------------------------------------
The corresponding coefÔ¨Åcients from the Ô¨Åtted polynomials
are given in Table 1.2, showing that regularization has the desired effect of reducing
the magnitude of the coefÔ¨Åcients The impact of the regularization term on the generalization error can be seen by
plotting the value of the RMS error (1.3) for both training and test sets against ln,
as shown in Figure 1.10 We see that now controls the effective complexity of the
model and hence determines the degree of over-Ô¨Åtting THE DEEP LEARNING REVOLUTION
Figure 1.10 Graph of the root-mean-
square error (1.3) versus lnfor theM= 9
polynomial ‚àí30 -20 -10 -0
M00.5ERMSTraining
Test
1.2.6
Model selection
The quantity is an example of a hyperparameter whose values are Ô¨Åxed during
the minimization of the error function to determine the model parameters w We
cannot simply determine the value of by minimizing the error function jointly with
respect to wandsince this will lead to ‚Üí0and an over-Ô¨Åtted model with small
or zero training error Similarly, the order Mof the polynomial is a hyperparameter
of the model, and simply optimizing the training set error with respect to Mwill
lead to large values of Mand associated over-Ô¨Åtting We therefore need to Ô¨Ånd a
way to determine suitable values for hyperparameters The results above suggest a
simple way of achieving this, namely by taking the available data and partitioning it
into a training set, used to determine the coefÔ¨Åcients w, and a separate validation set,
also called a hold-out set or a development set We then select the model having the
lowest error on the validation set If the model design is iterated many times using a
data set of limited size, then some over-Ô¨Åtting to the validation data can occur, and
so it may be necessary to keep aside a third test set on which the performance of the
selected model can Ô¨Ånally be evaluated For some applications, the supply of data for training and testing will be limited To build a good model, we should use as much of the available data as possible for
training However, if the validation set is too small, it will give a relatively noisy
estimate of predictive performance

============================================================

=== CHUNK 027 ===
Palavras: 365
Caracteres: 2149
--------------------------------------------------
One solution to this dilemma is to use cross-
Table 1.2 Table of the coefÔ¨Åcients w?for
M= 9 polynomials with various
values for the regularization param-
eter Note that ln=‚àí‚àû cor-
responds to a model with no regu-
larization, i.e., to the graph at the
bottom right in Figure 1.6 We see
that, as the value of increases,
the magnitude of a typical coefÔ¨Å-
cient gets smaller.ln=‚àí‚àû ln=‚àí
18 ln= 0
w 1‚àí
66:13 0:64 ‚àí0:07
w 2 1
;665:69 43:68 ‚àí0:09
w 3‚àí
15;566:61‚àí144:00‚àí0:07
w 4 76
;321:23 57:90 ‚àí0:05
w 5‚àí
217;389:15 117:36 ‚àí0:04
w 6 370
;626:48 9:87 ‚àí0:02
w 7‚àí
372;051:47‚àí90:02‚àí0:01
w 8 202
;540:70‚àí70:90‚àí0:01
w 9‚àí
46;080:94 75:26 0:00
1.2 A Tutorial Example 15
Figure 1.11 The technique of S-fold cross-validation, illus-
trated here for the case of S= 4, involves tak-
ing the available data and partitioning it into S
groups of equal size Then S‚àí1of the groups
are used to train a set of models that are then
evaluated on the remaining group This proce-
dure is then repeated for all Spossible choices
for the held-out group, indicated here by the
red blocks, and the performance scores from
theSruns are then averaged run1
run
2
run 3
run 4
validation
, which is illustrated in Figure 1.11 This allows a proportion (S‚àí1)=S of
the available data to be used for training while making use of all of the data to assess
performance When data is particularly scarce, it may be appropriate to consider the
caseS=N, whereNis the total number of data points, which gives the leave-one-
outtechnique The main drawback of cross-validation is that the number of training runs that
must be performed is increased by a factor of S, and this can prove problematic for
models in which the training is itself computationally expensive A further problem
with techniques such as cross-validation that use separate data to assess performance
is that we might have multiple complexity hyperparameters for a single model (for
instance, there might be several regularization hyperparameters) Exploring combi-
nations of settings for such hyperparameters could, in the worst case, require a num-
ber of training runs that is exponential in the number of hyperparameters

============================================================

=== CHUNK 028 ===
Palavras: 359
Caracteres: 2326
--------------------------------------------------
The state
of the art in modern machine learning involves extremely large models, trained on
commensurately large data sets Consequently, there is limited scope for exploration
of hyperparameter settings, and heavy reliance is placed on experience obtained with
smaller models and on heuristics This simple example of Ô¨Åtting a polynomial to a synthetic data set generated
from a sinusoidal function has illustrated many key ideas from machine learning,
and we will make further use of this example in future chapters However, real-
world applications of machine learning differ in several important respects The size
of the data sets used for training can be many orders of magnitude larger, and there
will generally be many more input variables, perhaps numbering in the millions for
image analysis, for example, as well as multiple output variables The learnable
function that relates outputs to inputs is governed by a class of models known as
neural networks, and these may have a large number of parameters perhaps num-
bering in the hundreds of billions, and the error function will be a highly nonlinear
function of those parameters The error function can no longer be minimized through
a closed-form solution and instead must be minimized through iterative optimization
techniques based on evaluation of the derivatives of the error function with respect
to the parameters, all of which may require specialist computational hardware and
incur substantial computational cost THE DEEP LEARNING REVOLUTION
Figure 1.12 Schematic illustration
showing two neurons from the human
brain These electrically active cells
communicate through junctions called
synapses whose strengths change as
the network learns A Brief History of Machine Learning
Machine learning has a long and rich history, including the pursuit of multiple al-
ternative approaches Here we focus on the evolution of machine learning methods
based on neural networks as these represent the foundation of deep learning and
have proven to be the most effective approach to machine learning for real-world
applications Neural network models were originally inspired by studies of information pro-
cessing in the brains of humans and other mammals The basic processing units in
the brain are electrically active cells called neurons, as illustrated in Figure 1.12

============================================================

=== CHUNK 029 ===
Palavras: 374
Caracteres: 2351
--------------------------------------------------
When a neuron ‚ÄòÔ¨Åres‚Äô, it sends an electrical impulse down the axon where it reaches
junctions, called synapses, which form connections with other neurons Chemical
signals called neurotransmitters are released at the synapses, and these can stimu-
late, or inhibit, the Ô¨Åring of subsequent neurons A human brain contains around 90 billion neurons in total, each of which has on
average several thousand synapses with other neurons, creating a complex network
having a total of around 100 trillion ( 1014) synapses If a particular neuron receives
sufÔ¨Åcient stimulation from the Ô¨Åring of other neurons then it too can be induced to
Ô¨Åre However, some synapses have a negative, or inhibitory, effect whereby the Ô¨Åring
of the input neuron makes it less likely that the output neuron will Ô¨Åre The extent to
which one neuron can cause another to Ô¨Åre depends on the strength of the synapse,
and it is changes in these strengths that represents a key mechanism whereby the
brain can store information and learn from experience These properties of neurons have been captured in very simple mathematical
models, known as artiÔ¨Åcial neural networks, which then form the basis for compu-
tational approaches to learning (McCulloch and Pitts, 1943) Many of these models
describe the properties of a single neuron by forming a linear combination of the
outputs of other neurons, which is then transformed using a nonlinear function A Brief History of Machine Learning 17
Figure 1.13 A simple neural network diagram representing the trans-
formations (1.5) and (1.6) describing a single neuron The
polynomial function (1.1) can be seen as a special case of
this model.xM x2
x1ywM
w2
w1
can
be expressed mathematically in the form
a=M/summationdisplay
i=1wixi (1.5)
y=f(a) (1.6)
wherex1;:::;xMrepresentMinputs corresponding to the activities of other neu-
rons that send connections to this neuron, and w1;:::;wMare continuous variables,
called weights, which represent the strengths of the associated synapses The quan-
tityais called the pre-activation, the nonlinear function f(¬∑)is called the activation
function, and the output yis called the activation We can see that the polynomial
(1.1) can be viewed as a speciÔ¨Åc instance of this representation in which the inputs
xiare given by powers of a single variable x, and the function f(¬∑)is just the identity
f(a) =a

============================================================

=== CHUNK 030 ===
Palavras: 360
Caracteres: 2168
--------------------------------------------------
The simple mathematical formulation given by (1.5) and (1.6) has formed
the basis of neural network models from the 1960s up to the present day, and can be
represented in diagram form as shown in Figure 1.13 1.3.1 Single-layer networks
The history of artiÔ¨Åcial neural networks can broadly be divided into three distinct
phases according to the level of sophistication of the networks as measured by the
number of ‚Äòlayers‚Äô of processing A simple neural model described by (1.5) and (1.6)
can be viewed as having a single layer of processing corresponding to the single layer
of connections in Figure 1.13 One of the most important such models in the history
of neural computing is the perceptron (Rosenblatt, 1962) in which the activation
functionf(¬∑)is a step function of the form
f(a) =/braceleftBigg
0;ifa60;
1;ifa>0:(1.7)
This can be viewed as a simpliÔ¨Åed model of neural Ô¨Åring in which a neuron Ô¨Åres if,
and only if, the total weighted input exceeds a threshold of 0 The perceptron was
pioneered by Rosenblatt (1962), who developed a speciÔ¨Åc training algorithm that
has the interesting property that if there exists a set of weight values for which the
perceptron can achieve perfect classiÔ¨Åcation of its training data then the algorithm
is guaranteed to Ô¨Ånd the solution in a Ô¨Ånite number of steps (Bishop, 2006) As
well as a learning algorithm, the perceptron also had a dedicated analogue hardware
18 1 THE DEEP LEARNING REVOLUTION
Figure 1.14 Illustration of the Mark 1 perceptron hardware The photograph on the left shows how the inputs
were obtained using a simple camera system in which an input scene, in this case a printed character, was
illuminated by powerful lights, and an image focused onto a 20√ó20array of cadmium sulphide photocells,
giving a primitive 400-pixel image The perceptron also had a patch board, shown in the middle photograph,
which allowed different conÔ¨Ågurations of input features to be tried Often these were wired up at random to
demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern
digital computer The photograph on the right shows one of the racks of learnable weights

============================================================

=== CHUNK 031 ===
Palavras: 363
Caracteres: 2358
--------------------------------------------------
Each weight was
implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby
allowing the value of the weight to be adjusted automatically by the learning algorithm implementation, as shown in Figure 1.14 A typical perceptron conÔ¨Åguration had
multiple layers of processing, but only one of those layers was learnable from data,
and so the perceptron is considered to be a ‚Äòsingle-layer‚Äô neural network At Ô¨Årst, the ability of perceptrons to learn from data in a brain-like way was con-
sidered remarkable However, it became apparent that the model also has major lim-
itations The properties of perceptrons were analysed by Minsky and Papert (1969),
who gave formal proofs of the limited capabilities of single-layer networks Unfortu-
nately, they also speculated that similar limitations would extend to networks having
multiple layers of learnable parameters Although this latter conjecture proved to
be wildly incorrect, the effect was to dampen enthusiasm for neural network mod-
els, and this contributed to the lack of interest, and funding, for neural networks
during the 1970s and early 1980s Furthermore, researchers were unable to explore
the properties of multilayered networks due to the lack of an effective algorithm
for training them, since techniques such as the perceptron algorithm were speciÔ¨Åc
to single-layer models Note that although perceptrons have long disappeared from
practical machine learning, the name lives on because a modern neural network is
also sometimes called a multilayer perceptron orMLP 1.3.2 Backpropagation
The solution to the problem of training neural networks having more than one
layer of learnable parameters came from the use of differential calculus and the appli-
cation of gradient-based optimization methods An important change was to replace
the step function (1.7) with continuous differentiable activation functions having a
non-zero gradient Another key modiÔ¨Åcation was to introduce differentiable error
functions that deÔ¨Åne how well a given choice of parameter values predicts the target
variables in the training set We saw an example of such an error function when we
1.3 A Brief History of Machine Learning 19
Figure 1.15 A neural network having two lay-
ers of parameters in which arrows
denote the direction of information
Ô¨Çow through the network

============================================================

=== CHUNK 032 ===
Palavras: 359
Caracteres: 2187
--------------------------------------------------
Each of
the hidden units and each of the
output units computes a function of
the form given by (1.5) and (1.6) in
which the activation function f(¬∑)is
differentiable.inputs
...hidden
units...outputs used
the sum-of-squares error function (1.2) to Ô¨Åt polynomials Section 1.2.3
With these changes, we now have an error function whose derivatives with re-
spect to each of the parameters in the network can be evaluated We can now consider
networks having more than one layer of parameters Figure 1.15 shows a simple net-
work with two processing layers Nodes in the middle layer called hidden units
because their values do not appear in the training set, which only provides values
for inputs and outputs Each of the hidden units and each of the output units in
Figure 1.15 computes a function of the form given by (1.5) and (1.6) For a given
set of input values, the states of all of the hidden and output units can be evaluated
by repeated application of (1.5) and (1.6) in which information is Ô¨Çowing forward
through the network in the direction of the arrows For this reason, such models are
sometimes also called feed-forward neural networks To train such a network the parameters are Ô¨Årst initialized using a random num-
ber generator and are then iteratively updated using gradient-based optimization
techniques This involves evaluating the derivatives of the error function, which
can be done efÔ¨Åciently in a process known as error backpropagation In backpropa- Chapter 8
gation, information Ô¨Çows backwards through the network from the outputs towards
the inputs (Rumelhart, Hinton, and Williams, 1986) There exist many different op-
timization algorithms that make use of gradients of the function to be optimized, but
the one that is most prevalent in machine learning is also the simplest and is known
asstochastic gradient descent Chapter 7
The ability to train neural networks having multiple layers of weights was a
breakthrough that led to a resurgence of interest in the Ô¨Åeld starting around the mid-
1980s This was also a period in which the Ô¨Åeld moved beyond a focus on neurobio-
logical inspiration and developed a more rigorous and principled foundation (Bishop,
1995b)

============================================================

=== CHUNK 033 ===
Palavras: 373
Caracteres: 2368
--------------------------------------------------
In particular, it was recognized that probability theory, and ideas from the
Ô¨Åeld of statistics, play a central role in neural networks and machine learning One
key insight is that learning from data involves background assumptions, sometimes
called prior knowledge orinductive biases These might be incorporated explicitly,
for example by designing the structure of a neural network such that the classiÔ¨Åca-
tion of a skin lesion does not depend on the location of the lesion within the image,
or they might take the form of implicit assumptions that arise from the mathematical
20 1 THE DEEP LEARNING REVOLUTION
form of the model or the way it is trained The development of backpropagation and gradient-based optimization dramati-
cally increased the capability of neural networks to solve practical problems How-
ever, it was also observed that in networks with many layers, it was only weights in
the Ô¨Ånal two layers that would learn useful values With a few exceptions, notably
models used for image analysis known as convolutional neural networks (LeCun Chapter 10
et al., 1998), there were very few successful applications of networks having more
than two layers Again, this constrained the complexity of the problems that could
be addressed effectively with these kinds of network To achieve reasonable perfor-
mance on many applications, it was necessary to use hand-crafted pre-processing to
transform the input variables into some new space where, it was hoped, the machine
learning problem would be easier to solve This pre-processing stage is sometimes
also called feature extraction Although this approach was sometimes effective, it
would clearly be much better if features could be learned from the data rather than
being hand-crafted By the start of the new millennium, the available neural network methods were
once again reaching the limits of their capability Researchers began to explore a
raft of alternatives to neural networks, such as kernel methods, support vector ma-
chines, Gaussian processes, and many others Neural networks fell into disfavour
once again, although a core of enthusiastic researchers continued to pursue the goal
of a truly effective approach to training networks with many layers 1.3.3 Deep networks
The third, and current, phase in the development of neural networks began dur-
ing the second decade of the 21st century

============================================================

=== CHUNK 034 ===
Palavras: 359
Caracteres: 2262
--------------------------------------------------
A series of developments allowed neural
networks with many layers of weights to be trained effectively, thereby removing
previous limitations on the capabilities of these techniques Networks with many lay-
ers of weights are called deep neural networks and the sub-Ô¨Åeld of machine learning
that focuses on such networks is called deep learning (LeCun, Bengio, and Hinton,
2015) One important theme in the origins of deep learning was a signiÔ¨Åcant increase
in the scale of neural networks, measured in terms of the number of parameters Al-
though networks with a few hundred or a few thousand parameters were common in
the 1980s, this steadily rose to the millions, and then billions, whereas current state-
of-the-art models can have in the region of one trillion (1012) parameters Networks
with many parameters require commensurately large data sets so that the training
signals can produced good values for those parameters The combination of massive
models and massive data sets in turn requires computation on a massive scale when
training the model Specialist processors called graphics processing units, or GPUs,
which had been developed for very fast rendering of graphical data for applications
such as video games, proved to be well suited to the training of neural networks be-
cause the functions computed by the units in one layer of a network can be evaluated
in parallel, and this maps well onto the massive parallelism of GPUs (Krizhevsky,
Sutskever, and Hinton, 2012) Today, training for the largest models is performed on
large arrays of thousands of GPUs linked by specialist high-speed interconnections A Brief History of Machine Learning 21
Figure 1.16 Plot of the number of compute cycles, measured in petaÔ¨Çop/s-days, needed to train a state-of-the-
art neural network as a function of date, showing two distinct phases of exponential growth [From OpenAI with
permission.]
Figure 1.16 illustrates how the number of compute cycles needed to train a state-
of-the-art neural network has grown over the years, showing two distinct phases of
growth The vertical axis has an exponential scale and has units of petaÔ¨Çop/s-days,
where a petaÔ¨Çop represents 1015(a thousand trillion) Ô¨Çoating point operations, and a
petaÔ¨Çop/s is one petaÔ¨Çop per second

============================================================

=== CHUNK 035 ===
Palavras: 366
Caracteres: 2235
--------------------------------------------------
One petaÔ¨Çop/s-day represents computation at
the rate of a petaÔ¨Çop/s for a period of 24 hours, which is roughly 1020Ô¨Çoating point
operations, and therefore, the top line of the graph represents an impressive 1024
Ô¨Çoating point operations A straight line on the graph represents exponential growth,
and we see that from the era of the perceptron up to around 2012, the doubling time
was around 2 years, which is consistent with the general growth of computing power
as a consequence of Moore‚Äôs law From 2012 onward, which marks the era of deep
learning, we again see exponential growth but the doubling time is now 3.4 months
corresponding to a factor of 10 increase in compute power every year It is often found that improvements in performance due to innovations in the
architecture or incorporation of more sophisticated forms of inductive bias are soon
22 1 THE DEEP LEARNING REVOLUTION
superseded simply by scaling up the quantity of training data, along with commen-
surate scaling of the model size and associated compute power used for training
(Sutton, 2019) Not only can large models have superior performance on a speciÔ¨Åc
task but they may be capable of solving a broader range of different problems with
the same trained neural network Large language models are a notable example as a
single network not only has an extraordinary breadth of capability but is even able to
outperform specialist networks designed to solve speciÔ¨Åc problems Section 12.3.5
We have seen that depth plays an important role in allowing neural networks to
achieve high performance One way to view the role of the hidden layers in a deep
neural network is that of representation learning (Bengio, Courville, and Vincent,
2012) in which the network learns to transform input data into a new representation
that is semantically meaningful thereby creating a much easier problem for the Ô¨Ånal
layer or layers to solve Such internal representations can be repurposed to allow for
the solution of related problems through transfer learning, as we saw for skin lesion
classiÔ¨Åcation It is interesting to note that neural networks used to process images
may learn internal representations that are remarkably like those observed in the
mammalian visual cortex

============================================================

=== CHUNK 036 ===
Palavras: 353
Caracteres: 2274
--------------------------------------------------
Large neural networks that can be adapted or Ô¨Åne-tuned Section 10.3
to a range of downstream tasks are called foundation models, and can take advan-
tage of large, heterogeneous data sets to create models having broad applicability
(Bommasani et al., 2021) In addition to scaling, there were other developments that helped in the suc-
cess of deep learning For example, in simple neural networks, the training signals
become weaker as they are backpropagated through successive layers of a deep net-
work One technique for addressing this is the introduction of residual connections Section 9.5
(Heet al., 2015a) that facilitate the training of networks having hundreds of layers Another key development was the introduction of automatic differentiation methods
in which the code that performs backpropagation to evaluate error function gradients
is generated automatically from the code used to specify the forward propagation This allows researchers to experiment rapidly with different architectures for a neural
network and to combine different architectural elements in multiple ways very easily
since only the relatively simple forward propagation functions need to be coded ex-
plicitly Also, much of the research in machine learning has been conducted through
open source, allowing researchers to build on the work of others, thereby further
accelerating the rate of progress in the Ô¨Åeld 2
Probabilities
In almost every application of machine learning we have to deal with uncertainty For
example, a system that classiÔ¨Åes images of skin lesions as benign or malignant can
never in practice achieve perfect accuracy We can distinguish between two kinds of
uncertainty The Ô¨Årst is epistemic uncertainty (derived from the Greek word episteme
meaning knowledge), sometimes called systematic uncertainty It arises because we
only get to see data sets of Ô¨Ånite size As we observe more data, for instance more
examples of benign and malignant skin lesion images, we are better able to predict
the class of a new example However, even with an inÔ¨Ånitely large data set, we would
still not be able to achieve perfect accuracy due to the second kind of uncertainty
known as aleatoric uncertainty, also called intrinsic orstochastic uncertainty, or
sometimes simply called noise

============================================================

=== CHUNK 037 ===
Palavras: 354
Caracteres: 2121
--------------------------------------------------
Generally speaking, the noise arises because we are
able to observe only partial information about the world, and therefore, one way to
reduce this source of uncertainty is to gather different kinds of data This is illustrated
23 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_2    
24 2 PROBABILITIES
x1
x2y
(a)
x1y
 (b)
x1
 (c)
Figure
2.1 An extension of the simple sine curve regression problem to two dimensions (a) A plot of the
functiony(x1;x2) = sin(2x 1) sin(2x2) Data is generated by selecting values for x1andx2, computing the
corresponding value of y(x1;x2), and then adding Gaussian noise (b) Plot of 100 data points in which x2is
unobserved showing high levels of noise (c) Plot of 100 data points in which x2is Ô¨Åxed to the value x2=
2,
sim
ulating the effect of being able to measure x2as well asx1, showing much lower levels of noise using an extension of the sine curve example to two dimensions in Figure 2.1 Section 1.2
As a practical example of this, a biopsy sample of the skin lesion is much more
informative than the image alone and might greatly improve the accuracy with which
we can determine if a new lesion is malignant Given both the image and the biopsy
data, the intrinsic uncertainty might be very small, and by collecting a large training
data set, we may be able to reduce the systematic uncertainty to a low level and
thereby make predictions of the class of the lesion with high accuracy Both kinds of uncertainty can be handled using the framework of probability
theory, which provides a consistent paradigm for the quantiÔ¨Åcation and manipula-
tion of uncertainty and therefore forms one of the central foundations for machine
learning We will see that probabilities are governed by two simple formulae known Section 2.1
as the sum rule and the product rule When coupled with decision theory, these rules Section 5.2
allow us, at least in principle, to make optimal predictions given all the information
available to us, even though that information may be incomplete or ambiguous

============================================================

=== CHUNK 038 ===
Palavras: 352
Caracteres: 1919
--------------------------------------------------
The concept of probability is often introduced in terms of frequencies of repeat-
able events Consider, for example, the bent coin shown in Figure 2.2, and suppose
that the shape of the coin is such that if it is Ô¨Çipped a large number of times, it lands
concave side up 60% of the time, and therefore lands convex side up 40% of the
time We say that the probability of landing concave side up is 60% or 0.6 Strictly,
the probability is deÔ¨Åned in the limit of an inÔ¨Ånite number of ‚Äòtrials‚Äô or coin Ô¨Çips
in this case Because the coin must land either concave side up or convex side up,
these probabilities add to 100% or 1.0 This deÔ¨Ånition of probability in terms of the
frequency of repeatable events is the basis for the frequentist view of statistics Now suppose that, although we know that the probability that the coin will land
concave side up is 0.6, we are not allowed to look at the coin itself and we do not
2.1 The Rules of Probability 25
Figure 2.2 Probability can be viewed ei-
ther as a frequency associated
with a repeatable event or as
a quantiÔ¨Åcation of uncertainty A bent coin can be used to il-
lustrate the difference, as dis-
cussed in the text 60% 40%
know which side is heads and which is tails If asked to take a bet on whether the coin
will land heads or tails when Ô¨Çipped, then symmetry suggests that our bet should be
based on the assumption that the probability of seeing heads is 0.5, and indeed a
more careful analysis shows that, in the absence of any additional information, this
is indeed the rational choice Here we are using probabilities in a more general sense
than simply the frequency of events Whether the convex side of the coin is heads or
tails is not itself a repeatable event, it is simply unknown The use of probability as a
quantiÔ¨Åcation of uncertainty is the Bayesian perspective and is more general in that Section 2.6
it includes frequentist probability as a special case

============================================================

=== CHUNK 039 ===
Palavras: 357
Caracteres: 2033
--------------------------------------------------
We can learn about which side
of the coin is heads if we are given results from a sequence of coin Ô¨Çips by making
use of Bayesian reasoning The more results we observe, the lower our uncertainty Exercise 2.40
as to which side of the coin is which Having introduced the concept of probability informally, we turn now to a more
detailed exploration of probabilities and discuss how to use them quantitatively Con-
cepts developed in the remainder of this chapter will form a core foundation for many
of the topics discussed throughout the book The Rules of Probability
In this section we will derive two simple rules that govern the behaviour of proba-
bilities However, in spite of their apparent simplicity, these rules will prove to be
very powerful and widely applicable We will motivate the rules of probability by
Ô¨Årst introducing a simple example 2.1.1 A medical screening example
Consider the problem of screening a population in order to provide early detec-
tion of cancer, and let us suppose that 1% of the population actually have cancer Ideally our test for cancer would give a positive result for anyone who has cancer
and a negative result for anyone who does not However, tests are not perfect, so
we will suppose that when the test is given to people who are free of cancer, 3% of
them will test positive These are known as false positives Similarly, when the test
is given to people who do have cancer, 10% of them will test negative These are
called false negatives The various error rates are illustrated in Figure 2.3 Given this information, we might ask the following questions: (1) ‚ÄòIf we screen
the population, what is the probability that someone will test positive?‚Äô, (2) ‚ÄòIf some-
26 2 PROBABILITIES
Figure 2.3 Illustration of the accuracy of
a cancer test Out of ev-
ery hundred people taking the
test who do not have cancer,
shown on the left, on average
three will test positive For
those who have cancer, shown
on the right, out of every hun-
dred people taking the test, on
average 90 will test positive

============================================================

=== CHUNK 040 ===
Palavras: 350
Caracteres: 2067
--------------------------------------------------
No Cancer Cancer
one receives a positive test result, what is the probability that they actually have can-
cer?‚Äô We could answer such questions by working through the cancer screening case
in detail Instead, however, we will pause our discussion of this speciÔ¨Åc example and
Ô¨Årst derive the general rules of probability, known as the sum rule of probability and
theproduct rule We will then illustrate the use of these rules by answering our two
questions 2.1.2 The sum and product rules
To derive the rules of probability, consider the slightly more general example
shown in Figure 2.4 involving two variables XandY In our cancer example, X
could represent the presence or absence of cancer, and Ycould be a variable de-
noting the outcome of the test Because the values of these variables can vary from
one person to another in a way that is generally unknown, they are called random
variables orstochastic variables We will suppose that Xcan take any of the values
xiwherei= 1;:::;L and thatYcan take the values yjwherej= 1;:::;M Con-
sider a total of Ntrials in which we sample both of the variables XandY, and let
the number of such trials in which X=xiandY=yjbenij Also, let the number
of trials in which Xtakes the value xi(irrespective of the value that Ytakes) be
denoted byci, and similarly let the number of trials in which Ytakes the value yjbe
denoted byrj The probability that Xwill take the value xiandYwill take the value yjis
writtenp(X=xi;Y=yj)and is called the joint probability of X=xiand
Y=yj It is given by the number of points falling in the cell i,jas a fraction of the
total number of points, and hence
p(X=xi;Y=yj) =nij
N: (2.1)
Here we are implicitly considering the limit N‚Üí‚àû Similarly, the probability that
Xtakes the value xiirrespective of the value of Yis written as p(X=xi)and is
2.1 The Rules of Probability 27
Figure 2.4 We can derive the sum and product rules
of probability by considering a random vari-
ableX, which takes the values {xi}where
i= 1;:::;L , and a second random variable
Y, which takes the values {yj}wherej=
1;:::;M

============================================================

=== CHUNK 041 ===
Palavras: 359
Caracteres: 2168
--------------------------------------------------
In this illustration, we have L= 5
andM= 3 If we consider the total number
Nof instances of these variables, then we de-
note the number of instances where X=xi
andY=yjbynij, which is the number of in-
stances in the corresponding cell of the array The number of instances in column i, corre-
sponding to X=xi, is denoted by ci, and the
number of instances in row j, corresponding
toY=yj, is denoted by rj ggci
rj yj
xinij
gi
ven by the fraction of the total number of points that fall in column i, so that
p(X=xi) =ci
N: (2.2)
Since/summationtext
ici=N,
we see that
L/summationdisplay
i=1p(X=xi) = 1 (2.3)
and, hence, the probabilities sum to one as required Because the number of instances
in columniin Figure 2.4 is just the sum of the number of instances in each cell of
that column, we have ci=/summationtext
jnijand therefore, from (2.1) and (2.2), we have
p(X=xi) =M/summationdisplay
j=1p(X=xi;Y=yj); (2.4)
which is the sum rule of probability Note that p(X =xi)is sometimes called the
marginal probability and is obtained by marginalizing, or summing out, the other
variables (in this case Y) If we consider only those instances for which X=xi, then the fraction of
such instances for which Y=yjis writtenp(Y =yj|X=xi)and is called the
conditional probability of Y=yjgivenX=xi It is obtained by Ô¨Ånding the
fraction of those points in column ithat fall in cell i,jand, hence, is given by
p(Y=yj|X=xi) =nij
ci: (2.5)
Summing
both sides over jand using/summationtext
jnij=ci, we obtain
M/summationdisplay
j=1p(Y=yj|X=xi) = 1 (2.6)
28 2 PROBABILITIES
showing that the conditional probabilities are correctly normalized From (2.1),
(2.2), and (2.5), we can then derive the following relationship:
p(X=xi;Y=yj) =nij
N=nij
ci¬∑ci
N
=p(Y=yj|X=xi)p(X =xi); (2.7)
which is the product rule of probability So far, we have been quite careful to make a distinction between a random vari-
able, such as X, and the values that the random variable can take, for example xi Thus, the probability that Xtakes the value xiis denotedp(X =xi) Although
this helps to avoid ambiguity, it leads to a rather cumbersome notation, and in many
cases there will be no need for such pedantry

============================================================

=== CHUNK 042 ===
Palavras: 370
Caracteres: 2371
--------------------------------------------------
Instead, we may simply write p(X)to
denote a distribution over the random variable X, orp(xi)to denote the distribution
evaluated for the particular value xi, provided that the interpretation is clear from the
context With this more compact notation, we can write the two fundamental rules of
probability theory in the following form:
sum rule p(X) =/summationdisplay
Yp(X;Y ) (2.8)
product rule p(X;Y ) =p(Y|X)p(X ): (2.9)
Herep(X;Y )is a joint probability and is verbalized as ‚Äòthe probability of Xand
Y‚Äô Similarly, the quantity p(Y|X)is a conditional probability and is verbalized as
‚Äòthe probability of YgivenX‚Äô Finally, the quantity p(X)is a marginal probability
and is simply ‚Äòthe probability of X‚Äô These two simple rules form the basis for all of
the probabilistic machinery that we will use throughout this book 2.1.3 Bayes‚Äô theorem
From the product rule, together with the symmetry property p(X;Y ) =p(Y;X ),
we immediately obtain the following relationship between conditional probabilities:
p(Y|X) =p(X|Y)p(Y )
p(X); (2.10)
which is called Bayes‚Äô theorem and which plays an important role in machine learn-
ing Note how Bayes‚Äô theorem relates the conditional distribution p(Y|X)on the
left-hand side of the equation, to the ‚Äòreversed‚Äô conditional distribution p(X|Y)on
the right-hand side Using the sum rule, the denominator in Bayes‚Äô theorem can be
expressed in terms of the quantities appearing in the numerator:
p(X) =/summationdisplay
Yp(X|Y)p(Y ): (2.11)
Thus, we can view the denominator in Bayes‚Äô theorem as being the normalization
constant required to ensure that the sum over the conditional probability distribution
on the left-hand side of (2.10) over all values of Yequals one The Rules of Probability 29
p(X, Y )
XY=
2
Y= 1
p(Y)
p(X)
X
Xp(X|Y=1)
Figure
2.5 An illustration of a distribution over two variables, X, which takes nine possible values, and Y,
which takes two possible values The top left Ô¨Ågure shows a sample of 60points drawn from a joint probability
distribution over these variables The remaining Ô¨Ågures show histogram estimates of the marginal distributions
p(X)andp(Y), as well as the conditional distribution p(X|Y= 1) corresponding to the bottom row in the top left
Ô¨Ågure InFigure 2.5, we show a simple example involving a joint distribution over two
variables to illustrate the concept of marginal and conditional distributions

============================================================

=== CHUNK 043 ===
Palavras: 460
Caracteres: 2523
--------------------------------------------------
Here a
Ô¨Ånite sample of N= 60 data points has been drawn from the joint distribution and
is shown in the top left In the top right is a histogram of the fractions of data points
having each of the two values of Y From the deÔ¨Ånition of probability, these frac-
tions would equal the corresponding probabilities p(Y)in the limit when the sample
sizeN‚Üí‚àû We can view the histogram as a simple way to model a probability
distribution given only a Ô¨Ånite number of points drawn from that distribution The Section 3.5.1
remaining two plots in Figure 2.5 show the corresponding histogram estimates of
p(X)andp(X|Y= 1) PROBABILITIES
2.1.4 Medical screening revisited
Let us now return to our cancer screening example and apply the sum and prod-
uct rules of probability to answer our two questions For clarity, when working
through this example, we will once again be explicit about distinguishing between
the random variables and their instantiations We will denote the presence or absence
of cancer by the variable C, which can take two values: C= 0 corresponds to ‚Äòno
cancer‚Äô and C= 1 corresponds to ‚Äòcancer‚Äô We have assumed that one person in a
hundred in the population has cancer, and so we have
p(C= 1) = 1=100 (2.12)
p(C= 0) = 99=100; (2.13)
respectively Note that these satisfy p(C= 0) +p(C= 1) = 1 Now let us introduce a second random variable Trepresenting the outcome of a
screening test, where T= 1denotes a positive result, indicative of cancer, and T= 0
a negative result, indicative of the absence of cancer As illustrated in Figure 2.3, we
know that for those who have cancer the probability of a positive test result is 90%,
while for those who do not have cancer the probability of a positive test result is 3% We can therefore write out all four conditional probabilities:
p(T= 1|C = 1) = 90=100 (2.14)
p(T= 0|C = 1) = 10=100 (2.15)
p(T= 1|C = 0) = 3=100 (2.16)
p(T= 0|C = 0) = 97=100: (2.17)
Again, note that these probabilities are normalized so that
p(T= 1|C = 1) +p(T= 0|C = 1) = 1 (2.18)
and similarly
p(T= 1|C = 0) +p(T= 0|C = 0) = 1: (2.19)
We can now use the sum and product rules of probability to answer our Ô¨Årst
question and evaluate the overall probability that someone who is tested at random
will have a positive test result:
p(T= 1) =p(T= 1|C = 0)p(C= 0) +p(T= 1|C = 1)p(C= 1)
=3
100√ó99
100+90
100√ó1
100=387
10
;000= 0:0387: (2.20)
We see that if a person is tested at random there is a roughly 4% chance that the
test will be positive even though there is a 1% chance that they actually have cancer

============================================================

=== CHUNK 044 ===
Palavras: 365
Caracteres: 2089
--------------------------------------------------
From this it follows, using the sum rule, that p(T = 0) = 1‚àí387=10; 000 =
9613=10; 000 = 0:9613 and, hence, there is a roughly 96% chance that the do not
have cancer Now consider our second question, which is the one that is of particular interest
to a person being screened: if a test is positive, what is the probability that the person
2.1 The Rules of Probability 31
has cancer This requires that we evaluate the probability of cancer conditional
on the outcome of the test, whereas the probabilities in (2.14) to (2.17) give the
probability distribution over the test outcome conditioned on whether the person has
cancer We can solve the problem of reversing the conditional probability by using
Bayes‚Äô theorem (2.10) to give
p(C= 1|T = 1) =p(T= 1|C = 1)p(C= 1)
p
(T= 1)(2.21)
=90
100√ó1
100√ó10
;000
387=90
387/similarequal0
:23 (2.22)
so that if a person is tested at random and the test is positive, there is a 23% proba-
bility that they actually have cancer From the sum rule, it then follows that p(C=
0|T= 1) = 1‚àí90=387 = 297=387 /similarequal0:77, which is a 77% chance that they do not
have cancer 2.1.5 Prior and posterior probabilities
We can use the cancer screening example to provide an important interpretation
of Bayes‚Äô theorem as follows If we had been asked whether someone is likely to
have cancer, before they have received a test, then the most complete information we
have available is provided by the probability p(C) We call this the prior probability
because it is the probability available before we observe the result of the test Once
we are told that this person has received a positive test, we can then use Bayes‚Äô theo-
rem to compute the probability p(C|T), which we will call the posterior probability
because it is the probability obtained after we have observed the test result T In this example, the prior probability of having cancer is 1% However, once we
have observed that the test result is positive, we Ô¨Ånd that the posterior probability of
cancer is now 23%, which is a substantially higher probability of cancer, as we would
intuitively expect

============================================================

=== CHUNK 045 ===
Palavras: 370
Caracteres: 2257
--------------------------------------------------
We note, however, that a person with a positive test still has only a
23% change of actually having cancer, even though the test appears, from Figure 2.3
to be reasonably ‚Äòaccurate‚Äô This conclusion seems counter-intuitive to many people Exercise 2.1
The reason has to do with the low prior probability of having cancer Although
the test provides strong evidence of cancer, this has to be combined with the prior
probability using Bayes‚Äô theorem to arrive at the correct posterior probability 2.1.6 Independent variables
Finally, if the joint distribution of two variables factorizes into the product of the
marginals, so that p(X;Y ) =p(X)p(Y ), thenXandYare said to be independent An example of independent events would be the successive Ô¨Çips of a coin From
the product rule, we see that p(Y|X) =p(Y), and so the conditional distribution
ofYgivenXis indeed independent of the value of X In our cancer screening
example, if the probability of a positive test is independent of whether the person has
cancer, then p(T|C) =p(T), which means that from Bayes‚Äô theorem (2.10) we have
p(C|T) =p(C), and therefore probability of cancer is not changed by observing the
test outcome Of course, such a test would be useless because the outcome of the
test tells us nothing about whether the person has cancer PROBABILITIES
Figure 2.6 The concept of probability for
discrete variables can be ex-
tended to that of a probabil-
ity densityp(x) over a contin-
uous variable xand is such
that the probability of xlying
in the interval (x;x +x)is
given byp(x)x forx‚Üí0 The probability density can
be expressed as the deriva-
tive of a cumulative distribu-
tion function P(x) Pr
obability Densities
As
well as considering probabilities deÔ¨Åned over discrete sets of values, we also
wish to consider probabilities with respect to continuous variables For instance, we
might wish to predict what dose of drug to give to a patient Since there will be
uncertainty in this prediction, we want to quantify this uncertainty and again we can
make use of probabilities However, we cannot simply apply the concepts of proba-
bility discussed so far directly, since the probability of observing a speciÔ¨Åc value for
a continuous variable, to inÔ¨Ånite precision, will effectively be zero

============================================================

=== CHUNK 046 ===
Palavras: 357
Caracteres: 2308
--------------------------------------------------
Instead, we need
to introduce the concept of a probability density Here we will limit ourselves to a
relatively informal discussion We deÔ¨Åne the probability density p(x) over a continuous variable xto be such
that the probability of xfalling in the interval (x;x +x)is given byp(x)x for
x‚Üí0 This is illustrated in Figure 2.6 The probability that xwill lie in an interval
(a;b) is then given by
p(x‚àà(a;b)) =/integraldisplayb
ap(x) dx: (2.23)
Because probabilities are non-negative, and because the value of xmust lie some-
where on the real axis, the probability density p(x) must satisfy the two conditions
p(x)>0 (2.24)/integraldisplay‚àû
‚àí‚àûp(x) dx= 1: (2.25)
The probability that xlies in the interval (‚àí‚àû;z )is given by the cumulative
distribution function deÔ¨Åned by
P(z) =/integraldisplayz
‚àí‚àûp(x) dx; (2.26)
2.2 Probability Densities 33
which satisÔ¨Åes P/prime(x) =p(x), as shown in Figure 2.6 If we have several continuous variables x1;:::;xD, denoted collectively by the
vector x, then we can deÔ¨Åne a joint probability density p(x) =p(x1;:::;xD)such
that the probability of xfalling in an inÔ¨Ånitesimal volume xcontaining the point x
is given byp(x)x This multivariate probability density must satisfy
p(x) >0 (2.27)/integraldisplay
p(x) d x= 1 (2.28)
in which the integral is taken over the whole of xspace More generally, we can also
consider joint probability distributions over a combination of discrete and continuous
variables The sum and product rules of probability, as well as Bayes‚Äô theorem, also apply
to probability densities as well as to combinations of discrete and continuous vari-
ables If xandyare two real variables, then the sum and product rules take the
form
sum rule p(x) =/integraldisplay
p(x;y) dy (2.29)
product rule p(x;y) =p(y|x)p(x): (2.30)
Similarly, Bayes‚Äô theorem can be written in the form
p(y|x) =p(x|y )p(y)
p
(x)(2.31)
where the denominator is given by
p(x) =/integraldisplay
p(x|y )p(y) dy: (2.32)
A formal justiÔ¨Åcation of the sum and product rules for continuous variables re-
quires a branch of mathematics called measure theory (Feller, 1966) and lies outside
the scope of this book Its validity can be seen informally, however, by dividing each
real variable into intervals of width ‚àÜand considering the discrete probability dis-
tribution over these intervals

============================================================

=== CHUNK 047 ===
Palavras: 357
Caracteres: 2185
--------------------------------------------------
Taking the limit ‚àÜ‚Üí0then turns sums into integrals
and gives the desired result 2.2.1 Example distributions
There are many forms of probability density that are in widespread use and
that are important both in their own right and as building blocks for more complex
probabilistic models The simplest form would be one in which p(x) is a constant,
independent of x, but this cannot be normalized because the integral in (2.28) will
be divergent Distributions that cannot be normalized are called improper We can,
however, have the uniform distribution that is constant over a Ô¨Ånite region, say (c;d),
and zero elsewhere, in which case (2.28) implies
p(x) = 1=(d‚àíc); x‚àà(c;d): (2.33)
34 2 PROBABILITIES
Figure 2.7 Plots of a uniform distribution over
the range (‚àí1; 1), shown in red,
the exponential distribution with
= 1, shown in blue, and a
Laplace distribution with = 1
and
= 1, shown in green ‚àí2‚àí1 0 1 2 3 4
x0.00.51.0
p(x)
Another
simple form of density is the exponential distribution given by
p(x|) =exp(‚àíx); x >0: (2.34)
A variant of the exponential distribution, known as the Laplace distribution, allows
the peak to be moved to a location and is given by
p(x|;
 ) =1
2

exp/parenleftbigg
‚àí|x‚àí|

/parenrightbigg
: (2.35)
The
constant, exponential, and Laplace distributions are illustrated in Figure 2.7 Another important distribution is the Dirac delta function, which is written
p(x|) =(x‚àí): (2.36)
This is deÔ¨Åned to be zero everywhere except at x=and to have the property of in-
tegrating to unity according to (2.28) Informally, we can think of this as an inÔ¨Ånitely
narrow and inÔ¨Ånitely tall spike located at x=with the property of having unit area Finally, if we have a Ô¨Ånite set of observations of xgiven byD={x1;:::;xN}then
we can use the delta function to construct the empirical distribution given by
p(x|D ) =1
NN/summationdisplay
n
=1(x‚àíxn); (2.37)
which consists of a Dirac delta function centred on each of the data points The
probability density deÔ¨Åned by (2.37) integrates to one as required Exercise 2.6
2.2.2 Expectations and covariances
One of the most important operations involving probabilities is that of Ô¨Ånding
weighted averages of functions

============================================================

=== CHUNK 048 ===
Palavras: 355
Caracteres: 2378
--------------------------------------------------
The weighted average of some function f(x)under
a probability distribution p(x) is called the expectation off(x)and will be denoted
byE[f] For a discrete distribution, it is given by summing over all possible values
ofxin the form
E[f] =/summationdisplay
xp(x)f (x) (2.38)
2.2 Probability Densities 35
where the average is weighted by the relative probabilities of the different values of
x For continuous variables, expectations are expressed in terms of an integration
with respect to the corresponding probability density:
E[f] =/integraldisplay
p(x)f (x) dx: (2.39)
In either case, if we are given a Ô¨Ånite number Nof points drawn from the probability
distribution or probability density, then the expectation can be approximated as a
Ô¨Ånite sum over these points: Exercise 2.7
E[f]/similarequal1
NN/summationdisplay
n=1f(xn): (2.40)
The approximation in (2.40) becomes exact in the limit N‚Üí‚àû Sometimes we will be considering expectations of functions of several variables,
in which case we can use a subscript to indicate which variable is being averaged
over, so that for instance
Ex[f(x;y)] (2.41)
denotes the average of the function f(x;y)with respect to the distribution of x Note
thatEx[f(x;y)]will be a function of y We can also consider a conditional expectation with respect to a conditional
distribution, so that
Ex[f|y] =/summationdisplay
xp(x|y )f(x); (2.42)
which is also a function of y For continuous variables, the conditional expectation
takes the form
Ex[f|y] =/integraldisplay
p(x|y )f(x) dx: (2.43)
Thevariance off(x)is deÔ¨Åned by
var[f ] =E/bracketleftBig
(f(x)‚àíE[f(x)])2/bracketrightBig
(2.44)
and provides a measure of how much f(x)varies around its mean value E[f(x)] Expanding out the square, we see that the variance can also be written in terms of
the expectations of f(x)andf(x)2: Exercise 2.8
var[f ] =E[f(x)2]‚àíE[f(x)]2: (2.45)
In particular, we can consider the variance of the variable xitself, which is given by
var[x] = E[x2]‚àíE[x]2: (2.46)
For two random variables xandy, the covariance measures the extent to which
the two variables vary together and is deÔ¨Åned by
cov[x;y ] = Ex;y[{x‚àíE[x]}{y‚àíE[y]}]
=Ex;y[xy]‚àíE[x]E[y]: (2.47)
36 2 PROBABILITIES
Figure 2.8 Plot of a Gaussian distribu-
tion for a single continuous
variablexshowing the mean
and the standard deviation
 N(xj;2)
x2

Ifxandyare
independent, then their covariance equals zero

============================================================

=== CHUNK 049 ===
Palavras: 362
Caracteres: 2465
--------------------------------------------------
Exercise 2.9
For two vectors xandy, their covariance is a matrix given by
cov[x; y] = Ex;y/bracketleftbig
{x‚àíE[x]}{yT‚àíE[yT]}/bracketrightbig
=Ex;y[xyT]‚àíE[x]E[yT]: (2.48)
If we consider the covariance of the components of a vector xwith each other, then
we use a slightly simpler notation cov[x]‚â°cov[x; x] The
Gaussian Distribution
One
of the most important probability distributions for continuous variables is called
thenormal orGaussian distribution, and we will make extensive use of this distribu-
tion throughout the rest of the book For a single real-valued variable x, the Gaussian
distribution is deÔ¨Åned by
N/parenleftbig
x|;2/parenrightbig
=1
(2
2)1=2exp/braceleftbigg
‚àí1
2
2(x‚àí)2/bracerightbigg
; (2.49)
which represents a probability density over xgoverned by two parameters: , called
themean, and2, called the variance The square root of the variance, given by
, is called the standard deviation, and the reciprocal of the variance, written as
= 1=2, is called the precision We will see the motivation for this terminology
shortly Figure 2.8 shows a plot of the Gaussian distribution Although the form
of the Gaussian distribution might seem arbitrary, we will see later that it arises
naturally from the concept of maximum entropy and from the perspective of the Section 2.5.4
central limit theorem Section 3.2
From (2.49) we see that the Gaussian distribution satisÔ¨Åes
N(x|;2)>0: (2.50)
2.3 The Gaussian Distribution 37
Also, it is straightforward to show that the Gaussian is normalized, so that Exercise 2.12
/integraldisplay‚àû
‚àí‚àûN/parenleftbig
x|;2/parenrightbig
dx= 1: (2.51)
Thus, (2.49) satisÔ¨Åes the two requirements for a valid probability density 2.3.1 Mean and variance
We can readily Ô¨Ånd expectations of functions of xunder the Gaussian distribu-
tion In particular, the average value of xis given by Exercise 2.13
E[x] =/integraldisplay‚àû
‚àí‚àûN/parenleftbig
x|;2/parenrightbig
xdx=: (2.52)
Because the parameter represents the average value of xunder the distribution, it
is referred to as the mean The integral in (2.52) is known as the Ô¨Årst-order moment
of the distribution because it is the expectation of xraised to the power one We can
similarly evaluate the second-order moment given by
E[x2] =/integraldisplay‚àû
‚àí‚àûN/parenleftbig
x|;2/parenrightbig
x2dx=2+2: (2.53)
From (2.52) and (2.53), it follows that the variance of xis given by
var[x ] =E[x2]‚àíE[x]2=2(2.54)
and hence2is referred to as the variance parameter

============================================================

=== CHUNK 050 ===
Palavras: 360
Caracteres: 2289
--------------------------------------------------
The maximum of a distribution
is known as its mode For a Gaussian, the mode coincides with the mean Exercise 2.14
2.3.2 Likelihood function
Suppose that we have a data set of observations represented as a row vector
x= (x 1;:::;xN), representing Nobservations of the scalar variable x Note that
we are using the typeface xto distinguish this from a single observation of a D-
dimensional vector-valued variable, which we represent by a column vector x=
(x1;:::;xD)T We will suppose that the observations are drawn independently from
a Gaussian distribution whose mean and variance 2are unknown, and we would
like to determine these parameters from the data set The problem of estimating a
distribution, given a Ô¨Ånite set of observations, is known as density estimation It
should be emphasized that the problem of density estimation is fundamentally ill-
posed, because there are inÔ¨Ånitely many probability distributions that could have
given rise to the observed Ô¨Ånite data set Indeed, any distribution p(x) that is non-
zero at each of the data points x1;:::;xNis a potential candidate Here we constrain
the space of distributions to be Gaussians, which leads to a well-deÔ¨Åned solution Data points that are drawn independently from the same distribution are said to
beindependent and identically distributed, which is often abbreviated to i.i.d We have seen that the joint probability of two independent events is given by
the product of the marginal probabilities for each event separately Because our data
38 2 PROBABILITIES
Figure 2.9 Illustration of the likelihood func-
tion for the Gaussian distribution
shown by the red curve Here
the grey points denote a data set
of values{xn}, and the likelihood
function (2.55) is given by the
product of the corresponding val-
ues ofp(x) denoted by the blue
points Maximizing the likelihood
involves adjusting the mean and
variance of the Gaussian so as to
maximize this product xn
N/parenleftbig
xn|¬µ,œÉ2/parenrightbig
xp(x)
setxis
i.i.d., we can therefore write the probability of the data set, given and2,
in the form
p(x|;2) =N/productdisplay
n=1N/parenleftbig
xn|;2/parenrightbig
: (2.55)
When viewed as a function of and2, this is called the likelihood function for the
Gaussian and is interpreted diagrammatically in Figure 2.9

============================================================

=== CHUNK 051 ===
Palavras: 357
Caracteres: 2283
--------------------------------------------------
One common approach for determining the parameters in a probability distribu-
tion using an observed data set, known as maximum likelihood, is to Ô¨Ånd the param-
eter values that maximize the likelihood function This might appear to be a strange
criterion because, from our foregoing discussion of probability theory, it would seem
more natural to maximize the probability of the parameters given the data, not the
probability of the data given the parameters In fact, these two criteria are related Section 2.6.2
To start with, however, we will determine values for the unknown parameters 
and2in the Gaussian by maximizing the likelihood function (2.55) In practice,
it is more convenient to maximize the log of the likelihood function Because the
logarithm is a monotonically increasing function of its argument, maximizing the
log of a function is equivalent to maximizing the function itself Taking the log not
only simpliÔ¨Åes the subsequent mathematical analysis, but it also helps numerically
because the product of a large number of small probabilities can easily underÔ¨Çow the
numerical precision of the computer, and this is resolved by computing the sum of
the log probabilities instead From (2.49) and (2.55), the log likelihood function can
be written in the form
lnp/parenleftbig
x|;2/parenrightbig
=‚àí1
2
2N/summationdisplay
n=1(xn‚àí)2‚àíN
2ln2‚àíN
2ln(2
): (2.56)
Maximizing (2.56) with respect to , we obtain the maximum likelihood solution
given by Exercise 2.15
ML=1
NN/summationdisplay
n
=1xn; (2.57)
2.3 The Gaussian Distribution 39
which is the sample mean, i.e., the mean of the observed values {xn} Similarly,
maximizing (2.56) with respect to 2, we obtain the maximum likelihood solution
for the variance in the form
2
ML=1
NN/summationdisplay
n
=1(xn‚àíML)2; (2.58)
which is the sample variance measured with respect to the sample mean ML Note
that we are performing a joint maximization of (2.56) with respect to and2, but
for a Gaussian distribution, the solution for decouples from that for 2so that we
can Ô¨Årst evaluate (2.57) and then subsequently use this result to evaluate (2.58) 2.3.3 Bias of maximum likelihood
The technique of maximum likelihood is widely used in deep learning and forms
the foundation for most machine learning algorithms

============================================================

=== CHUNK 052 ===
Palavras: 370
Caracteres: 2300
--------------------------------------------------
However, it has some limita-
tions, which we can illustrate using a univariate Gaussian We Ô¨Årst note that the maximum likelihood solutions MLand2
MLare functions
of the data set values x1;:::;xN Suppose that each of these values has been gen-
erated independently from a Gaussian distribution whose true parameters are and
2 Now consider the expectations of MLand2
MLwith respect to these data set
values It is straightforward to show that Exercise 2.16
E[ML] = (2.59)
E[2
ML] =/parenleftbiggN‚àí1
N/parenrightbigg
2: (2.60)
W
e see that, when averaged over data sets of a given size, the maximum likelihood
solution for the mean will equal the true mean However, the maximum likelihood
estimate of the variance will underestimate the true variance by a factor (N‚àí1)=N This is an example of a phenomenon called bias in which the estimator of a random
quantity is systematically different from the true value The intuition behind this
result is given by Figure 2.10 Note that bias arises because the variance is measured relative to the maximum
likelihood estimate of the mean, which itself is tuned to the data Suppose instead
we had access to the true mean and we used this to determine the variance using
the estimator
/hatwide2=1
NN/summationdisplay
n
=1(xn‚àí)2: (2.61)
Then we Ô¨Ånd that Exercise 2.17
E/bracketleftbig
/hatwide2/bracketrightbig
=2; (2.62)
which is unbiased Of course, we do not have access to the true mean but only
to the observed data values From the result (2.60) it follows that for a Gaussian
distribution, the following estimate for the variance parameter is unbiased:
/tildewide2=N
N‚àí12
ML=1
N‚àí1N/summationdisplay
n
=1(xn‚àíML)2: (2.63)
40 2 PROBABILITIES
¬µ
¬µ
¬µ
Figure
2.10 Illustration of how bias arises when using maximum likelihood to determine the mean and variance
of a Gaussian The red curves show the true Gaussian distribution from which data is generated, and the three
blue curves show the Gaussian distributions obtained by Ô¨Åtting to three data sets, each consisting of two data
points shown in green, using the maximum likelihood results (2.57) and (2.58) Averaged across the three data
sets, the mean is correct, but the variance is systematically underestimated because it is measured relative to
the sample mean and not relative to the true mean

============================================================

=== CHUNK 053 ===
Palavras: 360
Caracteres: 2190
--------------------------------------------------
Correcting for the bias of maximum likelihood in complex models such as neural
networks is not so easy, however Note that the bias of the maximum likelihood solution becomes less signiÔ¨Åcant
as the number Nof data points increases In the limit N‚Üí ‚àû the maximum
likelihood solution for the variance equals the true variance of the distribution that
generated the data In the case of the Gaussian, for anything other than small N, this
bias will not prove to be a serious problem However, throughout this book we will
be interested in complex models with many parameters, for which the bias problems
associated with maximum likelihood will be much more severe In fact, the issue of
bias in maximum likelihood is closely related to the problem of over-Ô¨Åtting Section 2.6.3
2.3.4 Linear regression
We have seen how the problem of linear regression can be expressed in terms of
error minimization Here we return to this example and view it from a probabilistic Section 1.2
perspective, thereby gaining some insights into error functions and regularization The goal in the regression problem is to be able to make predictions for the
target variable tgiven some new value of the input variable xby using a set of
training data comprising Ninput values x= (x1;:::;xN)and their corresponding
target values t= (t 1;:::;tN) We can express our uncertainty over the value of the
target variable using a probability distribution For this purpose, we will assume that,
given the value of x, the corresponding value of thas a Gaussian distribution with
a mean equal to the value y(x;w)of the polynomial curve given by (1.1), where w
are the polynomial coefÔ¨Åcients, and a variance 2 Thus, we have
p(t|x; w;2) =N/parenleftbig
t|y(x;w);2/parenrightbig
: (2.64)
This is illustrated schematically in Figure 2.11 We now use the training data {x;t}to determine the values of the unknown
parameters wand2by maximum likelihood If the data is assumed to be drawn
2.3 The Gaussian Distribution 41
Figure 2.11 Schematic illustration of a Gaus-
sian conditional distribution for t
givenx, deÔ¨Åned by (2.64), in
which the mean is given by the
polynomial function y(x;w), and
the variance is given by the pa-
rameter2

============================================================

=== CHUNK 054 ===
Palavras: 361
Caracteres: 2431
--------------------------------------------------
t
x
x0p(t|x0,w,œÉ2)y(x,w)
independently from the distribution (2.64), then the likelihood function is given by
p(t|x; w;2) =N/productdisplay
n=1N/parenleftbig
tn|y(xn;w);2/parenrightbig
: (2.65)
As we did for the simple Gaussian distribution earlier, it is convenient to maximize
the logarithm of the likelihood function Substituting for the Gaussian distribution,
given by (2.49), we obtain the log likelihood function in the form
lnp(t|x; w;2) =‚àí1
22N/summationdisplay
n=1{y(xn;w)‚àítn}2‚àíN
2ln2‚àíN
2ln(2 ):(2.66)
Consider Ô¨Årst the evaluation of the maximum likelihood solution for the polynomial
coefÔ¨Åcients, which will be denoted by wML These are determined by maximizing
(2.66) with respect to w For this purpose, we can omit the last two terms on the
right-hand side of (2.66) because they do not depend on w Also, note that scaling
the log likelihood by a positive constant coefÔ¨Åcient does not alter the location of the
maximum with respect to w, and so we can replace the coefÔ¨Åcient 1=22with1=2 Finally, instead of maximizing the log likelihood, we can equivalently minimize the
negative log likelihood We therefore see that maximizing the likelihood is equiva-
lent, so far as determining wis concerned, to minimizing the sum-of-squares error
function deÔ¨Åned by
E(w) =1
2N/summationdisplay
n=1{y(xn;w)‚àítn}2: (2.67)
Thus, the sum-of-squares error function has arisen as a consequence of maximizing
the likelihood under the assumption of a Gaussian noise distribution PROBABILITIES
We can also use maximum likelihood to determine the variance parameter 2 Maximizing (2.66) with respect to 2gives Exercise 2.18
2
ML=1
NN/summationdisplay
n=1{y(xn;wML)‚àítn}2: (2.68)
Note that we can Ô¨Årst determine the parameter vector wMLgoverning the mean,
and subsequently use this to Ô¨Ånd the variance 2
MLas was the case for the simple
Gaussian distribution Having determined the parameters wand2, we can now make predictions for
new values of x Because we now have a probabilistic model, these are expressed
in terms of the predictive distribution that gives the probability distribution over t,
rather than simply a point estimate, and is obtained by substituting the maximum
likelihood parameters into (2.64) to give
p(t|x; wML;2
ML) =N/parenleftbig
t|y(x;wML);2
ML/parenrightbig
: (2.69)
2.4 Transformation of Densities
We turn now to a discussion of how a probability density transforms under a nonlin-
ear change of variable

============================================================

=== CHUNK 055 ===
Palavras: 362
Caracteres: 2648
--------------------------------------------------
This property will play a crucial role when we discuss a class
of generative models called normalizing Ô¨Çows It also highlights that a probability Chapter 18
density has a different behaviour than a simple function under such transformations Consider a single variable xand suppose we make a change of variables x=
g(y), then a function f(x)becomes a new function /tildewidef(y)deÔ¨Åned by
/tildewidef(y) =f(g(y)): (2.70)
Now consider a probability density px(x), and again change variables using x=
g(y), giving rise to a density py(y)with respect to the new variable y, where the
sufÔ¨Åxes denote that px(x)andpy(y)are different densities Observations falling in
the range (x;x +x)will, for small values of x, be transformed into the range
(y;y +y), wherex=g(y), andpx(x)x/similarequalpy(y)y Hence, if we take the limit
x‚Üí0, we obtain
py(y) =px(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingledx
dy/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=px(g(y))/vextendsingle/vextendsingle/vextendsingle/vextendsingledg
dy/vextendsingle/vextendsingle/vextendsingle/vextendsingle: (2.71)
Here the modulus|¬∑|arises because the derivative dy=dxcould be negative, whereas
the density is scaled by the ratio of lengths, which is always positive Transformation of Densities 43
This procedure for transforming densities can be very powerful Any density
p(y)can be obtained from a Ô¨Åxed density q(x)that is everywhere non-zero by mak-
ing a nonlinear change of variable y=f(x)in whichf(x)is a monotonic function
so that 06f/prime(x)<‚àû Exercise 2.19
One consequence of the transformation property (2.71) is that the concept of the
maximum of a probability density is dependent on the choice of variable Suppose
f(x)has a mode (i.e., a maximum) at /hatwidexso thatf/prime(/hatwidex) = 0 The corresponding mode
of/tildewidef(y)will occur for a value /hatwideyobtained by differentiating both sides of (2.70) with
respect toy:
/tildewidef/prime(/hatwidey) =f/prime(g(/hatwidey))g/prime(/hatwidey) = 0: (2.72)
Assumingg/prime(/hatwidey)/negationslash= 0 at the mode, then f/prime(g(/hatwidey)) = 0 However, we know that
f/prime(/hatwidex) = 0 , and so we see that the locations of the mode expressed in terms of each
of the variables xandyare related by/hatwidex=g(/hatwidey), as one would expect Thus, Ô¨Ånding
a mode with respect to the variable xis equivalent to Ô¨Årst transforming to the variable
y, then Ô¨Ånding a mode with respect to y, and then transforming back to x Now consider the behaviour of a probability density px(x)under the change of
variablesx=g(y), where the density with respect to the new variable is py(y)and
is given by (2.71)

============================================================

=== CHUNK 056 ===
Palavras: 371
Caracteres: 2259
--------------------------------------------------
To deal with the modulus in (2.71) we can write g/prime(y) =s|g/prime(y)|
wheres‚àà{‚àí 1;+1} Then (2.71) can be written as
py(y) =px(g(y))sg/prime(y)
where we have used 1=s=s Differentiating both sides with respect to ythen gives
p/prime
y(y) =sp/prime
x(g(y)){g/prime(y)}2+spx(g(y))g/prime/prime(y): (2.73)
Due to the presence of the second term on the right-hand side of (2.73), the rela-
tionship/hatwidex=g(/hatwidey)no longer holds Thus, the value of xobtained by maximizing
px(x)will not be the value obtained by transforming to py(y)then maximizing with
respect toyand then transforming back to x This causes modes of densities to be
dependent on the choice of variables However, for a linear transformation, the sec-
ond term on the right-hand side of (2.73) vanishes, and so in this case the location of
the maximum transforms according to /hatwidex=g(/hatwidey) This effect can be illustrated with a simple example, as shown in Figure 2.12 We
begin by considering a Gaussian distribution px(x)overxshown by the red curve
inFigure 2.12 Next we draw a sample of N= 50;000 points from this distribution
and plot a histogram of their values, which as expected agrees with the distribution
px(x) Now consider a nonlinear change of variables from xtoygiven by
x=g(y) = ln(y )‚àíln(1‚àíy) + 5: (2.74)
The inverse of this function is given by
y=g‚àí1(x) =1
1
+ exp(‚àíx + 5); (2.75)
which is a logistic sigmoid function and is shown in Figure 2.12 by the blue curve PROBABILITIES
Figure 2.12 Example of the transformation of
the mode of a density under a
nonlinear change of variables, il-
lustrating the different behaviour
compared to a simple function 0 5 1000.51
g‚àí1(x)
px(x)py(y)
y
x
If
we simply transform px(x)as a function of xwe obtain the green curve
px(g(y))shown in Figure 2.12, and we see that the mode of the density px(x)is
transformed via the sigmoid function to the mode of this curve However, the den-
sity overytransforms instead according to (2.71) and is shown by the magenta curve
on the left side of the diagram Note that this has its mode shifted relative to the mode
of the green curve To conÔ¨Årm this result, we take our sample of 50;000 values ofx, evaluate the
corresponding values of yusing (2.75), and then plot a histogram of their values

============================================================

=== CHUNK 057 ===
Palavras: 376
Caracteres: 2291
--------------------------------------------------
We
see that this histogram matches the magenta curve in Figure 2.12 and not the green
curve 2.4.1 Multivariate distributions
We can extend the result (2.71) to densities deÔ¨Åned over multiple variables Con-
sider a density p(x) over aD-dimensional variable x= (x1;:::;xD)T, and suppose
we transform to a new variable y= (y 1;:::;yD)Twhere x=g(y) Here we will
limit ourselves to the case where xandyhave the same dimensionality The trans-
formed density is then given by the generalization of (2.71) in the form
py(y) =px(x)|detJ| (2.76)
where Jis the Jacobian matrix whose elements are given by the partial derivatives
Jij=@gi=@yj, so that
J=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞@g1
@
y1:::@g1
@
yD @gD
@
y1:::@gD
@
yDÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª: (2.77)
Intuitively, we can view the change of variables as expanding some regions of space
and contracting others, with an inÔ¨Ånitesimal region ‚àÜxaround a point xbeing trans-
formed to a region ‚àÜyaround the point y=g(x) The absolute value of the deter-
minant of the Jacobian represents the ratio of these volumes and is the same factor
2.4 Transformation of Densities 45
x1
x2
y1y2
x1
x2
y1y2
x1
x2
y1y2
Figure
2.13 Illustration of the effect of a change of variables on a probability distribution in two dimensions The left column shows the transforming of the variables whereas the middle and right columns show the corre-
sponding effects on a Gaussian distribution and on samples from that distribution, respectively that arises when changing variables within an integral The formula (2.77) follows
from the fact that the probability mass in region ‚àÜxis the same as the probability
mass in ‚àÜy Once again, we take the modulus to ensure that the density is non-
negative We can illustrate this by applying a change of variables to a Gaussian distribution
in two dimensions, as shown in the top row in Figure 2.13 Here the transformation
fromxtoyis given by Exercise 2.20
y1=x1+ tanh(5x 1) (2.78)
y2=x2+ tanh(5x 2) +x3
1
3: (2.79)
Also
shown on the bottom row are samples from a Gaussian distribution in x-space
along with the corresponding transformed samples in y-space Information Theory
Probability theory forms the basis for another important framework called informa-
tion theory, which quantiÔ¨Åes the information present in a data set and which plays
an important role in machine learning

============================================================

=== CHUNK 058 ===
Palavras: 389
Caracteres: 2286
--------------------------------------------------
Here we give a brief introduction to some of
the key elements of information theory that we will need later in the book, including
the important concept of entropy in its various forms For a more comprehensive in-
troduction to information theory, with connections to machine learning, see MacKay
(2003) 2.5.1 Entropy
We begin by considering a discrete random variable xand we ask how much
information is received when we observe a speciÔ¨Åc value for this variable The
amount of information can be viewed as the ‚Äòdegree of surprise‚Äô on learning the
value ofx If we are told that a highly improbable event has just occurred, we will
have received more information than if we were told that some very likely event has
just occurred, and if we knew that the event was certain to happen, we would receive
no information Our measure of information content will therefore depend on the
probability distribution p(x), and so we look for a quantity h(x) that is a monotonic
function of the probability p(x) and that expresses the information content The form
ofh(¬∑) can be found by noting that if we have two events xandythat are unrelated,
then the information gained from observing both of them should be the sum of the
information gained from each of them separately, so that h(x;y ) =h(x) +h(y) Two unrelated events are statistically independent and so p(x;y ) =p(x)p(y ) From
these two relationships, it is easily shown that h(x) must be given by the logarithm
ofp(x) and so we have Exercise 2.21
h(x) =‚àílog2p(x) (2.80)
where the negative sign ensures that information is positive or zero Note that low
probability events xcorrespond to high information content The choice of base for
the logarithm is arbitrary, and for the moment we will adopt the convention prevalent
in information theory of using logarithms to the base of 2 In this case, as we will
see shortly, the units of h(x)are bits (‚Äòbinary digits‚Äô) Now suppose that a sender wishes to transmit the value of a random variable to
a receiver The average amount of information that they transmit in the process is
obtained by taking the expectation of (2.80) with respect to the distribution p(x) and
is given by
H[x] =‚àí/summationdisplay
xp(x) log2p(x): (2.81)
This important quantity is called the entropy of the random variable x

============================================================

=== CHUNK 059 ===
Palavras: 355
Caracteres: 2017
--------------------------------------------------
Note that
lim‚Üí0(ln) = 0 and so we will take p(x) lnp(x) = 0 whenever we encounter a
value forxsuch thatp(x) = 0 So far, we have given a rather heuristic motivation for the deÔ¨Ånition of informa-
tion (2.80) and the corresponding entropy (2.81) We now show that these deÔ¨Ånitions
2.5 Information Theory 47
indeed possess useful properties Consider a random variable xhaving eight possible
states, each of which is equally likely To communicate the value of xto a receiver,
we would need to transmit a message of length 3 bits Notice that the entropy of this
variable is given by
H[x] =‚àí8√ó1
8log21
8= 3 bits:
Now consider an example (Cover and Thomas, 1991) of a variable having eight
possible states{a;b;c;d;e;f;g;h }for which the respective probabilities are given
by(1
2;1
4;1
8;1
16;1
64;1
64;1
64;1
64) The entropy in this case is given by
H[x] =‚àí1
2log21
2‚àí1
4log21
4‚àí1
8log21
8‚àí1
16log21
16‚àí4
64log21
64= 2 bits:
We see that the nonuniform distribution has a smaller entropy than the uniform one,
and we will gain some insight into this shortly when we discuss the interpretation of
entropy in terms of disorder For the moment, let us consider how we would transmit
the identity of the variable‚Äôs state to a receiver We could do this, as before, using
a 3-bit number However, we can take advantage of the nonuniform distribution by
using shorter codes for the more probable events, at the expense of longer codes
for the less probable events, in the hope of getting a shorter average code length This can be done by representing the states {a;b;c;d;e;f;g;h} using, for instance,
the following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, and
111111 The average length of the code that has to be transmitted is then
average code length =1
2√ó1 +1
4√ó2 +1
8√ó3 +1
16√ó4 + 4√ó1
64√ó6 = 2 bits;
which again is the same as the entropy of the random variable Note that shorter code
strings cannot be used because it must be possible to disambiguate a concatenation
of such strings into its component parts

============================================================

=== CHUNK 060 ===
Palavras: 370
Caracteres: 2284
--------------------------------------------------
For instance, 11001110 decodes uniquely
into the state sequence c,a,d This relation between entropy and shortest coding
length is a general one The noiseless coding theorem (Shannon, 1948) states that
the entropy is a lower bound on the number of bits needed to transmit the state of a
random variable From now on, we will switch to the use of natural logarithms in deÔ¨Åning entropy,
as this will provide a more convenient link with ideas elsewhere in this book In this
case, the entropy is measured in units of nats (from ‚Äònatural logarithm‚Äô) instead of
bits, which differ simply by a factor of ln 2 2.5.2 Physics perspective
We have introduced the concept of entropy in terms of the average amount of
information needed to specify the state of a random variable In fact, the concept of
entropy has much earlier origins in physics where it was introduced in the context
of equilibrium thermodynamics and later given a deeper interpretation as a measure
of disorder through developments in statistical mechanics We can understand this
alternative view of entropy by considering a set of Nidentical objects that are to be
divided amongst a set of bins, such that there are niobjects in the ith bin PROBABILITIES
the number of different ways of allocating the objects to the bins There are N
ways to choose the Ô¨Årst object, (N‚àí1)ways to choose the second object, and
so on, leading to a total of N!ways to allocate all Nobjects to the bins, where N (pronounced ‚ÄòN factorial‚Äô) denotes the product N√ó(N‚àí1)√ó¬∑¬∑¬∑√ó 2√ó1 However,
we do not wish to distinguish between rearrangements of objects within each bin In
theith bin there are ni!ways of reordering the objects, and so the total number of
ways of allocating the Nobjects to the bins is given by
W=N!/producttext
ini!; (2.82)
which
is called the multiplicity The entropy is then deÔ¨Åned as the logarithm of the
multiplicity scaled by a constant factor 1=N so that
H =1
NlnW=1
NlnN!‚àí1
N/summationdisplay
ilnni : (2.83)
We now consider the limit N‚Üí‚àû, in which the fractions ni=Nare held Ô¨Åxed, and
apply Stirling‚Äôs approximation:
lnN!/similarequalNlnN‚àíN; (2.84)
which gives
H =‚àílim
N‚Üí‚àû/summationdisplay
i/parenleftBigni
N/parenrightBig
ln/parenleftBigni
N/parenrightBig
=‚àí/summationdisplay
ipilnpi (2.85)
where
we have used/summationtext
ini=N

============================================================

=== CHUNK 061 ===
Palavras: 356
Caracteres: 2304
--------------------------------------------------
Herepi= limN‚Üí‚àû(ni=N)is the probability of
an object being assigned to the ith bin In physics terminology, the speciÔ¨Åc allocation
of objects into bins is called a microstate, and the overall distribution of occupation
numbers, expressed through the ratios ni=N, is called a macrostate The multiplicity
W, which expresses the number of microstates in a given macrostate, is also known
as the weight of the macrostate We can interpret the bins as the states xiof a discrete random variable X, where
p(X=xi) =pi The entropy of the random variable Xis then
H[p] =‚àí/summationdisplay
ip(xi) lnp(xi): (2.86)
Distributions p(xi)that are sharply peaked around a few values will have a relatively
low entropy, whereas those that are spread more evenly across many values will have
higher entropy, as illustrated in Figure 2.14 Because 06pi61, the entropy is non-negative, and it will equal its minimum
value of 0 when one of the pi= 1 and all other pj/negationslash=i= 0 The maximum entropy
conÔ¨Åguration can be found by maximizing Husing a Lagrange multiplier to enforce Appendix C
the normalization constraint on the probabilities Thus, we maximize
/tildewideH =‚àí/summationdisplay
ip(xi) lnp(xi) +/parenleftBigg/summationdisplay
ip(xi)‚àí1/parenrightBigg
(2.87)
2.5 Information Theory 49
probabilitiesH = 1.77
00.250.5
probabilitiesH = 3.09
00.250.5
Figure 2.14 Histograms of two probability distributions over 30bins illustrating the higher value of the entropy
Hfor the broader distribution The largest entropy would arise from a uniform distribution which would give
H =‚àíln(1=30) = 3:40 from which we Ô¨Ånd that all of the p(xi)are equal and are given by p(xi) = 1=M
whereMis the total number of states xi The corresponding value of the entropy
is then H = lnM This result can also be derived from Jensen‚Äôs inequality (to be Exercise 2.22
discussed shortly) To verify that the stationary point is indeed a maximum, we can Exercise 2.23
evaluate the second derivative of the entropy, which gives
@/tildewideH
@p(xi)@p(xj)=‚àíIij1
pi(2.88)
whereIijare the elements of the identity matrix We see that these values are all
negative and, hence, the stationary point is indeed a maximum 2.5.3 Differential entropy
We can extend the deÔ¨Ånition of entropy to include distributions p(x) over con-
tinuous variables xas follows

============================================================

=== CHUNK 062 ===
Palavras: 361
Caracteres: 2509
--------------------------------------------------
First divide xinto bins of width ‚àÜ Then, assuming
thatp(x) is continuous, the mean value theorem (Weisstein, 1999) tells us that, for
each such bin, there must exist a value xiin the range i‚àÜ6xi6(i+ 1)‚àÜ such that
/integraldisplay(i+1)
ip(x) dx=p(xi)‚àÜ: (2.89)
We can now quantize the continuous variable xby assigning any value xto the value
xiwheneverxfalls in theith bin The probability of observing the value xiis then
50 2 This gives a discrete distribution for which the entropy takes the form
H=‚àí/summationdisplay
ip(xi)‚àÜ ln (p(xi)‚àÜ) =‚àí/summationdisplay
ip(xi)‚àÜ lnp(xi)‚àíln ‚àÜ (2.90)
where we have used/summationtext
ip(xi)‚àÜ = 1, which follows from (2.89) and (2.25) We now
omit the second term ‚àíln ‚àÜ on the right-hand side of (2.90), since it is independent
ofp(x), and then consider the limit ‚àÜ‚Üí0 The Ô¨Årst term on the right-hand side of
(2.90) will approach the integral of p(x) lnp(x) in this limit so that
lim
‚Üí0/braceleftBigg
‚àí/summationdisplay
ip(xi)‚àÜ lnp(xi)/bracerightBigg
=‚àí/integraldisplay
p(x) lnp(x) dx (2.91)
where the quantity on the right-hand side is called the differential entropy We see
that the discrete and continuous forms of the entropy differ by a quantity ln ‚àÜ, which
diverges in the limit ‚àÜ‚Üí0 This reÔ¨Çects that specifying a continuous variable
very precisely requires a large number of bits For a density deÔ¨Åned over multiple
continuous variables, denoted collectively by the vector x, the differential entropy is
given by
H[x] =‚àí/integraldisplay
p(x) lnp(x) d x: (2.92)
2.5.4 Maximum entropy
We saw for discrete distributions that the maximum entropy conÔ¨Åguration cor-
responds to a uniform distribution of probabilities across the possible states of the
variable Let us now consider the corresponding result for a continuous variable If
this maximum is to be well deÔ¨Åned, it will be necessary to constrain the Ô¨Årst and
second moments of p(x) and to preserve the normalization constraint We therefore
maximize the differential entropy with the three constraints:
/integraldisplay‚àû
‚àí‚àûp(x) dx= 1 (2.93)
/integraldisplay‚àû
‚àí‚àûxp(x) dx= (2.94)
/integraldisplay‚àû
‚àí‚àû(x‚àí)2p(x) dx=2: (2.95)
The constrained maximization can be performed using Lagrange multipliers so that Appendix C
we maximize the following functional with respect to p(x):
‚àí/integraldisplay‚àû
‚àí‚àûp(x) lnp(x) dx+1/parenleftbigg/integraldisplay‚àû
‚àí‚àûp(x) dx‚àí1/parenrightbigg
+2/parenleftbigg/integraldisplay‚àû
‚àí‚àûxp(x) dx‚àí/parenrightbigg
+3/parenleftbigg/integraldisplay‚àû
‚àí‚àû(x‚àí)2p(x) dx‚àí2/parenrightbigg
:(2.96)
2.5

============================================================

=== CHUNK 063 ===
Palavras: 374
Caracteres: 2505
--------------------------------------------------
Information Theory 51
Using the calculus of variations, we set the derivative of this functional to zero giving Appendix B
p(x) = exp/braceleftbig
‚àí1 +1+2x+3(x‚àí)2/bracerightbig
: (2.97)
The Lagrange multipliers can be found by back-substitution of this result into the
three constraint equations, leading Ô¨Ånally to the result: Exercise 2.24
p(x) =1
(2
2)1=2exp/braceleftbigg
‚àí(x‚àí)2
2
2/bracerightbigg
; (2.98)
and so the distribution that maximizes the differential entropy is the Gaussian Note
that we did not constrain the distribution to be non-negative when we maximized the
entropy However, because the resulting distribution is indeed non-negative, we see
with hindsight that such a constraint is not necessary If we evaluate the differential entropy of the Gaussian, we obtain Exercise 2.25
H[x] =1
2/braceleftbig
1
+ ln(22)/bracerightbig
: (2.99)
Thus, we see again that the entropy increases as the distribution becomes broader,
i.e., as2increases This result also shows that the differential entropy, unlike the
discrete entropy, can be negative, because H(x)<0in (2.99) for 2<1=(2e) 2.5.5 Kullback‚ÄìLeibler divergence
So far in this section, we have introduced a number of concepts from informa-
tion theory, including the key notion of entropy We now start to relate these ideas
to machine learning Consider some unknown distribution p(x), and suppose that
we have modelled this using an approximating distribution q(x) If we use q(x)to
construct a coding scheme for transmitting values of xto a receiver, then the average
additional amount of information (in nats) required to specify the value of x(assum-
ing we choose an efÔ¨Åcient coding scheme) as a result of using q(x)instead of the
true distribution p(x) is given by
KL(p/bardblq ) =‚àí/integraldisplay
p(x) lnq(x) dx‚àí/parenleftbigg
‚àí/integraldisplay
p(x) lnp(x) d x/parenrightbigg
=‚àí/integraldisplay
p(x) ln/braceleftbiggq(x)
p
(x)/bracerightbigg
dx: (2.100)
This is known as the relative entropy orKullback‚ÄìLeibler divergence, or KL diver-
gence (Kullback and Leibler, 1951), between the distributions p(x) andq(x) Note
that it is not a symmetrical quantity, that is to say KL(p/bardblq )/negationslash‚â°KL(q/bardblp) We now show that the Kullback‚ÄìLeibler divergence satisÔ¨Åes KL(p/bardblq )>0with
equality if, and only if, p(x) =q(x) To do this we Ô¨Årst introduce the concept of
convex functions A function f(x)is said to be convex if it has the property that
every chord lies on or above the function, as shown in Figure 2.15

============================================================

=== CHUNK 064 ===
Palavras: 364
Caracteres: 2454
--------------------------------------------------
Any value of xin the interval from x=atox=bcan be written in the
forma+ (1‚àí)bwhere 0661 The corresponding point on the chord
52 2 PROBABILITIES
Figure 2.15 A convex function f(x)is one for which ev-
ery chord (shown in blue) lies on or above
the function (shown in red) x
a b xcho
rd
xf(x)
is given by f(a) + (1‚àí)f(b), and the corresponding value of the function is
f(a+ (1‚àí)b) Convexity then implies
f(a+ (1‚àí)b)6f(a) + (1‚àí)f(b): (2.101)
This is equivalent to the requirement that the second derivative of the function be
everywhere positive Examples of convex functions are xlnx(forx>0) andx2 A Exercise 2.32
function is called strictly convex if the equality is satisÔ¨Åed only for = 0and= 1 If a function has the opposite property, namely that every chord lies on or below the
function, it is called concave, with a corresponding deÔ¨Ånition for strictly concave If
a functionf(x)is convex, then‚àíf(x)will be concave Using the technique of proof by induction, we can show from (2.101) that a Exercise 2.33
convex function f(x)satisÔ¨Åes
f/parenleftBiggM/summationdisplay
i=1ixi/parenrightBigg
6M/summationdisplay
i=1if(xi) (2.102)
wherei>0and/summationtext
ii= 1, for any set of points {xi} The result (2.102) is known
asJensen‚Äôs inequality If we interpret the ias the probability distribution over a
discrete variable xtaking the values{xi}, then (2.102) can be written
f(E[x]) 6E[f(x)] (2.103)
where E[¬∑]denotes the expectation For continuous variables, Jensen‚Äôs inequality
takes the form
f/parenleftbigg/integraldisplay
xp(x) d x/parenrightbigg
6/integraldisplay
f(x)p(x) d x: (2.104)
We can apply Jensen‚Äôs inequality in the form (2.104) to the Kullback‚ÄìLeibler
divergence (2.100) to give
KL(p/bardblq ) =‚àí/integraldisplay
p(x) ln/braceleftbiggq(x)
p(x)/bracerightbigg
dx>‚àíln/integraldisplay
q(x) dx= 0 (2.105)
2.5 Information Theory 53
where we have used ‚àílnxis a convex function, together with the normalization
condition/integraltext
q(x) dx= 1 In fact,‚àílnxis a strictly convex function, so the equality
will hold if, and only if, q(x) =p(x) for all x Thus, we can interpret the Kullback‚Äì
Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and
q(x) We see that there is an intimate relationship between data compression and den-
sity estimation (i.e., the problem of modelling an unknown probability distribution)
because the most efÔ¨Åcient compression is achieved when we know the true distri-
bution

============================================================

=== CHUNK 065 ===
Palavras: 370
Caracteres: 2423
--------------------------------------------------
If we use a distribution that is different from the true one, then we must
necessarily have a less efÔ¨Åcient coding, and on average the additional information
that must be transmitted is (at least) equal to the Kullback‚ÄìLeibler divergence be-
tween the two distributions Suppose that data is being generated from an unknown distribution p(x) that we
wish to model We can try to approximate this distribution using some parametric
distribution q(x|), governed by a set of adjustable parameters  One way to de-
termineis to minimize the Kullback‚ÄìLeibler divergence between p(x) andq(x|)
with respect to  We cannot do this directly because we do not know p(x) Suppose,
however, that we have observed a Ô¨Ånite set of training points xn, forn= 1;:::;N ,
drawn fromp(x) Then the expectation with respect to p(x) can be approximated by
a Ô¨Ånite sum over these points, using (2.40), so that
KL(p/bardblq )/similarequal1
NN/summationdisplay
n=1/braceleftBig
‚àílnq(xn|) + lnp(xn)/bracerightBig
: (2.106)
The second term on the right-hand side of (2.106) is independent of , and the Ô¨Årst
term is the negative log likelihood function for under the distribution q(x|)eval-
uated using the training set Thus, we see that minimizing this Kullback‚ÄìLeibler
divergence is equivalent to maximizing the log likelihood function Exercise 2.34
2.5.6 Conditional entropy
Now consider the joint distribution between two sets of variables xandygiven
byp(x;y)from which we draw pairs of values of xandy If a value of xis already
known, then the additional information needed to specify the corresponding value of
yis given by‚àílnp(y|x) Thus the average additional information needed to specify
ycan be written as
H[y|x] =‚àí/integraldisplay/integraldisplay
p(y;x) lnp(y|x) dydx; (2.107)
which is called the conditional entropy ofygiven x It is easily seen, using the
product rule, that the conditional entropy satisÔ¨Åes the relation: Exercise 2.35
H[x;y] = H[ y|x] + H[x] (2.108)
where H[x;y]is the differential entropy of p(x;y)andH[x] is the differential en-
tropy of the marginal distribution p(x) Thus, the information needed to describe x
andyis given by the sum of the information needed to describe xalone plus the
additional information required to specify ygiven x PROBABILITIES
2.5.7 Mutual information
When two variables xandyare independent, their joint distribution will factor-
ize into the product of their marginals p(x;y) =p(x)p(y )

============================================================

=== CHUNK 066 ===
Palavras: 350
Caracteres: 2238
--------------------------------------------------
If the variables are not
independent, we can gain some idea of whether they are ‚Äòclose‚Äô to being independent
by considering the Kullback‚ÄìLeibler divergence between the joint distribution and
the product of the marginals, given by
I[x;y]‚â°KL(p(x; y)/bardblp(x)p(y ))
=‚àí/integraldisplay/integraldisplay
p(x;y) ln/parenleftbiggp(x)p(y )
p
(x;y)/parenrightbigg
dxdy; (2.109)
which is called the mutual information between the variables xandy From the
properties of the Kullback‚ÄìLeibler divergence, we see that I[x;y]>0with equal-
ity if, and only if, xandyare independent Using the sum and product rules of
probability, we see that the mutual information is related to the conditional entropy
through Exercise 2.38
I[x;y] = H[ x]‚àíH[x|y ] = H[ y]‚àíH[y|x]: (2.110)
Thus, the mutual information represents the reduction in the uncertainty about xby
virtue of being told the value of y(or vice versa) From a Bayesian perspective, we
can viewp(x) as the prior distribution for xandp(x|y )as the posterior distribution
after we have observed new data y The mutual information therefore represents the
reduction in uncertainty about xas a consequence of the new observation y Ba
yesian Probabilities
When
we considered the bent coin in Figure 2.2, we introduced the concept of prob-
ability in terms of the frequencies of random, repeatable events, such as the prob-
ability of the coin landing concave side up We will refer to this as the classical
orfrequentist interpretation of probability We also introduced the more general
Bayesian view, in which probabilities provide a quantiÔ¨Åcation of uncertainty In this
case, our uncertainty is whether the concave side of the coin is heads or tails The use of probability to represent uncertainty is not an ad hoc choice but is
inevitable if we are to respect common sense while making rational and coherent
inferences For example, Cox (1946) showed that if numerical values are used to
represent degrees of belief, then a simple set of axioms encoding common sense
properties of such beliefs leads uniquely to a set of rules for manipulating degrees of
belief that are equivalent to the sum and product rules of probability It is therefore
natural to refer to these quantities as (Bayesian) probabilities

============================================================

=== CHUNK 067 ===
Palavras: 351
Caracteres: 2111
--------------------------------------------------
For the bent coin we assumed, in the absence of further information, that the
probability of the concave side of the coin being heads is 0.5 Now suppose we
are told the results of Ô¨Çipping the coin a few times Intuitively, it seems that this
should provide us with some information as to whether the concave side is heads For instance, suppose we see many more Ô¨Çips that land tails than land heads Bayesian Probabilities 55
that the coin is more likely to land concave side up, this provides evidence to suggest
that the concave side is more likely to be tails In fact, this intuition is correct, and
furthermore, we can quantify this using the rules of probability Bayes‚Äô theorem now Exercise 2.40
acquires a new signiÔ¨Åcance, because it allows us to convert the prior probability for
the concave side being heads into a posterior probability by incorporating the data
provided by the coin Ô¨Çips Moreover, this process is iterative, meaning the posterior
probability becomes the prior for incorporating data from further coin Ô¨Çips One aspect of the Bayesian viewpoint is that the inclusion of prior knowledge
arises naturally Suppose, for instance, that a fair-looking coin is tossed three times
and lands heads each time The maximum likelihood estimate of the probability
of landing heads would give 1, implying that all future tosses will land heads By Section 3.1.2
contrast, a Bayesian approach with any reasonable prior will lead to a less extreme
conclusion 2.6.1 Model parameters
The Bayesian perspective provides valuable insights into several aspects of ma-
chine learning, and we can illustrate these using the sine curve regression example Section 1.2
Here we denote the training data set by D We have already seen in the context of
linear regression that the parameters can be chosen using maximum likelihood, in
which wis set to the value that maximizes the likelihood function p(D|w ) This
corresponds to choosing the value of wfor which the probability of the observed
data set is maximized In the machine learning literature, the negative log of the
likelihood function is called an error function

============================================================

=== CHUNK 068 ===
Palavras: 380
Caracteres: 2345
--------------------------------------------------
Because the negative logarithm is a
monotonically decreasing function, maximizing the likelihood is equivalent to min-
imizing the error This leads to a speciÔ¨Åc choice of parameter values, denoted wML,
which are then used to make predictions for new data We have seen that different choices of training data set, for example containing
different numbers of data points, give rise to different solutions for wML From a
Bayesian perspective, we can also use the machinery of probability theory to describe
this uncertainty in the model parameters We can capture our assumptions about w,
before observing the data, in the form of a prior probability distribution p(w) The
effect of the observed data Dis expressed through the likelihood function p(D|w ),
and Bayes‚Äô theorem now takes the form
p(w|D) =p(D|w )p(w )
p(D); (2.111)
which allows us to evaluate the uncertainty in wafter we have observed Din the
form of the posterior probability p(w|D) It is important to emphasize that the quantity p(D|w )is called the likelihood
function when it is viewed as a function of the parameter vector w, and it expresses
how probable the observed data set is for different values of w Note that the likeli-
hoodp(D|w )is not a probability distribution over w, and its integral with respect to
wdoes not (necessarily) equal one Given this deÔ¨Ånition of likelihood, we can state Bayes‚Äô theorem in words:
posterior‚àùlikelihood√óprior (2.112)
56 2 PROBABILITIES
where all of these quantities are viewed as functions of w The denominator in
(2.111) is the normalization constant, which ensures that the posterior distribution
on the left-hand side is a valid probability density and integrates to one Indeed, by
integrating both sides of (2.111) with respect to w, we can express the denominator
in Bayes‚Äô theorem in terms of the prior distribution and the likelihood function:
p(D) =/integraldisplay
p(D|w )p(w ) dw: (2.113)
In both the Bayesian and frequentist paradigms, the likelihood function p(D|w )
plays a central role However, the manner in which it is used is fundamentally dif-
ferent in the two approaches In a frequentist setting, wis considered to be a Ô¨Åxed
parameter, whose value is determined by some form of ‚Äòestimator‚Äô, and error bars on
this estimate are determined (conceptually, at least) by considering the distribution
of possible data sets D

============================================================

=== CHUNK 069 ===
Palavras: 371
Caracteres: 2398
--------------------------------------------------
By contrast, from the Bayesian viewpoint there is only a
single data setD(namely the one that is actually observed), and the uncertainty in
the parameters is expressed through a probability distribution over w 2.6.2 Regularization
We can use this Bayesian perspective to gain insight into the technique of regu-
larization that was used in the sine curve regression example to reduce over-Ô¨Åtting Section 1.2.5
Instead of choosing the model parameters by maximizing the likelihood function
with respect to w, we can maximize the posterior probability (2.111) This technique
is called the maximum a posteriori estimate, or simply MAP estimate Equivalently,
we can minimize the negative log of the posterior probability Taking negative logs
of both sides of (2.111), we have
‚àílnp(w|D) =‚àílnp(D|w )‚àílnp(w) + lnp(D): (2.114)
The Ô¨Årst term on the right-hand side of (2.114) is the usual log likelihood The third
term can be omitted since it does not depend on w The second term takes the form
of a function of w, which is added to the log likelihood, and we can recognize this
as a form of regularization To make this more explicit, suppose we choose the prior
distributionp(w)to be the product of independent zero-mean Gaussian distributions
for each of the elements of wsuch that each has the same variance s2so that
p(w|s) =M/productdisplay
i=0N(wi|0;s2) =M/productdisplay
i=0/parenleftbigg1
2s2/parenrightbigg1=2
exp/braceleftbigg
‚àíw2
i
2s2/bracerightbigg
: (2.115)
Substituting into (2.114), we obtain
‚àílnp(w|D) =‚àílnp(D|w ) +1
2s2M/summationdisplay
i=0w2
i+ const: (2.116)
If we consider the particular case of the linear regression model whose log likeli-
hood is given by (2.66), then we Ô¨Ånd that maximizing the posterior distribution is
equivalent to minimizing the function Exercise 2.41
2.6 Bayesian Probabilities 57
E(w) =1
22N/summationdisplay
n=1{y(xn;w)‚àítn}2+1
2s2wTw: (2.117)
We see that this takes the form of the regularized sum-of-squares error function en-
countered earlier in the form (1.4) 2.6.3 Bayesian machine learning
The Bayesian perspective has allowed us to motivate the use of regularization
and to derive a speciÔ¨Åc form for the regularization term However, the use of Bayes‚Äô
theorem alone does not constitute a truly Bayesian treatment of machine learning
since it is still Ô¨Ånding a single solution for wand does not therefore take account
of uncertainty in the value of w

============================================================

=== CHUNK 070 ===
Palavras: 369
Caracteres: 2414
--------------------------------------------------
Suppose we have a training data set Dand our
goal is to predict some target variable tgiven a new input value x We are therefore
interested in the distribution of tgiven bothxandD From the sum and product
rules of probability, we have
p(t|x;D) =/integraldisplay
p(t|x; w)p(w|D) dw: (2.118)
We see that the prediction is obtained by taking a weighted average p(t|x; w)over all
possible values of win which the weighting function is given by the posterior prob-
ability distribution p(w|D) The key difference that distinguishes Bayesian methods
is this integration over the space of parameters By contrast, conventional frequentist
methods use point estimates for parameters obtained by optimizing a loss function
such as a regularized sum-of-squares This fully Bayesian treatment of machine learning offers some powerful in-
sights For example, the problem of over-Ô¨Åtting, encountered earlier in the context
of polynomial regression, is an example of a pathology arising from the use of max- Section 1.2
imum likelihood, and does not arise when we marginalize over parameters using the
Bayesian approach Similarly, we may have multiple potential models that we could
use to solve a given problem, such as polynomials of different orders in the regres-
sion example A maximum likelihood approach simply picks the model that gives
the highest probability of the data, but this favours ever more complex models, lead-
ing to over-Ô¨Åtting A fully Bayesian treatment involves averaging over all possible
models, with the contribution of each model weighted by its posterior probability Section 9.6
Moreover, this probability is typically highest for models of intermediate complexity Very simple models (such as polynomials of low order) have low probability as they
are unable to Ô¨Åt the data well, whereas very complex models (such as polynomials
of very high order) also have low probability because the Bayesian integration over
parameters automatically and elegantly penalizes complexity For a comprehensive
overview of Bayesian methods applied to machine learning, including neural net-
works, see Bishop (2006) Unfortunately, there is a major drawback with the Bayesian framework, and
this is apparent in (2.118), which involves integrating over the space of parameters Modern deep learning models can have millions or billions of parameters and even
simple approximations to such integrals are typically infeasible

============================================================

=== CHUNK 071 ===
Palavras: 353
Caracteres: 2067
--------------------------------------------------
In fact, given a
58 2 PROBABILITIES
limited compute budget and an ample source of training data, it will often be better to
apply maximum likelihood techniques, generally augmented with one or more forms
of regularization, to a large neural network rather than apply a Bayesian treatment to
a much smaller model Exercises
2.1 (?)In the cancer screening example, we used a prior probability of cancer of p(C=
1) = 0:01 In reality, the prevalence of cancer is generally very much lower Con-
sider a situation in which p(C= 1) = 0:001, and recompute the probability of
having cancer given a positive test p(C= 1|T= 1) Intuitively, the result can ap-
pear surprising to many people since the test seems to have high accuracy and yet a
positive test still leads to a low probability of having cancer 2.2 (??) Deterministic numbers satisfy the property of transitivity, so that if x>y and
y > z then it follows that x > z When we go to random numbers, however, this
property need no longer apply Figure 2.16 shows a set of four cubical dice that have
been arranged in a cyclic order Show that each of the four dice has a 2/3 probability
of rolling a higher number than the previous die in the cycle Such dice are known
asnon-transitive dice, and the speciÔ¨Åc examples shown here are called Efron dice Figure 2.16 An example of non-transitive cu-
bical dice, in which each die
has been ‚ÄòÔ¨Çattened‚Äô to reveal the
numbers on each of the faces The dice have been arranged in
a cycle, such that each die has a
2/3 probability of rolling a higher
number than the previous die in
the cycle 2.3 (?)Consider a variable ygiven by the sum of two independent random variables
y=u+vwhere u‚àºpu(u)andv‚àºpv(v) Show that the distribution py(y)is
given by
p(y) =/integraldisplay
pu(u)pv(y‚àíu) du: (2.119)
This is known as the convolution ofpu(u)andpv(v) 2.4 (??) Verify that the uniform distribution (2.33) is correctly normalized, and Ô¨Ånd
expressions for its mean and variance 2.5 (??) Verify that the exponential distribution (2.34) and the Laplace distribution (2.35)
are correctly normalized

============================================================

=== CHUNK 072 ===
Palavras: 369
Caracteres: 2558
--------------------------------------------------
Exercises 59
2.6 (?)Using the properties of the Dirac delta function, show that the empirical density
(2.37) is correctly normalized 2.7 (?)By making use of the empirical density (2.37), show that the expectation given
by (2.39) can be approximated by a sum over a Ô¨Ånite set of samples drawn from the
density in the form (2.40) 2.8 (?)Using the deÔ¨Ånition (2.44), show that var[f (x)]satisÔ¨Åes (2.45) 2.9 (?)Show that if two variables xandyare independent, then their covariance is zero 2.10 (?)Suppose that the two variables xandzare statistically independent Show that
the mean and variance of their sum satisÔ¨Åes
E[x+z] = E[x] +E[z] (2.120)
var[x +z] = var[x] + var[z]: (2.121)
2.11 (?)Consider two variables xandywith joint distribution p(x;y ) Prove the follow-
ing two results:
E[x] = Ey[Ex[x|y]] (2.122)
var[x] = Ey[varx[x|y]] + vary[Ex[x|y]]: (2.123)
HereEx[x|y]denotes the expectation of xunder the conditional distribution p(x|y ),
with a similar notation for the conditional variance 2.12 (???) In this exercise, we prove the normalization condition (2.51) for the univariate
Gaussian To do this consider, the integral
I=/integraldisplay‚àû
‚àí‚àûexp/parenleftbigg
‚àí1
22x2/parenrightbigg
dx (2.124)
which we can evaluate by Ô¨Årst writing its square in the form
I2=/integraldisplay‚àû
‚àí‚àû/integraldisplay‚àû
‚àí‚àûexp/parenleftbigg
‚àí1
22x2‚àí1
22y2/parenrightbigg
dxdy: (2.125)
Now make the transformation from Cartesian coordinates (x;y)to polar coordinates
(r;)and then substitute u=r2 Show that, by performing the integrals over and
uand then taking the square root of both sides, we obtain
I=/parenleftbig
22/parenrightbig1=2: (2.126)
Finally, use this result to show that the Gaussian distribution N(x|;2)is normal-
ized PROBABILITIES
2.13 (??) By using a change of variables, verify that the univariate Gaussian distribution
given by (2.49) satisÔ¨Åes (2.52) Next, by differentiating both sides of the normaliza-
tion condition /integraldisplay‚àû
‚àí‚àûN/parenleftbig
x|;2/parenrightbig
dx= 1 (2.127)
with respect to 2, verify that the Gaussian satisÔ¨Åes (2.53) Finally, show that (2.54)
holds 2.14 (?)Show that the mode (i.e., the maximum) of the Gaussian distribution (2.49) is
given by 2.15 (?)By setting the derivatives of the log likelihood function (2.56) with respect to 
and2equal to zero, verify the results (2.57) and (2.58) 2.16 (??) Using the results (2.52) and (2.53), show that
E[xnxm] =2+Inm2(2.128)
wherexnandxmdenote data points sampled from a Gaussian distribution with mean
and variance 2andInmsatisÔ¨ÅesInm= 1 ifn=mandInm= 0 otherwise

============================================================

=== CHUNK 073 ===
Palavras: 350
Caracteres: 2141
--------------------------------------------------
Hence prove the results (2.59) and (2.60) 2.17 (??) Using the deÔ¨Ånition (2.61), prove the result (2.62) which shows that the expec-
tation of the variance estimator for a Gaussian distribution based on the true mean is
given by the true variance 2 2.18 (?)Show that maximizing (2.66) with respect to 2gives the result (2.68) 2.19 (??) Use the transformation property (2.71) of a probability density under a change
of variable to show that any density p(y)can be obtained from a Ô¨Åxed density q(x)
that is everywhere non-zero by making a nonlinear change of variable y=f(x)in
whichf(x)is a monotonic function so that 06f/prime(x)<‚àû Write down the differ-
ential equation satisÔ¨Åed by f(x)and draw a diagram illustrating the transformation
of the density 2.20 (?)Evaluate the elements of the Jacobian matrix for the transformation deÔ¨Åned by
(2.78) and (2.79) 2.21 (?)In Section 2.5, we introduced the idea of entropy h(x) as the information gained
on observing the value of a random variable xhaving distribution p(x) We saw
that, for independent variables xandyfor whichp(x;y ) =p(x)p(y ), the entropy
functions are additive, so that h(x;y ) =h(x) +h(y) In this exercise, we derive the
relation between handpin the form of a function h(p) First show that h(p2) =
2h(p) and, hence, by induction that h(pn) =nh(p) wherenis a positive integer Hence, show that h(pn=m) = (n=m)h(p) wheremis also a positive integer This
implies that h(px) =xh(p) wherexis a positive rational number and, hence, by
continuity when it is a positive real number Finally, show that this implies h(p)
must take the form h(p)‚àùlnp Exercises 61
2.22 (?)Use a Lagrange multiplier to show that maximization of the entropy (2.86) for a
discrete variable gives a distribution in which all of the probabilities p(xi)are equal
and that the corresponding value of the entropy is then lnM 2.23 (?)Consider an M-state discrete random variable x, and use Jensen‚Äôs inequality in
the form (2.102) to show that the entropy of its distribution p(x) satisÔ¨Åes H[x]6
lnM 2.24 (??) Use the calculus of variations to show that the stationary point of the functional
(2.96) is given by (2.97)

============================================================

=== CHUNK 074 ===
Palavras: 365
Caracteres: 2388
--------------------------------------------------
Then use the constraints (2.93), (2.94), and (2.95) to elim-
inate the Lagrange multipliers and, hence, show that the maximum entropy solution
is given by the Gaussian (2.98) 2.25 (?)Use the results (2.94) and (2.95) to show that the entropy of the univariate Gaus-
sian (2.98) is given by (2.99) 2.26 (??) Suppose that p(x) is some Ô¨Åxed distribution and that we wish to approximate it
using a Gaussian distribution q(x) =N(x|; ) By writing down the form of the
Kullback‚ÄìLeibler divergence KL(p/bardblq )for a Gaussian q(x)and then differentiating,
show that minimization of KL(p/bardblq )with respect to andleads to the result that
is given by the expectation of xunderp(x) and that is given by the covariance 2.27 (??) Evaluate the Kullback‚ÄìLeibler divergence (2.100) between the two Gaussians
p(x) =N(x|;2)andq(x) =N(x|m;s2) 2.28 (??) Thealpha family of divergences is deÔ¨Åned by
D(p/bardblq) =4
1‚àí2/parenleftbigg
1‚àí/integraldisplay
p(x)(1+)=2q(x)(1‚àí)=2dx/parenrightbigg
(2.129)
where‚àí‚àû<  <‚àûis a continuous parameter Show that the Kullback‚ÄìLeibler
divergence KL(p/bardblq )corresponds to ‚Üí1 This can be done by writing p=
exp( lnp) = 1 +lnp+O(2)and then taking ‚Üí0 Similarly, show that
KL(q/bardblp)corresponds to ‚Üí‚àí1 2.29 (??) Consider two variables xandyhaving joint distribution p(x;y) Show that the
differential entropy of this pair of variables satisÔ¨Åes
H[x;y]6H[x] + H[y ] (2.130)
with equality if, and only if, xandyare statistically independent 2.30 (?)Consider a vector xof continuous variables with distribution p(x) and corre-
sponding entropy H[x] Suppose that we make a non-singular linear transformation
ofxto obtain a new variable y=Ax Show that the corresponding entropy is given
byH[y] = H[ x] + ln det Awhere detAdenotes the determinant of A 2.31 (??) Suppose that the conditional entropy H[y|x]between two discrete random vari-
ablesxandyis zero Show that, for all values of xsuch thatp(x)>0, the variable
ymust be a function of x In other words, for each xthere is only one value of y
such thatp(y|x)/negationslash= 0 PROBABILITIES
2.32 (?)A strictly convex function is deÔ¨Åned as one for which every chord lies above the
function Show that this is equivalent to the condition that the second derivative of
the function is positive 2.33 (??) Using proof by induction, show that the inequality (2.101) for convex functions
implies the result (2.102)

============================================================

=== CHUNK 075 ===
Palavras: 357
Caracteres: 2255
--------------------------------------------------
2.34 (?)Show that, up to an additive constant, the Kullback‚ÄìLeibler divergence (2.100)
between the empirical distribution (2.37) and a model distribution q(x|)is equal to
the negative log likelihood function 2.35 (?)Using the deÔ¨Ånition (2.107) together with the product rule of probability, prove
the result (2.108) 2.36 (???) Consider two binary variables xandyhaving the joint distribution given by
y
0
1
x01/3
1/3
1 0
1/3
Evaluate the following quantities:
(a)H[x] (c)H[y|x] (e)H[x;y ]
(b)H[y] (d)H[x|y ] (f)I[x;y ] Draw a Venn diagram to show the relationship between these various quantities 2.37 (?)By applying Jensen‚Äôs inequality (2.102) with f(x) = lnx, show that the arith-
metic mean of a set of real numbers is never less than their geometrical mean 2.38 (?)Using the sum and product rules of probability, show that the mutual information
I(x;y)satisÔ¨Åes the relation (2.110) 2.39 (??) Suppose that two variables z1andz2are independent so that p(z1;z2) =
p(z1)p(z 2) Show that the covariance matrix between these variables is diagonal This shows that independence is a sufÔ¨Åcient condition for two variables to be uncor-
related Now consider two variables y1andy2wherey1is symmetrically distributed
around 0andy2=y2
1 Write down the conditional distribution p(y2|y1)and observe
that this is dependent on y1, thus showing that the two variables are not independent Now show that the covariance matrix between these two variables is again diagonal To do this, use the relation p(y1;y2) =p(y1)p(y 2|y1)to show that the off-diagonal
terms are zero This counterexample shows that zero correlation is not a sufÔ¨Åcient
condition for independence 2.40 (?)Consider the bent coin in Figure 2.2 Assume that the prior probability that the
convex side is heads is 0:1 Now suppose the coin is Ô¨Çipped 10 times and we are
told that eight of the Ô¨Çips landed heads up and two of the Ô¨Çips landed tails up Use
Bayes‚Äô theorem to evaluate the posterior probability that the concave side is heads Calculate the probability that the next Ô¨Çip will land heads up Exercises 63
2.41 (?)By substituting (2.115) into (2.114) and making use of the result (2.66) for the log
likelihood of the linear regression model, derive the result (2.117) for the regularized
error function

============================================================

=== CHUNK 076 ===
Palavras: 361
Caracteres: 2368
--------------------------------------------------
3
Standard
Distributions
In this chapter we discuss some speciÔ¨Åc examples of probability distributions and
their properties As well as being of interest in their own right, these distributions
can form building blocks for more complex models and will be used extensively
throughout the book One role for the distributions discussed in this chapter is to model the prob-
ability distribution p(x)of a random variable x, given a Ô¨Ånite set x1;:::;xNof
observations This problem is known as density estimation It should be emphasized
that the problem of density estimation is fundamentally ill-posed, because there are
inÔ¨Ånitely many probability distributions that could have given rise to the observed Ô¨Å-
nite data set Indeed, any distribution p(x)that is non-zero at each of the data points
x1;:::;xNis a potential candidate The issue of choosing an appropriate distribu-
tion relates to the problem of model selection, which has already been encountered
in the context of polynomial curve Ô¨Åtting and which is a central issue in machine Section 1.2
65 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_3    
66 3 STANDARD DISTRIBUTIONS
learning We begin by considering distributions for discrete variables before exploring
the Gaussian distribution for continuous variables These are speciÔ¨Åc examples of
parametric distributions, so called because they are governed by a relatively small
number of adjustable parameters, such as the mean and variance of a Gaussian To
apply such models to the problem of density estimation, we need a procedure for
determining suitable values for the parameters, given an observed data set, and our
main focus will be on maximizing the likelihood function In this chapter, we will
assume that the data observations are independent and identically distributed (i.i.d.),
whereas in future chapters we will explore more complex scenarios involving struc-
tured data where this assumption no longer holds One limitation of the parametric approach is that it assumes a speciÔ¨Åc functional
form for the distribution, which may turn out to be inappropriate for a particular
application An alternative approach is given by nonparametric density estimation
methods in which the form of the distribution typically depends on the size of the data
set

============================================================

=== CHUNK 077 ===
Palavras: 379
Caracteres: 2453
--------------------------------------------------
Such models still contain parameters, but these control the model complexity
rather than the form of the distribution We end this chapter by brieÔ¨Çy considering
three nonparametric methods based respectively on histograms, nearest neighbours,
and kernels A major limitation of nonparametric techniques such as these is that
they involve storing all the training data In other words, the number of parameters
grows with the size of the data set, so that the method become very inefÔ¨Åcient for
large data sets Deep learning combines the efÔ¨Åciency of parametric models with the
generality of nonparametric methods by considering Ô¨Çexible distributions based on
neural networks having a large, but Ô¨Åxed, number of parameters Discrete
Variables
W
e begin by considering simple distributions for discrete variables, starting with
binary variables and then moving on to multi-state variables 3.1.1 Bernoulli distribution
Consider a single binary random variable x‚àà{0; 1} For example, xmight
describe the outcome of Ô¨Çipping a coin, with x= 1representing ‚Äòheads‚Äô and x= 0
representing ‚Äòtails‚Äô If this were a damaged coin, such as the one shown in Figure 2.2,
the probability of landing heads is not necessarily the same as that of landing tails The probability of x= 1will be denoted by the parameter so that
p(x= 1|) = (3.1)
where 0661, from which it follows that p(x= 0|) = 1‚àí The probability
distribution over xcan therefore be written in the form
Bern(x|) =x(1‚àí)1‚àíx; (3.2)
which is known as the Bernoulli distribution It is easily veriÔ¨Åed that this distribution Exercise 3.1
3.1 Discrete Variables 67
is normalized and that it has mean and variance given by
E[x] = (3.3)
var[x] = (1‚àí): (3.4)
Now suppose we have a data set D={x1;:::;xN}of observed values of x We can construct the likelihood function, which is a function of , on the assumption
that the observations are drawn independently from p(x|), so that
p(D|) =N/productdisplay
n=1p(xn|) =N/productdisplay
n=1xn(1‚àí)1‚àíxn: (3.5)
We can estimate a value for by maximizing the likelihood function or equivalently
by maximizing the logarithm of the likelihood, since the log is a monotonic function The log likelihood function of the Bernoulli distribution is given by
lnp(D|) =N/summationdisplay
n=1lnp(xn|) =N/summationdisplay
n=1{xnln+ (1‚àíxn) ln(1‚àí)}: (3.6)
At this point, note that the log likelihood function depends on the Nobservations xn
only through their sum/summationtext
nxn

============================================================

=== CHUNK 078 ===
Palavras: 356
Caracteres: 2153
--------------------------------------------------
This sum provides an example of a sufÔ¨Åcient statistic
for the data under this distribution If we set the derivative of lnp(D|) with respect Section 3.4
toequal to zero, we obtain the maximum likelihood estimator:
ML=1
NN/summationdisplay
n=1xn; (3.7)
which is also known as the sample mean Denoting the number of observations of
x= 1(heads) within this data set by m, we can write (3.7) in the form
ML=m
N(3.8)
so that the probability of landing heads is given, in this maximum likelihood frame-
work, by the fraction of observations of heads in the data set 3.1.2 Binomial distribution
We can also work out the distribution for the binary variable xof the number
mof observations of x= 1, given that the data set has size N This is called the
binomial distribution, and from (3.5) we see that it is proportional to m(1‚àí)N‚àím To obtain the normalization coefÔ¨Åcient, note that out of Ncoin Ô¨Çips, we have to add
up all of the possible ways of obtaining mheads, so that the binomial distribution
can be written as
Bin(m|N; ) =/parenleftbiggN
m/parenrightbigg
m(1‚àí)N‚àím(3.9)
68 3 STANDARD DISTRIBUTIONS
Figure 3.1 Histogram plot of the binomial
distribution (3.9) as a function of
mforN= 10 and= 0:25 m0 1 2 3 4 5 6 7 8 91000.10.20.3
where /parenleftbiggN
m/parenrightbigg
‚â°N (
N‚àím)!m!(3.10)
is the number of ways of choosing mobjects out of a total of Nidentical objects
without replacement Figure 3.1 shows a plot of the binomial distribution for N= 10 Exercise 3.3
and= 0:25 The mean and variance of the binomial distribution can be found by using the
results that, for independent events, the mean of the sum is the sum of the means and
the variance of the sum is the sum of the variances Because m=x1+:::+xN Exercise 2.10
and because for each observation the mean and variance are given by (3.3) and (3.4),
respectively, we have
E[m]‚â°N/summationdisplay
m=0mBin(m|N; ) =N (3.11)
var[m]‚â°N/summationdisplay
m=0(m‚àíE[m])2Bin(m|N; ) =N(1‚àí): (3.12)
These results can also be proved directly by using calculus Exercise 3.4
3.1.3 Multinomial distribution
Binary variables can be used to describe quantities that can take one of two
possible values

============================================================

=== CHUNK 079 ===
Palavras: 373
Caracteres: 2594
--------------------------------------------------
Often, however, we encounter discrete variables that can take on
one ofKpossible mutually exclusive states Although there are various alternative
ways to express such variables, we will see shortly that a particularly convenient
representation is the 1-of-K scheme, sometimes called ‚Äòone-hot encoding‚Äô, in which
the variable is represented by a K-dimensional vector xin which one of the elements
xkequals 1and all remaining elements equal 0 So, for instance, if we have a variable
that can take K= 6states and a particular observation of the variable happens to
3.1 Discrete Variables 69
correspond to the state where x3= 1, then xwill be represented by
x= (0;0;1;0;0;0)T: (3.13)
Note that such vectors satisfy/summationtextK
k=1xk= 1 If we denote the probability of xk= 1
by the parameter k, then the distribution of xis given by
p(x|) =K/productdisplay
k=1xk
k(3.14)
where= (1;:::;K)T, and the parameters kare constrained to satisfy k>0
and/summationtext
kk= 1, because they represent probabilities The distribution (3.14) can be
regarded as a generalization of the Bernoulli distribution to more than two outcomes It is easily seen that the distribution is normalized:
/summationdisplay
xp(x|) =K/summationdisplay
k=1k= 1 (3.15)
and that
E[x|] =/summationdisplay
xp(x|)x =: (3.16)
Now consider a data set DofNindependent observations x1;:::;xN The
corresponding likelihood function takes the form
p(D|) =N/productdisplay
n=1K/productdisplay
k=1xnk
k=K/productdisplay
k=1(P
nxnk)
k=K/productdisplay
k=1mk
k(3.17)
where we see that the likelihood function depends on the Ndata points only through
theKquantities:
mk=N/summationdisplay
n=1xnk; (3.18)
which represent the number of observations of xk= 1 These are called the sufÔ¨Åcient
statistics for this distribution Note that the variables mkare subject to the constraint Section 3.4
K/summationdisplay
k=1mk=N: (3.19)
To Ô¨Ånd the maximum likelihood solution for , we need to maximize lnp(D|)
with respect to ktaking account of the constraint (3.15) that the kmust sum to
one This can be achieved using a Lagrange multiplier and maximizing Appendix C
K/summationdisplay
k=1mklnk+/parenleftBiggK/summationdisplay
k=1k‚àí1/parenrightBigg
: (3.20)
70 3 STANDARD DISTRIBUTIONS
Setting the derivative of (3.20) with respect to kto zero, we obtain
k=‚àímk=: (3.21)
We can solve for the Lagrange multiplier by substituting (3.21) into the constraint/summationtext
kk= 1to give=‚àíN Thus, we obtain the maximum likelihood solution for
kin the form
ML
k=mk
N; (3.22)
which is the fraction of the Nobservations for which xk= 1

============================================================

=== CHUNK 080 ===
Palavras: 350
Caracteres: 2353
--------------------------------------------------
We can also consider the joint distribution of the quantities m1;:::;mK, condi-
tioned on the parameter vector and on the total number Nof observations From
(3.17), this takes the form
Mult(m 1;m2;:::;mK|;N ) =/parenleftbiggN
m1m2:::mK/parenrightbiggK/productdisplay
k=1mk
k; (3.23)
which is known as the multinomial distribution The normalization coefÔ¨Åcient is the
number of ways of partitioning Nobjects into Kgroups of size m1;:::;mKand is
given by/parenleftbiggN
m1m2:::mK/parenrightbigg
=N m1!m2!:::mK!: (3.24)
Note that two-state quantities can be represented either as binary variables and
modelled using the binomial distribution (3.9) or as 1-of-2 variables and modelled
using the distribution (3.14) with K= 2 The Multivariate Gaussian
The Gaussian, also known as the normal distribution, is a widely used model for
the distribution of continuous variables We have already seen that for of a single
variablex, the Gaussian distribution can be written in the form Section 2.3
N(x|;2) =1
(22)1=2exp/braceleftbigg
‚àí1
22(x‚àí)2/bracerightbigg
(3.25)
whereis the mean and 2is the variance For a D-dimensional vector x, the
multivariate Gaussian distribution takes the form
N(x|; ) =1
(2)D=21
||1=2exp/braceleftbigg
‚àí1
2(x‚àí)T‚àí1(x‚àí)/bracerightbigg
(3.26)
whereis theD-dimensional mean vector, is theD√óDcovariance matrix, and
detdenotes the determinant of  The Gaussian distribution arises in many different contexts and can be motivated
from a variety of different perspectives For example, we have already seen that for Section 2.5
3.2 The Multivariate Gaussian 71
N= 1
0 0.5 10123
N= 2
0 0.5 10123
N= 10
0 0.5 10123
Figure
3.2 Histogram plots of the mean of Nuniformly distributed numbers for various values of N We
observe that as Nincreases, the distribution tends towards a Gaussian a single real variable, the distribution that maximizes the entropy is the Gaussian This property applies also to the multivariate Gaussian Exercise 3.8
Another situation in which the Gaussian distribution arises is when we consider
the sum of multiple random variables The central limit theorem tells us that, subject
to certain mild conditions, the sum of a set of random variables, which is of course
itself a random variable, has a distribution that becomes increasingly Gaussian as the
number of terms in the sum increases (Walker, 1969)

============================================================

=== CHUNK 081 ===
Palavras: 371
Caracteres: 2458
--------------------------------------------------
We can illustrate this by con-
sideringNvariablesx1;:::;xNeach of which has a uniform distribution over the
interval [0;1]and then considering the distribution of the mean (x1+¬∑¬∑¬∑+xN)=N For largeN, this distribution tends to a Gaussian, as illustrated in Figure 3.2 In
practice, the convergence to a Gaussian as Nincreases can be very rapid One con-
sequence of this result is that the binomial distribution (3.9), which is a distribution
overmdeÔ¨Åned by the sum of Nobservations of the random binary variable x, will
tend to a Gaussian as N‚Üí‚àû (seeFigure 3.1 forN= 10) The Gaussian distribution has many important analytical properties, and we will
consider several of these in detail As a result, this section will be rather more tech-
nically involved than some of the earlier sections and will require familiarity with
various matrix identities Appendix A
3.2.1 Geometry of the Gaussian
We begin by considering the geometrical form of the Gaussian distribution The
functional dependence of the Gaussian on xis through the quadratic form
‚àÜ2= (x‚àí)T‚àí1(x‚àí); (3.27)
which appears in the exponent The quantity ‚àÜis called the Mahalanobis distance
fromtox It reduces to the Euclidean distance when is the identity matrix The Gaussian distribution is constant on surfaces in x-space for which this quadratic
form is constant First, note that the matrix can be taken to be symmetric, without loss of gen-
erality, because any antisymmetric component would disappear from the exponent Exercise 3.11
Now consider the eigenvector equation for the covariance matrix
ui=iui (3.28)
72 3 STANDARD DISTRIBUTIONS
wherei= 1;:::;D Because is a real, symmetric matrix, its eigenvalues will be
real, and its eigenvectors can be chosen to form an orthonormal set, so that Exercise 3.12
uT
iuj=Iij (3.29)
whereIijis thei;jelement of the identity matrix and satisÔ¨Åes
Iij=/braceleftbigg
1;ifi=j
0;otherwise.(3.30)
The covariance matrix can be expressed as an expansion in terms of its eigenvec-
tors in the form Exercise 3.13
=D/summationdisplay
i=1iuiuT
i (3.31)
and similarly the inverse covariance matrix ‚àí1can be expressed as
‚àí1=D/summationdisplay
i=11
iuiuT
i: (3.32)
Substituting
(3.32) into (3.27), the quadratic form becomes
‚àÜ2=D/summationdisplay
i=1y2
i
i(3.33)
where
we have deÔ¨Åned
yi=uT
i(x‚àí): (3.34)
We can interpret{yi}as a new coordinate system deÔ¨Åned by the orthonormal vectors
uithat are shifted and rotated with respect to the original xicoordinates

============================================================

=== CHUNK 082 ===
Palavras: 383
Caracteres: 2567
--------------------------------------------------
Forming
the vector y= (y1;:::;yD)T, we have
y=U(x‚àí) (3.35)
where Uis a matrix whose rows are given by uT
i From (3.29) it follows that Uis
anorthogonal matrix, i.e., it satisÔ¨Åes UUT=UTU=I, where Iis the identity Appendix A
matrix The quadratic form, and hence the Gaussian density, is constant on surfaces for
which (3.33) is constant If all the eigenvalues iare positive, then these surfaces
represent ellipsoids, with their centres at and their axes oriented along ui, and with
scaling factors in the directions of the axes given by 1=2
i, as illustrated in Figure 3.3 For the Gaussian distribution to be well deÔ¨Åned, it is necessary for all the eigen-
valuesiof the covariance matrix to be strictly positive, otherwise the distribution
cannot be properly normalized A matrix whose eigenvalues are strictly positive is
said to be positive deÔ¨Ånite When we discuss latent variable models, we will en- Chapter 16
counter Gaussian distributions for which one or more of the eigenvalues are zero, in
3.2 The Multivariate Gaussian 73
Figure 3.3 The red curve shows the ellip-
tical surface of constant proba-
bility density for a Gaussian in
a two-dimensional space x=
(x1;x2)on which the density is
exp(‚àí1=2) of its value at x=
 The axes of the ellipse are
deÔ¨Åned by the eigenvectors ui
of the covariance matrix, with
corresponding eigenvalues i x1x2
1=2
11=2
2y1y2u1u2

which case the distribution is singular and is conÔ¨Åned to a subspace of lower dimen-
sionality If all the eigenvalues are non-negative, then the covariance matrix is said
to be positive semideÔ¨Ånite Now consider the form of the Gaussian distribution in the new coordinate system
deÔ¨Åned by the yi In going from the xto the ycoordinate system, we have a Jacobian
matrix Jwith elements given by
Jij=@xi
@yj=Uji (3.36)
whereUjiare the elements of the matrix UT Using the orthonormality property of
the matrix U, we see that the square of the determinant of the Jacobian matrix is
|J|2=/vextendsingle/vextendsingleUT/vextendsingle/vextendsingle2=/vextendsingle/vextendsingleUT/vextendsingle/vextendsingle|U|=/vextendsingle/vextendsingleUTU/vextendsingle/vextendsingle=|I|= 1 (3.37)
and, hence,|J|= 1 Also, the determinant ||of the covariance matrix can be
written as the product of its eigenvalues, and hence
||1=2=D/productdisplay
j=11=2
j: (3.38)
Thus, in the yjcoordinate system, the Gaussian distribution takes the form
p(y) =p(x)|J| =D/productdisplay
j=11
(2j)1=2exp/braceleftbigg
‚àíy2
j
2j/bracerightbigg
; (3.39)
which is the product of Dindependent univariate Gaussian distributions

============================================================

=== CHUNK 083 ===
Palavras: 387
Caracteres: 2769
--------------------------------------------------
The eigen-
vectors therefore deÔ¨Åne a new set of shifted and rotated coordinates with respect
to which the joint probability distribution factorizes into a product of independent
distributions The integral of the distribution in the ycoordinate system is then
/integraldisplay
p(y) dy=D/productdisplay
j=1/integraldisplay‚àû
‚àí‚àû1
(2j)1=2exp/braceleftbigg
‚àíy2
j
2j/bracerightbigg
dyj= 1 (3.40)
74 3 STANDARD DISTRIBUTIONS
where we have used the result (2.51) for the normalization of the univariate Gaussian This conÔ¨Årms that the multivariate Gaussian (3.26) is indeed normalized 3.2.2 Moments
We now look at the moments of the Gaussian distribution and thereby provide an
interpretation of the parameters and The expectation of xunder the Gaussian
distribution is given by
E[x] =1
(2)D=21
||1=2/integraldisplay
exp/braceleftbigg
‚àí1
2(x‚àí)T‚àí1(x‚àí)/bracerightbigg
xdx
=1
(2)D=21
||1=2/integraldisplay
exp/braceleftbigg
‚àí1
2zT‚àí1z/bracerightbigg
(z+) dz (3.41)
where we have changed variables using z=x‚àí Note that the exponent is an even
function of the components of z, and because the integrals over these are taken over
the range (‚àí‚àû;‚àû), the term in zin the factor (z+)will vanish by symmetry Thus,
E[x] =; (3.42)
and so we refer to as the mean of the Gaussian distribution We now consider second-order moments of the Gaussian In the univariate case,
we considered the second-order moment given by E[x2] For the multivariate Gaus-
sian, there are D2second-order moments given by E[xixj], which we can group
together to form the matrix E[xxT] This matrix can be written as
E[xxT] =1
(2)D=21
||1=2/integraldisplay
exp/braceleftbigg
‚àí1
2(x‚àí)T‚àí1(x‚àí)/bracerightbigg
xxTdx
=1
(2)D=21
||1=2/integraldisplay
exp/braceleftbigg
‚àí1
2zT‚àí1z/bracerightbigg
(z+)(z +)Tdz (3.43)
where again we have changed variables using z=x‚àí Note that the cross-terms
involvingzTandTzwill again vanish by symmetry The term Tis constant
and can be taken outside the integral, which itself is unity because the Gaussian
distribution is normalized Consider the term involving zzT Again, we can make
use of the eigenvector expansion of the covariance matrix given by (3.28), together
with the completeness of the set of eigenvectors, to write
z=D/summationdisplay
j=1yjuj (3.44)
3.2 The Multivariate Gaussian 75
whereyj=uT
jz, which gives
1
(2
)D=21
|
|1=2/integraldisplay
exp/braceleftbigg
‚àí1
2zT‚àí
1z/bracerightbigg
zzTdz
=1
(2
)D=21
|
|1=2D/summationdisplay
i=1D/summationdisplay
j=1uiuT
j/integraldisplay
exp/braceleftBigg
‚àíD/summationdisplay
k=1y2
k
2
k/bracerightBigg
yiyjdy
=D/summationdisplay
i=1uiuT
ii= (3.45)
where we have made use of the eigenvector equation (3.28), together with the fact
that the integral on the middle line vanishes by symmetry unless i=j

============================================================

=== CHUNK 084 ===
Palavras: 357
Caracteres: 2306
--------------------------------------------------
In the Ô¨Ånal
line we have made use of the results (2.53) and (3.38), together with (3.31) Thus,
we have
E[xxT] =T+: (3.46)
When deÔ¨Åning the variance for a single random variable, we subtracted the mean
before taking the second moment Similarly, in the multivariate case it is again
convenient to subtract off the mean, giving rise to the covariance of a random vector
xdeÔ¨Åned by
cov[x] = E/bracketleftbig
(x‚àíE[x])(x‚àíE[x])T/bracketrightbig
: (3.47)
For the speciÔ¨Åc case of a Gaussian distribution, we can make use of E[x] =,
together with the result (3.46), to give
cov[x] = : (3.48)
Because the parameter matrix governs the covariance of xunder the Gaussian
distribution, it is called the covariance matrix 3.2.3 Limitations
Although the Gaussian distribution (3.26) is often used as a simple density
model, it suffers from some signiÔ¨Åcant limitations Consider the number of free
parameters in the distribution A general symmetric covariance matrix will have
D(D+ 1)=2 independent parameters, and there are another Dindependent parame- Exercise 3.15
ters in, givingD(D+ 3)=2 parameters in total For large D, the total number of
parameters therefore grows quadratically with D, and the computational task of ma-
nipulating and inverting the large matrices can become prohibitive One way to ad-
dress this problem is to use restricted forms of the covariance matrix If we consider
covariance matrices that are diagonal, so that = diag(2
i), we then have a total
of2Dindependent parameters in the density model The corresponding contours of
constant density are given by axis-aligned ellipsoids We could further restrict the
covariance matrix to be proportional to the identity matrix, =2I, known as an
isotropic covariance, giving D+ 1 independent parameters in the model together
with spherical surfaces of constant density The three possibilities of general, diag-
onal, and isotropic covariance matrices are illustrated in Figure 3.4 STANDARD DISTRIBUTIONS
Figure 3.4 Contours of constant
probability density for a Gaussian
distribution in two dimensions in
which the covariance matrix is (a)
of general form, (b) diagonal, in
which case the elliptical contours
are aligned with the coordinate axes,
and (c) proportional to the identity
matrix, in which case the contours
are concentric circles

============================================================

=== CHUNK 085 ===
Palavras: 355
Caracteres: 2313
--------------------------------------------------
x1x2(a)
x1x2 (b)
x1x2 (c)
whereas such approaches limit the number of degrees of freedom in the distribu-
tion and make inversion of the covariance matrix a much faster operation, they also
greatly restrict the form of the probability density and limit its ability to capture
interesting correlations in the data A further limitation of the Gaussian distribution is that it is intrinsically uni-
modal (i.e., has a single maximum) and so is unable to provide a good approximation
to multimodal distributions Thus, the Gaussian distribution can be both too Ô¨Çexible,
in the sense of having too many parameters, and too limited in the range of distribu-
tions that it can adequately represent We will see later that the introduction of latent
variables, also called hidden variables or unobserved variables, allows both of these
problems to be addressed In particular, a rich family of multimodal distributions is
obtained by introducing discrete latent variables leading to mixtures of Gaussians Section 3.2.9
Similarly, the introduction of continuous latent variables leads to models in which the
number of free parameters can be controlled independently of the dimensionality D
of the data space while still allowing the model to capture the dominant correlations
in the data set Chapter 16
3.2.4 Conditional distribution
An important property of a multivariate Gaussian distribution is that if two sets
of variables are jointly Gaussian, then the conditional distribution of one set condi-
tioned on the other is again Gaussian Similarly, the marginal distribution of either
set is also Gaussian First, consider the case of conditional distributions Suppose that xis aD-
dimensional vector with Gaussian distribution N(x|; )and that we partition x
into two disjoint subsets xaandxb Without loss of generality, we can take xa
to form the Ô¨Årst Mcomponents of x, with xbcomprising the remaining D‚àíM
components, so that
x=/parenleftbigg
xa
xb/parenrightbigg
: (3.49)
We also deÔ¨Åne corresponding partitions of the mean vector given by
=/parenleftbigg
a
b/parenrightbigg
(3.50)
3.2 The Multivariate Gaussian 77
and of the covariance matrix given by
=/parenleftbigg
aaab
babb/parenrightbigg
: (3.51)
Note that the symmetry T=of the covariance matrix implies that aaandbb
are symmetric and that ba=T
ab

============================================================

=== CHUNK 086 ===
Palavras: 358
Caracteres: 2343
--------------------------------------------------
In many situations, it will be convenient to work with the inverse of the covari-
ance matrix:
‚â°‚àí1; (3.52)
which is known as the precision matrix In fact, we will see that some properties
of Gaussian distributions are most naturally expressed in terms of the covariance,
whereas others take a simpler form when viewed in terms of the precision We
therefore also introduce the partitioned form of the precision matrix:
=/parenleftbigg
aaab
babb/parenrightbigg
(3.53)
corresponding to the partitioning (3.49) of the vector x Because the inverse of a
symmetric matrix is also symmetric, we see that aaandbbare symmetric and Exercise 3.16
thatba=T
ab It should be stressed at this point that, for instance, aais not
simply given by the inverse of aa In fact, we will shortly examine the relation
between the inverse of a partitioned matrix and the inverses of its partitions We begin by Ô¨Ånding an expression for the conditional distribution p(xa|xb) From the product rule of probability, we see that this conditional distribution can be
evaluated from the joint distribution p(x) =p(xa;xb)simply by Ô¨Åxing xbto the
observed value and normalizing the resulting expression to obtain a valid probability
distribution over xa Instead of performing this normalization explicitly, we can
obtain the solution more efÔ¨Åciently by considering the quadratic form in the exponent
of the Gaussian distribution given by (3.27) and then reinstating the normalization
coefÔ¨Åcient at the end of the calculation If we make use of the partitioning (3.49),
(3.50), and (3.53), we obtain
‚àí1
2(x‚àí)T‚àí1(x‚àí) =
‚àí1
2(xa‚àía)Taa(xa‚àía)‚àí1
2(xa‚àía)Tab(xb‚àíb)
‚àí1
2(xb‚àíb)Tba(xa‚àía)‚àí1
2(xb‚àíb)Tbb(xb‚àíb):(3.54)
We see that as a function of xa, this is again a quadratic form, and hence, the cor-
responding conditional distribution p(xa|xb)will be Gaussian Because this distri-
bution is completely characterized by its mean and its covariance, our goal will be
to identify expressions for the mean and covariance of p(xa|xb)by inspection of
(3.54) This is an example of a rather common operation associated with Gaussian
distributions, sometimes called ‚Äòcompleting the square‚Äô, in which we are given a
78 3 STANDARD DISTRIBUTIONS
quadratic form deÔ¨Åning the exponent terms in a Gaussian distribution and we need
to determine the corresponding mean and covariance

============================================================

=== CHUNK 087 ===
Palavras: 418
Caracteres: 2736
--------------------------------------------------
Such problems can be solved
straightforwardly by noting that the exponent in a general Gaussian distribution
N(x|; )can be written as
‚àí1
2(x‚àí)T‚àí1(x‚àí) =‚àí1
2xT‚àí1x+xT‚àí1+ const (3.55)
where ‚Äòconst‚Äô denotes terms that are independent of x, We have also made use of
the symmetry of  Thus, if we take our general quadratic form and express it in
the form given by the right-hand side of (3.55), then we can immediately equate the
matrix of coefÔ¨Åcients entering the second-order term in xto the inverse covariance
matrix ‚àí1and the coefÔ¨Åcient of the linear term in xto‚àí1, from which we can
obtain Now let us apply this procedure to the conditional Gaussian distribution p(xa|xb)
for which the quadratic form in the exponent is given by (3.54) We will denote the
mean and covariance of this distribution by a|banda|b, respectively Consider
the functional dependence of (3.54) on xain which xbis regarded as a constant If
we pick out all terms that are second order in xa, we have
‚àí1
2xT
aaaxa (3.56)
from which we can immediately conclude that the covariance (inverse precision) of
p(xa|xb)is given by
a|b=‚àí1
aa: (3.57)
Now consider all the terms in (3.54) that are linear in xa:
xT
a{aaa‚àíab(xb‚àíb)} (3.58)
where we have used T
ba=ab From our discussion of the general form (3.55),
the coefÔ¨Åcient of xain this expression must equal ‚àí1
a|ba|band, hence,
a|b=a|b{aaa‚àíab(xb‚àíb)}
=a‚àí‚àí1
aaab(xb‚àíb) (3.59)
where we have made use of (3.57) The results (3.57) and (3.59) are expressed in terms of the partitioned precision
matrix of the original joint distribution p(xa;xb) We can also express these results
in terms of the corresponding partitioned covariance matrix To do this, we make use
of the following identity for the inverse of a partitioned matrix: Exercise 3.18
/parenleftbigg
A B
C D/parenrightbigg‚àí1
=/parenleftbigg
M‚àíMBD‚àí1
‚àíD‚àí1CM D‚àí1+D‚àí1CMBD‚àí1/parenrightbigg
(3.60)
where we have deÔ¨Åned
M= (A‚àíBD‚àí1C)‚àí1: (3.61)
3.2 The Multivariate Gaussian 79
The quantity M‚àí1is known as the Schur complement of the matrix on the left-hand
side of (3.60) with respect to the submatrix D Using the deÔ¨Ånition
/parenleftbigg
aaab
babb/parenrightbigg‚àí1
=/parenleftbigg
aaab
babb/parenrightbigg
(3.62)
and making use of (3.60), we have
aa= (aa‚àíab‚àí1
bbba)‚àí1(3.63)
ab=‚àí(aa‚àíab‚àí1
bbba)‚àí1ab‚àí1
bb: (3.64)
From these we obtain the following expressions for the mean and covariance of the
conditional distribution p(xa|xb):
a|b=a+ab‚àí1
bb(xb‚àíb) (3.65)
a|b=aa‚àíab‚àí1
bbba: (3.66)
Comparing (3.57) and (3.66), we see that the conditional distribution p(xa|xb)takes
a simpler form when expressed in terms of the partitioned precision matrix than
when it is expressed in terms of the partitioned covariance matrix

============================================================

=== CHUNK 088 ===
Palavras: 381
Caracteres: 2340
--------------------------------------------------
Note that the
mean of the conditional distribution p(xa|xb), given by (3.65), is a linear function of
xband that the covariance, given by (3.66), is independent of xb This represents an
example of a linear-Gaussian model Section 11.1.4
3.2.5 Marginal distribution
We have seen that if a joint distribution p(xa;xb)is Gaussian, then the condi-
tional distribution p(xa|xb)will again be Gaussian Now we turn to a discussion of
the marginal distribution given by
p(xa) =/integraldisplay
p(xa;xb) dxb; (3.67)
which, as we will see, is also Gaussian Once again, our strategy for calculating this
distribution will be to focus on the quadratic form in the exponent of the joint distri-
bution and thereby to identify the mean and covariance of the marginal distribution
p(xa) The quadratic form for the joint distribution can be expressed, using the parti-
tioned precision matrix, in the form (3.54) Our goal is to integrate out xb, which is
most easily achieved by Ô¨Årst considering the terms involving xband then completing
the square to facilitate the integration Picking out just those terms that involve xb,
we have
‚àí1
2xT
bbbxb+xT
bm=‚àí1
2(xb‚àí‚àí1
bbm)Tbb(xb‚àí‚àí1
bbm)+1
2mT‚àí1
bbm(3.68)
where we have deÔ¨Åned
m=bbb‚àíba(xa‚àía): (3.69)
80 3 STANDARD DISTRIBUTIONS
We see that the dependence on xbhas been cast into the standard quadratic form of a
Gaussian distribution corresponding to the Ô¨Årst term on the right-hand side of (3.68)
plus a term that does not depend on xb(but that does depend on xa) Thus, when
we take the exponential of this quadratic form, we see that the integration over xb
required by (3.67) will take the form
/integraldisplay
exp/braceleftbigg
‚àí1
2(xb‚àí‚àí1
bbm)Tbb(xb‚àí‚àí1
bbm)/bracerightbigg
dxb: (3.70)
This integration is easily performed by noting that it is the integral over an unnor-
malized Gaussian, and so the result will be the reciprocal of the normalization coef-
Ô¨Åcient We know from the form of the normalized Gaussian given by (3.26) that this
coefÔ¨Åcient is independent of the mean and depends only on the determinant of the
covariance matrix Thus, by completing the square with respect to xb, we can inte-
grate out xbso that the only term remaining from the contributions on the left-hand
side of (3.68) that depends on xais the last term on the right-hand side of (3.68) in
which mis given by (3.69)

============================================================

=== CHUNK 089 ===
Palavras: 354
Caracteres: 2531
--------------------------------------------------
Combining this term with the remaining terms from
(3.54) that depend on xa, we obtain
1
2[bbb‚àíba(xa‚àía)]T‚àí1
bb[bbb‚àíba(xa‚àía)]
‚àí1
2xT
aaaxa+xT
a(aaa+abb) + const
=‚àí1
2xT
a(aa‚àíab‚àí1
bbba)xa
+xT
a(aa‚àíab‚àí1
bbba)a+ const (3.71)
where ‚Äòconst‚Äô denotes quantities independent of xa Again, by comparison with
(3.55), we see that the covariance of the marginal distribution p(xa)is given by
a= (aa‚àíab‚àí1
bbba)‚àí1: (3.72)
Similarly, the mean is given by
a(aa‚àíab‚àí1
bbba)a=a (3.73)
where we have used (3.72) The covariance (3.72) is expressed in terms of the par-
titioned precision matrix given by (3.53) We can rewrite this in terms of the cor-
responding partitioning of the covariance matrix given by (3.51), as we did for the
conditional distribution These partitioned matrices are related by
/parenleftbigg
aaab
babb/parenrightbigg‚àí1
=/parenleftbigg
aaab
babb/parenrightbigg
: (3.74)
Making use of (3.60), we then have
/parenleftbig
aa‚àíab‚àí1
bbba/parenrightbig‚àí1=aa: (3.75)
3.2 The Multivariate Gaussian 81
Thus, we obtain the intuitively satisfying result that the marginal distribution p(xa)
has mean and covariance given by
E[xa] =a (3.76)
cov[xa] = aa: (3.77)
We see that for a marginal distribution, the mean and covariance are most simply ex-
pressed in terms of the partitioned covariance matrix, in contrast to the conditional
distribution for which the partitioned precision matrix gives rise to simpler expres-
sions Our results for the marginal and conditional distributions of a partitioned Gaus-
sian can be summarized as follows Given a joint Gaussian distribution N(x|; )
with‚â°‚àí1and the following partitions
x=/parenleftbigg
xa
xb/parenrightbigg
;=/parenleftbigg
a
b/parenrightbigg
(3.78)
=/parenleftbigg
aaab
babb/parenrightbigg
;=/parenleftbigg
aaab
babb/parenrightbigg
(3.79)
then the conditional distribution is given by
p(xa|xb) =N(x|a|b;‚àí1
aa) (3.80)
a|b=a‚àí‚àí1
aaab(xb‚àíb) (3.81)
and the marginal distribution is given by
p(xa) =N(xa|a;aa): (3.82)
We illustrate the idea of conditional and marginal distributions associated with
a multivariate Gaussian using an example involving two variables in Figure 3.5 3.2.6 Bayes‚Äô theorem
In Sections 3.2.4 and 3.2.5 we considered a Gaussian p(x) in which we parti-
tioned the vector xinto two subvectors x= (xa;xb)and then found expressions
for the conditional distribution p(xa|xb)and the marginal distribution p(xa) We
noted that the mean of the conditional distribution p(xa|xb)was a linear function of
xb

============================================================

=== CHUNK 090 ===
Palavras: 430
Caracteres: 2941
--------------------------------------------------
Here we will suppose that we are given a Gaussian marginal distribution p(x)
and a Gaussian conditional distribution p(y|x)in whichp(y|x)has a mean that is a
linear function of xand a covariance that is independent of x This is an example
of a linear-Gaussian model (Roweis and Ghahramani, 1999) We wish to Ô¨Ånd the Section 11.1.4
marginal distribution p(y)and the conditional distribution p(x|y ) This is a struc-
ture that arises in several types of generative model and it will prove convenient to Chapter 16
derive the general results here We will take the marginal and conditional distributions to be
p(x) =N/parenleftbig
x|;‚àí1/parenrightbig
(3.83)
p(y|x) =N/parenleftbig
y|Ax+b;L‚àí1/parenrightbig
(3.84)
82 3 STANDARD DISTRIBUTIONS
0 0.5 100.51
xaxbxb= 0.7
p(xa,xb)
(a)
0 0.5 10510
xap(xa)p(xa|xb= 0.7) (b)
Figure 3.5 (a) Contours of a Gaussian distribution p(xa;xb)over two variables (b) The marginal distribution
p(xa)(blue curve) and the conditional distribution p(xa|xb)forxb= 0:7(red curve) where,A, and bare parameters governing the means, and andLare precision
matrices If xhas dimensionality Mandyhas dimensionality D, then the matrix A
has sizeD√óM First we Ô¨Ånd an expression for the joint distribution over xandy To do this, we
deÔ¨Åne
z=/parenleftbigg
x
y/parenrightbigg
(3.85)
and then consider the log of the joint distribution:
lnp(z) = ln p(x) + lnp(y|x)
=‚àí1
2(x‚àí)T(x‚àí)
‚àí1
2(y‚àíAx‚àíb)TL(y‚àíAx‚àíb) + const (3.86)
where ‚Äòconst‚Äô denotes terms independent of xandy As before, we see that this is a
quadratic function of the components of z, and hence, p(z) is Gaussian distribution To Ô¨Ånd the precision of this Gaussian, we consider the second-order terms in (3.86),
which can be written as
‚àí1
2xT(+ATLA)x‚àí1
2yTLy+1
2yTLAx +1
2xTATLy
=‚àí1
2/parenleftbigg
x
y/parenrightbiggT/parenleftbigg
+ATLA‚àíATL
‚àíLA L/parenrightbigg/parenleftbigg
x
y/parenrightbigg
=‚àí1
2zTRz (3.87)
and so the Gaussian distribution over zhas precision (inverse covariance) matrix
3.2 The Multivariate Gaussian 83
given by
R=/parenleftbigg
+ATLA‚àíATL
‚àíLA L/parenrightbigg
: (3.88)
The covariance matrix is found by taking the inverse of the precision, which can be
done using the matrix inversion formula (3.60) to give Exercise 3.23
cov[z] = R‚àí1=/parenleftbigg
‚àí1‚àí1AT
A‚àí1L‚àí1+A‚àí1AT/parenrightbigg
: (3.89)
Similarly, we can Ô¨Ånd the mean of the Gaussian distribution over zby identify-
ing the linear terms in (3.86), which are given by
xT‚àíxTATLb+yTLb=/parenleftbigg
x
y/parenrightbiggT/parenleftbigg
‚àíATLb
Lb/parenrightbigg
: (3.90)
Using our earlier result (3.55) obtained by completing the square over the quadratic
form of a multivariate Gaussian, we Ô¨Ånd that the mean of zis given by
E[z] = R‚àí1/parenleftbigg
‚àíATLb
Lb/parenrightbigg
: (3.91)
Making use of (3.89), we then obtain Exercise 3.24
E[z] =/parenleftbigg

A+b/parenrightbigg
: (3.92)
Next we Ô¨Ånd an expression for the marginal distribution p(y)in which we have
marginalized over x

============================================================

=== CHUNK 091 ===
Palavras: 360
Caracteres: 2305
--------------------------------------------------
Recall that the marginal distribution over a subset of the com-
ponents of a Gaussian random vector takes a particularly simple form when ex-
pressed in terms of the partitioned covariance matrix SpeciÔ¨Åcally, its mean and Section 3.2
covariance are given by (3.76) and (3.77), respectively Making use of (3.89) and
(3.92), we see that the mean and covariance of the marginal distribution p(y)are
given by
E[y] = A+b (3.93)
cov[y ] = L‚àí1+A‚àí1AT: (3.94)
A special case of this result is when A=I, in which case the marginal distribution
reduces to the convolution of two Gaussians, for which we see that the mean of the
convolution is the sum of the means of the two Gaussians and the covariance of the
convolution is the sum of their covariances Finally, we seek an expression for the conditional p(x|y ) Recall that the results
for the conditional distribution are most easily expressed in terms of the partitioned
precision matrix, using (3.57) and (3.59) Applying these results to (3.89) and (3.92), Section 3.2
we see that the conditional distribution p(x|y )has mean and covariance given by
E[x|y ] = ( +ATLA)‚àí1/braceleftbig
ATL(y‚àíb) +/bracerightbig
(3.95)
cov[x|y ] = ( +ATLA)‚àí1: (3.96)
84 3 STANDARD DISTRIBUTIONS
The evaluation of this conditional distribution can be seen as an example of
Bayes‚Äô theorem, in which we interpret p(x) as a prior distribution over x If the
variable yis observed, then the conditional distribution p(x|y )represents the corre-
sponding posterior distribution over x Having found the marginal and conditional
distributions, we have effectively expressed the joint distribution p(z) =p(x)p(y|x)
in the formp(x|y )p(y) These results can be summarized as follows Given a marginal Gaussian distri-
bution for xand a conditional Gaussian distribution for ygiven xin the form
p(x) =N(x|; ‚àí1) (3.97)
p(y|x) =N(y|Ax+b;L‚àí1); (3.98)
then the marginal distribution of yand the conditional distribution of xgiven yare
given by
p(y) =N(y|A+b;L‚àí1+A‚àí1AT) (3.99)
p(x|y ) =N(x|{ATL(y‚àíb) +}; ) (3.100)
where
= (+ATLA)‚àí1: (3.101)
3.2.7 Maximum likelihood
Given a data set X= (x 1;:::;xN)Tin which the observations {xn}are as-
sumed to be drawn independently from a multivariate Gaussian distribution, we can
estimate the parameters of the distribution by maximum likelihood

============================================================

=== CHUNK 092 ===
Palavras: 362
Caracteres: 2377
--------------------------------------------------
The log likeli-
hood function is given by
lnp(X|; ) =‚àíND
2ln(2 )‚àíN
2ln||‚àí1
2N/summationdisplay
n=1(xn‚àí)T‚àí1(xn‚àí):(3.102)
By simple rearrangement, we see that the likelihood function depends on the data set
only through the two quantities
N/summationdisplay
n=1xn;N/summationdisplay
n=1xnxT
n: (3.103)
These are known as the sufÔ¨Åcient statistics for the Gaussian distribution Using
(A.19), the derivative of the log likelihood with respect to is given by Appendix A
@
@lnp(X|; ) =N/summationdisplay
n=1‚àí1(xn‚àí); (3.104)
and setting this derivative to zero, we obtain the solution for the maximum likelihood
estimate of the mean:
ML=1
NN/summationdisplay
n=1xn; (3.105)
3.2 The Multivariate Gaussian 85
which is the mean of the observed set of data points The maximization of (3.102)
with respect to is rather more involved The simplest approach is to ignore the
symmetry constraint and show that the resulting solution is symmetric as required Exercise 3.28
Alternative derivations of this result, which impose the symmetry and positive deÔ¨Å-
niteness constraints explicitly, can be found in Magnus and Neudecker (1999) The
result is as expected and takes the form
ML=1
NN/summationdisplay
n=1(xn‚àíML)(xn‚àíML)T; (3.106)
which involves MLbecause this is the result of a joint maximization with respect
toand Note that the solution (3.105) for MLdoes not depend on ML, and so
we can Ô¨Årst evaluate MLand then use this to evaluate ML If we evaluate the expectations of the maximum likelihood solutions under the
true distribution, we obtain the following results Exercise 3.29
E[ML] = (3.107)
E[ ML] =N‚àí1
N: (3.108)
We see that the expectation of the maximum likelihood estimate for the mean is equal
to the true mean However, the maximum likelihood estimate for the covariance has
an expectation that is less than the true value, and hence, it is biased We can correct
this bias by deÔ¨Åning a different estimator /tildewidegiven by
/tildewide=1
N‚àí1N/summationdisplay
n=1(xn‚àíML)(xn‚àíML)T: (3.109)
Clearly from (3.106) and (3.108), the expectation of /tildewideis equal to  3.2.8 Sequential estimation
Our discussion of the maximum likelihood solution represents a batch method
in which the entire training data set is considered at once An alternative is to use
sequential methods, which allow data points to be processed one at a time and then
discarded

============================================================

=== CHUNK 093 ===
Palavras: 362
Caracteres: 2164
--------------------------------------------------
These are important for online applications and for large data when the
batch processing of all data points at once is infeasible Consider the result (3.105) for the maximum likelihood estimator of the mean
ML, which we will denote by (N)
MLwhen it is based on Nobservations STANDARD DISTRIBUTIONS
Figure 3.6 Plots of the Old Faith-
ful data in which the red curves are
contours of constant probability den-
sity (a) A single Gaussian distribu-
tion which has been Ô¨Åtted to the data
using maximum likelihood Note that
this distribution fails to capture the
two clumps in the data and indeed
places much of its probability mass
in the central region between the
clumps where the data are relatively
sparse (b) The distribution given by
a linear combination of two Gaus-
sians, also Ô¨Åtted by maximum likeli-
hood, which gives a better represen-
tation of the data 1 2 3 4 5 6406080100(a)
1 2 3 4 5 6406080100 (b)
dissect
out the contribution from the Ô¨Ånal data point xN, we obtain
(N)
ML =1
NN/summationdisplay
n
=1xn
=1
NxN+1
NN‚àí
1/summationdisplay
n=1xn
=1
NxN+N‚àí1
N(
N‚àí1)
ML
=(N‚àí1)
ML +1
N(
xN‚àí(N‚àí1)
ML ): (3.110)
This result has a nice interpretation, as follows After observing N‚àí1data points,
we estimateby(N‚àí1)
ML We now observe data point xN, and we obtain our revised
estimate(N)
MLby moving the old estimate a small amount, proportional to 1=N, in
the direction of the ‚Äòerror signal‚Äô (xN‚àí(N‚àí1)
ML ) Note that, as Nincreases, so the
contributions from successive data points get smaller 3.2.9 Mixtures of Gaussians
Although the Gaussian distribution has some important analytical properties, it
suffers from signiÔ¨Åcant limitations when used to model modelling real data sets Consider the example shown in Figure 3.6(a) This is known as the ‚ÄòOld Faithful‚Äô
data set, and comprises 272measurements of the eruption of the Old Faithful geyser
in Yellowstone National Park in the USA Each measurement gives the duration of
the eruption in minutes (horizontal axis) and the time in minutes to the next eruption
(vertical axis) We see that the data set forms two dominant clumps, and that a simple
Gaussian distribution is unable to capture this structure

============================================================

=== CHUNK 094 ===
Palavras: 363
Caracteres: 2331
--------------------------------------------------
We might expect that a superposition of two Gaussian distributions would be
able to do a much better job of representing the structure in this data set, and indeed
3.2 The Multivariate Gaussian 87
Figure 3.7 Example of a Gaussian mixture distri-
bution in one dimension showing three
Gaussians (each scaled by a coefÔ¨Åcient)
in blue and their sum in red tp(t|x)
this
proves to be the case, as can be seen from Figure 3.6(b) Such superpositions,
formed by taking linear combinations of more basic distributions such as Gaussians,
can be formulated as probabilistic models known as mixture distributions In this sec- Chapter 15
tion we will consider Gaussians to illustrate the framework of mixture models More
generally, mixture models can comprise linear combinations of other distributions,
for example mixtures of Bernoulli distributions for binary variables In Figure 3.7 we
see that a linear combination of Gaussians can give rise to very complex densities By using a sufÔ¨Åcient number of Gaussians and by adjusting their means and covari-
ances as well as the coefÔ¨Åcients in the linear combination, almost any continuous
distribution can be approximated to arbitrary accuracy We therefore consider a superposition of KGaussian densities of the form
p(x) =K/summationdisplay
k=1kN(x|k;k); (3.111)
which is called a mixture of Gaussians Each Gaussian density N(x|k;k)is
called a component of the mixture and has its own mean kand covariance k Contour and surface plots for a Gaussian mixture in two dimensions having three
components are shown in Figure 3.8 The parameters kin (3.111) are called mixing coefÔ¨Åcients If we integrate both
sides of (3.111) with respect to x, and note that both p(x) and the individual Gaussian
components are normalized, we obtain
K/summationdisplay
k=1k= 1: (3.112)
Also, given thatN(x|k;k)>0, a sufÔ¨Åcient condition for the requirement p(x)>
0is thatk>0for allk Combining this with the condition (3.112), we obtain
06k61: (3.113)
We can therefore see that the mixing coefÔ¨Åcients satisfy the requirements to be prob-
abilities, and we will show that this probabilistic interpretation of mixture distribu-
tions is very powerful STANDARD DISTRIBUTIONS
x1x2
œÄ1= 0.5œÄ2= 0.3œÄ3= 0.2
(a)
x1x2
p(x) (b)
x1x2 (c)
Figure 3.8 Illustration of a mixture of three Gaussians in a two-dimensional space

============================================================

=== CHUNK 095 ===
Palavras: 369
Caracteres: 2538
--------------------------------------------------
(a) Contours of constant
density for each of the mixture components, in which the three components are denoted red, blue, and green, and
the values of the mixing coefÔ¨Åcients are shown below each component (b) Contours of the marginal probability
densityp(x)of the mixture distribution (c) A surface plot of the distribution p(x) From the sum and product rules of probability, the marginal density can be writ-
ten as
p(x) =K/summationdisplay
k=1p(k)p(x|k); (3.114)
which is equivalent to (3.111) in which we can view k=p(k)as the prior proba-
bility of picking the kth component, and the density N(x|k;k) =p(x|k)as the
probability of xconditioned on k As we will see in later chapters, an important role
is played by the corresponding posterior probabilities p(k|x), which are also known
asresponsibilities From Bayes‚Äô theorem, these are given by

k(x)‚â°p(k|x)
=p(k)p(x|k)/summationtext
lp(l)p(x|l)
=kN(x|k;k)/summationtext
llN(x|l;l): (3.115)
The form of the Gaussian mixture distribution is governed by the parameters ,
, and, where we have used the notation ‚â°{1;:::;K},‚â°{1;:::;K},
and‚â°{1;:::K} One way to set the values of these parameters is to use
maximum likelihood From (3.111), the log of the likelihood function is given by
lnp(X|;;) =N/summationdisplay
n=1ln/braceleftBiggK/summationdisplay
k=1kN(xn|k;k)/bracerightBigg
(3.116)
where X={x1;:::;xN} We immediately see that the situation is now much more
complex than with a single Gaussian, due to the summation over kinside the log-
3.3 Periodic Variables 89
arithm As a result, the maximum likelihood solution for the parameters no longer
has a closed-form analytical solution One approach for maximizing the likelihood
function is to use iterative numerical optimization techniques Alternatively, we can
employ a powerful framework called expectation maximization, which has wide ap- Chapter 15
plicability to a variety of different deep generative models P
eriodic Variables
Although
Gaussian distributions are of great practical signiÔ¨Åcance, both in their own
right and as building blocks for more complex probabilistic models, there are situa-
tions in which they are inappropriate as density models for continuous variables One
important case, which arises in practical applications, is that of periodic variables An example of a periodic variable is the wind direction at a particular geographi-
cal location We might, for instance, measure the wind direction at multiple locations
and wish to summarize this data using a parametric distribution

============================================================

=== CHUNK 096 ===
Palavras: 355
Caracteres: 2183
--------------------------------------------------
Another example
is calendar time, where we may be interested in modelling quantities that are be-
lieved to be periodic over 24 hours or over an annual cycle Such quantities can
conveniently be represented using an angular (polar) coordinate 06<2 We might be tempted to treat periodic variables by choosing some direction
as the origin and then applying a conventional distribution such as the Gaussian Such an approach, however, would give results that were strongly dependent on the
arbitrary choice of origin Suppose, for instance, that we have two observations at
1= 1‚ó¶and2= 359‚ó¶, and we model them using a standard univariate Gaussian
distribution If we place the origin at 0‚ó¶, then the sample mean of this data set will be
180‚ó¶with standard deviation 179‚ó¶, whereas if we place the origin at 180‚ó¶, then the
mean will be 0‚ó¶and the standard deviation will be 1‚ó¶ We clearly need to develop a
special approach for periodic variables 3.3.1 Von Mises distribution
Let us consider the problem of evaluating the mean of a set of observations
D={1;:::;N}of a periodic variable whereis measured in radians We have
already seen that the simple average (1+¬∑¬∑¬∑+N)=N will be strongly coordinate
dependent To Ô¨Ånd an invariant measure of the mean, note that the observations
can be viewed as points on the unit circle and can therefore be described instead by
two-dimensional unit vectors x1;:::;xNwhere/bardblxn/bardbl= 1 forn= 1;:::;N , as
illustrated in Figure 3.9 We can average the vectors {xn}instead to give
x=1
NN/summationdisplay
n
=1xn (3.117)
and then Ô¨Ånd the corresponding angle of
this average Clearly, this deÔ¨Ånition will
ensure that the location of the mean is independent of the origin of the angular coor-
dinate Note that xwill
typically lie inside the unit circle The Cartesian coordinates
90 3 STANDARD DISTRIBUTIONS
Figure 3.9 Illustration of the representation of val-
uesnof a periodic variable as two-
dimensional vectors xnliving on the unit
circle Also shown is the average xof
those vectors x1x2
x1x2x3x4
x
r

of the observations are given by xn= (cosn;sinn), and we can write the Carte-
sian coordinates of the sample mean in the form x= (rcos;rsin)

============================================================

=== CHUNK 097 ===
Palavras: 351
Caracteres: 2381
--------------------------------------------------
Substituting
into (3.117) and equating the x1andx2components then gives
x1=rcos=1
NN/summationdisplay
n=1cosn;x2=rsin=1
NN/summationdisplay
n=1sinn: (3.118)
Taking the ratio, and using the identity tan= sin=cos, we can solve for to
give
= tan‚àí1/braceleftbigg/summationtext
nsinn/summationtext
ncosn/bracerightbigg
: (3.119)
Shortly, we will see how this result arises naturally as a maximum likelihood estima-
tor First, we need to deÔ¨Åne a periodic generalization of the Gaussian called the
von Mises distribution Here we will limit our attention to univariate distributions,
although analogous periodic distributions can also be found over hyperspheres of
arbitrary dimension (Mardia and Jupp, 2000) By convention, we will consider distributions p()that have period 2 Any
probability density p()deÔ¨Åned over must not only be non-negative and integrate
to one, but it must also be periodic Thus, p()must satisfy the three conditions:
p()>0 (3.120)
/integraldisplay2
0p() d= 1 (3.121)
p(+ 2 ) =p(): (3.122)
From (3.122), it follows that p(+M2) =p()for any integer M We can easily obtain a Gaussian-like distribution that satisÔ¨Åes these three prop-
erties as follows Consider a Gaussian distribution over two variables x= (x1;x2)
3.3 Periodic Variables 91
Figure 3.10 The von Mises distribution can be derived by considering
a two-dimensional Gaussian of the form (3.123), whose
density contours are shown in blue, and conditioning on
the unit circle shown in red x1x2
p(x)
r=1
ha
ving mean= (1;2)and a covariance matrix =2Iwhere Iis the 2√ó2
identity matrix, so that
p(x1;x2) =1
2
2exp/braceleftbigg
‚àí(x1‚àí1)2+ (x 2‚àí2)2
2
2/bracerightbigg
: (3.123)
The contours of constant p(x) are circles, as illustrated in Figure 3.10 Now suppose we consider the value of this distribution along a circle of Ô¨Åxed
radius Then by construction, this distribution will be periodic, although it will not
be normalized We can determine the form of this distribution by transforming from
Cartesian coordinates (x1;x2)to polar coordinates (r;)so that
x1=rcos; x 2=rsin: (3.124)
We also map the mean into polar coordinates by writing
1=r0cos0;  2=r0sin0: (3.125)
Next we substitute these transformations into the two-dimensional Gaussian distribu-
tion (3.123), and then condition on the unit circle r= 1, noting that we are interested
only in the dependence on 

============================================================

=== CHUNK 098 ===
Palavras: 414
Caracteres: 2806
--------------------------------------------------
Focusing on the exponent in the Gaussian distribution
we have
‚àí1
2
2/braceleftbig
(rcos‚àír0cos0)2+ (rsin‚àír0sin0)2/bracerightbig
=‚àí1
2
2/braceleftbig
1 +r2
0‚àí2r0coscos0‚àí2r0sinsin0/bracerightbig
=r0
2cos(
‚àí0) + const (3.126)
where ‚Äòconst‚Äô denotes terms independent of  We have made use of the following
trigonometrical identities:
cos2A+ sin2A= 1 (3.127)
cosAcosB+ sinAsinB= cos(A‚àíB): (3.128)
If we now deÔ¨Åne m=r0=2, we obtain our Ô¨Ånal expression for the distribution of
p()along the unit circle r= 1in the form
p(|0;m) =1
2
I0(m)exp{mcos(‚àí0)}; (3.129)
92 3.STANDARD DISTRIBUTIONS
  
m= 5,Œ∏0=œÄ/4
m= 1,Œ∏0=3œÄ/4
2œÄ0œÄ/
4
3œÄ/4
  
m=5,Œ∏0=œÄ /4
m= 1,Œ∏0= 3œÄ/4
Figure 3.11 The von Mises distribution plotted for two different parameter values, shown as a Cartesian plot
on the left and as the corresponding polar plot on the right which is called the von Mises distribution or the circular normal Here the param-
eter0corresponds to the mean of the distribution, whereas m, which is known
as the concentration parameter, is analogous to the inverse variance (i.e the pre-
cision) for the Gaussian The normalization coefÔ¨Åcient in (3.129) is expressed in
terms ofI0(m), which is the zeroth-order modiÔ¨Åed Bessel function of the Ô¨Årst kind
(Abramowitz and Stegun, 1965) and is deÔ¨Åned by
I0(m) =1
2/integraldisplay2
0exp{mcos}d: (3.130)
For largem, the distribution becomes approximately Gaussian The von Mises dis- Exercise 3.31
tribution is plotted in Figure 3.11 , and the function I0(m)is plotted in Figure 3.12 Now consider the maximum likelihood estimators for the parameters 0andm
for the von Mises distribution The log likelihood function is given by
lnp(D|0;m) =‚àíNln(2)‚àíNlnI0(m) +mN/summationdisplay
n=1cos(n‚àí0): (3.131)
Setting the derivative with respect to 0equal to zero gives
N/summationdisplay
n=1sin(n‚àí0) = 0: (3.132)
To solve for 0, we make use of the trigonometric identity
sin(A‚àíB) = cosBsinA‚àícosAsinB (3.133)
from which we obtain Exercise 3.32
3.3 Periodic Variables 93
I0(m)
m0 5 100100020003000
A(m)
m0 5 1000.51
Figure
3.12 Plot of the Bessel function I0(m)deÔ¨Åned by (3.130), together with the function A(m)deÔ¨Åned by
(3.136) ML
0= tan‚àí1/braceleftbigg/summationtext
nsinn/summationtext
ncosn/bracerightbigg
; (3.134)
which
we recognize as the result (3.119) obtained earlier for the mean of the obser-
vations viewed in a two-dimensional Cartesian space Similarly, maximizing (3.131) with respect to mand making use of I/prime
0(m) =
I1(m) (Abramowitz and Stegun, 1965), we have
A(m ML) =1
NN/summationdisplay
n
=1cos(n‚àíML
0) (3.135)
where we have substituted for the maximum likelihood solution for ML
0(recalling
that we are performing a joint optimization over andm), and we have deÔ¨Åned
A(m) =I1(m)
I0(
m): (3.136)
The function A(m) is plotted in Figure 3.12

============================================================

=== CHUNK 099 ===
Palavras: 352
Caracteres: 2303
--------------------------------------------------
Making use of the trigonometric iden-
tity (3.128), we can write (3.135) in the form
A(m ML) =/parenleftBigg
1
NN/summationdisplay
n
=1cosn/parenrightBigg
cosML
0+/parenleftBigg
1
NN/summationdisplay
n
=1sinn/parenrightBigg
sinML
0: (3.137)
The right-hand side of (3.137) is easily evaluated, and the function A(m) can be in-
verted numerically One limitation of the von Mises distribution is that it is unimodal By forming mixtures of von Mises distributions, we obtain a Ô¨Çexible framework for
modelling periodic variables that can handle multimodality For completeness, we mention brieÔ¨Çy some alternative techniques for construct-
ing periodic distributions The simplest approach is to use a histogram of observa-
tions in which the angular coordinate is divided into Ô¨Åxed bins This has the virtue of
94 3 STANDARD DISTRIBUTIONS
simplicity and Ô¨Çexibility but also suffers from signiÔ¨Åcant limitations, as we will see
when we discuss histogram methods in more detail later Another approach starts, Section 3.5
like the von Mises distribution, from a Gaussian distribution over a Euclidean space
but now marginalizes onto the unit circle rather than conditioning (Mardia and Jupp,
2000) However, this leads to more complex forms of distribution and will not be
discussed further Finally, any valid distribution over the real axis (such as a Gaus-
sian) can be turned into a periodic distribution by mapping successive intervals of
width 2onto the periodic variable (0;2), which corresponds to ‚Äòwrapping‚Äô the
real axis around the unit circle Again, the resulting distribution is more complex to
handle than the von Mises distribution The Exponential Family
The probability distributions that we have studied so far in this chapter (with the
exception of mixture models) are speciÔ¨Åc examples of a broad class of distributions
called the exponential family (Duda and Hart, 1973; Bernardo and Smith, 1994) Members of the exponential family have many important properties in common, and
it is illuminating to discuss these properties in some generality The exponential family of distributions over x, given parameters , is deÔ¨Åned to
be the set of distributions of the form
p(x| ) =h(x)g () exp/braceleftbig
Tu(x)/bracerightbig
(3.138)
where xmay be scalar or vector and may be discrete or continuous

============================================================

=== CHUNK 100 ===
Palavras: 360
Caracteres: 2449
--------------------------------------------------
Here are called
thenatural parameters of the distribution, and u(x) is some function of x The
functiong()can be interpreted as the coefÔ¨Åcient that ensures that the distribution
is normalized, and therefore, it satisÔ¨Åes
g()/integraldisplay
h(x) exp/braceleftbig
Tu(x)/bracerightbig
dx= 1 (3.139)
where the integration is replaced by summation if xis a discrete variable We begin by taking some examples of the distributions introduced earlier in
the chapter and showing that they are indeed members of the exponential family Consider Ô¨Årst the Bernoulli distribution:
p(x|) = Bern( x|) =x(1‚àí)1‚àíx: (3.140)
Expressing the right-hand side as the exponential of the logarithm, we have
p(x|) = exp{xln+ (1‚àíx) ln(1‚àí)}
= (1‚àí) exp/braceleftbigg
ln/parenleftbigg
1‚àí/parenrightbigg
x/bracerightbigg
: (3.141)
Comparison with (3.138) allows us to identify
= ln/parenleftbigg
1‚àí/parenrightbigg
(3.142)
3.4 The Exponential Family 95
which we can solve for to give=(), where
() =1
1 + exp(‚àí )(3.143)
is called the logistic sigmoid function Thus, we can write the Bernoulli distribution
using the standard representation (3.138) in the form
p(x| ) =(‚àí) exp(x) (3.144)
where we have used 1‚àí() =(‚àí), which is easily proved from (3.143) Com-
parison with (3.138) shows that
u(x) =x (3.145)
h(x) = 1 (3.146)
g() =(‚àí): (3.147)
Next consider the multinomial distribution which, for a single observation x,
takes the form
p(x|) =M/productdisplay
k=1xk
k= exp/braceleftBiggM/summationdisplay
k=1xklnk/bracerightBigg
(3.148)
where x= (x 1;:::;xM)T Again, we can write this in the standard representation
(3.138) so that
p(x| ) = exp(Tx) (3.149)
wherek= lnk, and we have deÔ¨Åned = (1;:::;M)T Again, comparing with
(3.138) we have
u(x) = x (3.150)
h(x) = 1 (3.151)
g() = 1: (3.152)
Note that the parameters kare not independent because the parameters kare sub-
ject to the constraint
M/summationdisplay
k=1k= 1 (3.153)
so that, given any M‚àí1of the parameters k, the value of the remaining parameter
is Ô¨Åxed In some circumstances, it will be convenient to remove this constraint by
expressing the distribution in terms of only M‚àí1parameters This can be achieved
by using the relationship (3.153) to eliminate Mby expressing it in terms of the
remaining{k}wherek= 1;:::;M‚àí1, thereby leaving M‚àí1parameters Note
that these remaining parameters are still subject to the constraints
06k61;M‚àí1/summationdisplay
k=1k61: (3.154)
96 3

============================================================

=== CHUNK 101 ===
Palavras: 404
Caracteres: 3270
--------------------------------------------------
STANDARD DISTRIBUTIONS
Making use of the constraint (3.153), the multinomial distribution in this representa-
tion then becomes
exp/braceleftBiggM/summationdisplay
k=1xklnk/bracerightBigg
= exp/braceleftBiggM‚àí1/summationdisplay
k=1xklnk+/parenleftBigg
1‚àíM‚àí1/summationdisplay
k=1xk/parenrightBigg
ln/parenleftBigg
1‚àíM‚àí1/summationdisplay
k=1k/parenrightBigg/bracerightBigg
= exp/braceleftBiggM‚àí1/summationdisplay
k=1xkln/parenleftBigg
k
1‚àí/summationtextM‚àí1
j=1j/parenrightBigg
+ ln/parenleftBigg
1‚àíM‚àí1/summationdisplay
k=1k/parenrightBigg/bracerightBigg
:(3.155)
We now identify
ln/parenleftBigg
k
1‚àí/summationtext
jj/parenrightBigg
=k; (3.156)
which we can solve for kby Ô¨Årst summing both sides over kand then rearranging
and back-substituting to give
k=exp(k)
1 +/summationtext
jexp(j): (3.157)
This is called the softmax function or the normalized exponential In this representa-
tion, the multinomial distribution therefore takes the form
p(x| ) =/parenleftBigg
1 +M‚àí1/summationdisplay
k=1exp(k)/parenrightBigg‚àí1
exp(Tx): (3.158)
This is the standard form of the exponential family, with parameter vector =
(1;:::;M‚àí1)Tin which
u(x) = x (3.159)
h(x) = 1 (3.160)
g() =/parenleftBigg
1 +M‚àí1/summationdisplay
k=1exp(k)/parenrightBigg‚àí1
: (3.161)
Finally, let us consider the Gaussian distribution For the univariate Gaussian,
we have
p(x|;2) =1
(22)1=2exp/braceleftbigg
‚àí1
22(x‚àí)2/bracerightbigg
(3.162)
=1
(22)1=2exp/braceleftbigg
‚àí1
22x2+
2x‚àí1
222/bracerightbigg
;(3.163)
which, after some simple rearranging, can be cast in the standard exponential family
form (3.138) with Exercise 3.35
3.4 The Exponential Family 97
=/parenleftbigg
=2
‚àí1=22/parenrightbigg
(3.164)
u(x) =/parenleftbigg
x
x2/parenrightbigg
(3.165)
h(x) = (2 )‚àí1=2(3.166)
g() = (‚àí2 2)1=2exp/parenleftbigg2
1
42/parenrightbigg
: (3.167)
Finally, we shall sometimes make use of a restricted form of (3.138) in which
we choose u(x) = x However, this can be somewhat generalized by noting that if
f(x)is a normalized density then
1
sf/parenleftbigg1
sx/parenrightbigg
(3.168)
is also a normalized density, where s>0is a scale parameter Combining these, we
arrive at a restricted set of exponential family class-conditional densities of the form
p(x|k;s) =1
sh/parenleftbigg1
sx/parenrightbigg
g(k) exp/braceleftbigg1
sT
kx/bracerightbigg
: (3.169)
Note that we are allowing each class to have its own parameter vector kbut we are
assuming that the classes share the same scale parameter s 3.4.1 SufÔ¨Åcient statistics
Let us now consider the problem of estimating the parameter vector in the gen-
eral exponential family distribution (3.138) using the technique of maximum likeli-
hood Taking the gradient of both sides of (3.139) with respect to , we have
‚àág()/integraldisplay
h(x) exp/braceleftbig
Tu(x)/bracerightbig
dx
+g()/integraldisplay
h(x) exp/braceleftbig
Tu(x)/bracerightbig
u(x) d x= 0: (3.170)
Rearranging and making use again of (3.139) then gives
‚àí1
g()‚àág() =g()/integraldisplay
h(x) exp/braceleftbig
Tu(x)/bracerightbig
u(x) d x=E[u(x)]: (3.171)
We therefore obtain the result
‚àí‚àálng() =E[u(x)]: (3.172)
Note that the covariance of u(x) can be expressed in terms of the second derivatives
ofg(), and similarly for higher-order moments

============================================================

=== CHUNK 102 ===
Palavras: 383
Caracteres: 2447
--------------------------------------------------
Thus, provided we can normalize a Exercise 3.36
distribution from the exponential family, we can always Ô¨Ånd its moments by simple
differentiation STANDARD DISTRIBUTIONS
Now consider a set of independent identically distributed data denoted by X=
{x1;:::;xn}, for which the likelihood function is given by
p(X| ) =/parenleftBiggN/productdisplay
n=1h(xn)/parenrightBigg
g()Nexp/braceleftBigg
TN/summationdisplay
n=1u(xn)/bracerightBigg
: (3.173)
Setting the gradient of lnp(X| )with respect to to zero, we get the following
condition to be satisÔ¨Åed by the maximum likelihood estimator ML:
‚àí‚àálng(ML) =1
NN/summationdisplay
n
=1u(xn); (3.174)
which can in principle be solved to obtain ML We see that the solution for the
maximum likelihood estimator depends on the data only through/summationtext
nu(xn), which
is therefore called the sufÔ¨Åcient statistic of the distribution (3.138) We do not need
to store the entire data set itself but only the value of the sufÔ¨Åcient statistic For
the Bernoulli distribution, for example, the function u(x) is given just by xand
so we need only keep the sum of the data points {xn}, whereas for the Gaussian
u(x) = (x;x2)T, and so we should keep both the sum of {xn}and the sum of{x2
n} If we consider the limit N‚Üí‚àû, then the right-hand side of (3.174) becomes
E[u(x)], and so by comparing with (3.172) we see that in this limit, MLwill equal
the true value Nonparametric
Methods
Throughout
this chapter, we have focused on the use of probability distributions
having speciÔ¨Åc functional forms governed by a small number of parameters whose
values are to be determined from a data set This is called the parametric approach
to density modelling An important limitation of this approach is that the chosen
density might be a poor model of the distribution that generates the data, which can
result in poor predictive performance For instance, if the process that generates the
data is multimodal, then this aspect of the distribution can never be captured by a
Gaussian, which is necessarily unimodal In this Ô¨Ånal section, we consider some
nonparametric approaches to density estimation that make few assumptions about
the form of the distribution 3.5.1 Histograms
Let us start with a discussion of histogram methods for density estimation, which
we have already encountered in the context of marginal and conditional distributions
inFigure 2.5 and in the context of the central limit theorem in Figure 3.2

============================================================

=== CHUNK 103 ===
Palavras: 362
Caracteres: 2110
--------------------------------------------------
Here we
explore the properties of histogram density models in more detail, focusing on cases
with a single continuous variable x Standard histograms simply partition xinto
distinct bins of width ‚àÜiand then count the number niof observations of xfalling
3.5 Nonparametric Methods 99
Figure 3.13 An illustration of the histogram
approach to density estimation,
in which a data set of 50data
points is generated from the dis-
tribution shown by the green
curve Histogram density esti-
mates, based on (3.175) with a
common bin width , are shown
for various values of  To turn this count into a normalized probability density, we simply divide
by the total number Nof observations and by the width ‚àÜiof the bins to obtain
probability values for each bin:
pi=ni
N‚àÜi(3.175)
for
which it is easily seen that/integraltext
p(x) dx= 1 This gives a model for the density
p(x) that is constant over the width of each bin Often the bins are chosen to have
the same width ‚àÜi= ‚àÜ InFigure 3.13, we show an example of histogram density estimation Here the
data is drawn from the distribution corresponding to the green curve, which is formed
from a mixture of two Gaussians Also shown are three examples of histogram
density estimates corresponding to three different choices for the bin width ‚àÜ We
see that when ‚àÜis very small (top Ô¨Ågure), the resulting density model is very spiky,
with a lot of structure that is not present in the underlying distribution that generated
the data set Conversely, if ‚àÜis too large (bottom Ô¨Ågure) then the result is a model
that is too smooth and consequently fails to capture the bimodal property of the
green curve The best results are obtained for some intermediate value of ‚àÜ(middle
Ô¨Ågure) In principle, a histogram density model is also dependent on the choice of
edge location for the bins, though this is typically much less signiÔ¨Åcant than the bin
width ‚àÜ Note that the histogram method has the property (unlike the methods to be dis-
cussed shortly) that, once the histogram has been computed, the data set itself can
be discarded, which can be advantageous if the data set is large

============================================================

=== CHUNK 104 ===
Palavras: 373
Caracteres: 2326
--------------------------------------------------
Also, the histogram
approach is easily applied if the data points arrive sequentially In practice, the histogram technique can be useful for obtaining a quick visual-
ization of data in one or two dimensions but is unsuited to most density estimation
applications One obvious problem is that the estimated density has discontinuities
that are due to the bin edges rather than any property of the underlying distribution
that generated the data A major limitation of the histogram approach is its scal-
ing with dimensionality If we divide each variable in a D-dimensional space into
100 3 STANDARD DISTRIBUTIONS
Mbins, then the total number of bins will be MD This exponential scaling with
Dis an example of the curse of dimensionality In a space of high dimensionality, Section 6.1.1
the quantity of data needed to provide meaningful estimates of the local probability
density would be prohibitive The histogram approach to density estimation does, however, teach us two im-
portant lessons First, to estimate the probability density at a particular location,
we should consider the data points that lie within some local neighbourhood of that
point Note that the concept of locality requires that we assume some form of dis-
tance measure, and here we have been assuming Euclidean distance For histograms,
this neighbourhood property was deÔ¨Åned by the bins, and there is a natural ‚Äòsmooth-
ing‚Äô parameter describing the spatial extent of the local region, in this case the bin
width Second, to obtain good results, the value of the smoothing parameter should
be neither too large nor too small This is reminiscent of the choice of model com-
plexity in polynomial regression where the degree Mof the polynomial, or alterna- Chapter 1
tively the value of the regularization parameter, was optimal for some intermediate
value, neither too large nor too small Armed with these insights, we turn now to a
discussion of two widely used nonparametric techniques for density estimation, ker-
nel estimators and nearest neighbours, which have better scaling with dimensionality
than the simple histogram model 3.5.2 Kernel densities
Let us suppose that observations are being drawn from some unknown probabil-
ity densityp(x) in someD-dimensional space, which we will take to be Euclidean,
and we wish to estimate the value of p(x)

============================================================

=== CHUNK 105 ===
Palavras: 353
Caracteres: 2194
--------------------------------------------------
From our earlier discussion of locality,
let us consider some small region Rcontaining x The probability mass associated
with this region is given by
P=/integraldisplay
Rp(x) d x: (3.176)
Now suppose that we have collected a data set comprising Nobservations drawn
fromp(x) Because each data point has a probability Pof falling withinR, the total
numberKof points that lie inside Rwill be distributed according to the binomial
distribution: Section 3.1.2
Bin(K|N;P ) =N K!(N‚àíK)!PK(1‚àíP)N‚àíK: (3.177)
Using (3.11), we see that the mean fraction of points falling inside the region is
E[K=N ] =P, and similarly using (3.12), we see that the variance around this mean
isvar[K=N ] =P(1‚àíP)=N For largeN, this distribution will be sharply peaked
around the mean and so
K/similarequalNP: (3.178)
If, however, we also assume that the region Ris sufÔ¨Åciently small so that the proba-
bility density p(x) is roughly constant over the region, then we have
P/similarequalp(x)V (3.179)
3.5 Nonparametric Methods 101
whereVis the volume ofR Combining (3.178) and (3.179), we obtain our density
estimate in the form
p(x) =K
NV: (3.180)
Note that the validity of (3.180) depends on two contradictory assumptions, namely
that the regionRis sufÔ¨Åciently small that the density is approximately constant over
the region and yet sufÔ¨Åciently large (in relation to the value of that density) that the
numberKof points falling inside the region is sufÔ¨Åcient for the binomial distribution
to be sharply peaked We can exploit the result (3.180) in two different ways Either we can Ô¨Åx Kand
determine the value of Vfrom the data, which gives rise to the K-nearest-neighbour
technique discussed shortly, or we can Ô¨Åx Vand determine Kfrom the data, giv-
ing rise to the kernel approach It can be shown that both the K-nearest-neighbour
density estimator and the kernel density estimator converge to the true probability
density in the limit N‚Üí‚àû provided that Vshrinks with Nand thatKgrows with
N, at an appropriate rate (Duda and Hart, 1973) We begin by discussing the kernel method in detail To start with we take the
regionRto be a small hypercube centred on the point xat which we wish to de-
termine the probability density

============================================================

=== CHUNK 106 ===
Palavras: 366
Caracteres: 2245
--------------------------------------------------
To count the number Kof points falling within this
region, it is convenient to deÔ¨Åne the following function:
k(u) =/braceleftbigg
1;|ui|61=2,i= 1;:::;D ,
0;otherwise;(3.181)
which represents a unit cube centred on the origin The function k(u)is an example
of a kernel function , and in this context, it is also called a Parzen window From
(3.181), the quantity k((x‚àíxn)=h) will be 1 if the data point xnlies inside a cube
of sidehcentred on x, and zero otherwise The total number of data points lying
inside this cube will therefore be
K=N/summationdisplay
n=1k/parenleftbiggx‚àíxn
h/parenrightbigg
: (3.182)
Substituting this expression into (3.180) then gives the following result for the esti-
mated density at x:
p(x) =1
NN/summationdisplay
n=11
hDk/parenleftbiggx‚àíxn
h/parenrightbigg
(3.183)
where we have used V=hDfor the volume of a hypercube of side hinDdi-
mensions Using the symmetry of the function k(u), we can now reinterpret this
equation, not as a single cube centred on xbut as the sum over Ncubes centred on
theNdata points xn As it stands, the kernel density estimator (3.183) will suffer from one of the same
problems that the histogram method suffered from, namely the presence of artiÔ¨Åcial
discontinuities, in this case at the boundaries of the cubes We can obtain a smoother
102 3 STANDARD DISTRIBUTIONS
Figure 3.14 Illustration of the kernel den-
sity model (3.184) applied to the
same data set used to demon-
strate the histogram approach in
Figure 3.13 We see that hacts
as a smoothing parameter and
that if it is set too small (top
panel), the result is a very noisy
density model, whereas if it is
set too large (bottom panel), then
the bimodal nature of the under-
lying distribution from which the
data is generated (shown by the
green curve) is washed out The
best density model is obtained
for some intermediate value of h
(middle panel) h= 0.005
0 0.5 105
h= 0.07
0 0.5 105
h=0.2
0 0.5 105
density
model if we choose a smoother kernel function, and a common choice is the
Gaussian, which gives rise to the following kernel density model:
p(x) =1
NN/summationdisplay
n
=11
(2
h2)D=2exp/braceleftbigg
‚àí/bardblx‚àíxn/bardbl2
2
h2/bracerightbigg
(3.184)
wherehrepresents the standard deviation of the Gaussian components

============================================================

=== CHUNK 107 ===
Palavras: 355
Caracteres: 2112
--------------------------------------------------
Thus, our
density model is obtained by placing a Gaussian over each data point, adding up the
contributions over the whole data set, and then dividing by Nso that the density
is correctly normalized In Figure 3.14, we apply the model (3.184) to the data
set used earlier to demonstrate the histogram technique We see that, as expected,
the parameter hplays the role of a smoothing parameter, and there is a trade-off
between sensitivity to noise at small hand over-smoothing at large h Again, the
optimization of his a problem in model complexity, analogous to the choice of bin
width in histogram density estimation or the degree of the polynomial used in curve
Ô¨Åtting We can choose any other kernel function k(u)in (3.183) subject to the condi-
tions
k(u)>0; (3.185)/integraldisplay
k(u) du= 1; (3.186)
which ensure that the resulting probability distribution is non-negative everywhere
and integrates to one The class of density model given by (3.183) is called a kernel
density estimator or Parzen estimator It has a great merit that there is no computation
involved in the ‚Äòtraining‚Äô phase because this simply requires the training set to be
stored However, this is also one of its great weaknesses because the computational
cost of evaluating the density grows linearly with the size of the data set Nonparametric Methods 103
Figure 3.15 Illustration of K-nearest-
neighbour density estimation
using the same data set as in
Figures 3.14 and3.13 We see
that the parameter Kgoverns
the degree of smoothing, so
that a small value of Kleads
to a very noisy density model
(top panel), whereas a large
value (bottom panel) smooths
out the bimodal nature of the true
distribution (shown by the green
curve) from which the data set
was generated K= 1
0 0.5 105
K=5
0 0.5 105
K=30
0 0.5 105
3.5.3
Nearest-neighbours
One of the difÔ¨Åculties with the kernel approach to density estimation is that the
parameterhgoverning the kernel width is Ô¨Åxed for all kernels In regions of high
data density, a large value of hmay lead to over-smoothing and a washing out of
structure that might otherwise be extracted from the data

============================================================

=== CHUNK 108 ===
Palavras: 364
Caracteres: 2144
--------------------------------------------------
However, reducing hmay
lead to noisy estimates elsewhere in the data space where the density is smaller Thus, the optimal choice for hmay be dependent on the location within the data
space This issue is addressed by nearest-neighbour methods for density estimation We therefore return to our general result (3.180) for local density estimation,
and instead of Ô¨Åxing Vand determining the value of Kfrom the data, we consider
a Ô¨Åxed value of Kand use the data to Ô¨Ånd an appropriate value for V To do this,
we consider a small sphere centred on the point xat which we wish to estimate the
densityp(x), and we allow the radius of the sphere to grow until it contains precisely
Kdata points The estimate of the density p(x) is then given by (3.180) with V
set to the volume of the resulting sphere This technique is known as Knearest
neighbours and is illustrated in Figure 3.15 for various choices of the parameter K
using the same data set as used in Figures 3.13 and3.14 We see that the value of K
now governs the degree of smoothing and that again there is an optimum choice for
Kthat is neither too large nor too small Note that the model produced by Knearest
neighbours is not a true density model because the integral over all space diverges Exercise 3.38
We close this chapter by showing how the K-nearest-neighbour technique for
density estimation can be extended to the problem of classiÔ¨Åcation To do this, we
apply theK-nearest-neighbour density estimation technique to each class separately
and then make use of Bayes‚Äô theorem Let us suppose that we have a data set com-
prisingNkpoints in classCkwithNpoints in total, so that/summationtext
kNk=N If we
wish to classify a new point x, we draw a sphere centred on xcontaining precisely
Kpoints irrespective of their class Suppose this sphere has volume Vand contains
Kkpoints from classCk Then (3.180) provides an estimate of the density associated
104 3 STANDARD DISTRIBUTIONS
Figure 3.16 (a) In theK-nearest-
neighbour classiÔ¨Åer, a new point,
shown by the black diamond, is clas-
siÔ¨Åed according to the majority class
membership of the Kclosest train-
ing data points, in this case K=
3

============================================================

=== CHUNK 109 ===
Palavras: 379
Caracteres: 2324
--------------------------------------------------
(b) In the nearest-neighbour
(K= 1) approach to classiÔ¨Åcation,
the resulting decision boundary is
composed of hyperplanes that form
perpendicular bisectors of pairs of
points from different classes x1x2
(a)
x1x2
(b)
with
each class:
p(x|Ck) =Kk
NkV: (3.187)
Similarly
, the unconditional density is given by
p(x) =K
N
V(3.188)
and the class priors are given by
p(Ck) =Nk
N: (3.189)
W
e can now combine (3.187), (3.188), and (3.189) using Bayes‚Äô theorem to obtain
the posterior probability of class membership:
p(Ck|x) =p(x|Ck)p(Ck)
p
(x)=Kk
K: (3.190)
W
e can minimize the probability of misclassiÔ¨Åcation by assigning the test point xto
the class having the largest posterior probability, corresponding to the largest value
ofKk=K Thus, to classify a new point, we identify the Knearest points from the
training data set and then assign the new point to the class having the largest number
of representatives amongst this set Ties can be broken at random The particular
case ofK= 1 is called the nearest-neighbour rule, because a test point is simply
assigned to the same class as the nearest point from the training set These concepts
are illustrated in Figure 3.16 An interesting property of the nearest-neighbour (K = 1) classiÔ¨Åer is that, in the
limitN‚Üí‚àû, the error rate is never more than twice the minimum achievable error
rate of an optimal classiÔ¨Åer, i.e., one that uses the true class distributions (Cover and
Hart, 1967) As discussed so far, both the K-nearest-neighbour method and the kernel den-
sity estimator require the entire training data set to be stored, leading to expensive
Exercises 105
computation if the data set is large This effect can be offset, at the expense of some
additional one-off computation, by constructing tree-based search structures to allow
(approximate) near neighbours to be found efÔ¨Åciently without doing an exhaustive
search of the data set Nevertheless, these nonparametric methods are still severely
limited On the other hand, we have seen that simple parametric models are very
restricted in terms of the forms of distribution that they can represent We therefore
need to Ô¨Ånd density models that are very Ô¨Çexible and yet for which the complexity
of the models can be controlled independently of the size of the training set, and this
can be achieved using deep neural networks

============================================================

=== CHUNK 110 ===
Palavras: 383
Caracteres: 2628
--------------------------------------------------
Exercises
3.1 (?)Verify that the Bernoulli distribution (3.2) satisÔ¨Åes the following properties:
1/summationdisplay
x=0p(x|) = 1 (3.191)
E[x] = (3.192)
var[x ] =(1‚àí): (3.193)
Show that the entropy H[x] of a Bernoulli-distributed random binary variable xis
given by
H[x] =‚àíln‚àí(1‚àí) ln(1‚àí): (3.194)
3.2 (??) The form of the Bernoulli distribution given by (3.2) is not symmetric between
the two values of x In some situations, it will be more convenient to use an equiva-
lent formulation for which x‚àà{‚àí 1;1}, in which case the distribution can be written
p(x|) =/parenleftbigg1‚àí
2/parenrightbigg(1‚àíx)=2/parenleftbigg1 +
2/parenrightbigg(1+x)=2
(3.195)
where‚àà[‚àí1;1] Show that the distribution (3.195) is normalized, and evaluate its
mean, variance, and entropy 3.3 (??) In this exercise, we prove that the binomial distribution (3.9) is normalized First, use the deÔ¨Ånition (3.10) of the number of combinations of midentical objects
chosen from a total of Nto show that
/parenleftbiggN
m/parenrightbigg
+/parenleftbiggN
m‚àí1/parenrightbigg
=/parenleftbiggN+ 1
m/parenrightbigg
: (3.196)
Use this result to prove by induction the following result:
(1 +x)N=N/summationdisplay
m=0/parenleftbiggN
m/parenrightbigg
xm; (3.197)
106 3 STANDARD DISTRIBUTIONS
which is known as the binomial theorem and which is valid for all real values of x Finally, show that the binomial distribution is normalized, so that
N/summationdisplay
m=0/parenleftbiggN
m/parenrightbigg
m(1‚àí)N‚àím= 1; (3.198)
which can be done by Ô¨Årst pulling a factor (1‚àí)Nout of the summation and then
making use of the binomial theorem 3.4 (??) Show that the mean of the binomial distribution is given by (3.11) To do this,
differentiate both sides of the normalization condition (3.198) with respect to and
then rearrange to obtain an expression for the mean of n Similarly, by differentiating
(3.198) twice with respect to and making use of the result (3.11) for the mean of
the binomial distribution, prove the result (3.12) for the variance of the binomial 3.5 (?)Show that the mode of the multivariate Gaussian (3.26) is given by  3.6 (??) Suppose that xhas a Gaussian distribution with mean and covariance  Show that the linearly transformed variable Ax+bis also Gaussian, and Ô¨Ånd its
mean and covariance 3.7 (???) Show that the Kullback‚ÄìLeibler divergence between two Gaussian distribu-
tionsq(x) =N(x|q;q)andp(x) =N(x|p;p)is given by
KL (q(x)/bardblp(x))
=1
2/braceleftbigg
ln|p|
|q|‚àíD+Tr/parenleftbig
‚àí1
pq/parenrightbig
+ (p‚àíq)T‚àí1
p(p‚àíq)/bracerightbigg
(3.199)
where Tr(¬∑) denotes the trace of a matrix, and Dis the dimensionality of x

============================================================

=== CHUNK 111 ===
Palavras: 364
Caracteres: 2394
--------------------------------------------------
3.8 (??) This exercise demonstrates that the multivariate distribution with maximum
entropy, for a given covariance, is a Gaussian The entropy of a distribution p(x) is
given by
H[x] =‚àí/integraldisplay
p(x) lnp(x) d x: (3.200)
We wish to maximize H[x] over all distributions p(x) subject to the constraints that
p(x) is normalized and that it has a speciÔ¨Åc mean and covariance, so that
/integraldisplay
p(x) d x= 1 (3.201)
/integraldisplay
p(x)x dx= (3.202)
/integraldisplay
p(x)(x‚àí)(x‚àí)Tdx=: (3.203)
By performing a variational maximization of (3.200) and using Lagrange multipliers
to enforce the constraints (3.201), (3.202), and (3.203), show that the maximum
likelihood distribution is given by the Gaussian (3.26) Exercises 107
3.9 (???) Show that the entropy of the multivariate Gaussian N(x|; )is given by
H[x] =1
2ln||+D
2(1 + ln(2 )) (3.204)
whereDis the dimensionality of x 3.10 (???) Consider two random variables x1andx2having Gaussian distributions with
means1and2and precisions 1and2, respectively Derive an expression for the
differential entropy of the variable x=x1+x2 To do this, Ô¨Årst Ô¨Ånd the distribution
ofxby using the relation
p(x) =/integraldisplay‚àû
‚àí‚àûp(x|x 2)p(x 2) dx2 (3.205)
and completing the square in the exponent Then observe that this represents the
convolution of two Gaussian distributions, which itself will be Gaussian, and Ô¨Ånally
make use of the result (2.99) for the entropy of the univariate Gaussian 3.11 (?)Consider the multivariate Gaussian distribution given by (3.26) By writing the
precision matrix (inverse covariance matrix) as the sum of a symmetric and an anti-
symmetric matrix, show that the antisymmetric term does not appear in the exponent
of the Gaussian, and hence, that the precision matrix may be taken to be symmetric
without loss of generality Because the inverse of a symmetric matrix is also sym-
metric (see Exercise 3.16), it follows that the covariance matrix may also be chosen
to be symmetric without loss of generality 3.12 (???) Consider a real, symmetric matrix whose eigenvalue equation is given by
(3.28) By taking the complex conjugate of this equation, subtracting the original
equation, and then forming the inner product with eigenvector ui, show that the
eigenvalues iare real Similarly, use the symmetry property of to show that two
eigenvectors uiandujwill be orthogonal provided j/negationslash=i

============================================================

=== CHUNK 112 ===
Palavras: 353
Caracteres: 2253
--------------------------------------------------
Finally, show that,
without loss of generality, the set of eigenvectors can be chosen to be orthonormal,
so that they satisfy (3.29), even if some of the eigenvalues are zero 3.13 (??) Show that a real, symmetric matrix having the eigenvector equation (3.28)
can be expressed as an expansion in the eigenvectors, with coefÔ¨Åcients given by the
eigenvalues, of the form (3.31) Similarly, show that the inverse matrix ‚àí1has a
representation of the form (3.32) 3.14 (??) A positive deÔ¨Ånite matrix can be deÔ¨Åned as one for which the quadratic form
aTa (3.206)
is positive for any real value of the vector a Show that a necessary and sufÔ¨Åcient
condition for to be positive deÔ¨Ånite is that all the eigenvalues iof, deÔ¨Åned by
(3.28), are positive 3.15 (?)Show that a real, symmetric matrix of size D√óDhasD(D+ 1)=2 independent
parameters STANDARD DISTRIBUTIONS
3.16 (?)Show that the inverse of a symmetric matrix is itself symmetric 3.17 (??) By diagonalizing the coordinate system using the eigenvector expansion (3.31),
show that the volume contained within the hyperellipsoid corresponding to a constant
Mahalanobis distance ‚àÜis given by
VD||1=2‚àÜD(3.207)
whereVDis the volume of the unit sphere in Ddimensions, and the Mahalanobis
distance is deÔ¨Åned by (3.27) 3.18 (??) Prove the identity (3.60) by multiplying both sides by the matrix
/parenleftbigg
A B
C D/parenrightbigg
(3.208)
and making use of the deÔ¨Ånition (3.61) 3.19 (???) In Sections 3.2.4 and 3.2.5, we considered the conditional and marginal distri-
butions for a multivariate Gaussian More generally, we can consider a partitioning
of the components of xinto three groups xa,xb, andxc, with a corresponding par-
titioning of the mean vector and of the covariance matrix in the form
=Ô£´
Ô£≠a
b
cÔ£∂
Ô£∏; =Ô£´
Ô£≠aaabac
babbbc
cacbccÔ£∂
Ô£∏: (3.209)
By making use of the results of Section 3.2, Ô¨Ånd an expression for the conditional
distributionp(xa|xb)in which xchas been marginalized out 3.20 (??) A very useful result from linear algebra is the Woodbury matrix inversion for-
mula given by
(A+BCD )‚àí1=A‚àí1‚àíA‚àí1B(C‚àí1+DA‚àí1B)‚àí1DA‚àí1: (3.210)
By multiplying both sides by (A+BCD), prove the correctness of this result 3.21 (?)Letxandzbe two independent random vectors, so that p(x;z) =p(x)p(z)

============================================================

=== CHUNK 113 ===
Palavras: 353
Caracteres: 2265
--------------------------------------------------
Show that the mean of their sum y=x+zis given by the sum of the means of each
of the variables separately Similarly, show that the covariance matrix of yis given
by the sum of the covariance matrices of xandz 3.22 (???) Consider a joint distribution over the variable
z=/parenleftbigg
x
y/parenrightbigg
(3.211)
whose mean and covariance are given by (3.92) and (3.89), respectively By making
use of the results (3.76) and (3.77), show that the marginal distribution p(x) is given
by (3.83) Similarly, by making use of the results (3.65) and (3.66), show that the
conditional distribution p(y|x)is given by (3.84) Exercises 109
3.23 (??) Using the partitioned matrix inversion formula (3.60), show that the inverse of
the precision matrix (3.88) is given by the covariance matrix (3.89) 3.24 (??) By starting from (3.91) and making use of the result (3.89), verify the result
(3.92) 3.25 (??) Consider two multi-dimensional random vectors xandzhaving Gaussian dis-
tributionsp(x) =N(x|x;x)andp(z) =N(z|z;z), respectively, together
with their sum y=x+z By considering the linear-Gaussian model comprising
the product of the marginal distribution p(x) and the conditional distribution p(y|x)
and making use of the results (3.93) and (3.94), show that the marginal distribution
ofp(y)is given by
p(y) =N(y|x+z;x+z): (3.212)
3.26 (???) This exercise and the next provide practice at manipulating the quadratic
forms that arise in linear-Gaussian models, and they also serve as an independent
check of results derived in the main text Consider a joint distribution p(x;y)de-
Ô¨Åned by the marginal and conditional distributions given by (3.83) and (3.84) By
examining the quadratic form in the exponent of the joint distribution and using the
technique of ‚Äòcompleting the square‚Äô discussed in Section 3.2, Ô¨Ånd expressions for
the mean and covariance of the marginal distribution p(y)in which the variable x
has been integrated out To do this, make use of the Woodbury matrix inversion
formula (3.210) Verify that these results agree with (3.93) and (3.94) 3.27 (???) Consider the same joint distribution as in Exercise 3.26, but now use the tech-
nique of completing the square to Ô¨Ånd expressions for the mean and covariance of
the conditional distribution p(x|y )

============================================================

=== CHUNK 114 ===
Palavras: 400
Caracteres: 2593
--------------------------------------------------
Again, verify that these agree with the corre-
sponding expressions (3.95) and (3.96) 3.28 (??) To Ô¨Ånd the maximum likelihood solution for the covariance matrix of a mul-
tivariate Gaussian, we need to maximize the log likelihood function (3.102) with
respect to , noting that the covariance matrix must be symmetric and positive def-
inite Here we proceed by ignoring these constraints and doing a straightforward
maximization Using the results (A.21), (A.26), and (A.28) from Appendix A, show
that the covariance matrix that maximizes the log likelihood function (3.102) is
given by the sample covariance (3.106) We note that the Ô¨Ånal result is necessarily
symmetric and positive deÔ¨Ånite (provided the sample covariance is non-singular) 3.29 (??) Use the result (3.42) to prove (3.46) Now, using the results (3.42) and (3.46),
show that
E[xnxT
m] =T+Inm (3.213)
where xndenotes a data point sampled from a Gaussian distribution with mean 
and covariance , andInmdenotes the (n;m) element of the identity matrix Hence,
prove the result (3.108) 3.30 (?)The various trigonometric identities used in the discussion of periodic variables
in this chapter can be proven easily from the relation
exp(iA) = cos A+isinA (3.214)
110 3 STANDARD DISTRIBUTIONS
in whichiis the square root of minus one By considering the identity
exp(iA) exp(‚àíiA) = 1 (3.215)
prove the result (3.127) Similarly, using the identity
cos(A‚àíB) =/Rfracturexp{i(A‚àíB)} (3.216)
where/Rfracturdenotes the real part, prove (3.128) Finally, by using sin(A‚àíB) =
/Ifracturexp{i(A‚àíB)}, where/Ifracturdenotes the imaginary part, prove the result (3.133) 3.31 (??) For largem, the von Mises distribution (3.129) becomes sharply peaked around
the mode0 By deÔ¨Åning =m1=2(‚àí0)and taking the Taylor expansion of the
cosine function given by
cos= 1‚àí2
2+O(
4) (3.217)
show that as m‚Üí‚àû, the von Mises distribution tends to a Gaussian 3.32 (?)Using the trigonometric identity (3.133), show that solution of (3.132) for 0is
given by (3.134) 3.33 (?)By computing the Ô¨Årst and second derivatives of the von Mises distribution
(3.129), and using I0(m)>0form> 0, show that the maximum of the distribution
occurs when =0and that the minimum occurs when =0+(mod 2 ) 3.34 (?)By making use of the result (3.118) together with (3.134) and the trigonometric
identity (3.128), show that the maximum likelihood solution mMLfor the concentra-
tion of the von Mises distribution satisÔ¨Åes A(m ML) =rwhereris
the radius of the
mean of the observations viewed as unit vectors in the two-dimensional Euclidean
plane, as illustrated in Figure 3.9

============================================================

=== CHUNK 115 ===
Palavras: 380
Caracteres: 2478
--------------------------------------------------
3.35 (?)Verify that the multivariate Gaussian distribution can be cast in exponential fam-
ily form (3.138), and derive expressions for ,u(x),h(x), andg()analogous to
(3.164) to (3.167) 3.36 (?)The result (3.172) showed that the negative gradient of lng()for the exponential
family is given by the expectation of u(x) By taking the second derivatives of
(3.139), show that
‚àí‚àá‚àá lng() =E[u(x)u(x)T]‚àíE[u(x)]E[u(x)T] = cov[ u(x)]: (3.218)
3.37 (??) Consider a histogram-like density model in which the space xis divided into
Ô¨Åxed regions for which the density p(x) takes the constant value hiover theith re-
gion The volume of region iis denoted ‚àÜi Suppose we have a set of Nobservations
ofxsuch thatniof these observations fall in region i Using a Lagrange multiplier
to enforce the normalization constraint on the density, derive an expression for the
maximum likelihood estimator for the {hi} 3.38 (?)Show that the K-nearest-neighbour density model deÔ¨Ånes an improper distribu-
tion whose integral over all space is divergent 4
Single-layer
Networks:
Regression
In this chapter we discuss some of the basic ideas behind neural networks using the
framework of linear regression, which we encountered brieÔ¨Çy in the context of poly- Section 1.2
nomial curve Ô¨Åtting We will see that a linear regression model corresponds to a sim-
ple form of neural network having a single layer of learnable parameters Although
single-layer networks have very limited practical applicability, they have simple an-
alytical properties and provide an excellent framework for introducing many of the
core concepts that will lay a foundation for our discussion of deep neural networks
in later chapters 111 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_4    
112 4 SINGLE-LAYER NETWORKS: REGRESSION
4.1 Linear
Regression
The
goal of regression is to predict the value of one or more continuous target vari-
ablestgiven the value of a D-dimensional vector xofinput variables Typically we
are given a training data set comprising Nobservations{xn}, wheren= 1;:::;N ,
together with corresponding target values {tn}, and the goal is to predict the value of
tfor a new value of x To do this, we formulate a function y(x;w)whose values for
new inputs xconstitute the predictions for the corresponding values of t, and where
wrepresents a vector of parameters that can be learned from the training data

============================================================

=== CHUNK 116 ===
Palavras: 396
Caracteres: 2356
--------------------------------------------------
The simplest model for regression is one that involves a linear combination of
the input variables:
y(x;w) =w0+w1x1+:::+wDxD (4.1)
where x= (x1;:::;xD)T The term linear regression sometimes refers speciÔ¨Åcally
to this form of model The key property of this model is that it is a linear function
of the parameters w0;:::;wD It is also, however, a linear function of the input
variablesxi, and this imposes signiÔ¨Åcant limitations on the model 4.1.1 Basis functions
We can extend the class of models deÔ¨Åned by (4.1) by considering linear com-
binations of Ô¨Åxed nonlinear functions of the input variables, of the form
y(x;w) =w0+M‚àí1/summationdisplay
j=1wjj(x) (4.2)
wherej(x)are known as basis functions By denoting the maximum value of the
indexjbyM‚àí1, the total number of parameters in this model will be M The parameter w0allows for any Ô¨Åxed offset in the data and is sometimes called
abias parameter (not to be confused with bias in a statistical sense) It is often Section 4.3
convenient to deÔ¨Åne an additional dummy basis function 0(x)whose value is Ô¨Åxed
at0(x) = 1 so that (4.2) becomes
y(x;w) =M‚àí1/summationdisplay
j=0wjj(x) = wT(x) (4.3)
where w= (w 0;:::;wM‚àí1)Tand= ( 0;:::;M‚àí1)T We can represent the
model (4.3) using a neural network diagram, as shown in Figure 4.1 By using nonlinear basis functions, we allow the function y(x;w)to be a non-
linear function of the input vector x Functions of the form (4.2) are called linear
models, however, because they are linear in w It is this linearity in the parameters
that will greatly simplify the analysis of this class of models However, it also leads
to some signiÔ¨Åcant limitations Linear Regression 113
Figure 4.1 The linear regression model (4.3) can be ex-
pressed as a simple neural network diagram
involving a single layer of parameters Here
each basis function j(x)is represented by
an input node, with the solid node repre-
senting the ‚Äòbias‚Äô basis function 0, and the
functiony(x;w)is represented by an output
node Each of the parameters wjis shown
by a line connecting the corresponding basis
function to the output 1(
x)
0(
x)y(
x;w)wM‚àí
1
w1
w0
Before
the advent of deep learning it was common practice in machine learning
to use some form of Ô¨Åxed pre-processing of the input variables x, also known as fea-
ture extraction, expressed in terms of a set of basis functions {j(x)}

============================================================

=== CHUNK 117 ===
Palavras: 357
Caracteres: 2291
--------------------------------------------------
The goal was
to choose a sufÔ¨Åciently powerful set of basis functions that the resulting learning task
could be solved using a simple network model Unfortunately, it is very difÔ¨Åcult to
hand-craft suitable basis functions for anything but the simplest applications Deep
learning avoids this problem by learning the required nonlinear transformations of
the data from the data set itself We have already encountered an example of a regression problem when we dis-
cussed curve Ô¨Åtting using polynomials The polynomial function (1.1) can be ex- Chapter 1
pressed in the form (4.3) if we consider a single input variable xand if we choose
basis functions deÔ¨Åned by j(x) =xj There are many other possible choices for
the basis functions, for example
j(x) = exp/braceleftbigg
‚àí(x‚àíj)2
2
s2/bracerightbigg
(4.4)
where thejgovern the locations of the basis functions in input space, and the
parametersgoverns their spatial scale These are usually referred to as ‚ÄòGaussian‚Äô
basis functions, although it should be noted that they are not required to have a
probabilistic interpretation In particular the normalization coefÔ¨Åcient is unimportant
because these basis functions will be multiplied by learnable parameters wj Another possibility is the sigmoidal basis function of the form
j(x) =/parenleftbiggx‚àíj
s/parenrightbigg
(4.5)
where(
a)is the logistic sigmoid function deÔ¨Åned by
(a) =1
1
+ exp(‚àía): (4.6)
Equivalently, we can use the tanh function because this is related to the logistic
sigmoid by tanh(a) = 2 (2a)‚àí1, and so a general linear combination of logistic
sigmoid functions is equivalent to a general linear combination of tanh functions in Exercise 4.3
the sense that they can represent the same class of input‚Äìoutput functions These
various choices of basis function are illustrated in Figure 4.2 SINGLE-LAYER NETWORKS: REGRESSION
‚àí1 0 1‚àí1‚àí0.500.51
‚àí1 0 10   0.250.5 0.751   
‚àí1 0 100.250.50.751
Figure 4.2 Examples of basis functions, showing polynomials on the left, Gaussians of the form (4.4) in the
centre, and sigmoidal basis functions of the form (4.5) on the right Yet another possible choice of basis function is the Fourier basis, which leads to
an expansion in sinusoidal functions Each basis function represents a speciÔ¨Åc fre-
quency and has inÔ¨Ånite spatial extent

============================================================

=== CHUNK 118 ===
Palavras: 365
Caracteres: 2315
--------------------------------------------------
By contrast, basis functions that are localized
to Ô¨Ånite regions of input space necessarily comprise a spectrum of different spatial
frequencies In signal processing applications, it is often of interest to consider basis
functions that are localized in both space and frequency, leading to a class of func-
tions known as wavelets (Ogden, 1997; Mallat, 1999; Vidakovic, 1999) These are
also deÔ¨Åned to be mutually orthogonal, to simplify their application Wavelets are
most applicable when the input values live on a regular lattice, such as the successive
time points in a temporal sequence or the pixels in an image Most of the discussion in this chapter, however, is independent of the choice of
basis function set, and so we will not specify the particular form of the basis func-
tions, except for numerical illustration Furthermore, to keep the notation simple, we
will focus on the case of a single target variable t, although we will brieÔ¨Çy outline
the modiÔ¨Åcations needed to deal with multiple target variables Section 4.1.7
4.1.2 Likelihood function
We solved the problem of Ô¨Åtting a polynomial function to data by minimizing
a sum-of-squares error function, and we also showed that this error function could Section 1.2
be motivated as the maximum likelihood solution under an assumed Gaussian noise
model We now return to this discussion and consider the least-squares approach,
and its relation to maximum likelihood, in more detail As before, we assume that the target variable tis given by a deterministic func-
tiony(x;w)with additive Gaussian noise so that
t=y(x;w) + (4.7)
whereis a zero-mean Gaussian random variable with variance 2 Thus, we can
write
p(t|x; w;2) =N(t|y(x;w);2): (4.8)
4.1 Linear Regression 115
Now consider a data set of inputs X={x1;:::;xN}with corresponding target
valuest1;:::;tN We group the target variables {tn}into a column vector that we
denote by twhere the typeface is chosen to distinguish it from a single observation
of a multivariate target, which would be denoted t Making the assumption that these
data points are drawn independently from the distribution (4.8), we obtain an expres-
sion for the likelihood function, which is a function of the adjustable parameters w
and2:
p(t|X; w;2) =N/productdisplay
n=1N(tn|wT(xn);2) (4.9)
where we have used (4.3)

============================================================

=== CHUNK 119 ===
Palavras: 359
Caracteres: 2491
--------------------------------------------------
Taking the logarithm of the likelihood function and mak-
ing use of the standard form (2.49) for the univariate Gaussian, we have
lnp(t|X; w;2) =N/summationdisplay
n=1lnN(tn|wT(xn);2)
=‚àíN
2ln2‚àíN
2ln(2 )‚àí1
2ED(w) (4.10)
where the sum-of-squares error function is deÔ¨Åned by
ED(w) =1
2N/summationdisplay
n=1{tn‚àíwT(xn)}2: (4.11)
The Ô¨Årst two terms in (4.10) can be treated as constants when determining wbe-
cause they are independent of w Therefore, as we saw previously, maximizing the Section 2.3.4
likelihood function under a Gaussian noise distribution is equivalent to minimizing
the sum-of-squares error function (4.11) 4.1.3 Maximum likelihood
Having written down the likelihood function, we can use maximum likelihood
to determine wand2 Consider Ô¨Årst the maximization with respect to w The
gradient of the log likelihood function (4.10) with respect to wtakes the form
‚àáwlnp(t|X; w;2) =1
2N/summationdisplay
n=1/braceleftbig
tn‚àíwT(xn)/bracerightbig
(xn)T: (4.12)
Setting this gradient to zero gives
0 =N/summationdisplay
n=1tn(xn)T‚àíwT/parenleftBiggN/summationdisplay
n=1(xn)(xn)T/parenrightBigg
: (4.13)
Solving for wwe obtain
wML=/parenleftbig
T/parenrightbig‚àí1Tt; (4.14)
116 4 SINGLE-LAYER NETWORKS: REGRESSION
which are known as the normal equations for the least-squares problem Here is an
N√óMmatrix, called the design matrix, whose elements are given by Œ¶nj=j(xn),
so that
=Ô£´
Ô£¨Ô£¨Ô£≠0(x1)1(x1)¬∑¬∑¬∑M‚àí1(x1)
0(x2)1(x2)¬∑¬∑¬∑M‚àí1(x2) 0(xN)1(xN)¬∑¬∑¬∑M‚àí1(xN)Ô£∂
Ô£∑Ô£∑Ô£∏: (4.15)
The quantity
‚Ä†‚â°/parenleftbig
T/parenrightbig‚àí1T(4.16)
is known as the Moore‚ÄìPenrose pseudo-inverse of the matrix (Rao and Mitra,
1971; Golub and Van Loan, 1996) It can be regarded as a generalization of the no-
tion of a matrix inverse to non-square matrices Indeed, if is square and invertible,
then using the property (AB)‚àí1=B‚àí1A‚àí1we see that ‚Ä†‚â°‚àí1 At this point, we can gain some insight into the role of the bias parameter w0 If
we make the bias parameter explicit, then the error function (4.11) becomes
ED(w) =1
2N/summationdisplay
n=1{tn‚àíw0‚àíM‚àí1/summationdisplay
j=1wjj(xn)}2: (4.17)
Setting the derivative with respect to w0equal to zero and solving for w0, we obtain
w0=t‚àíM‚àí1/summationdisplay
j=1wjj (4.18)
where we have deÔ¨Åned
t=1
NN/summationdisplay
n=1tn;j=1
NN/summationdisplay
n=1j(xn): (4.19)
Thus, the bias w0compensates for the difference between the averages (over the
training set) of the target values and the weighted sum of the averages of the basis
function values

============================================================

=== CHUNK 120 ===
Palavras: 350
Caracteres: 2138
--------------------------------------------------
We can also maximize the log likelihood function (4.10) with respect to the
variance2, giving
2
ML=1
NN/summationdisplay
n=1{tn‚àíwT
ML(xn)}2; (4.20)
and so we see that the maximum likelihood value of the variance parameter is given
by the residual variance of the target values around the regression function Linear Regression 117
Figure 4.3 Geometrical interpretation of the least-
squares solution in an N-dimensional space
whose axes are the values of t1;:::;tN The
least-squares regression function is obtained
by Ô¨Ånding the orthogonal projection of the
data vector tonto the subspace spanned by
the basis functions j(x)in which each basis
function is viewed as a vector 'jof lengthN
with elements j(xn) S
t
y '1'2
4.1.4
Geometry of least squares
At this point, it is instructive to consider the geometrical interpretation of the
least-squares solution To do this, we consider an N-dimensional space whose axes
are given by the tn, so that t= (t 1;:::;tN)Tis a vector in this space Each basis
functionj(xn), evaluated at the Ndata points, can also be represented as a vector in
the same space, denoted by 'j, as illustrated in Figure 4.3 Note that 'jcorresponds
to thejth column of , whereas(xn)corresponds to the transpose of the nth row of
 If the number Mof basis functions is smaller than the number Nof data points,
then theMvectorsj(xn)will span a linear subspace Sof dimensionality M We
deÔ¨Åne yto be anN-dimensional vector whose nth element is given by y(xn;w),
wheren= 1;:::;N Because yis an arbitrary linear combination of the vectors
'j, it can live anywhere in the M-dimensional subspace The sum-of-squares error
(4.11) is then equal (up to a factor of 1=2) to the squared Euclidean distance between
yandt Thus, the least-squares solution for wcorresponds to that choice of ythat
lies in subspaceSand is closest to t Intuitively, from Figure 4.3, we anticipate that
this solution corresponds to the orthogonal projection of tonto the subspaceS This
is indeed the case, as can easily be veriÔ¨Åed by noting that the solution for yis given
byw MLand then conÔ¨Årming that this takes the form of an orthogonal projection

============================================================

=== CHUNK 121 ===
Palavras: 375
Caracteres: 2397
--------------------------------------------------
Exercise 4.4
In practice, a direct solution of the normal equations can lead to numerical difÔ¨Å-
culties when Tis close to singular In particular, when two or more of the basis
vectors'jare co-linear, or nearly so, the resulting parameter values can have large
magnitudes Such near degeneracies will not be uncommon when dealing with real
data sets The resulting numerical difÔ¨Åculties can be addressed using the technique
ofsingular value decomposition, or SVD (Deisenroth, Faisal, and Ong, 2020) Note
that the addition of a regularization term ensures that the matrix is non-singular, even
in the presence of degeneracies 4.1.5 Sequential learning
The maximum likelihood solution (4.14) involves processing the entire training
set in one go and is known as a batch method This can become computationally
costly for large data sets If the data set is sufÔ¨Åciently large, it may be worthwhile
to use sequential algorithms, also known as online algorithms, in which the data
points are considered one at a time and the model parameters updated after each
such presentation Sequential learning is also appropriate for real-time applications
in which the data observations arrive in a continuous stream and predictions must be
118 4 SINGLE-LAYER NETWORKS: REGRESSION
made before all the data points are seen We can obtain a sequential learning algorithm by applying the technique of
stochastic gradient descent , also known as sequential gradient descent, as follows If Chapter 7
the error function comprises a sum over data points E=/summationtext
nEn, then after presenta-
tion of data point n, the stochastic gradient descent algorithm updates the parameter
vector wusing
w(+1)=w()‚àí‚àáEn (4.21)
wheredenotes the iteration number, and is a suitably chosen learning rate pa-
rameter The value of wis initialized to some starting vector w(0) For the sum-of-
squares error function (4.11), this gives
w(+1)=w()+(tn‚àíw()Tn)n (4.22)
wheren=(xn) This is known as the least-mean-squares or the LMS algorithm 4.1.6 Regularized least squares
We have previously introduced the idea of adding a regularization term to an Section 1.2
error function to control over-Ô¨Åtting, so that the total error function to be minimized
takes the form
ED(w) +EW(w) (4.23)
whereis the regularization coefÔ¨Åcient that controls the relative importance of the
data-dependent error ED(w)and the regularization term EW(w)

============================================================

=== CHUNK 122 ===
Palavras: 383
Caracteres: 2344
--------------------------------------------------
One of the sim-
plest forms of regularizer is given by the sum of the squares of the weight vector
elements:
EW(w) =1
2/summationdisplay
jw2
j=1
2wTw: (4.24)
If we also consider the sum-of-squares error function given by
ED(w) =1
2N/summationdisplay
n=1{tn‚àíwT(xn)}2; (4.25)
then the total error function becomes
1
2N/summationdisplay
n=1{tn‚àíwT(xn)}2+
2wTw: (4.26)
In statistics, this regularizer provides an example of a parameter shrinkage method
because it shrinks parameter values towards zero It has the advantage that the error
function remains a quadratic function of w, and so its exact minimizer can be found
in closed form SpeciÔ¨Åcally, setting the gradient of (4.26) with respect to wto zero
and solving for was before, we obtain Exercise 4.6
w=/parenleftbig
I+T/parenrightbig‚àí1Tt: (4.27)
This represents a simple extension of the least-squares solution (4.14) Linear Regression 119
Figure 4.4 Representation of a linear regres-
sion model as a neural network hav-
ing a single layer of connections Each basis function is represented
by a node, with the solid node rep-
resenting the ‚Äòbias‚Äô basis function
0 Likewise each output y1;:::;yK
is represented by a node The
links between the nodes represent
the corresponding weight and bias
parameters y1(
x;w)
4.1.7
Multiple outputs
So far, we have considered situations with a single target variable t In some
applications, we may wish to predict K > 1target variables, which we denote col-
lectively by the target vector t= (t 1;:::;tK)T This could be done by introducing
a different set of basis functions for each component of t, leading to multiple, inde-
pendent regression problems However, a more common approach is to use the same
set of basis functions to model all of the components the target vector so that
y(x;w) =WT(x) (4.28)
where yis aK-dimensional column vector, Wis anM√óKmatrix of parameters,
and(x) is anM-dimensional column vector with elements j(x)with0(x) = 1
as before Again, this can be represented as a neural network having a single layer
of parameters, as shown in Figure 4.4 Suppose we take the conditional distribution of the target vector to be an isotropic
Gaussian of the form
p(t|x; W;2) =N(t|WT(x);2I): (4.29)
If we have a set of observations t1;:::;tN, we can combine these into a matrix T
of sizeN√óKsuch that the nth row is given by tT
n

============================================================

=== CHUNK 123 ===
Palavras: 359
Caracteres: 2458
--------------------------------------------------
Similarly, we can combine the
input vectors x1;:::;xNinto a matrix X The log likelihood function is then given
by
lnp(T|X; W;2) =N/summationdisplay
n=1lnN(tn|WT(xn);2I)
=‚àíNK
2ln/parenleftbig
2
2/parenrightbig
‚àí1
2
2N/summationdisplay
n=1/vextenddouble/vextenddoubletn‚àíWT(xn)/vextenddouble/vextenddouble2:(4.30)
As before, we can maximize this function with respect to W, giving
WML=/parenleftbig
T/parenrightbig‚àí1TT (4.31)
where we have combined the input feature vectors (x 1);:::;(xN)into a matrix
 If we examine this result for each target variable tk, we have
wk=/parenleftbig
T/parenrightbig‚àí1Ttk=‚Ä†tk (4.32)
120 4 SINGLE-LAYER NETWORKS: REGRESSION
where tkis anN-dimensional column vector with components tnkforn= 1;:::N Thus, the solution to the regression problem decouples between the different target
variables, and we need compute only a single pseudo-inverse matrix ‚Ä†, which is
shared by all the vectors wk The extension to general Gaussian noise distributions having arbitrary covari-
ance matrices is straightforward Again, this leads to a decoupling into Kinde- Exercise 4.7
pendent regression problems This result is unsurprising because the parameters W
deÔ¨Åne only the mean of the Gaussian noise distribution, and we know that the max-
imum likelihood solution for the mean of a multivariate Gaussian is independent of
the covariance From now on, we will therefore consider a single target variable t Section 3.2.7
for simplicity Decision theory
We have formulated the regression task as one of modelling a conditional proba-
bility distribution p(t|x), and we have chosen a speciÔ¨Åc form for the conditional
probability, namely a Gaussian (4.8) with an x-dependent mean y(x;w)governed
by parameters wand with variance given by the parameter 2 Both wand2can be
learned from data using maximum likelihood The result is a predictive distribution
given by
p(t|x; wML;2
ML) =N(t|y(x;wML);2
ML): (4.33)
The predictive distribution expresses our uncertainty over the value of tfor some
new input x However, for many practical applications we need to predict a speciÔ¨Åc
value fortrather than returning an entire distribution, particularly where we must
take a speciÔ¨Åc action For example, if our goal is to determine the optimal level of
radiation to use for treating a tumour and our model predicts a probability distri-
bution over radiation dose, then we must use that distribution to decide the speciÔ¨Åc
dose to be administered

============================================================

=== CHUNK 124 ===
Palavras: 359
Caracteres: 2208
--------------------------------------------------
Our task therefore breaks down into two stages In the Ô¨Årst
stage, called the inference stage, we use the training data to determine a predictive
distribution p(t|x) In the second stage, known as the decision stage, we use this
predictive distribution to determine a speciÔ¨Åc value f(x), which will be dependent
on the input vector x, that is optimal according to some criterion We can do this
by minimizing a loss function that depends on both the predictive distribution p(t|x)
and onf Intuitively we might choose the mean of the conditional distribution, so that
we would use f(x) =y(x;wML) In some cases this intuition will be correct, but
in other situations it can give very poor results It is therefore useful to formalize
this so that we can understand when it applies and under what assumptions, and the
framework for doing this is called decision theory Suppose that we choose a value f(x)for our prediction when the true value is
t In doing so, we incur some form of penalty or cost This is determined by a
loss, which we denote L(t;f (x)) Of course, we do not know the true value of t, so
instead of minimizing Litself, we minimize the average, or expected, loss which is
4.2 Decision theory 121
Figure 4.5 The regression function f?(x),
which minimizes the expected
squared loss, is given by the
mean of the conditional distribu-
tionp(t|x) t
xp(t|x0,w,œÉ2)f‚ãÜ(x)
gi
ven by
E[L] =/integraldisplay/integraldisplay
L(t;f (x))p(x;t ) dxdt (4.34)
where we are averaging over the distribution of both input and target variables,
weighted by their joint distribution p(x;t) A common choice of loss function in
regression problems is the squared loss given by L(t;f (x)) ={f(x)‚àít}2 In this
case, the expected loss can be written
E[L] =/integraldisplay/integraldisplay
{f(x)‚àít}2p(x;t ) dxdt: (4.35)
It is important not to confuse the squared-loss function with the sum-of-squares
error function introduced earlier The error function is used to set the parameters
during training in order to determine the conditional probability distribution p(t|x),
whereas the loss function governs how the conditional distribution is used to arrive
at a predictive function f(x)specifying a prediction for each value of x

============================================================

=== CHUNK 125 ===
Palavras: 361
Caracteres: 2323
--------------------------------------------------
Our goal is to choose f(x)so as to minimize E[L] If we assume a completely
Ô¨Çexible function f(x), we can do this formally using the calculus of variations to Appendix B
give
E[L]

f(x)= 2/integraldisplay
{f(x)‚àít}p(x;t) dt= 0: (4.36)
Solving forf(x)and using the sum and product rules of probability, we obtain
f?(x) =1
p
(x)/integraldisplay
tp(x;t) dt=/integraldisplay
tp(t|x) dt=Et[t|x]; (4.37)
which is the conditional average of tconditioned on xand is known as the regression
function This result is illustrated in Figure 4.5 It can readily be extended to multiple
target variables represented by the vector t, in which case the optimal solution is the
conditional average f?(x) = Et[t|x] For a Gaussian conditional distribution of the Exercise 4.8
122 4 SINGLE-LAYER NETWORKS: REGRESSION
form (4.8), the conditional mean will be simply
E[t|x] =/integraldisplay
tp(t|x) dt=y(x;w): (4.38)
The use of calculus of variations to derive (4.37) implies that we are optimiz-
ing over all possible functions f(x) Although any parametric model that we can
implement in practice is limited in the range of functions that it can represent, the
framework of deep neural networks, discussed extensively in later chapters, provides
a highly Ô¨Çexible class of functions that, for many practical purposes, can approxi-
mate any desired function to high accuracy We can derive this result in a slightly different way, which will also shed light
on the nature of the regression problem Armed with the knowledge that the optimal
solution is the conditional expectation, we can expand the square term as follows
{f(x)‚àít}2={f(x)‚àíE[t|x] + E[t|x]‚àít}2
={f(x)‚àíE[t|x]}2+ 2{f (x)‚àíE[t|x]}{E[t|x]‚àít}+{E[t|x]‚àít}2
where, to keep the notation uncluttered, we use E[t|x] to denote Et[t|x] Substituting
into the loss function (4.35) and performing the integral over t, we see that the cross-
term vanishes and we obtain an expression for the loss function in the form
E[L] =/integraldisplay
{f(x)‚àíE[t|x]}2p(x) d x+/integraldisplay
var [t|x]p(x) d x: (4.39)
The function f(x)we seek to determine appears only in the Ô¨Årst term, which will be
minimized when f(x)is equal to E[t|x], in which case this term will vanish This is
simply the result that we derived previously, and shows that the optimal least-squares
predictor is given by the conditional mean

============================================================

=== CHUNK 126 ===
Palavras: 352
Caracteres: 2147
--------------------------------------------------
The second term is the variance of the
distribution of t, averaged over x, and represents the intrinsic variability of the target
data and can be regarded as noise Because it is independent of f(x), it represents
the irreducible minimum value of the loss function The squared loss is not the only possible choice of loss function for regression Here we consider brieÔ¨Çy one simple generalization of the squared loss, called the
Minkowski loss, whose expectation is given by
E[Lq] =/integraldisplay/integraldisplay
|f(x)‚àít|qp(x;t) d xdt; (4.40)
which reduces to the expected squared loss for q= 2 The function |f‚àít|qis
plotted against f‚àítfor various values of qinFigure 4.6 The minimum of E[Lq]is
given by the conditional mean for q= 2, the conditional median for q= 1, and the
conditional mode for q‚Üí0 Exercise 4.12
Note that the Gaussian noise assumption implies that the conditional distribution
oftgiven xis unimodal, which may be inappropriate for some applications In
this case a squared loss can lead to very poor results and we need to develop more
sophisticated approaches For example, we can extend this model by using mixtures
4.3 The Bias‚ÄìVariance Trade-off 123
‚àí2‚àí1 0 1 2
f‚àít012|f‚àít|0.3q= 0.3
‚àí2‚àí1 0 1 2
f‚àít012|f‚àít|1q= 1
‚àí2‚àí1 0 1 2
f‚àít012|f‚àít|2q= 2
‚àí2‚àí1 0 1 2
f‚àít012|f‚àít|10q= 10
Figure 4.6 Plots of the quantity Lq=|f‚àít|qfor various values of q of Gaussians to give multimodal conditional distributions, which often arise in the Section 6.5
solution of inverse problems Our focus in this section has been on decision theory
for regression problems, and in the next chapter we shall develop analogous concepts
for classiÔ¨Åcation tasks The Bias‚ÄìVariance Trade-off
So far in our discussion of linear models for regression, we have assumed that the
form and number of basis functions are both given We have also seen that the use Section 1.2
of maximum likelihood can lead to severe over-Ô¨Åtting if complex models are trained
using data sets of limited size However, limiting the number of basis functions
to avoid over-Ô¨Åtting has the side effect of limiting the Ô¨Çexibility of the model to
capture interesting and important trends in the data

============================================================

=== CHUNK 127 ===
Palavras: 368
Caracteres: 2338
--------------------------------------------------
Although a regularization term
can control over-Ô¨Åtting for models with many parameters, this raises the question of
how to determine a suitable value for the regularization coefÔ¨Åcient  SINGLE-LAYER NETWORKS: REGRESSION
solution that minimizes the regularized error function with respect to both the weight
vector wand the regularization coefÔ¨Åcient is clearly not the right approach, since
this leads to the unregularized solution with = 0 It is instructive to consider a frequentist viewpoint of the model complexity is-
sue, known as the bias‚Äìvariance trade-off Although we will introduce this concept
in the context of linear basis function models, where it is easy to illustrate the ideas
using simple examples, the discussion has very general applicability Note, however,
that over-Ô¨Åtting is really an unfortunate property of maximum likelihood and does
not arise when we marginalize over parameters in a Bayesian setting (Bishop, 2006) When we discussed decision theory for regression problems, we considered var- Section 4.2
ious loss functions, each of which leads to a corresponding optimal prediction once
we are given the conditional distribution p(t|x) A popular choice is the squared-loss
function, for which the optimal prediction is given by the conditional expectation,
which we denote by h(x) and is given by
h(x) = E[t|x] =/integraldisplay
tp(t|x) dt: (4.41)
We have also seen that the expected squared loss can be written in the form
E[L] =/integraldisplay
{f(x)‚àíh(x)}2p(x) d x+/integraldisplay/integraldisplay
{h(x)‚àít}2p(x;t) d xdt: (4.42)
Recall that the second term, which is independent of f(x), arises from the intrin-
sic noise on the data and represents the minimum achievable value of the expected
loss The Ô¨Årst term depends on our choice for the function f(x), and we will seek a
solution for f(x)that makes this term a minimum Because it is non-negative, the
smallest value that we can hope to achieve for this term is zero If we had an unlim-
ited supply of data (and unlimited computational resources), we could in principle
Ô¨Ånd the regression function h(x) to any desired degree of accuracy, and this would
represent the optimal choice for f(x) However, in practice we have a data set D
containing only a Ô¨Ånite number Nof data points, and consequently, we cannot know
the regression function h(x) exactly

============================================================

=== CHUNK 128 ===
Palavras: 355
Caracteres: 2451
--------------------------------------------------
If we were to model h(x) using a function governed by a parameter vector w,
then from a Bayesian perspective, the uncertainty in our model would be expressed
through a posterior distribution over w A frequentist treatment, however, involves
making a point estimate of wbased on the data set Dand tries instead to interpret the
uncertainty of this estimate through the following thought experiment Suppose we
had a large number of data sets each of size Nand each drawn independently from
the distribution p(t;x) For any given data set D, we can run our learning algorithm
and obtain a prediction function f(x;D) Different data sets from the ensemble will
give different functions and consequently different values of the squared loss The
performance of a particular learning algorithm is then assessed by taking the average
over this ensemble of data sets Consider the integrand of the Ô¨Årst term in (4.42), which for a particular data set
Dtakes the form
{f(x;D)‚àíh(x)}2: (4.43)
4.3 The Bias‚ÄìVariance Trade-off 125
Because this quantity will be dependent on the particular data set D, we take its aver-
age over the ensemble of data sets If we add and subtract the quantity ED[f(x;D)]
inside the braces, and then expand, we obtain
{f(x;D)‚àíED[f(x;D)] +ED[f(x;D)]‚àíh(x)}2
={f(x;D)‚àíED[f(x;D)]}2+{ED[f(x;D)]‚àíh(x)}2
+ 2{f (x;D)‚àíED[f(x;D)]}{ED[f(x;D)]‚àíh(x)}: (4.44)
We now take the expectation of this expression with respect to Dand note that the
Ô¨Ånal term will vanish, giving
ED/bracketleftbig
{f(x;D)‚àíh(x)}2/bracketrightbig
={ED[f(x;D)]‚àíh(x)}2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(bias)2+ED/bracketleftbig
{f(x;D)‚àíED[f(x;D)]}2/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
variance: (4.45)
We see that the expected squared difference between f(x;D)and the regression
functionh(x) can be expressed as the sum of two terms The Ô¨Årst term, called the
squared bias, represents the extent to which the average prediction over all data sets
differs from the desired regression function The second term, called the variance,
measures the extent to which the solutions for individual data sets vary around their
average, and hence, this measures the extent to which the function f(x;D)is sen-
sitive to the particular choice of data set We will provide some intuition to support
these deÔ¨Ånitions shortly when we consider a simple example So far, we have considered a single input value x

============================================================

=== CHUNK 129 ===
Palavras: 377
Caracteres: 2360
--------------------------------------------------
If we substitute this expansion
back into (4.42), we obtain the following decomposition of the expected squared
loss:
expected loss = (bias)2+variance +noise (4.46)
where
(bias)2=/integraldisplay
{ED[f(x;D)]‚àíh(x)}2p(x) d x (4.47)
variance =/integraldisplay
ED/bracketleftbig
{f(x;D)‚àíED[f(x;D)]}2/bracketrightbig
p(x) d x (4.48)
noise =/integraldisplay/integraldisplay
{h(x)‚àít}2p(x;t ) dxdt (4.49)
and the bias and variance terms now refer to integrated quantities Our goal is to minimize the expected loss, which we have decomposed into the
sum of a (squared) bias, a variance, and a constant noise term As we will see, there is
a trade-off between bias and variance, with very Ô¨Çexible models having low bias and
high variance, and relatively rigid models having high bias and low variance The
model with the optimal predictive capability is the one that leads to the best balance
between bias and variance This is illustrated by considering the sinusoidal data set
introduced earlier Here we independently generate 100 data sets, each containing Section 1.2
126 4 SINGLE-LAYER NETWORKS: REGRESSION
N= 25 data points, from the sinusoidal curve h(x) = sin(2x) The data sets are
indexed byl= 1;:::;L , whereL= 100 For each data set D(l), we Ô¨Åt a model
withM= 24 Gaussian basis functions along with a constant ‚Äòbias‚Äô basis function to
give a total of 25parameters By minimizing the regularized error function (4.26),
we obtain a prediction function f(l)(x), as shown in Figure 4.7 The top row corresponds to a large value of the regularization coefÔ¨Åcient that
gives low variance (because the red curves in the left plot look similar) but high
bias (because the two curves in the right plot are very different) Conversely on
the bottom row, for which is small, there is large variance (shown by the high
variability between the red curves in the left plot) but low bias (shown by the good
Ô¨Åt between the average model Ô¨Åt and the original sinusoidal function) Note that
the result of averaging many solutions for the complex model with M= 25 is a
very good Ô¨Åt to the regression function, which suggests that averaging may be a
beneÔ¨Åcial procedure Indeed, a weighted averaging of multiple solutions lies at the
heart of a Bayesian approach, although the averaging is with respect to the posterior
distribution of parameters, not with respect to multiple data sets

============================================================

=== CHUNK 130 ===
Palavras: 351
Caracteres: 2187
--------------------------------------------------
We can also examine the bias‚Äìvariance trade-off quantitatively for this example The average prediction is estimated from
f(
x) =1
LL/summationdisplay
l=1f(
l)(x); (4.50)
and the integrated squared bias and integrated variance are then given by
(bias)2=1
NN/summationdisplay
n
=1/braceleftbig
f(
xn)‚àíh(xn)/bracerightbig2(4.51)
variance =1
NN/summationdisplay
n
=11
LL/summationdisplay
l=1/braceleftbig
f(
l)(xn)‚àíf(
xn)/bracerightbig2(4.52)
where the integral over x, weighted by the distribution p(x), is approximated by a
Ô¨Ånite sum over data points drawn from that distribution These quantities, along with
their sum, are plotted as a function of lninFigure 4.8 We see that small values
ofallow the model to become Ô¨Ånely tuned to the noise on each individual data set
leading to large variance Conversely, a large value of pulls the weight parameters
towards zero leading to large bias Note that the bias‚Äìvariance decomposition is of limited practical value because
it is based on averages with respect to ensembles of data sets, whereas in practice
we have only the single observed data set If we had a large number of independent
training sets of a given size, we would be better off combining them into a single
larger training set, which of course would reduce the level of over-Ô¨Åtting for a given
model complexity Nevertheless, the bias‚Äìvariance decomposition often provides
useful insights into the model complexity issue, and although we have introduced it
in this chapter from the perspective of regression problems, the underlying intuition
has broad applicability The Bias‚ÄìVariance Trade-off 127
0 1x‚àí11
tlnŒª= 3
0 1x‚àí11
t
0 1x‚àí11
tlnŒª= 1
0 1x‚àí11
t
0 1x‚àí11
tlnŒª=‚àí3
0 1x‚àí11
t
Figure 4.7 Illustration of the dependence of bias and variance on model complexity governed by a regulariza-
tion parameter , using the sinusoidal data from Chapter 1 There are L= 100 data sets, each having N= 25
data points, and there are 24Gaussian basis functions in the model so that the total number of parameters is
M= 25 including the bias parameter The left column shows the result of Ô¨Åtting the model to the data sets for
various values of ln(for clarity, only 20 of the 100 Ô¨Åts are shown)

============================================================

=== CHUNK 131 ===
Palavras: 367
Caracteres: 2453
--------------------------------------------------
The right column shows the corresponding
average of the 100 Ô¨Åts (red) along with the sinusoidal function from which the data sets were generated (green) SINGLE-LAYER NETWORKS: REGRESSION
Figure 4.8 Plot of squared bias and vari-
ance, together with their sum, correspond-
ing to the results shown in Figure 4.7 Also
shown is the average test set error for a
test data set size of 1,000 points The min-
imum value of (bias)2+ variance occurs
around ln= 0:43, which is close to the
value that gives the minimum error on the
test data ‚àí3 0 3
lnŒª00.25
(bias)2
variance
(bias)2+ variance
test error
Ex
ercises
4.1 (?)Consider the sum-of-squares error function given by (1.2) in which the function
y(x;w)is given by the polynomial (1.1) Show that the coefÔ¨Åcients w={wi}that
minimize this error function are given by the solution to the following set of linear
equations:
M/summationdisplay
j=0Aijwj=Ti (4.53)
where
Aij=N/summationdisplay
n=1(xn)i+j; Ti=N/summationdisplay
n=1(xn)itn: (4.54)
Here a sufÔ¨Åx iorjdenotes the index of a component, whereas (x)idenotesxraised
to the power of i 4.2 (?)Write down the set of coupled linear equations, analogous to (4.53), satisÔ¨Åed by
the coefÔ¨Åcients withat minimize the regularized sum-of-squares error function given
by (1.4) 4.3 (?)Show that the tanh function deÔ¨Åned by
tanh(a) =ea‚àíe‚àía
ea+e‚àí
a(4.55)
and the logistic sigmoid function deÔ¨Åned by (4.6) are related by
tanh(a) = 2 (2a)‚àí1: (4.56)
Hence, show that a general linear combination of logistic sigmoid functions of the
form
y(x;w) =w0+M/summationdisplay
j=1wj/parenleftbiggx‚àíj
s/parenrightbigg
(4.57)
Exer
cises 129
is equivalent to a linear combination of tanh functions of the form
y(x;u) =u0+M/summationdisplay
j=1ujtanh/parenleftbiggx‚àíj
2
s/parenrightbigg
(4.58)
and Ô¨Ånd expressions to relate the new parameters {u1;:::;uM}to the original pa-
rameters{w1;:::;wM} 4.4 (???) Show that the matrix
(T)‚àí1T(4.59)
takes any vector vand projects it onto the space spanned by the columns of  Use
this result to show that the least-squares solution (4.14) corresponds to an orthogonal
projection of the vector tonto the manifoldS, as shown in Figure 4.3 4.5 (?)Consider a data set in which each data point tnis associated with a weighting
factorrn>0, so that the sum-of-squares error function becomes
ED(w) =1
2N/summationdisplay
n
=1rn/braceleftbig
tn‚àíwT(xn)/bracerightbig2: (4.60)
Find an expression for the solution w?that minimizes this error function

============================================================

=== CHUNK 132 ===
Palavras: 354
Caracteres: 2434
--------------------------------------------------
Give two
alternative interpretations of the weighted sum-of-squares error function in terms of
(i) data-dependent noise variance and (ii) replicated data points 4.6 (?)By setting the gradient of (4.26) with respect to wto zero, show that the exact
minimum of the regularized sum-of-squares error function for linear regression is
given by (4.27) 4.7 (??) Consider a linear basis function regression model for a multivariate target vari-
ablethaving a Gaussian distribution of the form
p(t|W;) =N(t|y(x;W);) (4.61)
where
y(x;W) =WT(x) (4.62)
together with a training data set comprising input basis vectors (xn)and corre-
sponding target vectors tn, withn= 1;:::;N Show that the maximum likelihood
solution WMLfor the parameter matrix Whas the property that each column is
given by an expression of the form (4.14), which was the solution for an isotropic
noise distribution Note that this is independent of the covariance matrix  Show
that the maximum likelihood solution for is given by
=1
NN/summationdisplay
n
=1/parenleftbig
tn‚àíWT
ML(xn)/parenrightbig/parenleftbig
tn‚àíWT
ML(xn)/parenrightbigT: (4.63)
130 4 SINGLE-LAYER NETWORKS: REGRESSION
4.8 (?)Consider the generalization of the squared-loss function (4.35) for a single target
variabletto multiple target variables described by the vector tgiven by
E[L(t; f(x))] =/integraldisplay/integraldisplay
/bardblf(x)‚àít/bardbl2p(x;t) dxdt: (4.64)
Using the calculus of variations, show that the function f(x)for which this expected
loss is minimized is given by
f(x) = Et[t|x]: (4.65)
4.9 (?)By expansion of the square in (4.64), derive a result analogous to (4.39) and,
hence, show that the function f(x)that minimizes the expected squared loss for a
vector tof target variables is again given by the conditional expectation of tin the
form (4.65) 4.10 (??) Rederive the result (4.65) by Ô¨Årst expanding (4.64) analogous to (4.39) 4.11 (??) The following distribution
p(x|2;q) =q
2(22)1=qŒì(1=q )exp/parenleftbigg
‚àí|x|q
22/parenrightbigg
(4.66)
is a generalization of the univariate Gaussian distribution Here Œì(x) is the gamma
function deÔ¨Åned by
Œì(x) =/integraldisplay‚àû
‚àí‚àûux‚àí1e‚àíudu: (4.67)
Show that this distribution is normalized so that/integraldisplay‚àû
‚àí‚àûp(x|2;q) dx= 1 (4.68)
and that it reduces to the Gaussian when q= 2 Consider a regression model in
which the target variable is given by t=y(x;w)+andis a random noise variable
drawn from the distribution (4.66)

============================================================

=== CHUNK 133 ===
Palavras: 364
Caracteres: 2335
--------------------------------------------------
Show that the log likelihood function over wand
2, for an observed data set of input vectors X={x1;:::;xN}and corresponding
target variables t= (t1;:::;tN)T, is given by
lnp(t|X; w;2) =‚àí1
22N/summationdisplay
n=1|y(xn;w)‚àítn|q‚àíN
qln(22) + const (4.69)
where ‚Äòconst‚Äô denotes terms independent of both wand2 Note that, as a function
ofw, this is theLqerror function considered in Section 4.2 4.12 (??) Consider the expected loss for regression problems under the Lqloss function
given by (4.40) Write down the condition that y(x)must satisfy to minimize E[Lq] Show that, for q= 1, this solution represents the conditional median, i.e., the func-
tiony(x)such that the probability mass for t < y (x)is the same as for t>y(x) Also show that the minimum expected Lqloss forq‚Üí0is given by the conditional
mode, i.e., by the function y(x)being equal to the value of tthat maximizes p(t|x)
for each x 5
Single-layer
Networks:
ClassiÔ¨Åcation
In the previous chapter, we explored a class of regression models in which the out-
put variables were linear functions of the model parameters and which can therefore
be expressed as simple neural networks having a single layer of weight and bias
parameters We turn now to a discussion of classiÔ¨Åcation problems, and in this chap-
ter, we will focus on an analogous class of models that again can be expressed as
single-layer neural networks These will allow us to introduce many of the key con-
cepts of classiÔ¨Åcation before dealing with more general deep neural networks in later
chapters The goal in classiÔ¨Åcation is to take an input vector x‚ààRDand assign it to one
ofKdiscrete classesCkwherek= 1;:::;K In the most common scenario, the
classes are taken to be disjoint, so that each input is assigned to one and only one
class The input space is thereby divided into decision regions whose boundaries are
called decision boundaries ordecision surfaces In this chapter, we consider linear
131 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_5    
132 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
models for classiÔ¨Åcation, by which we mean that the decision surfaces are linear
functions of the input vector xand, hence, are deÔ¨Åned by (D‚àí1)-dimensional
hyperplanes within the D-dimensional input space

============================================================

=== CHUNK 134 ===
Palavras: 357
Caracteres: 2425
--------------------------------------------------
Data sets whose classes can
be separated exactly by linear decision surfaces are said to be linearly separable Linear classiÔ¨Åcation models can be applied to data sets that are not linearly separable,
although not all inputs will be correctly classiÔ¨Åed We can broadly identify three distinct approaches to solving classiÔ¨Åcation prob-
lems The simplest involves constructing a discriminant function that directly assigns
each vector xto a speciÔ¨Åc class A more powerful approach, however, models the
conditional probability distributions p(Ck|x)in an inference stage and subsequently
uses these distributions to make optimal decisions Separating inference and deci-
sion brings numerous beneÔ¨Åts There are two different approaches to determining Section 5.2.4
the conditional probabilities p(Ck|x) One technique is to model them directly, for
example by representing them as parametric models and then optimizing the param-
eters using a training set This will be called a discriminative probabilistic model Alternatively, we can model the class-conditional densities p(x|Ck), together with
the prior probabilities p(Ck)for the classes, and then compute the required posterior
probabilities using Bayes‚Äô theorem:
p(Ck|x) =p(x|Ck)p(Ck)
p(x): (5.1)
This will be called a generative probabilistic model because it offers the opportunity
to generate samples from each of the class-conditional densities p(x|Ck) In this
chapter, we will discuss examples of all three approaches: discriminant functions,
generative probabilistic models, and discriminative probabilistic models Discriminant Functions
A discriminant is a function that takes an input vector xand assigns it to one of K
classes, denotedCk In this chapter, we will restrict attention to linear discriminants,
namely those for which the decision surfaces are hyperplanes To simplify the dis-
cussion, we consider Ô¨Årst two classes and then investigate the extension to K > 2
classes 5.1.1 Two classes
The simplest representation of a linear discriminant function is obtained by tak-
ing a linear function of the input vector so that
y(x) = wTx+w0 (5.2)
where wis called a weight vector, and w0is abias (not to be confused with bias in
the statistical sense) An input vector xis assigned to class C1ify(x)>0and to
classC2otherwise The corresponding decision boundary is therefore deÔ¨Åned by the
relationy(x) = 0 , which corresponds to a (D‚àí1)-dimensional hyperplane within
5.1

============================================================

=== CHUNK 135 ===
Palavras: 352
Caracteres: 2204
--------------------------------------------------
Discriminant Functions 133
Figure 5.1 Illustration of the geometry of
a linear discriminant function in two dimen-
sions The decision surface, shown in red,
is perpendicular to w, and its displacement
from the origin is controlled by the bias pa-
rameterw0 Also, the signed orthogonal
distance of a general point xfrom the deci-
sion surface is given by y(x)=/bardblw/bardbl x2
x1wx
y(x)
/bardblw/bardbl
x‚ä•
‚àíw0
/bardblw/bardbly=0
y
<0y > 0
R2R1
theD-dimensional
input space Consider two points xAandxBboth of which lie on
the decision surface Because y(xA) =y(xB) = 0 , we have wT(xA‚àíxB) = 0 and
hence the vector wis orthogonal to every vector lying within the decision surface,
and so wdetermines the orientation of the decision surface Similarly, if xis a point
on the decision surface, then y(x) = 0, and so the normal distance from the origin
to the decision surface is given by
wTx
/bardbl
w/bardbl=‚àíw0
/bardbl
w/bardbl: (5.3)
We therefore see that the bias parameter w0determines the location of the decision
surface These properties are illustrated for the case of D= 2inFigure 5.1 Furthermore, note that the value of y(x)gives a signed measure of the perpen-
dicular distance rof the point xfrom the decision surface To see this, consider an
arbitrary point xand let x‚ä•be its orthogonal projection onto the decision surface,
so that
x=x‚ä•+rw
/bardbl
w/bardbl: (5.4)
Multiplying both sides of this result by wTand addingw0, and making use of y(x) =
wTx+w0andy(x‚ä•) =wTx‚ä•+w0= 0, we have
r=y(x)
/bardbl
w/bardbl: (5.5)
This result is illustrated in Figure 5.1 As with linear regression models, it is sometimes convenient to use a more com- Section 4.1.1
pact notation in which we introduce an additional dummy ‚Äòinput‚Äô value x0= 1and
then deÔ¨Åne/tildewidew= (w0;w)and/tildewidex= (x0;x)so that
y(x) =/tildewidewT/tildewidex: (5.6)
134 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
R1
R2
R3 C1
notC1C2
notC2
R1
R2R3
?C1
C2C1C3
C2C3
Figure
5.2 Attempting to construct a K-class discriminant from a set of two-class discriminant functions leads
to ambiguous regions, as shown in green On the left is an example with two discriminant functions designed to
distinguish points in class Ckfrom points not in class Ck

============================================================

=== CHUNK 136 ===
Palavras: 361
Caracteres: 2314
--------------------------------------------------
On the right is an example involving three discriminant
functions each of which is used to separate a pair of classes CkandCj In this case, the decision surfaces are D-dimensional hyperplanes passing through
the origin of the (D+ 1)-dimensional expanded input space 5.1.2 Multiple classes
Now consider the extension of linear discriminant functions to K > 2classes We might be tempted to build a K-class discriminant by combining a number of
two-class discriminant functions However, this leads to some serious difÔ¨Åculties
(Duda and Hart, 1973), as we now show Consider a model with K‚àí1classiÔ¨Åers, each of which solves a two-class prob-
lem of separating points in a particular class Ckfrom points not in that class This
is known as a one-versus-the-rest classiÔ¨Åer The left-hand example in Figure 5.2
shows an example involving three classes where this approach leads to regions of
input space that are ambiguously classiÔ¨Åed An alternative is to introduce K(K‚àí1)=2 binary discriminant functions, one
for every possible pair of classes This is known as a one-versus-one classiÔ¨Åer Each
point is then classiÔ¨Åed according to a majority vote amongst the discriminant func-
tions However, this too runs into the problem of ambiguous regions, as illustrated
in the right-hand diagram of Figure 5.2 We can avoid these difÔ¨Åculties by considering a single K-class discriminant
comprisingKlinear functions of the form
yk(x) = wT
kx+wk0 (5.7)
and then assigning a point xto classCkifyk(x)>yj(x)for allj/negationslash=k The decision
boundary between class Ckand classCjis therefore given by yk(x) =yj(x)and
5.1 Discriminant Functions 135
Figure 5.3 Illustration of the decision regions for a
multi-class linear discriminant, with the
decision boundaries shown in red If
two points xAandxBboth lie inside the
same decision region Rk, then any point
bxthat lies on the line connecting these
two points must also lie in Rk, and hence,
the decision region must be singly con-
nected and convex RiRj
Rk
xAxB
bx
hence
corresponds to a (D‚àí1)-dimensional hyperplane deÔ¨Åned by
(wk‚àíwj)Tx+ (wk0‚àíwj0) = 0: (5.8)
This has the same form as the decision boundary for the two-class case discussed in
Section 5.1.1, and so analogous geometrical properties apply The decision regions of such a discriminant are always singly connected and
convex

============================================================

=== CHUNK 137 ===
Palavras: 351
Caracteres: 2241
--------------------------------------------------
To see this, consider two points xAandxBboth of which lie inside decision
regionRk, as illustrated in Figure 5.3 Any point /hatwidexthat lies on the line connecting
xAandxBcan be expressed in the form
/hatwidex=xA+ (1‚àí)xB (5.9)
where 0661 From the linearity of the discriminant functions, it follows that
yk(/hatwidex) =yk(xA) + (1‚àí)yk(xB): (5.10)
Because both xAandxBlie insideRk, it follows that yk(xA)> yj(xA)and that
yk(xB)>yj(xB), for allj/negationslash=k, and henceyk(/hatwidex)>yj(/hatwidex), and so/hatwidexalso lies inside
Rk Thus,Rkis singly connected and convex Note that for two classes, we can either employ the formalism discussed here,
based on two discriminant functions y1(x)andy2(x), or else use the simpler but
essentially equivalent formulation based on a single discriminant function y(x) Section 5.1.1
5.1.3 1-of-K coding
For regression problems, the target variable twas simply the vector of real num-
bers whose values we wish to predict In classiÔ¨Åcation, there are various ways of
using target values to represent class labels For two-class problems, the most conve-
nient is the binary representation in which there is a single target variable t‚àà{0; 1}
such thatt= 1 represents classC1andt= 0 represents classC2 We can interpret
the value oftas the probability that the class is C1, with the probability values taking
only the extreme values of 0 and 1 For K > 2classes, it is convenient to use a
1-of-K coding scheme, also known as the one-hot encoding scheme, in which tis
a vector of length Ksuch that if the class is Cj, then all elements tkoftare zero
136 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
except element tj, which takes the value 1 For instance, if we have K= 5classes,
then a data point from class 2would be given the target vector
t= (0;1;0;0;0)T: (5.11)
Again, we can interpret the value of tkas the probability that the class is Ckin which
the probabilities take only the values 0 and 1 5.1.4 Least squares for classiÔ¨Åcation
With linear regression models, the minimization of a sum-of-squares error func-
tion leads to a simple closed-form solution for the parameter values It is therefore Section 4.1.3
tempting to see if we can apply the same least-squares formalism to classiÔ¨Åcation
problems

============================================================

=== CHUNK 138 ===
Palavras: 362
Caracteres: 2573
--------------------------------------------------
Consider a general classiÔ¨Åcation problem with Kclasses and a 1-of-K
binary coding scheme for the target vector t One justiÔ¨Åcation for using least squares
in such a context is that it approximates the conditional expectation E[t|x] of the
target values given the input vector For a binary coding scheme, this conditional ex-
pectation is given by the vector of posterior class probabilities Unfortunately, these Exercise 5.1
probabilities are typically approximated rather poorly, and indeed the approxima-
tions can have values outside the range (0;1) However, it is instructional to explore
these simple models and to understand how these limitations arise Each classCkis described by its own linear model so that
yk(x) = wT
kx+wk0 (5.12)
wherek= 1;:::;K We can conveniently group these together using vector nota-
tion so that
y(x) =/tildewiderWT/tildewidex (5.13)
where/tildewiderWis a matrix whose kth column comprises the (D+ 1) -dimensional vector
/tildewidewk= (wk0;wT
k)Tand/tildewidexis the corresponding augmented input vector (1;xT)Twith
a dummy input x0= 1 A new input xis then assigned to the class for which the
outputyk=/tildewidewT
k/tildewidexis largest We now determine the parameter matrix /tildewiderWby minimizing a sum-of-squares
error function Consider a training data set {xn;tn}wheren= 1;:::;N , and
deÔ¨Åne a matrix Twhosenth row is the vector tT
n, together with a matrix /tildewideXwhose
nth row is/tildewidexT
n The sum-of-squares error function can then be written as
ED(/tildewiderW) =1
2Tr/braceleftBig
(/tildewideX/tildewiderW‚àíT)T(/tildewideX/tildewiderW‚àíT)/bracerightBig
: (5.14)
Setting the derivative with respect to /tildewiderWto zero and rearranging, we obtain the solu-
tion for/tildewiderWin the form
/tildewiderW= (/tildewideXT/tildewideX)‚àí1/tildewideXTT=/tildewideX‚Ä†T (5.15)
where/tildewideX‚Ä†is the pseudo-inverse of the matrix /tildewideX We then obtain the discriminant Section 4.1.3
5.1 Discriminant Functions 137
function in the form
y(x) =/tildewiderWT/tildewidex=TT/parenleftBig
/tildewideX‚Ä†/parenrightBigT
/tildewidex: (5.16)
An interesting property of least-squares solutions with multiple target variables
is that if every target vector in the training set satisÔ¨Åes some linear constraint
aTtn+b= 0 (5.17)
for some constants aandb, then the model prediction for any value of xwill satisfy
the same constraint so that Exercise 5.3
aTy(x) +b= 0: (5.18)
Thus, if we use a 1-of-K coding scheme for Kclasses, then the predictions made
by the model will have the property that the elements of y(x)will sum to 1 for any
value of x

============================================================

=== CHUNK 139 ===
Palavras: 401
Caracteres: 2481
--------------------------------------------------
However, this summation constraint alone is not sufÔ¨Åcient to allow the
model outputs to be interpreted as probabilities because they are not constrained to
lie within the interval (0;1) The least-squares approach gives an exact closed-form solution for the discrim-
inant function parameters However, even as a discriminant function (where we use
it to make decisions directly and dispense with any probabilistic interpretation), it
suffers from some severe problems We have seen that the sum-of-squares error
function can be viewed as the negative log likelihood under the assumption of a
Gaussian noise distribution If the true distribution of the data is markedly different Section 2.3.4
from being Gaussian, then least squares can give poor results In particular, least
squares is very sensitive to the presence of outliers, which are data points located a
long way from the bulk of the data This is illustrated in Figure 5.4 Here we see that
the additional data points in the right-hand Ô¨Ågure produce a signiÔ¨Åcant change in the
location of the decision boundary, even though these points would be correctly clas-
siÔ¨Åed by the original decision boundary in the left-hand Ô¨Ågure The sum-of-squares
error function gives too much weight to data points that are a long way from the
decision boundary, even though they are correctly classiÔ¨Åed Outliers can arise due
to rare events or may simply be due to mistakes in the data set Techniques that are
sensitive to a very few data points are said to lack robustness For comparison, Fig-
ure 5.4 also shows results from a technique called logistic regression, which is more Section 5.4.3
robust to outliers The failure of least squares should not surprise us when we recall that it cor-
responds to maximum likelihood under the assumption of a Gaussian conditional
distribution, whereas binary target vectors clearly have a distribution that is far from
Gaussian By adopting more appropriate probabilistic models, we can obtain clas-
siÔ¨Åcation techniques with much better properties than least squares, and which can
also be generalized to give Ô¨Çexible nonlinear neural network models, as we will see
in later chapters SINGLE-LAYER NETWORKS: CLASSIFICATION
‚àí4 ‚àí2 0 2 4 6 8‚àí8‚àí6‚àí4‚àí2024
‚àí4 ‚àí2 0 2 4 6 8‚àí8‚àí6‚àí4‚àí2024
Figure 5.4 The left-hand plot shows data from two classes, denoted by red crosses and blue circles, together
with the decision boundaries found by least squares (magenta curve) and by a logistic regression model (green
curve)

============================================================

=== CHUNK 140 ===
Palavras: 357
Caracteres: 2130
--------------------------------------------------
The right-hand plot shows the corresponding results obtained when extra data points are added at the
bottom right of the diagram, showing that least squares is highly sensitive to outliers, unlike logistic regression Decision Theory
When we discussed linear regression we saw how the process of making predictions
in machine learning can be broken down into the two stages of inference and de-
cision We now explore this perspective in much greater depth speciÔ¨Åcally in the Section 4.2
context of classiÔ¨Åers Suppose we have an input vector xtogether with a corresponding vector tof
target variables, and our goal is to predict tgiven a new value for x For regression
problems, twill comprise continuous variables and in general will be a vector as
we may wish to predict several related quantities For classiÔ¨Åcation problems, twill
represent class labels Again, twill in general be a vector if we have more than
two classes The joint probability distribution p(x;t)provides a complete summary
of the uncertainty associated with these variables Determining p(x;t)from a set
of training data is an example of inference and is typically a very difÔ¨Åcult problem
whose solution forms the subject of much of this book In a practical application,
however, we must often make a speciÔ¨Åc prediction for the value of tor more gen-
erally take a speciÔ¨Åc action based on our understanding of the values tis likely to
take, and this aspect is the subject of decision theory Consider, for example, our earlier medical diagnosis problem in which we have
taken an image of a skin lesion on a patient, and we wish to determine whether the
patient has cancer In this case, the input vector xis the set of pixel intensities in
5.2 Decision Theory 139
the image, and the output variable twill represent the absence of cancer, which we
denote by the class C1, or the presence of cancer, which we denote by the class C2 We might, for instance, choose tto be a binary variable such that t= 0corresponds
to classC1andt= 1 corresponds to class C2 We will see later that this choice of
label values is particularly convenient when working with probabilities

============================================================

=== CHUNK 141 ===
Palavras: 354
Caracteres: 2181
--------------------------------------------------
The gen-
eral inference problem then involves determining the joint distribution p(x;Ck), or
equivalently p(x;t), which gives us the most complete probabilistic description of
the variables Although this can be a very useful and informative quantity, ultimately,
we must decide either to give treatment to the patient or not, and we would like this
choice to be optimal according to some appropriate criterion (Duda and Hart, 1973) This is the decision step, and the aim of decision theory is that it should tell us how
to make optimal decisions given the appropriate probabilities We will see that the
decision stage is generally very simple, even trivial, once we have solved the in-
ference problem Here we give an introduction to the key ideas of decision theory
as required for the rest of the book Further background, as well as more detailed
accounts, can be found in Berger (1985) and Bather (2000) Before giving a more detailed analysis, let us Ô¨Årst consider informally how we
might expect probabilities to play a role in making decisions When we obtain the
skin image xfor a new patient, our goal is to decide which of the two classes to assign
the image to We are therefore interested in the probabilities of the two classes, given
the image, which are given by p(Ck|x) Using Bayes‚Äô theorem, these probabilities
can be expressed in the form
p(Ck|x) =p(x|Ck)p(Ck)
p(x): (5.19)
Note that any of the quantities appearing in Bayes‚Äô theorem can be obtained from
the joint distribution p(x;Ck)by either marginalizing or conditioning with respect to
the appropriate variables We can now interpret p(Ck)as the prior probability for the
classCkandp(Ck|x)as the corresponding posterior probability Thus, p(C1)repre-
sents the probability that a person has cancer, before the image is taken Similarly,
p(C1|x)is the posterior probability, revised using Bayes‚Äô theorem in light of the in-
formation contained in the image If our aim is to minimize the chance of assigning
xto the wrong class, then intuitively we would choose the class having the higher
posterior probability We now show that this intuition is correct, and we also discuss
more general criteria for making decisions

============================================================

=== CHUNK 142 ===
Palavras: 358
Caracteres: 2210
--------------------------------------------------
5.2.1 MisclassiÔ¨Åcation rate
Suppose that our goal is simply to make as few misclassiÔ¨Åcations as possible We need a rule that assigns each value of xto one of the available classes Such a
rule will divide the input space into regions Rkcalled decision regions, one for each
class, such that all points in Rkare assigned to class Ck The boundaries between
decision regions are called decision boundaries ordecision surfaces Note that each
decision region need not be contiguous but could comprise some number of disjoint
regions To Ô¨Ånd the optimal decision rule, consider Ô¨Årst the case of two classes, as in
the cancer problem, for instance A mistake occurs when an input vector belonging
140 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
to classC1is assigned to class C2or vice versa The probability of this occurring is
given by
p(mistake) = p(x‚ààR 1;C2) +p(x‚ààR 2;C1)
=/integraldisplay
R1p(x;C2) dx+/integraldisplay
R2p(x;C1) dx: (5.20)
We are free to choose the decision rule that assigns each point xto one of the
two classes Clearly, to minimize p(mistake) we should arrange that each xis as-
signed to whichever class has the smaller value of the integrand in (5.20) Thus, if
p(x;C1)> p(x;C2)for a given value of x, then we should assign that xto class
C1 From the product rule of probability, we have p(x;Ck) =p(Ck|x)p(x) Because
the factorp(x) is common to both terms, we can restate this result as saying that
the minimum probability of making a mistake is obtained if each value of xis as-
signed to the class for which the posterior probability p(Ck|x)is largest This result
is illustrated for two classes and a single input variable xinFigure 5.5 For the more general case of Kclasses, it is slightly easier to maximize the
probability of being correct, which is given by
p(correct) =K/summationdisplay
k=1p(x‚ààRk;Ck)
=K/summationdisplay
k=1/integraldisplay
Rkp(x;Ck) dx; (5.21)
which is maximized when the regions Rkare chosen such that each xis assigned
to the class for which p(x;Ck)is largest Again, using the product rule p(x;Ck) =
p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see
that each xshould be assigned to the class having the largest posterior probability
p(Ck|x)

============================================================

=== CHUNK 143 ===
Palavras: 356
Caracteres: 2144
--------------------------------------------------
5.2.2 Expected loss
For many applications, our objective will be more complex than simply mini-
mizing the number of misclassiÔ¨Åcations Let us consider again the medical diagnosis
problem We note that, if a patient who does not have cancer is incorrectly diagnosed
as having cancer, the consequences may be that they experience some distress plus
there is the need for further investigations Conversely, if a patient with cancer is
diagnosed as healthy, the result may be premature death due to lack of treatment Thus, the consequences of these two types of mistake can be dramatically different It would clearly be better to make fewer mistakes of the second kind, even if this was
at the expense of making more mistakes of the Ô¨Årst kind We can formalize such issues through the introduction of a loss function, also
called a cost function, which is a single, overall measure of loss incurred in taking
any of the available decisions or actions Our goal is then to minimize the total loss
incurred Note that some authors consider instead a utility function, whose value
5.2 Decision Theory 141
ÀÜx x0p(x,C1)
p(x,C2)R1 R2
(a)
ÀÜxp(x,C1)
p(x,C2)R1 R2
(b)
Figure 5.5 Schematic illustration of the joint probabilities p(x;Ck)for each of two classes plotted against x,
together with the decision boundary x=bx Values of x>bxare classiÔ¨Åed as class C2and hence belong to
decision regionR2, whereas points x <bxare classiÔ¨Åed asC1and belong toR1 Errors arise from the blue,
green, and red regions, so that for x <bx, the errors are due to points from class C2being misclassiÔ¨Åed as C1
(represented by the sum of the red and green regions) Conversely for points in the region x>bx, the errors are
due to points from class C1being misclassiÔ¨Åed as C2(represented by the blue region) By varying the location
bxof the decision boundary, as indicated by the red double-headed arrow in (a), the combined areas of the blue
and green regions remains constant, whereas the size of the red region varies The optimal choice for bxis where
the curves for p(x;C1)andp(x;C2)cross, as shown in (b) and corresponding to bx=x0, because in this case
the red region disappears

============================================================

=== CHUNK 144 ===
Palavras: 364
Caracteres: 2204
--------------------------------------------------
This is equivalent to the minimum misclassiÔ¨Åcation rate decision rule, which assigns
each value of xto the class having the higher posterior probability p(Ck|x) SINGLE-LAYER NETWORKS: CLASSIFICATION
Figure 5.6 An example of a loss matrix with elements
Lkjfor the cancer treatment problem The rows cor-
respond to the true class, whereas the columns corre-
spond to the assignment of class made by our decision
criterion./parenleftbiggnormal cancer
normal 0 1
cancer 100 0/parenrightbigg
they aim to maximize These are equivalent concepts if we take the utility to be
simply the negative of the loss Throughout this text we will use the loss function
convention Suppose that, for a new value of x, the true class isCkand that we assign
xto classCj(wherejmay or may not be equal to k) In so doing, we incur some
level of loss that we denote by Lkj, which we can view as the k;jelement of a loss
matrix For instance, in our cancer example, we might have a loss matrix of the form
shown in Figure 5.6 This particular loss matrix says that there is no loss incurred if
the correct decision is made, there is a loss of 1if a healthy patient is diagnosed as
having cancer, whereas there is a loss of 100if a patient having cancer is diagnosed
as healthy The optimal solution is the one that minimizes the loss function However, the
loss function depends on the true class, which is unknown For a given input vector x,
our uncertainty in the true class is expressed through the joint probability distribution
p(x;Ck), and so we seek instead to minimize the average loss, where the average is
computed with respect to this distribution and is given by
E[L] =/summationdisplay
k/summationdisplay
j/integraldisplay
RjLkjp(x;Ck) dx: (5.22)
Eachxcan be assigned independently to one of the decision regions Rj Our goal
is to choose the regions Rjto minimize the expected loss (5.22), which implies that
for each x, we should minimize/summationtext
kLkjp(x;Ck) As before, we can use the product
rulep(x;Ck) =p(Ck|x)p(x) to eliminate the common factor of p(x) Thus, the
decision rule that minimizes the expected loss assigns each new xto the classjfor
which the quantity/summationdisplay
kLkjp(Ck|x) (5.23)
is a minimum

============================================================

=== CHUNK 145 ===
Palavras: 352
Caracteres: 2132
--------------------------------------------------
Once we have chosen values for the loss matrix elements Lkj, this is
clearly trivial to do 5.2.3 The reject option
We have seen that classiÔ¨Åcation errors arise from the regions of input space
where the largest of the posterior probabilities p(Ck|x)is signiÔ¨Åcantly less than unity
or equivalently where the joint distributions p(x;Ck)have comparable values These
are the regions where we are relatively uncertain about class membership In some
applications, it will be appropriate to avoid making decisions on the difÔ¨Åcult cases
in anticipation of obtaining a lower error rate on those examples for which a classi-
Ô¨Åcation decision is made This is known as the reject option For example, in our
hypothetical cancer screening example, it may be appropriate to use an automatic
5.2 Decision Theory 143
Figure 5.7 Illustration of the reject option Inputs
xsuch that the larger of the two poste-
rior probabilities is less than or equal to
some threshold will be rejected xp(C1jx) p(C2jx)
0:01:0

reject region
system
to classify those images for which there is little doubt as to the correct class,
while requesting a biopsy to classify the more ambiguous cases We can achieve this
by introducing a threshold and rejecting those inputs xfor which the largest of
the posterior probabilities p(Ck|x)is less than or equal to  This is illustrated for
two classes and a single continuous input variable xinFigure 5.7 Note that setting
= 1will ensure that all examples are rejected, whereas if there are Kclasses, then
setting < 1=K will ensure that no examples are rejected Thus, the fraction of
examples that are rejected is controlled by the value of  We can easily extend the reject criterion to minimize the expected loss, when a
loss matrix is given, by taking account of the loss incurred when a reject decision is
made Exercise 5.10
5.2.4 Inference and decision
We have broken the classiÔ¨Åcation problem down into two separate stages, the
inference stage in which we use training data to learn a model for p(Ck|x)and the
subsequent decision stage in which we use these posterior probabilities to make op-
timal class assignments

============================================================

=== CHUNK 146 ===
Palavras: 357
Caracteres: 2380
--------------------------------------------------
An alternative possibility would be to solve both problems
together and simply learn a function that maps inputs xdirectly into decisions Such
a function is called a discriminant function In fact, we can identify three distinct approaches to solving decision problems,
all of which have been used in practical applications These are, in decreasing order
of complexity, as follows:
(a)First, solve the inference problem of determining the class-conditional den-
sitiesp(x|Ck)for each classCkindividually Separately infer the prior class
probabilities p(Ck) Then use Bayes‚Äô theorem in the form
p(Ck|x) =p(x|Ck)p(Ck)
p
(x)(5.24)
to Ô¨Ånd the posterior class probabilities p(Ck|x) As usual, the denominator in
144 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
Bayes‚Äô theorem can be found in terms of the quantities in the numerator, using
p(x) =/summationdisplay
kp(x|Ck)p(Ck): (5.25)
Equivalently, we can model the joint distribution p(x;Ck)directly and then
normalize to obtain the posterior probabilities Having found the posterior
probabilities, we use decision theory to determine the class membership for
each new input x Approaches that explicitly or implicitly model the distribu-
tion of inputs as well as outputs are known as generative models, because by
sampling from them, it is possible to generate synthetic data points in the input
space (b)First, solve the inference problem of determining the posterior class probabili-
tiesp(Ck|x), and then subsequently use decision theory to assign each new xto
one of the classes Approaches that model the posterior probabilities directly
are called discriminative models (c)Find a function f(x), called a discriminant function, that maps each input x
directly onto a class label For instance, for two-class problems, f(¬∑)might be
binary valued and such that f= 0 represents classC1andf= 1 represents
classC2 In this case, probabilities play no role Let us consider the relative merits of these three alternatives Approach (a) is the
most demanding because it involves Ô¨Ånding the joint distribution over both xand
Ck For many applications, xwill have high dimensionality, and consequently, we
may need a large training set to be able to determine the class-conditional densities to
reasonable accuracy Note that the class priors p(Ck)can often be estimated simply
from the fractions of the training set data points in each of the classes

============================================================

=== CHUNK 147 ===
Palavras: 369
Caracteres: 2353
--------------------------------------------------
One advantage
of approach (a), however, is that it also allows the marginal density of data p(x) to
be determined from (5.25) This can be useful for detecting new data points that
have low probability under the model and for which the predictions may be of low
accuracy, which is known as outlier detection ornovelty detection (Bishop, 1994;
Tarassenko, 1995) However, if we wish only to make classiÔ¨Åcation decisions, then it can be waste-
ful of computational resources and excessively demanding of data to Ô¨Ånd the joint
distribution p(x;Ck)when in fact we really need only the posterior probabilities
p(Ck|x), which can be obtained directly through approach (b) Indeed, the class-
conditional densities may contain a signiÔ¨Åcant amount of structure that has little ef-
fect on the posterior probabilities, as illustrated in Figure 5.8 There has been much
interest in exploring the relative merits of generative and discriminative approaches
to machine learning and in Ô¨Ånding ways to combine them (Jebara, 2004; Lasserre,
Bishop, and Minka, 2006) An even simpler approach is (c) in which we use the training data to Ô¨Ånd a
discriminant function f(x)that maps each xdirectly onto a class label, thereby
combining the inference and decision stages into a single learning problem In the
example of Figure 5.8, this would correspond to Ô¨Ånding the value of xshown by
5.2 Decision Theory 145
p(x| C 1)p(x|C2)
xclass densities
0 0.2 0.4 0.6 0.8 1012345
xp(C1|x) p(C2|x)
0 0.2 0.4 0.6 0.8 100.20.40.60.811.2
Figure 5.8 Example of the class-conditional densities for two classes having a single input variable x(left
plot) together with the corresponding posterior probabilities (right plot) Note that the left-hand mode of the
class-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities The
vertical green line in the right plot shows the decision boundary in xthat gives the minimum misclassiÔ¨Åcation
rate, assuming the prior class probabilities, p(C1)andp(C2), are equal the vertical green line, because this is the decision boundary giving the minimum
probability of misclassiÔ¨Åcation With option (c), however, we no longer have access to the posterior probabilities
p(Ck|x) There are many powerful reasons for wanting to compute the posterior
probabilities, even if we subsequently use them to make decisions

============================================================

=== CHUNK 148 ===
Palavras: 361
Caracteres: 2149
--------------------------------------------------
These include:
Minimizing risk Consider a problem in which the elements of the loss matrix are
subjected to revision from time to time (such as might occur in a Ô¨Ånancial
application) If we know the posterior probabilities, we can trivially revise the
minimum risk decision criterion by modifying (5.23) appropriately If we have
only a discriminant function, then any change to the loss matrix would require
that we return to the training data and solve the inference problem afresh Posterior probabilities allow us to determine a rejection criterion that
will minimize the misclassiÔ¨Åcation rate, or more generally the expected loss,
for a given fraction of rejected data points Compensating for class priors Consider our cancer screening example again, and Section 2.1.1
suppose that we have collected a large number of images from the general pop-
ulation for use as training data, which we use to build an automated screening
system Because cancer is rare amongst the general population, we might Ô¨Ånd
that, say, only 1 in every 1,000 examples corresponds to the presence of cancer SINGLE-LAYER NETWORKS: CLASSIFICATION
If we used such a data set to train an adaptive model, we could run into severe
difÔ¨Åculties due to the small proportion of those in the cancer class For in-
stance, a classiÔ¨Åer that assigned every point to the normal class would achieve
99.9% accuracy, and it may be difÔ¨Åcult to avoid this trivial solution Also, even
a large data set will contain very few examples of skin images corresponding
to cancer, and so the learning algorithm will not be exposed to a broad range
of examples of such images and hence is not likely to generalize well A bal-
anced data set with equal numbers of examples from each of the classes would
allow us to Ô¨Ånd a more accurate model However, we then have to compensate
for the effects of our modiÔ¨Åcations to the training data Suppose we have used
such a modiÔ¨Åed data set and found models for the posterior probabilities From
Bayes‚Äô theorem (5.24), we see that the posterior probabilities are proportional
to the prior probabilities, which we can interpret as the fractions of points in
each class

============================================================

=== CHUNK 149 ===
Palavras: 369
Caracteres: 2307
--------------------------------------------------
We can therefore simply take the posterior probabilities obtained
from our artiÔ¨Åcially balanced data set, divide by the class fractions in that data
set, and then multiply by the class fractions in the population to which we wish
to apply the model Finally, we need to normalize to ensure that the new poste-
rior probabilities sum to one Note that this procedure cannot be applied if we
have learned a discriminant function directly instead of determining posterior
probabilities For complex applications, we may wish to break the problem
into a number of smaller sub-problems each of which can be tackled by a sep-
arate module For example, in our hypothetical medical diagnosis problem,
we may have information available from, say, blood tests as well as skin im-
ages Rather than combine all of this heterogeneous information into one huge
input space, it may be more effective to build one system to interpret the im-
ages and a different one to interpret the blood data If each of the two models
gives posterior probabilities for the classes, then we can combine the outputs
systematically using the rules of probability One simple way to do this is to
assume that, for each class separately, the distributions of inputs for the im-
ages, denoted by xI, and the blood data, denoted by xB, are independent, so
that
p(xI;xB|Ck) =p(xI|Ck)p(x B|Ck): (5.26)
This is an example of a conditional independence property, because the in- Section 11.2
dependence holds when the distribution is conditioned on the class Ck The
posterior probability, given both the image and blood data, is then given by
p(Ck|xI;xB)‚àùp(xI;xB|Ck)p(Ck)
‚àùp(xI|Ck)p(x B|Ck)p(Ck)
‚àùp(Ck|xI)p(Ck|xB)
p(Ck): (5.27)
Thus, we need the class prior probabilities p(Ck), which we can easily estimate
from the fractions of data points in each class, and then we need to normalize
5.2 Decision Theory 147
Figure 5.9 The confusion matrix for the cancer treat-
ment problem, in which the rows correspond to the true
class and the columns correspond to the assignment
of class made by our decision criterion The elements
of the matrix show the numbers of true negatives, false
positives, false negatives, and true positives./parenleftbiggnormal cancer
normalNTNNFP
cancerNFNNTP/parenrightbigg
the resulting posterior probabilities so they sum to one

============================================================

=== CHUNK 150 ===
Palavras: 389
Caracteres: 2374
--------------------------------------------------
The particular condi-
tional independence assumption (5.26) is an example of a naive Bayes model Section 11.2.3
Note that the joint marginal distribution p(xI;xB)will typically not factorize
under this model We will see in later chapters how to construct models for
combining data that do not require the conditional independence assumption
(5.26) A further advantage of using models that output probabilities rather
than decisions is that they can easily be made differentiable with respect to
any adjustable parameters (such as the weight coefÔ¨Åcients in the polynomial
regression example), which allows them to be composed and trained jointly
using gradient-based optimization methods Chapter 7
5.2.5 ClassiÔ¨Åer accuracy
The simplest measure of performance for a classiÔ¨Åer is the fraction of test set
points that are correctly classiÔ¨Åed However, we have seen that different types of
error can have different consequences, as expressed through the loss matrix, and
often we therefore do not simply wish to minimize the number of misclassiÔ¨Åcations By changing the location of the decision boundary, we can make trade-offs between
different kinds of error, for example with the goal of minimizing an expected loss Because this is such an important concept, we will introduce some deÔ¨Ånitions and
terminology so that the performance of a classiÔ¨Åer can be better characterized We will consider again our cancer screening example For each person tested, Section 2.1.1
there is a ‚Äòtrue label‚Äô of whether they have cancer or not, and there is also the predic-
tion made by the classiÔ¨Åer If, for a particular person, the classiÔ¨Åer predicts cancer
and this is in fact the true label, then the prediction is called a true positive How-
ever, if the person does not have cancer it is a false positive Likewise, if the classiÔ¨Åer
predicts that a person does not have cancer and this is correct, then the prediction is
called a true negative , otherwise it is a false negative The false positives are also
known as type 1 errors whereas the false negatives are called type 2 errors If Nis
the total number of people taking the test, then NTPis the number of true positives,
NFPis the number of false positives, NTNis the number of true negatives, and NFN
is the number of false negatives, where
N=NTP+NFP+NTN+NFN: (5.28)
This can be represented as a confusion matrix as shown in Figure 5.9

============================================================

=== CHUNK 151 ===
Palavras: 365
Caracteres: 2251
--------------------------------------------------
Accuracy,
measured by the fraction of correct classiÔ¨Åcations, is then given by
Accuracy =NTP+NTN
NTP+NFP+NTN+NFN: (5.29)
148 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
We can see that accuracy can be misleading if there are strongly imbalanced classes In our cancer screening example, for instance, where only 1person in 1;000 has
cancer, a naive classiÔ¨Åer that simply decides that nobody has cancer will achieve
99:9% accuracy and yet is completely useless Several other quantities can be deÔ¨Åned in terms of these numbers, of which the
most commonly encountered are
Precision =NTP
NTP+NFP(5.30)
Recall =NTP
NTP+NFN(5.31)
F
alse positive rate =NFP
NFP+NTN(5.32)
F
alse discovery rate =NFP
NFP+NTP(5.33)
In
our cancer screening example, precision represents an estimate of the probability
that a person who has a positive test does indeed have cancer, whereas recall is an
estimate of the probability that a person who has cancer is correctly detected by
the test The false positive rate is an estimate of the probability that a person who is
normal will be classiÔ¨Åed as having cancer, whereas the false discovery rate represents
the fraction of those testing positive who do not in fact have cancer By altering the location of the decision boundary, we can change the trade-offs
between the two kinds of errors To understand this trade-off, we revisit Figure 5.5,
but now we label the various regions as shown in Figure 5.10 We can relate the
labelled regions to the various true and false rates as follows:
NFP=N=E (5.34)
NTP=N=D+E (5.35)
NFN=N=B+C (5.36)
NTN=N=A+C (5.37)
where we are implicitly considering the limit N‚Üí‚àû so that we can relate number
of observations to probabilities 5.2.6 ROC curve
A probabilistic classiÔ¨Åer will output a posterior probability, which can be con-
verted to a decision by setting a threshold As the value of the threshold is varied, we
can reduce type 1 errors at the expense of increasing type 2 errors, or vice versa To
better understand this trade-off, it is useful to plot the receiver operating characteris-
ticorROC curve (Fawcett, 2006), a name that originates from procedures to measure
the performance of radar receivers This is a graph of true positive rate versus false
positive rate, as shown in Figure 5.11

============================================================

=== CHUNK 152 ===
Palavras: 386
Caracteres: 2243
--------------------------------------------------
As the decision boundary in Figure 5.10 is moved from‚àí‚àû to‚àû, the ROC
curve is traced out and can then be generated by plotting the cumulative fraction of
5.2 Decision Theory 149
ÀÜx x0p(x,C1)
p(x,C2)R1 R2
AB
CD
E
Figure
5.10 As in Figure 5.5, with the various regions labelled In the cancer classiÔ¨Åcation problem, region R1
is assigned to the normal class whereas region R2is assigned to the cancer class correct detection of cancer on the y-axis versus the cumulative fraction of incorrect
detection on the x-axis Note that a speciÔ¨Åc confusion matrix represents one point
along the ROC curve The best possible classiÔ¨Åer would be represented by a point at
the top left corner of the ROC diagram The bottom left corner represents a simple
classiÔ¨Åer that assigns every point to the normal class and therefore has no true posi-
tives but also no false positives Similarly, the top right corner represents a classiÔ¨Åer
that assigns everything to the cancer class and therefore has no false negatives but
also no true negatives In Figure 5.11, the classiÔ¨Åers represented by the blue curve
are better than those of the red curve for any choice of, say, false positive rate It
is also possible, however, for such curves to cross over, in which case the choice of
which is better will depend on the choice of operating point As a baseline, we can consider a random classiÔ¨Åer that simply assigns each data
point to cancer with probability and to normal with probability 1‚àí As we vary
the value of it will trace out an ROC curve given by a diagonal straight line, as
shown in Figure 5.11 Any classiÔ¨Åer below the diagonal line performs worse than
random guessing Sometimes it is useful to have a single number that characterises the whole ROC
curve One approach is to measure the area under the curve (AUC) A value of 0:5
for the AUC represents random guessing whereas a value of 1:0represents a perfect
classiÔ¨Åer Another measure is the F-score, which is the geometric mean of precision and
150 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
Figure 5.11 The receiver operator characteristic
(ROC) curve is a plot of true positive
rate against false positive rate, and
it characterizes the trade-off between
type 1 and type 2 errors in a classiÔ¨Å-
cation problem

============================================================

=== CHUNK 153 ===
Palavras: 361
Caracteres: 2251
--------------------------------------------------
The upper blue curve
represents a better classiÔ¨Åer than the
lower red curve Here the dashed
curve represents the performance of
a simple random classiÔ¨Åer 0 1 False positive rate01 True positive rate
recall,
and is therefore deÔ¨Åned by
F=2√óprecision√órecall
precision +recall(5.38)
=2
NTP
2
NTP+NFP+NFN: (5.39)
Of course, we can also combine the confusion matrix in Figure 5.9 with the loss ma-
trix in Figure 5.6 to compute the expected loss by multiplying the elements pointwise
and summing the resulting products Although the ROC curve can be extended to more than two classes, it rapidly
becomes cumbersome as the number of classes increases Generative
ClassiÔ¨Åers
W
e turn next to a probabilistic view of classiÔ¨Åcation and show how models with
linear decision boundaries arise from simple assumptions about the distribution of
the data We have already discussed the distinction between the discriminative and
the generative approaches to classiÔ¨Åcation Here we will adopt a generative approach Section 5.2.4
in which we model the class-conditional densities p(x|Ck)as well as the class priors
p(Ck)and then use these to compute posterior probabilities p(Ck|x)through Bayes‚Äô
theorem First, consider problems having two classes The posterior probability for class
5.3 Generative ClassiÔ¨Åers 151
Figure 5.12 Plot of the logistic sigmoid
function(a)deÔ¨Åned by
(5.42), shown in red, together
with the scaled probit function
(a), for2==8, shown
in dashed blue, where (a)
is deÔ¨Åned by (5.86) The
scaling factor =8 is chosen
so that the derivatives of
the two curves are equal for
a= 0 ‚àí5 0 500.51
C1can
be written as
p(C1|x) =p(x|C 1)p(C 1)
p
(x|C 1)p(C 1) +p(x|C 2)p(C 2)
=1
1
+ exp(‚àía)=(a) (5.40)
where we have deÔ¨Åned
a= lnp(x|C 1)p(C 1)
p
(x|C 2)p(C 2)(5.41)
and(a)is the logistic sigmoid function deÔ¨Åned by
(a) =1
1
+ exp(‚àía); (5.42)
which is plotted in Figure 5.12 The term ‚Äòsigmoid‚Äô means S-shaped This type of
function is sometimes also called a ‚Äòsquashing function‚Äô because it maps the whole
real axis into a Ô¨Ånite interval The logistic sigmoid has been encountered already
in earlier chapters and plays an important role in many classiÔ¨Åcation algorithms It
satisÔ¨Åes the following symmetry property:
(‚àía) = 1‚àí(a) (5.43)
as is easily veriÔ¨Åed

============================================================

=== CHUNK 154 ===
Palavras: 380
Caracteres: 2525
--------------------------------------------------
The inverse of the logistic sigmoid is given by
a= ln/parenleftbigg
1‚àí/parenrightbigg
(5.44)
and
is known as the logit function It represents the log of the ratio of probabilities
ln [p(C 1|x)=p(C 2|x)]for the two classes, also known as the log odds Note that in (5.40), we have simply rewritten the posterior probabilities in an
equivalent form, and so the appearance of the logistic sigmoid may seem artiÔ¨Åcial SINGLE-LAYER NETWORKS: CLASSIFICATION
However, it will have signiÔ¨Åcance provided a(x) has a constrained functional form We will shortly consider situations in which a(x) is a linear function of x, in which
case the posterior probability is governed by a generalized linear model If there areK > 2classes, we have
p(Ck|x) =p(x|Ck)p(Ck)/summationtext
jp
(x|Cj)p(Cj)
=exp(ak)/summationtext
jexp(
aj); (5.45)
which is known as the normalized exponential and can be regarded as a multi-class
generalization of the logistic sigmoid Here the quantities akare deÔ¨Åned by
ak= ln (p(x|C k)p(Ck)): (5.46)
The normalized exponential is also known as the softmax function, as it represents
a smoothed version of the ‚Äòmax‚Äô function because, if ak/greatermuchajfor allj/negationslash=k, then
p(Ck|x)/similarequal1, andp(Cj|x)/similarequal0 We now investigate the consequences of choosing speciÔ¨Åc forms for the class-
conditional densities, looking Ô¨Årst at continuous input variables xand then dis-
cussing brieÔ¨Çy discrete inputs 5.3.1 Continuous inputs
Let us assume that the class-conditional densities are Gaussian We will then ex-
plore the resulting form for the posterior probabilities To start with, we will assume
that all classes share the same covariance matrix  Thus, the density for class Ckis
given by
p(x|Ck) =1
(2
)D=21
|
|1=2exp/braceleftbigg
‚àí1
2(
x‚àík)T‚àí1(x‚àík)/bracerightbigg
: (5.47)
First, suppose that we have two classes From (5.40) and (5.41), we have
p(C1|x) =(wTx+w0) (5.48)
where we have deÔ¨Åned
w=‚àí1(1‚àí2) (5.49)
w0=‚àí1
2T
1‚àí
11+1
2T
2‚àí
12+ lnp(C1)
p
(C2): (5.50)
We see that the quadratic terms in xfrom the exponents of the Gaussian densities
have cancelled (due to the assumption of common covariance matrices), leading to
a linear function of xin the argument of the logistic sigmoid This result is illus-
trated for a two-dimensional input space xinFigure 5.13 The resulting decision
boundaries correspond to surfaces along which the posterior probabilities p(Ck|x)
x1x2
01
x1x2Figure 5.13 The left-hand plot shows the class-conditional densities for two classes, denoted red and blue

============================================================

=== CHUNK 155 ===
Palavras: 374
Caracteres: 2372
--------------------------------------------------
On the right is the corresponding posterior probability p(C1|x), which is given by a logistic sigmoid of a linear
function of x The surface in the right-hand plot is coloured using a proportion of red ink given by p(C1|x)and a
proportion of blue ink given by p(C2|x) = 1‚àíp(C1|x) are constant and so will be given by linear functions of x, and therefore the decision
boundaries are linear in input space The prior probabilities p(Ck)enter only through
the bias parameter w0, so that changes in the priors have the effect of making par-
allel shifts of the decision boundary and more generally of the parallel contours of
constant posterior probability For the general case of Kclasses, the posterior probabilities are given by (5.45)
where, from (5.46) and (5.47), we have
ak(x) =wT
kx+wk0 (5.51)
in which we have deÔ¨Åned
wk=‚àí1k (5.52)
wk0=‚àí1
2T
k‚àí1k+ lnp(Ck): (5.53)
We see that the ak(x)are again linear functions of xas a consequence of the cancel-
lation of the quadratic terms due to the shared covariances The resulting decision
boundaries, corresponding to the minimum misclassiÔ¨Åcation rate, will occur when
two of the posterior probabilities (the two largest) are equal, and so will be deÔ¨Åned
by linear functions of x Thus, we again have a generalized linear model If we relax the assumption of a shared covariance matrix and allow each class-
conditional density p(x|Ck)to have its own covariance matrix k, then the earlier
cancellations will no longer occur, and we will obtain quadratic functions of x, giv-
ing rise to a quadratic discriminant The linear and quadratic decision boundaries
are illustrated in Figure 5.14 5.3.2 Maximum likelihood solution
Once we have speciÔ¨Åed a parametric functional form for the class-conditional
densitiesp(x|Ck), we can then determine the values of the parameters, together with5.3.Generative ClassiÔ¨Åers 153
154 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
x1x2
x1x2
Figure 5.14 The left-hand plot shows the class-conditional densities for three classes each having a Gaussian
distribution, coloured red, green, and blue, in which the red and blue classes have the same covariance matrix The right-hand plot shows the corresponding posterior probabilities, in which each point on the image is coloured
using proportions of red, blue, and green ink corresponding to the posterior probabilities for the respective
three classes

============================================================

=== CHUNK 156 ===
Palavras: 355
Caracteres: 2429
--------------------------------------------------
The decision boundaries are also shown Notice that the boundary between the red and blue
classes, which have the same covariance matrix, is linear, whereas those between the other pairs of classes are
quadratic the prior class probabilities p(Ck), using maximum likelihood This requires a data
set comprising observations of xalong with their corresponding class labels First, suppose we have two classes, each having a Gaussian class-conditional
density with a shared covariance matrix, and suppose we have a data set {xn;tn}
wheren= 1;:::;N Heretn= 1denotes classC1andtn= 0denotes classC2 We
denote the prior class probability p(C1) =, so thatp(C2) = 1‚àí For a data point
xnfrom classC1, we havetn= 1and hence
p(xn;C1) =p(C1)p(xn|C1) =N(xn|1;):
Similarly for class C2, we havetn= 0and hence
p(xn;C2) =p(C2)p(xn|C2) = (1‚àí)N(xn|2;):
Thus, the likelihood function is given by
p(t;X|;1;2;) =N/productdisplay
n=1[N(xn|1;)]tn[(1‚àí)N(xn|2;)]1‚àítn(5.54)
where t= (t1;:::;tN)T As usual, it is convenient to maximize the log of the
likelihood function Consider Ô¨Årst the maximization with respect to  Generative ClassiÔ¨Åers 155
the log likelihood function that depend on are
N/summationdisplay
n=1{tnln+ (1‚àítn) ln(1‚àí)}: (5.55)
Setting the derivative with respect to equal to zero and rearranging, we obtain
=1
NN/summationdisplay
n=1tn=N1
N=N1
N1+N2(5.56)
whereN1denotes the total number of data points in class C1, andN2denotes the
total number of data points in class C2 Thus, the maximum likelihood estimate
foris simply the fraction of points in class C1as expected This result is easily
generalized to the multi-class case where again the maximum likelihood estimate of
the prior probability associated with class Ckis given by the fraction of the training
set points assigned to that class Exercise 5.13
Now consider the maximization with respect to 1 Again, we can pick out of
the log likelihood function those terms that depend on 1:
N/summationdisplay
n=1tnlnN(xn|1;) =‚àí1
2N/summationdisplay
n=1tn(xn‚àí1)T‚àí1(xn‚àí1) + const: (5.57)
Setting the derivative with respect to 1to zero and rearranging, we obtain
1=1
N1N/summationdisplay
n=1tnxn; (5.58)
which is simply the mean of all the input vectors xnassigned to classC1 By a
similar argument, the corresponding result for 2is given by
2=1
N2N/summationdisplay
n=1(1‚àítn)xn; (5.59)
which again is the mean of all the input vectors xnassigned to classC2

============================================================

=== CHUNK 157 ===
Palavras: 361
Caracteres: 2570
--------------------------------------------------
Finally, consider the maximum likelihood solution for the shared covariance
matrix  Picking out the terms in the log likelihood function that depend on , we
have
‚àí1
2N/summationdisplay
n=1tnln||‚àí1
2N/summationdisplay
n=1tn(xn‚àí1)T‚àí1(xn‚àí1)
‚àí1
2N/summationdisplay
n=1(1‚àítn) ln||‚àí1
2N/summationdisplay
n=1(1‚àítn)(xn‚àí2)T‚àí1(xn‚àí2)
=‚àíN
2ln||‚àíN
2Tr/braceleftbig
‚àí1S/bracerightbig
(5.60)
156 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
where we have deÔ¨Åned
S=N1
NS1+N2
NS2 (5.61)
S1=1
N1/summationdisplay
n‚ààC 1(xn‚àí1)(xn‚àí1)T(5.62)
S2=1
N2/summationdisplay
n‚ààC 2(xn‚àí2)(xn‚àí2)T: (5.63)
Using the standard result for the maximum likelihood solution for a Gaussian distri-
bution, we see that =S, which represents a weighted average of the covariance
matrices associated with each of the two classes separately This result is easily extended to the K-class problem to obtain the corresponding
maximum likelihood solutions for the parameters in which each class-conditional
density is Gaussian with a shared covariance matrix Note that the approach of Ô¨Åtting Exercise 5.14
Gaussian distributions to the classes is not robust to outliers, because the maximum
likelihood estimation of a Gaussian is not robust Section 5.1.4
5.3.3 Discrete features
Let us now consider discrete feature values xi For simplicity, we begin by look-
ing at binary feature values xi‚àà{0;1}and discuss the extension to more general
discrete features shortly If there are Dinputs, then a general distribution would
correspond to a table of 2Dnumbers for each class and have 2D‚àí1independent
variables (due to the summation constraint) Because this grows exponentially with
the number of features, we can seek a more restricted representation Here we will
make the naive Bayes assumption in which the feature values are treated as indepen- Section 11.2.3
dent and conditioned on the class Ck Thus, we have class-conditional distributions
of the form
p(x|Ck) =D/productdisplay
i=1xi
ki(1‚àíki)1‚àíxi; (5.64)
which contain Dindependent parameters for each class Substituting into (5.46) then
gives
ak(x) =D/summationdisplay
i=1{xilnki+ (1‚àíxi) ln(1‚àíki)}+ lnp(Ck); (5.65)
which again are linear functions of the input values xi ForK= 2 classes, we can
alternatively consider the logistic sigmoid formulation given by (5.40) Analogous
results are obtained for discrete variables that take L>2states Exercise 5.16
5.3.4 Exponential family
As we have seen, for both Gaussian distributed and discrete inputs, the posterior
class probabilities are given by generalized linear models with logistic sigmoid ( K=
5.4

============================================================

=== CHUNK 158 ===
Palavras: 373
Caracteres: 2528
--------------------------------------------------
Discriminative ClassiÔ¨Åers 157
2classes) or softmax (K >2classes) activation functions These are particular cases
of a more general result obtained by assuming that the class-conditional densities
p(x|Ck)are members of the subset of the exponential family of distributions given Section 3.4
by
p(x|k;s) =1
sh/parenleftbigg1
sx/parenrightbigg
g(k) exp/braceleftbigg1
sT
kx/bracerightbigg
: (5.66)
Here the scaling parameter sis shared across all the classes For the two-class problem, we substitute this expression for the class-conditional
densities into (5.41) and we see that the posterior class probability is again given by
a logistic sigmoid acting on a linear function a(x), which is given by
a(x) = ( 1‚àí2)Tx+ lng(1)‚àílng(2) + lnp(C1)‚àílnp(C2): (5.67)
Similarly, for the K-class problem, we substitute the class-conditional density ex-
pression into (5.46) to give
ak(x) =T
kx+ lng(k) + lnp(Ck) (5.68)
and so again is a linear function of x Discriminative ClassiÔ¨Åers
For the two-class classiÔ¨Åcation problem, we have seen that the posterior probabil-
ity of classC1can be written as a logistic sigmoid acting on a linear function of
x, for a wide choice of class-conditional distributions p(x|Ck)from the exponential
family Similarly, for the multi-class case, the posterior probability of class Ckis
given by a softmax transformation of linear functions of x For speciÔ¨Åc choices of
the class-conditional densities p(x|Ck), we have used maximum likelihood to deter-
mine the parameters of the densities as well as the class priors p(Ck)and then used
Bayes‚Äô theorem to Ô¨Ånd the posterior class probabilities This represents an example
ofgenerative modelling, because we could take such a model and generate synthetic
data by drawing values of xfrom the marginal distribution p(x) or from any of the
class-conditional densities p(x|Ck) However, an alternative approach is to use the functional form of the general-
ized linear model explicitly and to determine its parameters directly by using maxi-
mum likelihood In this direct approach, we maximize a likelihood function deÔ¨Åned
through the conditional distribution p(Ck|x), which represents a form of discrimina-
tiveprobabilistic modelling One advantage of the discriminative approach is that
there will typically be fewer learnable parameters to be determined, as we will see
shortly It may also lead to improved predictive performance, particularly when the
assumed forms for the class-conditional densities represent a poor approximation to
the true distributions

============================================================

=== CHUNK 159 ===
Palavras: 360
Caracteres: 2265
--------------------------------------------------
SINGLE-LAYER NETWORKS: CLASSIFICATION
5.4.1 Activation functions
In linear regression, the model prediction y(x;w)is given by a linear function Chapter 4
of the parameters
y(x;w) =wTx+w0; (5.69)
which gives a continuous-valued output in the range (‚àí‚àû;‚àû) For classiÔ¨Åcation
problems, however, we wish to predict discrete class labels, or more generally pos-
terior probabilities that lie in the range (0;1) To achieve this, we consider a gener-
alization of this model in which we transform the linear function of wandw0using
a nonlinear function f(¬∑)so that
y(x;w) =f/parenleftbig
wTw+w+ 0/parenrightbig
: (5.70)
In the machine learning literature, f(¬∑)is known as an activation function, whereas
its inverse is called a link function in the statistics literature The decision surfaces
correspond to y(x) = constant , so that wTx= constant, and hence the decision
surfaces are linear functions of x, even if the function f(¬∑)is nonlinear For this
reason, the class of models described by (5.70) are called generalized linear models
(McCullagh and Nelder, 1989) However, in contrast to the models used for regres-
sion, they are no longer linear in the parameters due to the nonlinear function f(¬∑) This will lead to more complex analytical and computational properties than for
linear regression models Nevertheless, these models are still relatively simple com-
pared to the much more Ô¨Çexible nonlinear models that will be studied in subsequent
chapters 5.4.2 Fixed basis functions
So far in this chapter, we have considered classiÔ¨Åcation models that work di-
rectly with the original input vector x However, all the algorithms are equally ap-
plicable if we Ô¨Årst make a Ô¨Åxed nonlinear transformation of the inputs using a vector
of basis functions (x) The resulting decision boundaries will be linear in the fea-
ture space, and these correspond to nonlinear decision boundaries in the original x
space, as illustrated in Figure 5.15 Classes that are linearly separable in the feature
space(x) need not be linearly separable in the original observation space x Note that as in our discussion of linear models for regression, one of the basis
functions is typically set to a constant, say 0(x) = 1 , so that the corresponding
parameterw0plays the role of a bias

============================================================

=== CHUNK 160 ===
Palavras: 355
Caracteres: 2290
--------------------------------------------------
For many problems of practical interest, there is signiÔ¨Åcant overlap in x-space
between the class-conditional densities p(x|Ck) This corresponds to posterior prob-
abilitiesp(Ck|x), which, for at least some values of x, are not 0 or 1 In such cases,
the optimal solution is obtained by modelling the posterior probabilities accurately
and then applying standard decision theory Note that nonlinear transformations Section 5.2
(x) cannot remove such a class overlap, although they can increase the level of
overlap or create an overlap where none existed in the original observation space However, suitable choices of nonlinearity can make the process of modelling the
posterior probabilities easier However, such Ô¨Åxed basis function models have im-
portant limitations, and these will be resolved in later chapters by allowing the basis Section 6.1
functions themselves to adapt to the data Discriminative ClassiÔ¨Åers 159
x1x2
‚àí1 0 1‚àí101
œÜ1œÜ2
0 0.5 100.51
Figure 5.15 Illustration of the role of nonlinear basis functions in linear classiÔ¨Åcation models The left-hand
plot shows the original input space (x1;x2)together with data points from two classes labelled red and blue Two ‚ÄòGaussian‚Äô basis functions 1(x)and2(x)are deÔ¨Åned in this space with centres shown by the green
crosses and with contours shown by the green circles The right-hand plot shows the corresponding feature
space (1;2)together with the linear decision boundary obtained given by a logistic regression model of the
form discussed in Section 5.4.3 This corresponds to a nonlinear decision boundary in the original input space,
shown by the black curve in the left-hand plot 5.4.3 Logistic regression
We Ô¨Årst consider the problem of two-class classiÔ¨Åcation In our discussion of
generative approaches in Section 5.3, we saw that under rather general assumptions,
the posterior probability of class C1can be written as a logistic sigmoid acting on a
linear function of the feature vector so that
p(C1|) =y() =/parenleftbig
wT/parenrightbig
(5.71)
withp(C2|) = 1‚àíp(C1|) Here(¬∑)is the logistic sigmoid function deÔ¨Åned by
(5.42) In the terminology of statistics, this model is known as logistic regression,
although it should be emphasized that this is a model for classiÔ¨Åcation rather than
for continuous variable

============================================================

=== CHUNK 161 ===
Palavras: 354
Caracteres: 2265
--------------------------------------------------
For anM-dimensional feature space , this model has Madjustable parameters By contrast, if we had Ô¨Åtted Gaussian class-conditional densities using maximum
likelihood, we would have used 2M parameters for the means and M(M+ 1)=2
parameters for the (shared) covariance matrix Together with the class prior p(C1),
this gives a total of M(M+5)=2+1 parameters, which grows quadratically with M,
in contrast to the linear dependence on Mof the number of parameters in logistic
regression For large values of M, there is a clear advantage in working with the
logistic regression model directly SINGLE-LAYER NETWORKS: CLASSIFICATION
We now use maximum likelihood to determine the parameters of the logistic
regression model To do this, we will make use of the derivative of the logistic sig-
moid function, which can conveniently be expressed in terms of the sigmoid function
itself: Exercise 5.18
d
da=(1‚àí): (5.72)
For a data set{n;tn}, wheren=(xn)andtn‚àà{0; 1}, withn= 1;:::;N ,
the likelihood function can be written
p(t|w ) =N/productdisplay
n=1ytn
n{1‚àíyn}1‚àítn(5.73)
where t= (t 1;:::;tN)Tandyn=p(C1|n) As usual, we can deÔ¨Åne an error
function by taking the negative logarithm of the likelihood, which gives the cross-
entropy error function:
E(w) =‚àílnp(t|w ) =‚àíN/summationdisplay
n=1{tnlnyn+ (1‚àítn) ln(1‚àíyn)} (5.74)
whereyn=(an)andan=wTn Taking the gradient of the error function with
respect to w, we obtain Exercise 5.19
‚àáE(w) =N/summationdisplay
n=1(yn‚àítn)n (5.75)
where we have made use of (5.72) We see that the factor involving the derivative
of the logistic sigmoid has cancelled, leading to a simpliÔ¨Åed form for the gradient
of the log likelihood In particular, the contribution to the gradient from data point
nis given by the ‚Äòerror‚Äô yn‚àítnbetween the target value and the prediction of
the model times the basis function vector n Furthermore, comparison with (4.12)
shows that this takes precisely the same form as the gradient of the sum-of-squares
error function for the linear regression model Section 4.1.3
The maximum likelihood solution corresponds to ‚àáE(w) = 0 However, from
(5.75) we see that this no longer corresponds to a set of linear equations, due to
the nonlinearity in y(¬∑), and so this equation does not have a closed-form solution

============================================================

=== CHUNK 162 ===
Palavras: 405
Caracteres: 2623
--------------------------------------------------
One approach to Ô¨Ånding a maximum likelihood solution would be to use stochastic
gradient descent, in which ‚àáEnis thenth term on the right-hand side of (5.75) Chapter 7
Stochastic gradient descent will be the principal approach to training the highly non-
linear neural networks discussed in later chapters However, the maximum likelihood
equation is only ‚Äòslightly‚Äô nonlinear, and in fact the error function (5.74), in which the
model is deÔ¨Åned by (5.71), is a convex function of the parameters, which allows the
error function to be minimized using a simple algorithm called iterative reweighted
least squares or IRLS (Bishop, 2006) However, this does not easily generalize to
more complex models such as deep neural networks Discriminative ClassiÔ¨Åers 161
Note that maximum likelihood can exhibit severe over-Ô¨Åtting for data sets that
are linearly separable This arises because the maximum likelihood solution occurs
when the hyperplane corresponding to = 0:5, equivalent to wT= 0, separates
the two classes and the magnitude of wgoes to inÔ¨Ånity In this case, the logis-
tic sigmoid function becomes inÔ¨Ånitely steep in feature space, corresponding to a
Heaviside step function, so that every training point from each class kis assigned a
posterior probability p(Ck|x) = 1 Furthermore, there is typically a continuum of Exercise 5.20
such solutions because any separating hyperplane will give rise to the same posterior
probabilities at the training data points Maximum likelihood provides no way to
favour one such solution over another, and which solution is found in practice will
depend on the choice of optimization algorithm and on the parameter initialization Note that the problem will arise even if the number of data points is large compared
with the number of parameters in the model, so long as the training data set is lin-
early separable The singularity can be avoided by adding a regularization term to
the error function Chapter 9
5.4.4 Multi-class logistic regression
In our discussion of generative models for multi-class classiÔ¨Åcation, we have Section 5.3
seen that, for a large class of distributions from the exponential family, the posterior
probabilities are given by a softmax transformation of linear functions of the feature
variables, so that
p(Ck|) =yk() =exp(ak)/summationtext
jexp(aj)(5.76)
where the pre-activations akare given by
ak=wT
k: (5.77)
There we used maximum likelihood to determine separately the class-conditional
densities and the class priors and then found the corresponding posterior probabilities
using Bayes‚Äô theorem, thereby implicitly determining the parameters {wk}

============================================================

=== CHUNK 163 ===
Palavras: 350
Caracteres: 2320
--------------------------------------------------
Here we
consider the use of maximum likelihood to determine the parameters {wk}of this
model directly To do this, we will require the derivatives of ykwith respect to all
the pre-activations aj These are given by Exercise 5.21
@yk
@aj=yk(Ikj‚àíyj) (5.78)
whereIkjare the elements of the identity matrix Next we write down the likelihood function This is most easily done using
the 1-of-K coding scheme in which the target vector tnfor a feature vector n
belonging to classCkis a binary vector with all elements zero except for element k,
which equals one The likelihood function is then given by
p(T|w 1;:::;wK) =N/productdisplay
n=1K/productdisplay
k=1p(Ck|n)tnk=N/productdisplay
n=1K/productdisplay
k=1ytnk
nk(5.79)
162 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
Figure 5.16 Representation of a multi-class lin-
ear classiÔ¨Åcation model as a neu-
ral network having a single layer
of connections Each basis func-
tion is represented by a node,
with the solid node represent-
ing the ‚Äòbias‚Äô basis function 0,
whereas each output y1;:::;yNis
also represented by a node The
links between the nodes represent
the corresponding weight and bias
parameters y1(
x;w)
whereynk=yk(
n), and Tis anN√óKmatrix of target variables with elements
tnk Taking the negative logarithm then gives
E(w1;:::;wK) =‚àílnp(T|w 1;:::;wK) =‚àíN/summationdisplay
n=1K/summationdisplay
k=1tnklnynk; (5.80)
which is known as the cross-entropy error function for the multi-class classiÔ¨Åcation
problem We now take the gradient of the error function with respect to one of the param-
eter vectors wj Making use of the result (5.78) for the derivatives of the softmax
function, we obtain Exercise 5.22
‚àáwjE(w1;:::;wK) =N/summationdisplay
n=1(ynj‚àítnj)n (5.81)
where we have made use of/summationtext
ktnk= 1 Again, we could optimize the parameters
through stochastic gradient descent Chapter 7
Once again, we see the same form arising for the gradient as was found for the
sum-of-squares error function with the linear model and for the cross-entropy error
with the logistic regression model, namely the product of the error (ynj‚àítnj)times
the basis function activation n These are examples of a more general result that
we will explore later Section 5.4.6
Linear classiÔ¨Åcation models can be represented as single-layer neural networks
as shown in Figure 5.16

============================================================

=== CHUNK 164 ===
Palavras: 389
Caracteres: 2396
--------------------------------------------------
If we consider the derivative of the error function with
respect to a weight wik, which links basis function i(x)to output unit tk, we have
from (5.81)
@E(w1;:::;wK)
@
wij=N/summationdisplay
n=1(ynk‚àítnk)i(xn): (5.82)
Comparing this with Figure 5.16, we see that, for each data point nthis gradient
takes the form of the output of the basis function at the input end of the weight link
with the ‚Äòerror‚Äô (ynk‚àítnk)at the output end Discriminative ClassiÔ¨Åers 163
Figure 5.17 Schematic example of a probability den-
sityp()shown by the blue curve, given in this example
by a mixture of two Gaussians, along with its cumulative
distribution function f(a), shown by the red curve Note
that the value of the blue curve at any point, such as
that indicated by the vertical green line, corresponds to
the slope of the red curve at the same point Conversely,
the value of the red curve at this point corresponds to the
area under the blue curve indicated by the shaded green
region In the stochastic threshold model, the class label
takes the value t= 1if the value of a=wTexceeds
a threshold, otherwise it takes the value t= 0 This is
equivalent to an activation function given by the cumula-
tive distribution function f(a) 0 1 2 3 400.20.40.60.81
5.4.5
Probit regression
We have seen that, for a broad range of class-conditional distributions described
by the exponential family, the resulting posterior class probabilities are given by a
logistic (or softmax) transformation acting on a linear function of the feature vari-
ables However, not all choices of class-conditional density give rise to such a simple
form for the posterior probabilities, which suggests that it might be worth exploring
other types of discriminative probabilistic model Consider the two-class case, again
remaining within the framework of generalized linear models, so that
p(t= 1|a) =f(a) (5.83)
wherea=wT, andf(¬∑)is the activation function One way to motivate an alternative choice for the link function is to consider a
noisy threshold model, as follows For each input n, we evaluate an=wTnand
then we set the target value according to
/braceleftBigg
tn= 1; ifan>;
tn= 0; otherwise:(5.84)
If the value of is drawn from a probability density p(), then the corresponding
activation function will be given by the cumulative distribution function
f(a) =/integraldisplaya
‚àí‚àûp() d (5.85)
as illustrated in Figure 5.17

============================================================

=== CHUNK 165 ===
Palavras: 361
Caracteres: 2271
--------------------------------------------------
As a speciÔ¨Åc example, suppose that the density p()is given by a zero-mean,
unit-variance Gaussian The corresponding cumulative distribution function is given
by
Œ¶(a) =/integraldisplaya
‚àí‚àûN(|0;1) d; (5.86)
164 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
which is known as the probit function It has a sigmoidal shape and is compared
with the logistic sigmoid function in Figure 5.12 Note that the use of a Gaussian
distribution with general mean and variances does not change the model because this
is equivalent to a re-scaling of the linear coefÔ¨Åcients w Many numerical packages
can evaluate a closely related function deÔ¨Åned by
erf(a) =2‚àö/integraldisplaya
0exp(
‚àí2=2) d (5.87)
and known as the erf function orerror function (not to be confused with the error
function of a machine learning model) It is related to the probit function by Exercise 5.23
Œ¶(a) =1
2/braceleftbigg
1
+1‚àö
2erf
(a)/bracerightbigg
: (5.88)
The generalized linear model based on a probit activation function is known as probit
regression We can determine the parameters of this model using maximum likeli-
hood by a straightforward extension of the ideas discussed earlier In practice, the
results found using probit regression tend to be like those of logistic regression One issue that can occur in practical applications is that of outliers, which can
arise for instance through errors in measuring the input vector xor through misla-
belling of the target value t Because such points can lie a long way to the wrong side
of the ideal decision boundary, they can seriously distort the classiÔ¨Åer The logistic
and probit regression models behave differently in this respect because the tails of
the logistic sigmoid decay asymptotically like exp(‚àíx) for|x|‚Üí‚àû , whereas for
the probit activation function, they decay like exp(‚àíx2), and so the probit model
can be signiÔ¨Åcantly more sensitive to outliers 5.4.6 Canonical link functions
For the linear regression model with a Gaussian noise distribution, the error
function, corresponding to the negative log likelihood, is given by (4.11) If we
take the derivative with respect to the parameter vector wof the contribution to the
error function from a data point n, this takes the form of the ‚Äòerror‚Äô yn‚àítntimes the
feature vectorn, whereyn=wTn

============================================================

=== CHUNK 166 ===
Palavras: 352
Caracteres: 2320
--------------------------------------------------
Similarly, for the combination of the logistic-
sigmoid activation function and the cross-entropy error function (5.74) and for the
softmax activation function with the multi-class cross-entropy error function (5.80),
we again obtain this same simple form We now show that this is a general result
of assuming a conditional distribution for the target variable from the exponential
family along with a corresponding choice for the activation function known as the
canonical link function We again make use of the restricted form (3.169) of exponential family distri-
butions Note that here we are applying the assumption of exponential family distri-
bution to the target variable t, in contrast to Section 5.3.4 where we applied it to the
input vector x We therefore consider conditional distributions of the target variable
of the form
p(t|;s ) =1
sh/parenleftbiggt
s/parenrightbigg
g(
) exp/braceleftbiggt
s/bracerightbigg
: (5.89)
5.4 Discriminative ClassiÔ¨Åers 165
Using the same line of argument as led to the derivation of the result (3.172), we see
that the conditional mean of t, which we denote by y, is given by
y‚â°E[t| ] =‚àísd
dlng(): (5.90)
Thus,yandmust related, and we denote this relation through = (y) Following Nelder and Wedderburn (1972), we deÔ¨Åne a generalized linear model
to be one for which yis a nonlinear function of a linear combination of the input (or
feature) variables so that
y=f(wT) (5.91)
wheref(¬∑)is known as the activation function in the machine learning literature, and
f‚àí1(¬∑)is known as the link function in statistics Now consider the log likelihood function for this model, which, as a function of
, is given by
lnp(t|;s ) =N/summationdisplay
n=1lnp(tn|;s) =N/summationdisplay
n=1/braceleftbigg
lng(n) +ntn
s/bracerightbigg
+ const (5.92)
where we are assuming that all observations share a common scale parameter (which
corresponds to the noise variance for a Gaussian distribution, for instance) and so s
is independent of n The derivative of the log likelihood with respect to the model
parameters wis then given by
‚àáwlnp(t|;s ) =N/summationdisplay
n=1/braceleftbiggd
dnlng(n) +tn
s/bracerightbiggdn
dyndyn
dan‚àáwan
=N/summationdisplay
n=11
s{tn‚àíyn} /prime(yn)f/prime(an)n (5.93)
wherean=wTn, and we have used yn=f(an)together with the result (5.90)
forE[t| ]

============================================================

=== CHUNK 167 ===
Palavras: 376
Caracteres: 2344
--------------------------------------------------
We now see that there is a considerable simpliÔ¨Åcation if we choose a
particular form for the link function f‚àí1(y)given by
f‚àí1(y) = (y); (5.94)
which gives f( (y)) =yand hencef/prime( ) /prime(y) = 1 Also, because a=f‚àí1(y),
we havea= and hencef/prime(a) /prime(y) = 1 In this case, the gradient of the error
function reduces to
‚àálnE(w) =1
sN/summationdisplay
n=1{yn‚àítn}n: (5.95)
We have seen that there is a natural pairing between the choice of error function
and the choice of output-unit activation function Although we have derived this
result in the context of single-layer network models, the same considerations apply
to deep neural networks discussed in later chapters SINGLE-LAYER NETWORKS: CLASSIFICATION
Exercises
5.1 (?)Consider a classiÔ¨Åcation problem with Kclasses and a target vector tthat uses a
1-of-K binary coding scheme Show that the conditional expectation E[t|x] is given
by the posterior probability p(Ck|x) 5.2 (??) Given a set of data points {xn}, we can deÔ¨Åne the convex hull to be the set of
all points xgiven by
x=/summationdisplay
nnxn (5.96)
wheren>0and/summationtext
nn= 1 Consider a second set of points {yn}together with
their corresponding convex hull By deÔ¨Ånition, the two sets of points will be linearly
separable if there exists a vector /hatwidewand a scalar w0such that/hatwidewTxn+w0>0for all
xnand/hatwidewTyn+w0<0for all yn Show that if their convex hulls intersect, the two
sets of points cannot be linearly separable, and conversely that if they are linearly
separable, their convex hulls do not intersect 5.3 (??) Consider the minimization of a sum-of-squares error function (5.14), and sup-
pose that all the target vectors in the training set satisfy a linear constraint
aTtn+b= 0 (5.97)
where tncorresponds to the nth row of the matrix Tin (5.14) Show that as a
consequence of this constraint, the elements of the model prediction y(x)given by
the least-squares solution (5.16) also satisfy this constraint, so that
aTy(x) +b= 0: (5.98)
To do so, assume that one of the basis functions 0(x) = 1 so that the corresponding
parameterw0plays the role of a bias 5.4 (??) Extend the result of Exercise 5.3 to show that if multiple linear constraints are
satisÔ¨Åed simultaneously by the target vectors, then the same constraints will also be
satisÔ¨Åed by the least-squares prediction of a linear model

============================================================

=== CHUNK 168 ===
Palavras: 352
Caracteres: 2262
--------------------------------------------------
5.5 (?)Use the deÔ¨Ånition (5.38), along with (5.30) and (5.31) to derive the result (5.39)
for the F-score 5.6 (??) Consider two non-negative numbers aandb, and show that, if a6b, then
a6(ab)1=2 Use this result to show that, if the decision regions of a two-class
classiÔ¨Åcation problem are chosen to minimize the probability of misclassiÔ¨Åcation,
this probability will satisfy
p(mistake) 6/integraldisplay
{p(x;C1)p(x;C2)}1=2dx: (5.99)
5.7 (?)Given a loss matrix with elements Lkj, the expected risk is minimized if, for
eachx, we choose the class that minimizes (5.23) Verify that, when the loss matrix
Exercises 167
is given byLkj= 1‚àíIkj, whereIkjare the elements of the identity matrix, this
reduces to the criterion of choosing the class having the largest posterior probability What is the interpretation of this form of loss matrix 5.8 (?)Derive the criterion for minimizing the expected loss when there is a general loss
matrix and general prior probabilities for the classes 5.9 (?)Consider the average of the posterior probabilities over a set of Ndata points in
the form
1
NN/summationdisplay
N=1p(Ck|xn): (5.100)
By taking the limit N‚Üí ‚àû, show that this quantity approaches the prior class
probabilityp(Ck) 5.10 (??) Consider a classiÔ¨Åcation problem in which the loss incurred when an input
vector from classCkis classiÔ¨Åed as belonging to class Cjis given by the loss matrix
Lkjand for which the loss incurred in selecting the reject option is  Find the
decision criterion that will give the minimum expected loss Verify that this reduces
to the reject criterion discussed in Section 5.2.3 when the loss matrix is given by
Lkj= 1‚àíIkj What is the relationship between and the rejection threshold  5.11 (?)Show that the logistic sigmoid function (5.42) satisÔ¨Åes the property (‚àía) =
1‚àí(a)and that its inverse is given by ‚àí1(y) = ln{y=(1‚àíy)} 5.12 (?)Using (5.40) and (5.41), derive the result (5.48) for the posterior class probability
in the two-class generative model with Gaussian densities, and verify the results
(5.49) and (5.50) for the parameters wandw0 5.13 (?)Consider a generative classiÔ¨Åcation model for Kclasses deÔ¨Åned by prior class
probabilities p(Ck) =kand general class-conditional densities p(|Ck)where
is the input feature vector

============================================================

=== CHUNK 169 ===
Palavras: 366
Caracteres: 2389
--------------------------------------------------
Suppose we are given a training data set {n;tn}where
n= 1;:::;N , and tnis a binary target vector of length Kthat uses the 1-of-K
coding scheme, so that it has components tnj=Ijkif data point nis from classCk Assuming that the data points are drawn independently from this model, show that
the maximum-likelihood solution for the prior probabilities is given by
k=Nk
N(5.101)
whereNkis the number of data points assigned to class Ck 5.14 (??) Consider the classiÔ¨Åcation model of Exercise 5.13 and now suppose that the
class-conditional densities are given by Gaussian distributions with a shared covari-
ance matrix, so that
p(|Ck) =N(|k;): (5.102)
168 5 SINGLE-LAYER NETWORKS: CLASSIFICATION
Show that the maximum likelihood solution for the mean of the Gaussian distribution
for classCkis given by
k=1
NkN/summationdisplay
n=1tnkn; (5.103)
which represents the mean of those feature vectors assigned to class Ck Similarly,
show that the maximum likelihood solution for the shared covariance matrix is given
by
=K/summationdisplay
k=1Nk
NSk (5.104)
where
Sk=1
NkN/summationdisplay
n=1tnk(n‚àík)(n‚àík)T: (5.105)
Thus, is given by a weighted average of the covariances of the data associated with
each class, in which the weighting coefÔ¨Åcients are given by the prior probabilities of
the classes 5.15 (??) Derive the maximum likelihood solution for the parameters {ki}of the proba-
bilistic naive Bayes classiÔ¨Åer with discrete binary features described in Section 5.3.3 5.16 (??) Consider a classiÔ¨Åcation problem with Kclasses for which the feature vector
hasMcomponents each of which can take Ldiscrete states Let the values of the
components be represented by a 1-of- Lbinary coding scheme Further suppose that,
conditioned on the class Ck, theMcomponents of are independent, so that the
class-conditional density factorizes with respect to the feature vector components Show that the quantities akgiven by (5.46), which appear in the argument to the
softmax function describing the posterior class probabilities, are linear functions of
the components of Note that this represents an example of a naive Bayes model Section 11.2.3
5.17 (??) Derive the maximum likelihood solution for the parameters of the probabilistic
naive Bayes classiÔ¨Åer described in Exercise 5.16 5.18 (?)Verify the relation (5.72) for the derivative of the logistic sigmoid function de-
Ô¨Åned by (5.42)

============================================================

=== CHUNK 170 ===
Palavras: 362
Caracteres: 2243
--------------------------------------------------
5.19 (?)By making use of the result (5.72) for the derivative of the logistic sigmoid, show
that the derivative of the error function (5.74) for the logistic regression model is
given by (5.75) 5.20 (?)Show that for a linearly separable data set, the maximum likelihood solution
for the logistic regression model is obtained by Ô¨Ånding a vector wwhose decision
boundary wT(x) = 0 separates the classes and then taking the magnitude of wto
inÔ¨Ånity 5.21 (?)Show that the derivatives of the softmax activation function (5.76), where the ak
are deÔ¨Åned by (5.77), are given by (5.78) Exercises 169
5.22 (?)Using the result (5.78) for the derivatives of the softmax activation function, show
that the gradients of the cross-entropy error (5.80) are given by (5.81) 5.23 (?)Show that the probit function (5.86) and the erf function (5.87) are related by
(5.88) 5.24 (??) Suppose we wish to approximate the logistic sigmoid (a)deÔ¨Åned by (5.42)
by a scaled probit function Œ¶(a), where Œ¶(a) is deÔ¨Åned by (5.86) Show that if is
chosen so that the derivatives of the two functions are equal at a= 0, then2==8 6
Deep Neural
Networks
In recent years, neural networks have emerged as, by far, the most important ma-
chine learning technology for practical applications, and we therefore devote a large
fraction of this book to studying them Previous chapters have already laid many
of the foundations we will need In particular, we have seen that linear regression
models that comprise linear combinations of Ô¨Åxed nonlinear basis functions can be
expressed as neural networks having a single layer of weight and bias parameters Chapter 4
Likewise, classiÔ¨Åcation models based on linear combinations of basis functions can
also be viewed as single-layer neural networks These allowed us to introduce several Chapter 5
important concepts before we embark on a discussion of more complex multilayered
networks in this chapter Given a sufÔ¨Åcient number of suitably chosen basis functions, such linear models
can approximate any given nonlinear transformation from inputs to outputs to any
desired accuracy and might therefore appear to be sufÔ¨Åcient to tackle any practical
171 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C

============================================================

=== CHUNK 171 ===
Palavras: 378
Caracteres: 2412
--------------------------------------------------
Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_6    
172 6 DEEP NEURAL NETWORKS
application However, these models have some severe limitations, and so we will
begin our discussion of neural networks by exploring these limitations and under-
standing why it is necessary to use basis functions that are themselves learned from
data This leads naturally to a discussion of neural networks having more than one
layer of learnable parameters These are known as feed-forward networks ormulti- Section 6.3.6
layer perceptrons We will also discuss the beneÔ¨Åts of having many such layers of
processing, leading to the key concept of deep neural networks that now dominate
the Ô¨Åeld of machine learning Limitations of Fixed Basis Functions
Linear basis function models for classiÔ¨Åcation are based on linear combinations of Chapter 5
basis functions j(x)and take the form
y(x;w) =fÔ£´
Ô£≠M/summationdisplay
j=1wjj(x) +w0Ô£∂
Ô£∏ (6.1)
wheref(¬∑)is a nonlinear output activation function Linear models for regression
take the same form but with f(¬∑)replaced by the identity These models allow for Chapter 4
an arbitrary set of nonlinear basis functions {i(x)}, and because of the generality
of these basis functions, such models can in principle provide a solution to any re-
gression or classiÔ¨Åcation problem This is true in a trivial sense in that if one of the
basis functions corresponds to the desired input-to-output transformation, then the
learnable linear layer simply has to copy the value of this basis function to the output
of the model More generally, we would expect that a sufÔ¨Åciently large and rich set of basis
functions would allow any desired function to be approximated to arbitrary accu-
racy It would seem therefore that such linear models constitute a general purpose
framework for solving problems in machine learning Unfortunately, there are some
signiÔ¨Åcant shortcomings with linear models, which arise from the assumption that
the basis functions j(x)are Ô¨Åxed and independent of the training data To under-
stand these limitations, we start by looking at the behaviour of linear models as the
number of input variables is increased 6.1.1 The curse of dimensionality
Consider a simple regression model for a single input variable given by a poly-
nomial of order Min the form Section 1.2
y(x;w) =w0+w1x+w2x2+:::+wMxM(6.2)
and let us see what happens if we increase the number of inputs

============================================================

=== CHUNK 172 ===
Palavras: 364
Caracteres: 2222
--------------------------------------------------
If we have Dinput
variables{x1;:::;xD}, then a general polynomial with coefÔ¨Åcients up to order 3
6.1 Limitations of Fixed Basis Functions 173
Figure 6.1 Plot of the Iris data in which red,
green, and blue points denote
three species of iris Ô¨Çower and the
axes represent measurements of
the length and width of the sepal,
respectively Our goal is to clas-
sify a new test point such as the
one denoted by  sepal lengthsepal width
w
ould take the form
y(x;w) =w0+D/summationdisplay
i=1wixi+D/summationdisplay
i=1D/summationdisplay
j=1wijxixj+D/summationdisplay
i=1D/summationdisplay
j=1D/summationdisplay
k=1wijkxixjxk:(6.3)
AsDincreases, the growth in the number of independent coefÔ¨Åcients is O(D3),
whereas for a polynomial of order M, the growth in the number of coefÔ¨Åcients is
O(DM)(Bishop, 2006) We see that in spaces of higher dimensionality, polynomials
can rapidly become unwieldy and of little practical utility The severe difÔ¨Åculties that can arise in spaces of many dimensions is sometimes
called the curse of dimensionality (Bellman, 1961) It is not limited to polynomial
regression but is in fact quite general Consider the use of linear models for solv-
ing classiÔ¨Åcation problems Figure 6.1 shows a plot of data from the Iris data set
comprising 50 observations taken from each of three species of iris Ô¨Çowers Each
observation has four variables representing measurements of the sepal length, sepal
width, petal length, and petal width For this illustration, we consider only the sepal
length and sepal width variables Given these 150 observations as training data, our
goal is to classify a new test point, such as the one denoted by the cross in Figure 6.1,
by assigning it to one of the three species We observe that the cross is close to sev-
eral red points, and so we might suppose that it belongs to the red class However,
there are also some green points nearby, so we might think that it could instead be-
long to the green class It seems less likely that it belongs to the blue class The
intuition here is that the identity of the cross should be determined more strongly by
nearby points from the training set and less strongly by more distant points, and this
intuition turns out to be reasonable

============================================================

=== CHUNK 173 ===
Palavras: 350
Caracteres: 1988
--------------------------------------------------
One very simple way of converting this intuition into a learning algorithm would
be to divide the input space into regular cells, as indicated in Figure 6.2 When we
are given a test point and we wish to predict its class, we Ô¨Årst decide which cell it
174 6 DEEP NEURAL NETWORKS
Figure 6.2 Illustration of a simple approach
for solving classiÔ¨Åcation problems
in which the input space is di-
vided into cells and any new test
point is assigned to the class that
has the most representatives in
the same cell as the test point As we shall see shortly, this sim-
plistic approach has some severe
shortcomings sepal lengthsepal width
belongs
to, and then we Ô¨Ånd all the training data points that fall in the same cell The
identity of the test point is predicted to be the same as the class having the largest
number of training points in the same cell as the test point (with ties being broken
at random) We can view this as a basis function model in which there is a basis
functioni(x)for each grid cell, which simply returns zero if xlies outside the
grid cell, and otherwise returns the majority class of the training data points that fall
inside the cell The output of the model is then given by the sum of the outputs of all
the basis functions There are numerous problems with this naive approach, but one of the most
severe becomes apparent when we consider its extension to problems having larger
numbers of input variables, corresponding to input spaces of higher dimensionality The origin of the problem is illustrated in Figure 6.3, which shows that, if we divide a
region of a space into regular cells, then the number of such cells grows exponentially
with the dimensionality of the space The challenge with an exponentially large
number of cells is that we would need an exponentially large quantity of training
Figure 6.3 Illustration of the curse
of dimensionality, showing how the
number of regions of a regular grid
grows exponentially with the dimen-
sionalityDof the space

============================================================

=== CHUNK 174 ===
Palavras: 350
Caracteres: 1989
--------------------------------------------------
For clarity,
only a subset of the cubical regions
are shown for D= 3 x1
D=1
x1x2
D=2
x1x2
x3
D=3
6.1 Limitations of Fixed Basis Functions 175
Figure 6.4 Plot of the fraction of the volume
of a hypersphere of radius r= 1
lying in the range r= 1‚àíto
r= 1for various values of the di-
mensionality D /epsilon1volume fractionD= 1D=
2D= 5D= 20
0 0.2 0.4 0.6 0.8 100.20.40.60.81
data
to ensure that the cells are not empty We have already seen in Figure 6.2 that
some cells contain no training points Hence, a test point in such cells cannot be
classiÔ¨Åed Clearly, we have no hope of applying such a technique in a space of more
than a few variables The difÔ¨Åculties with both the polynomial regression example
and the Iris data classiÔ¨Åcation example arise because the basis functions were chosen
independently of the problem being solved We will need to be more sophisticated
in our choice of basis functions if we are to circumvent the curse of dimensionality Section 6.1.4
6.1.2 High-dimensional spaces
First, however, we will look more closely at the properties of spaces with higher
dimensionality where our geometrical intuitions, formed through a life spent in a
space of three dimensions, can fail badly As a simple example, consider a hyper-
sphere of radius r= 1 in a space of Ddimensions, and ask what is the fraction of
the volume of the hypersphere that lies between radius r= 1‚àíandr= 1 We can
evaluate this fraction by noting that the volume VD(r)of a hypersphere of radius r
inDdimensions must scale as rD, and so we write
VD(r) =KDrD(6.4)
where the constant KDdepends only on D Thus, the required fraction is given by Exercise 6.1
VD(1)‚àíVD(1‚àí)
VD(1)=
1‚àí(1‚àí)D; (6.5)
which is plotted as a function of for various values of DinFigure 6.4 We see that,
for largeD, this fraction tends to 1even for small values of  Thus, we arrive at
the remarkable result that, in spaces of high dimensionality, most of the volume of a
hypersphere is concentrated in a thin shell near the surface

============================================================

=== CHUNK 175 ===
Palavras: 368
Caracteres: 2217
--------------------------------------------------
DEEP NEURAL NETWORKS
Figure 6.5 Plot of the probability density with
respect to radius rof a Gaussian
distribution for various values of
the dimensionality D In a high-
dimensional space, most of the
probability mass of a Gaussian
is located within a thin shell at a
speciÔ¨Åc radius D= 1
D= 2
D= 20
rp(r)
0 2 4012
As
a further example of direct relevance to machine learning, consider the be-
haviour of a Gaussian distribution in a high-dimensional space If we transform from
Cartesian to polar coordinates and then integrate out the directional variables, we ob-
tain an expression for the density p(r)as a function of radius rfrom the origin Thus, Exercise 6.3
p(r)ris the probability mass inside a thin shell of thickness rlocated at radius r This distribution is plotted, for various values of D, inFigure 6.5, and we see that
for largeD, the probability mass of the Gaussian is concentrated in a thin shell at a
speciÔ¨Åc radius In this book, we make extensive use of illustrative examples involving one or two
variables, because this makes it particularly easy to visualize these spaces graphi-
cally The reader should be warned, however, that not all intuitions developed in
spaces of low dimensionality will generalize to situations involving many dimen-
sions Finally, although we have talked about the curse of dimensionality, there can
also be advantages to working in high-dimensional spaces Consider the situation
shown in Figure 6.6 We see that this data set, in which each data point consists
of a pair of values (x1;x2), is linearly separable, but when only the value of x1is
observed, the classes have a strong overlap The classiÔ¨Åcation problem is therefore
much easier in the higher-dimensional space 6.1.3 Data manifolds
With both the polynomial regression model and the grid-based classiÔ¨Åer in Fig-
ure 6.2, we saw that the number of basis functions grows rapidly with dimensionality,
making such methods impractical for applications involving even a few dozen vari-
ables, never mind the millions of inputs that often arise with, say, image processing The problem is that the basis functions are Ô¨Åxed ahead of time and do not depend on
the data, or indeed even on the speciÔ¨Åc problem being solved

============================================================

=== CHUNK 176 ===
Palavras: 375
Caracteres: 2293
--------------------------------------------------
We need to Ô¨Ånd a way
to create basis functions that are tuned to the particular application Although the curse of dimensionality certainly raises important issues for ma-
chine learning applications, it does not prevent us from Ô¨Ånding effective techniques
applicable to high-dimensional spaces One reason for this is that real data will
generally be conÔ¨Åned to a region of the data space having lower effective dimen-
6.1 Limitations of Fixed Basis Functions 177
x1x2
(a)
x1 (b)
Figure 6.6 Illustration of a data set in two dimensions (x1;x2)in which data points from the two classes de-
picted using green and red circles can be separated by a linear decision surface, as seen in (a) If, however, only
the variable x1is measured then the classes are no longer separable, as seen in (b) Consider the images shown in Figure 6.7 Each image is a point in a
high-dimensional space whose dimensionality is determined by the number of pix-
els Because the objects can occur at different vertical and horizontal positions within
the image and in different orientations, there are three degrees of freedom of vari-
ability between images, and a set of images will, to a Ô¨Årst approximation, live on
a three-dimensional manifold embedded within the high-dimensional space Due to
the complex relationships between the object position or orientation and the pixel
intensities, this manifold will be highly nonlinear In fact, the number of pixels is really an artefact of the image generation pro-
cess since they represent measurements of a continuous world Capturing the same
image at a higher resolution increases the dimensionality Dof the data space with-
out changing the fact that the images still live on a three-dimensional manifold If
we can associate localized basis functions with the data manifold, rather than with
the entire high-dimensional data space, we might expect that the number of required
basis functions would grow exponentially with the dimensionality of the manifold
rather than with the dimensionality of the data space Since the manifold will typi-
cally have a much lower dimensionality than the data space, this represents a huge
Figure 6.7 Examples of images of a hand-
written digit that differ in the
location of the digit within
the images as well as in
their orientation

============================================================

=== CHUNK 177 ===
Palavras: 350
Caracteres: 2131
--------------------------------------------------
This data
lives on a nonlinear three-
dimensional manifold within the
high-dimensional image space DEEP NEURAL NETWORKS
Figure 6.8 The top row shows examples of natural images of size 64√ó64pixels, whereas the bottom
row shows randomly generated images of the same size obtained by drawing pixel values
from a uniform probability distribution over the possible pixel colours Effectively, neural networks learn a set of basis functions that are
adapted to data manifolds Moreover, for a particular application, not all directions
within the manifold may be signiÔ¨Åcant For example, if we wish to determine only
the orientation, and not the position, of the object in Figure 6.7, then there is only one
relevant degree of freedom on the manifold and not three Neural networks are also
able to learn which directions on the manifold are relevant to predicting the desired
outputs Another way to see that real data is conÔ¨Åned to low-dimensional manifolds is
to consider the task of generating random images In Figure 6.8 we see examples
of natural images along with examples of synthetic images of the same resolution
generated by sampling each of the red, green, and blue intensities at each pixel inde-
pendently at random from a uniform distribution We see that none of the synthetic
images look at all like natural images The reason is that these random images lack
the very strong correlations between pixels that natural images exhibit For example,
two adjacent pixels in a natural image have a much higher probability of having the
same, or very similar, colour, than would two adjacent images in the random exam-
ples Each of the images in Figure 6.8 corresponds to a point in a high-dimensional
space, yet natural images cover only a tiny fraction of this space 6.1.4 Data-dependent basis functions
We have seen that simple basis functions that are chosen independently of the
problem being solved can run into signiÔ¨Åcant limitations, particularly in spaces of
high dimensionality If we want to use basis functions in such situations, then one
approach would be to use expert knowledge to hand-craft the basis functions in a
6.1

============================================================

=== CHUNK 178 ===
Palavras: 350
Caracteres: 2190
--------------------------------------------------
Limitations of Fixed Basis Functions 179
way that is speciÔ¨Åc to each application For many years, this was the mainstream
approach in machine learning Basis functions, often called features, would be de-
termined by a combination of domain knowledge and trial-and-error However, this
approach met with limited success and was superseded by data-driven approaches
in which basis functions are learned from the training data Domain knowledge still
plays a role in modern machine learning, but at a more qualitative level in designing
network architectures where it can capture appropriate inductive bias, as we will see Section 9.1
in later chapters Since data in a high-dimensional space may be conÔ¨Åned to a low-dimensional
manifold, we do not need basis functions that densely Ô¨Åll the whole input space,
but instead we can use basis functions that are themselves associated with the data
manifold One way to do this is to have one basis function associated with each data
point in the training set, which ensures that the basis functions are automatically
adapted to the underlying data manifold An example of such a model is that of
radial basis functions (Broomhead and Lowe, 1988), which have the property that
each basis function depends only on the radial distance (typically Euclidean) from a
central vector If the basis centres are chosen to be the input data values {xn}then
there is one basis function n(x)for each data point, which will therefore capture
the whole of the data manifold A typical choice for a radial basis function is
n(x) = exp/parenleftBigg
‚àí/bardblx‚àíxn/bardbl2
s2/parenrightBigg
(6.6)
wheresis a parameter controlling the width of the basis function Although it can be
quick to set up such a model, a major problem with this technique is that it becomes
computationally unwieldy for large data sets Moreover, the model needs careful
regularization to avoid severe over-Ô¨Åtting A related approach, called a support vector machine or SVM (Vapnik, 1995;
Sch¬®olkopf and Smola, 2002; Bishop, 2006), addresses this by again deÔ¨Åning basis
functions that are centred on each of the training data points and then selecting a
subset of these automatically during training

============================================================

=== CHUNK 179 ===
Palavras: 387
Caracteres: 2476
--------------------------------------------------
As a result, the effective number of
basis functions in the resulting models is generally much smaller than the number of
training points, although it is often still relatively large and typically increases with
the size of the training set Support vector machines also do not produce probabilistic
outputs, and they do not naturally generalize to more than two classes Methods such
as radial basis functions and support vector machines have been superseded by deep
neural networks, which are much better at exploiting very large data sets efÔ¨Åciently Moreover, as we will see later, neural networks are able to learn deep hierarchical
representations, which are crucial to achieving high prediction accuracy in more
complex applications DEEP NEURAL NETWORKS
6.2 Multilayer Networks
In the previous section, we saw that to apply linear models of the form (6.1) to prob-
lems involving large-scale data sets and high-dimensional spaces, we need to Ô¨Ånd a
set of basis functions that is tuned to the problem being solved The key idea behind
neural networks is to choose basis functions j(x)that themselves have learnable
parameters and then allow these parameters to be adjusted, along with the coefÔ¨Å-
cients{wj}, during training We then optimize the whole model by minimizing an
error function using gradient-based optimization methods, such as stochastic gradi-
ent descent, where the error function is deÔ¨Åned jointly across all the parameters in Chapter 7
the model There are, of course, many ways to construct parametric nonlinear basis func-
tions One key requirement is that they must be differentiable functions of their
learnable parameters so that we can apply gradient-based optimization The most
successful choice has been to use basis functions that follow the same form as (6.1),
so that each basis function is itself a nonlinear function of a linear combination of
the inputs, where the coefÔ¨Åcients in the linear combination are learnable parameters Note that this construction can clearly be extended recursively to give a hierarchical
model with many layers, which forms the basis for deep neural networks Section 6.3
Consider a basic neural network model having two layers of learnable parame-
ters First, we construct Mlinear combinations of the input variables x1;:::;xDin
the form
a(1)
j=D/summationdisplay
i=1w(1)
jixi+w(1)
j0 (6.7)
wherej= 1;:::;M , and the superscript (1)indicates that the corresponding pa-
rameters are in the Ô¨Årst ‚Äòlayer‚Äô of the network

============================================================

=== CHUNK 180 ===
Palavras: 386
Caracteres: 2576
--------------------------------------------------
We will refer to the parameters w(1)
ji
asweights and the parameters w(1)
j0asbiases, while the quantities a(1)
jare called Chapter 4
pre-activations Each of the quantities ajis then transformed using a differentiable,
nonlinear activation function h(¬∑) to give
z(1)
j=h(a(1)
j); (6.8)
which represent the outputs of the basis functions in (6.1) In the context of neu-
ral networks, these basis functions are called hidden units We will explore various
choices for the nonlinear function h(¬∑) shortly, but here we note that provided the
derivativeh/prime(¬∑)can be evaluated, then the overall network function will be differen-
tiable Following (6.1), these values are again linearly combined to give
a(2)
k=M/summationdisplay
j=1w(2)
kjz(1)
j+w(2)
k0(6.9)
wherek= 1;:::;K , andKis the total number of outputs This transformation
corresponds to the second layer of the network, and again the w(2)
k0are bias parame-
ters Finally, the{a(2)
k}are transformed using an appropriate output-unit activation
6.2 Multilayer Networks 181
Figure 6.9 Network diagram for a two-layer
neural network The input, hid-
den, and output variables are
represented by nodes, and the
weight parameters are repre-
sented by links between the
nodes The bias parame-
ters are denoted by links com-
ing from additional input and
hidden variables x0andz0
which are themselves denoted
by solid nodes Arrows denote
the direction of information Ô¨Çow
through the network during for-
ward propagation.xDInputs y1w(1)
M
D
w(1)
10w(2)
K
M
w(2)
10
functionf(
¬∑)to give a set of network outputs yk A two-layer neural network can be
represented in diagram form as shown in Figure 6.9 6.2.1 Parameter matrices
As we discussed in the context of linear regression models, the bias parameters Section 4.1.1
in (6.7) can be absorbed into the set of weight parameters by deÔ¨Åning an additional
input variable x0whose value is clamped at x0= 1, so that (6.7) takes the form
aj=D/summationdisplay
i=0w(1)
jixi: (6.10)
We can similarly absorb the second-layer biases into the second-layer weights, so
that the overall network function becomes
yk(x;w) =fÔ£´
Ô£≠M/summationdisplay
j=0w(2)
kjh/parenleftBiggD/summationdisplay
i=0w(1)
jixi/parenrightBiggÔ£∂
Ô£∏: (6.11)
Another notation that will prove convenient at various points in the book is to repre-
sent the inputs as a column vector x= (x1;:::;xN)Tand then to gather the weight
and bias parameters in (6.11) into matrices to give
y(x;w) =f/parenleftbig
W(2)h/parenleftbig
W(1)x/parenrightbig/parenrightbig
(6.12)
wheref(¬∑)andh(¬∑) are evaluated on each vector element separately

============================================================

=== CHUNK 181 ===
Palavras: 360
Caracteres: 2281
--------------------------------------------------
6.2.2 Universal approximation
The capability of a two-layer network to model a broad range of functions is
illustrated in Figure 6.10 This Ô¨Ågure also shows how individual hidden units work
collaboratively to approximate the Ô¨Ånal function The role of hidden units in a simple
classiÔ¨Åcation problem is illustrated in Figure 6.11 DEEP NEURAL NETWORKS
Figure 6.10 Illustration of the ca-
pability of a two-layer neural network
to approximate four different func-
tions: (a)f(x) =x2, (b)f(x) =
sin(x), (c), f(x) =|x|, and (d)
f(x) =H(x)whereH(x)is the
Heaviside step function In each
case,N= 50 data points, shown as
blue dots, have been sampled uni-
formly inxover the interval (‚àí1; 1)
and the corresponding values of
f(x)evaluated These data points
are then used to train a two-layer
network having three hidden units
with tanh activation functions and
linear output units The resulting
network functions are shown by the
red curves, and the outputs of the
three hidden units are shown by the
three dashed curves (a)
 (b)
(c)
 (d)
The approximation properties of two-layer feed-forward networks were widely
studied in the 1980s, with various theorems showing that, for a wide range of activa-
tion functions, such networks can approximate any function deÔ¨Åned over a continu-
ous subset of RDto arbitrary accuracy (Funahashi, 1989; Cybenko, 1989; Hornik,
Stinchcombe, and White, 1989; Leshno et al., 1993) A similar result holds for func-
tions from any Ô¨Ånite-dimensional discrete space to any another Neural networks are
therefore said to be universal approximators Although such theorems are reassuring, they tell us only that there exists a net-
work that can represent the required function In some cases, they may require net-
works that have an exponentially large number of hidden units Moreover, they say
nothing about whether such a network can be found by a learning algorithm Fur-
thermore, we will see later that the no free lunch theorem says that we can never Ô¨Ånd Section 9.1.2
a truly universal machine learning algorithm Finally, although networks having two
layers of weights are universal approximators, in a practical application, there can
be huge beneÔ¨Åts in considering networks having many more than two layers that can
learn hierarchical internal representations

============================================================

=== CHUNK 182 ===
Palavras: 378
Caracteres: 2332
--------------------------------------------------
All these points support the drive towards
deep learning 6.2.3 Hidden unit activation functions
We have seen that the activation functions for the output units are determined
by the kind of distribution being modelled For the hidden units, however, the only
requirement is that they need to be differentiable, which leaves a wide range of pos-
6.2 Multilayer Networks 183
Figure 6.11 Example of the solution of a sim-
ple two-class classiÔ¨Åcation prob-
lem involving synthetic data us-
ing a neural network having two
inputs, two hidden units with tanh
activation functions, and a single
output having a logistic-sigmoid
activation function The dashed
blue lines show the z= 0:5 con-
tours for each of the hidden units,
and the red line shows the y=
0:5decision surface for the net-
work For comparison, the green
lines denote the optimal decision
boundary computed from the dis-
tributions used to generate the
data ‚àí2 ‚àí1 0 1 2‚àí2‚àí10123
sibilities In most cases, all the hidden units in a network will be given the same
activation function, although in principle there is no reason why different choices
could not be applied in different parts of the network The simplest option for a hidden unit activation function is the identity function,
which means that all the hidden units become linear However, for any such network,
we can always Ô¨Ånd an equivalent network without hidden units This follows from
the fact that the composition of successive linear transformations is itself a linear
transformation, and so its representational capability is no greater than that of a sin-
gle linear layer However, if the number of hidden units is smaller than either the
number of input or output units, then the transformations that such a network can
generate are not the most general possible linear transformation from inputs to out-
puts because information is lost in the dimensionality reduction at the hidden units Consider a network with Ninputs,Mhidden units, and Koutputs, and where all ac-
tivation functions are linear Such a network has M(N+K)parameters, whereas a
linear transformation of inputs directly to outputs would have NK parameters If M
is small relative to NorK, or both, this leads to a two-layer linear network having
fewer parameters than the direct linear mapping, corresponding to a rank-deÔ¨Åcient
transformation

============================================================

=== CHUNK 183 ===
Palavras: 364
Caracteres: 2268
--------------------------------------------------
Such ‚Äòbottleneck‚Äô networks of linear units corresponds to a standard
data analysis technique called principal component analysis In general, however, Chapter 16
there is limited interest in using multilayer networks of linear units since the overall
function computed by such a network is still linear A simple, nonlinear differentiable function is the logistic sigmoid given by
(a) =1
1
+ exp(‚àía); (6.13)
which is plotted in Figure 5.12 This was widely used in the early years of work on
multilayer neural networks and was partly inspired by studies of the properties of
184 6 DEEP NEURAL NETWORKS
‚àí2.5 0.0 2.5‚àí2.50.02.5
tanh
(a)
‚àí2.5 0.0 2.5‚àí2.50.02.5
hard tanh (b)
‚àí2.5 0.0 2.5‚àí2.50.02.5
softplus (c)
‚àí2.5 0.0 2.5‚àí2.50.02.5
ReLU
(d)
‚àí2.5 0.0 2.5‚àí2.50.02.5
leaky ReLU (e)
‚àí2.5 0.0 2.5‚àí2.50.02.5
absolute (f)
Figure
6.12 A variety of nonlinear activation functions A closely related function is tanh, which is deÔ¨Åned by
tanh(a) =ea‚àíe‚àía
ea+e‚àí
a; (6.14)
which is plotted in Figure 6.12(a) This function differs from the logistic sigmoid
by a linear transformation of its input and its output values, and so for any network
with logistic-sigmoid hidden-unit activation functions, there is an equivalent network
with tanh activation functions However, when training a network, these are not Exercise 6.4
necessarily equivalent because for gradient-based optimization, the network weights
and biases need to be initialized, and so if the activation functions are changed, then
the initialization scheme must be adjusted accordingly A ‚Äòhard‚Äô version of the tanh
function (Collobert, 2004) is given by
h(a) = max (‚àí1; min(1;a )) (6.15)
and is plotted in Figure 6.12(b) A major drawback of both the logistic sigmoid and the tanh activation functions
is that the gradients go to zero exponentially when the inputs have either large pos-
itive or large negative values We will discuss this ‚Äòvanishing gradients‚Äô issue later, Section 7.4.2
6.2 Multilayer Networks 185
but for the moment, we note that it will generally be better to use activation func-
tions with non-zero gradients, at least when the input takes a large positive value One such choice is the softplus activation function given by Exercise 6.7
h(a) = ln (1 + exp(a)) ; (6.16)
which is plotted in Figure 6.12(c)

============================================================

=== CHUNK 184 ===
Palavras: 363
Caracteres: 2177
--------------------------------------------------
For a/greatermuch1, we haveh(a)/similarequala, and so the gradient
remains non-zero even when the input to the activation function is large and positive,
thereby helping to alleviate the vanishing gradients problem An even simpler choice of activation function is the rectiÔ¨Åed linear unit orReLU,
which is deÔ¨Åned by
h(a) = max(0 ;a) (6.17)
and which is plotted in Figure 6.12(d) Empirically, this is one of the best-performing
activation functions, and it is in widespread use Note that strictly speaking, the
derivative of the ReLU function is not deÔ¨Åned when a= 0, but in practice this can
be safely ignored The softplus function (6.16) can be viewed as a smoothed version
of the ReLU and is therefore also sometimes called soft ReLU Exercise 6.5
Although the ReLU has a non-zero gradient for positive input values, this is
not the case for negative inputs, which can mean that some hidden units receive no
‚Äòerror signal‚Äô during training A modiÔ¨Åcation of ReLU that seeks to avoid this issue
is called a leaky ReLU and is deÔ¨Åned by
h(a) = max(0 ;a) +min(0;a ); (6.18)
where 0<< 1 This function is plotted in Figure 6.12(e) Unlike ReLU, this has a
nonzero gradient for input values a<0, which ensures that there is a signal to drive
training A variant of this activation function uses =‚àí1, in which case h(a) =|a|,
which is plotted in Figure 6.12(f) Another variant allows each hidden unit to have its
own valuej, which can be learned during network training by evaluating gradients
with respect to the {j}along with the gradients with respect to the weights and
biases The introduction of ReLU gave a big improvement in training efÔ¨Åciency over
previous sigmoidal activation functions (Krizhevsky, Sutskever, and Hinton, 2012) As well as allowing deeper networks to be trained efÔ¨Åciently, it is much less sensitive
to the random initialization of the weights It is also well suited to a low-precision
implementation, such as 8-bit Ô¨Åxed versus 64-bit Ô¨Çoating point, and it is computa-
tionally cheap to evaluate Many practical applications simply use ReLU units as
the default unless the goal is explicitly to explore the effects of different choices of
activation function

============================================================

=== CHUNK 185 ===
Palavras: 367
Caracteres: 2219
--------------------------------------------------
6.2.4 Weight-space symmetries
One property of feed-forward networks is that multiple distinct choices for the
weight vector wcan all give rise to the same mapping function from inputs to outputs
(Chen, Lu, and Hecht-Nielsen, 1993) Consider a two-layer network of the form
shown in Figure 6.9 withMhidden units having tanh activation functions and full
connectivity in both layers If we change the sign of all the weights and the bias
186 6 DEEP NEURAL NETWORKS
feeding into a particular hidden unit, then, for a given input data point, the sign
of the pre-activation of the hidden unit will be reversed, and therefore so too will
the activation, because tanh is an odd function, so that tanh(‚àía) =‚àítanh(a) This transformation can be exactly compensated for by changing the sign of all the
weights leading out of that hidden unit Thus, by changing the signs of a particular
group of weights (and a bias), the input‚Äìoutput mapping function represented by
the network is unchanged, and so we have found two different weight vectors that
give rise to the same mapping function For Mhidden units, there will be Msuch
‚Äòsign-Ô¨Çip‚Äô symmetries, and thus, any given weight vector will be one of a set 2M
equivalent weight vectors Similarly, imagine that we interchange the values of all of the weights (and the
bias) leading both into and out of a particular hidden unit with the corresponding
values of the weights (and bias) associated with a different hidden unit Again, this
clearly leaves the network input‚Äìoutput mapping function unchanged, but it cor-
responds to a different choice of weight vector For Mhidden units, any given
weight vector will belong to a set of M√ó(M‚àí1)√ó¬∑¬∑¬∑√ó 2√ó1 =M!equivalent
weight vectors associated with this interchange symmetry, corresponding to the M different orderings of the hidden units The network will therefore have an overall
weight-space symmetry factor of M For networks with more than two layers
of weights, the total level of symmetry will be given by the product of such factors,
one for each layer of hidden units It turns out that these factors account for all the symmetries in weight space
(except for possible accidental symmetries due to speciÔ¨Åc choices for the weight
values)

============================================================

=== CHUNK 186 ===
Palavras: 352
Caracteres: 2284
--------------------------------------------------
Furthermore, the existence of these symmetries is not a particular property
of the tanh function but applies to a wide range of activation functions (Kurkov ¬¥a and
Kainen, 1994) In general, these symmetries in weight space are of little practical
consequence, since network training aims to Ô¨Ånd a speciÔ¨Åc setting for the parameters,
and the existence of other, equivalent, settings is of little consequence However,
weight-space symmetries do play a role when Bayesian methods are used to evaluate
the probability distribution over networks of different sizes (Bishop, 2006) Deep
Networks
W
e have motivated the development of neural networks by making the basis func-
tions of a linear regression or classiÔ¨Åcation model themselves be governed by learn-
able parameters, giving rise to the two-layer network model shown in Figure 6.9 For
many years, this was the most widely used architecture, primarily because it proved
difÔ¨Åcult to train networks with more than two layers effectively However, extend-
ing neural networks to have more than two layers, known as deep neural networks,
brings many advantages as we will discuss shortly, and recent advances in techniques
for training neural networks are effective for networks with many layers Chapter 7
We can easily extend the two-layer network architecture (6.12) to any Ô¨Ånite num-
berLof layers, in which layer l= 1;:::;L computes the following function:
z(l)=h(l)/parenleftbig
W(l)z(l‚àí1)/parenrightbig
(6.19)
6.3 Deep Networks 187
whereh(l)denotes the activation function associated with layer l, andW(l)denotes
the corresponding matrix of weight and bias parameters Also, z(0)=xrepresents
the input vector and z(L)=yrepresents the output vector Note that there has been some confusion in the literature regarding the termi-
nology for counting the number of layers in such networks Thus, the network in
Figure 6.9 is sometimes described as a three-layer network (which counts the num-
ber of layers of units and treats the inputs as units) or sometimes as a single-hidden-
layer network (which counts the number of layers of hidden units) We recommend
a terminology in which Figure 6.9 is called a two-layer network, because it is the
number of layers of learnable weights that is important for determining the network
properties

============================================================

=== CHUNK 187 ===
Palavras: 362
Caracteres: 2219
--------------------------------------------------
We have seen that a network of the form shown in Figure 6.9, having two layers
of learnable parameters, has universal approximation capabilities However, net-
works with more than two layers can sometimes represent a given function with far
fewer parameters than a two-layer network (2014) show that the
network function divides the input space into a number of regions that is exponential
in the depth of the network, but which is only polynomial in the width of the hidden
layers To represent the same function using a two-layer network would require an
exponential number of hidden units 6.3.1 Hierarchical representations
Although this is an interesting result, a more compelling reason to explore deep
neural networks is that the network architecture encodes a particular form of induc-
tive bias, namely that the outputs are related to the input space through a hierarchical
representation A good example is the task of recognizing objects in images The Chapter 10
relationship between the pixels of an image and a high-level concept such as ‚Äòcat‚Äô is
highly complex and nonlinear, and would be an extremely challenging problem for
a two-layer network However, a deep neural network can learn to detect low-level
features, such as edges, in the early layers, and can then combine these in subse-
quent layers to make higher-level features such as eyes or whiskers, which in turn
can be combined in later layers to detect the presence of a cat This can be viewed
as a compositional inductive bias, in which higher-level objects, such as a cat, are
composed of lower-level objects, such as eyes, which in turn have yet lower-level el-
ements such as edges We can also think of this in reverse by considering the process
of generating an image starting with low-level features such as edges, then combin-
ing these to form simple shapes such as circles, and then combining those in turn to
form higher-level objects such as cats At each stage there are many ways to com-
bine different components, giving an exponential gain in the number of possibilities
with increasing depth 6.3.2 Distributed representations
Neural networks can take advantage of another form of compositionality called
adistributed representation

============================================================

=== CHUNK 188 ===
Palavras: 364
Caracteres: 2216
--------------------------------------------------
Conceptually, each unit in a hidden layer can be thought
of as representing a ‚Äòfeature‚Äô at that level of the network, with a high value of the
188 6 DEEP NEURAL NETWORKS
activation indicating that the corresponding feature is present and a low value indi-
cating its absence With Munits in a given layer, such a network can represent M
different features However, the network could potentially learn a different represen-
tation in which combinations of hidden units represent features, thereby potentially
allowing a hidden layer with Munits to represent 2Mdifferent features, growing
exponentially with the number of units Consider, for example, a network designed
to process images of faces Each particular face image may or may not have glasses,
it may or may not have a hat, and it may or may not have a beard, leading to eight
different combinations Although this could be represented by eight units each of
which ‚Äòturns on‚Äô when it detects the corresponding combination, it could also be
represented more compactly by just three units, one for each attribute These can be
present independently of each other (although statistically their presence is likely to
be correlated to some degree) Later, we will explore in detail the kinds of internal
representations that deep learning networks discover for themselves during training Chapter 10
6.3.3 Representation learning
We can view the successive layers of a deep neural network as performing trans-
formations of the data, that make it easier to solve the desired task or tasks For
example, a neural network that successfully learns to classify skin lesions as benign
or malignant must have learned to transform the original image data into a new space, Section 1.1.1
represented by the outputs of the Ô¨Ånal layer of hidden units, such that the Ô¨Ånal layer
of the network can distinguish the two classes This Ô¨Ånal layer can be viewed as a
simple linear classiÔ¨Åer, and so in the representation of the last hidden layer, the two
classes must be well separated by a linear surface This ability to discover a nonlin-
ear transformation of the data that makes subsequent tasks easier to solve is called
representation learning (Bengio, Courville, and Vincent, 2012)

============================================================

=== CHUNK 189 ===
Palavras: 359
Caracteres: 2236
--------------------------------------------------
The learned repre-
sentation, sometimes called the embedding space, is given by the outputs of one of
the hidden layers of the network, so that any input vector, either from the training set
or from some new data set, can be transformed into this representation by forward
propagation through the network Representation learning is especially powerful because it allows us to exploit
unlabelled data Often it is easy to collect a large quantity of unlabelled data, but
acquiring the associated labels may be more difÔ¨Åcult For example, a video camera
on a vehicle can gather large numbers of images of urban scenes as the vehicle is
driven around a city, but taking those images and identifying relevant objects, such
as pedestrians and road signs, would require expensive and time-consuming human
labelling Learning from unlabelled data is called unsupervised learning, and many differ-
ent algorithms have been developed to do this For example, a neural network can be
trained to take images as input and to create the same images as the output To make
this into a non-trivial task, the network may use hidden layers with fewer units than
the number of pixels in the image, thereby forcing the network to learn some kind of
compression of the images Only unlabelled data is needed because each image in
the training set acts as both the input vector and the target vector Such networks are
known as autoencoders The goal is that this type of training will force the network Section 19.1
6.3 Deep Networks 189
to discover some internal representation for the data that is useful for solving other
tasks, such as image classiÔ¨Åcation Historically, unsupervised learning played an important role in enabling the Ô¨Årst
deep networks (apart from convolutional networks) to be successfully trained Each
layer of the network was Ô¨Årst pre-trained using unsupervised learning and then the
entire network was trained further using gradient-based supervised training It was
later discovered that the pre-training phase could be omitted and a deep network
could be trained from scratch purely using supervised learning given appropriate
conditions However, pre-training and representation learning remain central to deep learn-
ing in other contexts

============================================================

=== CHUNK 190 ===
Palavras: 393
Caracteres: 2377
--------------------------------------------------
The most notable example of pre-training is in natural language
processing in which transformer models are trained on large quantities of text and are Chapter 12
able to learn highly sophisticated internal representations of language that facilitates
an impressive range of capabilities at human level and beyond 6.3.4 Transfer learning
The internal representation learned for one particular task might also be useful
for related tasks For example, a network trained on a large labelled data set of
everyday objects can learn how to transform an image representation into one that is
much better suited for classifying objects Then, the Ô¨Ånal classiÔ¨Åcation layer of the
network can be retrained using a smaller labelled data set of skin lesion images to
create a lesion classiÔ¨Åer This is an example of transfer learning (Hospedales et al., Section 1.1.1
2021), which allows higher accuracy to be achieved than if only the lesion image
data were used for training, because the network can exploit commonalities shared
by natural images in general Transfer learning is illustrated in Figure 6.13 In general, transfer learning can be used to improve performance on some task
A, for which training data is in short supply, by using data from a related task B, for
which data is more plentiful The two tasks should have the same kind of inputs,
and there should be some commonality between the tasks so that low-level features,
or internal representations, learned from task B will be useful for task A When we
look at convolutional networks we will see that many image processing tasks require Chapter 10
similar low-level features corresponding to the early layers of a deep neural network,
whereas later layers are more specialized to a particular task, making such networks
well suited to transfer learning applications When data for task A is very scarce, we might simply re-train the Ô¨Ånal layer of
the network In contrast, if there are more data points, it is feasible to retrain several
layers The process of learning parameters using one task that are then applied to
one or more other tasks is called pre-training Note that for the new task, instead
of applying stochastic gradient descent to the whole network, it is much more ef-
Ô¨Åcient to send the new training data once through the Ô¨Åxed pre-trained network so
as to evaluate the training inputs in the new representation

============================================================

=== CHUNK 191 ===
Palavras: 353
Caracteres: 2068
--------------------------------------------------
Iterative gradient-based
optimization can then be applied just to the smaller network consisting of the Ô¨Ånal
layers As well as using a pre-trained network as a Ô¨Åxed pre-processor for a different
task, it is also possible to apply Ô¨Åne-tuning in which the whole network is adapted to
the data for task A This is generally done with a very small learning rate for a lim-
190 6 DEEP NEURAL NETWORKS
tree
cat
dog (a)
cancer
normal
(b)
Figure 6.13 Schematic illustration of transfer learning (a) A network is Ô¨Årst trained on a task with abundant
data, such as object classiÔ¨Åcation of natural images (b) The early layers of the network (shown in red) are
copied from the Ô¨Årst task and the Ô¨Ånal few layers of the network (shown in blue) are then retrained on a new task
such as skin lesion classiÔ¨Åcation for which training data is more scarce ited number of iterations to ensure that the network does not over-Ô¨Åt to the relatively
small data set available for the new task A related approach is multitask learning (Caruana, 1997) in which a network
jointly learns more than one related task at the same time For example, we might
wish to construct a spam email Ô¨Ålter that allows different users to have different
classiÔ¨Åers tuned to their particular preferences The training data may comprise ex-
amples of spam email and non-spam email for many different users, but the number
of examples for any one user may be quite limited, and therefore training a separate
classiÔ¨Åer for each user would give poor results Instead, we can combine the data
sets to train a single larger network that might, for example, share early layers but
have separate learnable parameters for the different users in later layers Sharing
data across tasks allows the network to exploit commonalities amongst the tasks,
thereby improving the accuracy for all users With a large number of training exam-
ples, a deeper network with more parameters can be used, again leading to improved
performance Learning across multiple tasks can be extended to meta-learning , which is also
called learning to learn

============================================================

=== CHUNK 192 ===
Palavras: 366
Caracteres: 2191
--------------------------------------------------
Whereas multitask learning aims to make predictions for
a Ô¨Åxed set of tasks, the aim of meta-learning is to make predictions for future tasks
that were not seen during training This can be done by not only learning a shared
6.3 Deep Networks 191
internal representation across tasks but also by learning the learning algorithm itself
(Hospedales et al., 2021) Meta-learning can be used to facilitate generalization of,
for example, a classiÔ¨Åcation model to new classes when there are very few labelled
examples of the new classes This is referred to as few-shot learning When only a
single labelled example is used it is called one-shot learning 6.3.5 Contrastive learning
One of the most common and powerful representation learning methods is con-
trastive learning (Gutmann and Hyv ¬®arinen, 2010; Oord, Li, and Vinyals, 2018;
Chen, Kornblith, et al., 2020) The idea is to learn a representation such that cer-
tain pairs of inputs, referred to as positive pairs, are close in the embedding space,
and other pairs of inputs, called negative pairs, are far apart The intuition is that
if we choose our positive pairs in such a way that they are semantically similar and
choose negative pairs that are semantically dissimilar, then we will learn a represen-
tation space in which similar inputs are close, making downstream tasks, such as
classiÔ¨Åcation, much easier As with other forms of representation learning, the out-
puts of the trained network are typically not used directly, and instead the activations
at some earlier layer are used to form the embedding space Contrastive learning is
unlike most other machine learning tasks, in that the error function for a given input
is deÔ¨Åned only with respect to other inputs, instead of having a per-input label or
target output Suppose we have a given data point xcalled the anchor, for which we have spec-
iÔ¨Åed another data point x+that together with xmakes up a positive pair We must
also specify a set of data points {x‚àí
1;:::;x‚àí
N}each of which makes up a negative
pair with x We now need a loss function that will reward close proximity between
the representations of xandx+while encouraging a large distance between each pair
{x;x‚àí
n}

============================================================

=== CHUNK 193 ===
Palavras: 351
Caracteres: 2218
--------------------------------------------------
One example of such a function, and the most commonly used loss func-
tion for contrastive learning, is called the InfoNCE loss (Gutmann and Hyv ¬®arinen,
2010; Oord, Li, and Vinyals, 2018), where NCE denotes ‚Äònoise contrastive estima-
tion‚Äô Suppose we have a neural network function fw(x)that maps points from the
input space xto a representation space, governed by learnable parameters w This
representation is normalized so that ||fw(x)|| = 1 Then, for a data point x, the
InfoNCE loss is deÔ¨Åned by
E(w) =‚àílnexp{f w(x)Tfw(x+)}
exp{f w(x)Tfw(x+)}+/summationtextN
n=1exp{f w(x)Tfw(x‚àín)}: (6.20)
We can see that in this function, the cosine similarity fw(x)Tfw(x+)between the
representation fw(x)of the anchor and the representation fw(x+)of the positive
example provides our measure of how close the positive pair examples are in the
learned space, and the same measure is used to assess how close the anchor is to the
negative examples Note that the function resembles a classiÔ¨Åcation cross-entropy
error function in which the cosine similarity of the positive pair gives the logit for
the label class and the cosine similarities for the negative pairs give the logits for the
incorrect classes Also note that the negative pairs are crucial as without them the
192 6 DEEP NEURAL NETWORKS
embedding would simply learn the degenerate solution of mapping every point to the
same representation A particular contrastive learning algorithm is deÔ¨Åned predominantly by how the
positive and negative pairs are chosen, which is how we use our prior knowledge to
specify what a good representation should be For example, consider the problem of
learning representations of images Here, a common choice is to create positive pairs
by corrupting the input images in ways that should preserve the semantic information
of the image while greatly altering the image in the pixel space (Wu et al., 2018; He
et al., 2019; Chen, Kornblith, et al., 2020) Corruptions are closely related to data
augmentations, and examples include rotation, translation, and colour shifts Other Section 9.1.3
images from the data set can then be used to create the negative pairs This approach
to contrastive learning is known as instance discrimination

============================================================

=== CHUNK 194 ===
Palavras: 356
Caracteres: 2251
--------------------------------------------------
If, however, we have access to class labels, then we can use images of the same
class as positive pairs and images of different classes as negative pairs This re-
laxes the reliance on specifying the augmentations that the representation should be
invariant to and also avoids treating two semantically similar images as a negative
pair This is referred to as supervised contrastive learning (Khosla et al., 2020) be-
cause of the reliance on the class labels, and it can often yield better results than
simply learning the representation using cross-entropy classiÔ¨Åcation The members of positive and negative pairs do not necessarily have to come
from the same data modality In contrastive-language image pretraining, or CLIP
(Radford et al., 2021), a positive pair consists of an image and its corresponding
text caption, and two separate functions, one for each modality, are used to map the
inputs to the same representation space Negative pairs are then mismatched images
and captions This is often referred to as weakly supervised, as it relies on captioned
images, which are often easier to obtain by scraping data from the internet than by
manually labelling images with their classes The loss function in this case is given
by
E(w) =‚àí1
2lnexp{f w(x+)Tg(y+)}
exp{f w(x+)Tg(y+)}+/summationtextN
n=1exp{f w(x‚àín)Tg(y+)}
‚àí1
2lnexp{f w(x+)Tg(y+)}
exp{f w(x+)Tg(y+)}+/summationtextM
m=1exp{f w(x+)Tg(y‚àím)}(6.21)
where x+andy+represent a positive pair in which xis an image and yis its corre-
sponding text caption, fwrepresents the mapping from images to the representation
space, and gis the mapping from text input to the representation space We also
require a set{x‚àí
1;:::;x‚àí
N}of other images from the data set, for which we can
assume the text caption y+is inappropriate, and a set {y‚àí
1;:::;y‚àí
M}of text cap-
tions that are similarly mismatched to the input image x The two terms in the loss
function ensure that (a) the representation of the image is close to its text caption
representation relative to other image representations and (b) the text caption rep-
resentation is close to the representation of the image it describes relative to other
representations of text captions Although CLIP uses text and image pairs, any data
6.3

============================================================

=== CHUNK 195 ===
Palavras: 399
Caracteres: 2506
--------------------------------------------------
Deep Networks 193
x x+x‚àífw(x)
fw(x+)fw(x‚àí)
(a)
x x+x‚àífw(x)
fw(x+)fw(x‚àí)
(b)
‚Äòa ginger
cat sat
on a wall‚Äô‚Äòa bike
leaning
on a
brick wall‚Äô
x+y+y‚àífw(x+)
g(y+)g(y‚àí)
(c)
Figure 6.14 Illustration of three different contrastive learning paradigms (a) The instance discrimination ap-
proach, where the positive pair is made up of the anchor and an augmented version of the same image These
are mapped to points in a normalized space that can be thought of as a unit hypersphere The coloured arrows
show that the loss encourages the representations of the positive pair to be closer together but pushes negative
pairs further apart (b) Supervised contrastive learning in which the positive pair consists of two different images
from the same class (c) The CLIP model in which the positive pair is made up of an image and an associated
text snippet set with paired modalities can be used to learn representations A comparison of the
different contrastive learning methods we have discussed is shown in Figure 6.14 6.3.6 General network architectures
So far, we have explored neural network architectures that are organized into a
sequence of fully-connected layers However, because there is a direct correspon-
dence between a network diagram and its mathematical function, we can develop
more general network mappings by considering more complex network diagrams These must be restricted to a feed-forward architecture, in other words to one having
no closed directed cycles, to ensure that the outputs are deterministic functions of
the inputs This is illustrated with a simple example in Figure 6.15 Each (hidden or
output) unit in such a network computes a function given by
zk=hÔ£´
Ô£≠/summationdisplay
j‚ààA(k)wkjzj+bkÔ£∂
Ô£∏ (6.22)
whereA(k)denotes the set of ancestors of nodek, in other words the set of units
that send connections to unit k, andbkdenotes the associated bias parameter For
a given set of values applied to the inputs of the network, successive application of
(6.22) allows the activations of all units in the network to be evaluated including
those of the output units DEEP NEURAL NETWORKS
Figure 6.15 Example of a neural network
having a general feed-forward topology Note
that each hidden and output unit has an asso-
ciated bias parameter (omitted for clarity).x1inputs
x2z1z2
z3z2
z3y1outputs
y2
6.3.7 Tensors
We see that linear algebra plays a central role in neural networks, with quantities
such as data sets, activations, and network parameters represented as scalars, vectors,
and matrices

============================================================

=== CHUNK 196 ===
Palavras: 402
Caracteres: 2635
--------------------------------------------------
However, we also encounter variables of higher dimensionality Con-
sider, for example, a data set of Ncolour images each of which is Ipixels high and
Jpixels wide Each pixel is indexed by its row and column within the image and
has red, green, and blue values We have one such value for each image in the data
set, and so we can represent a particular intensity value by a four-dimensional array
Xwith elements xijkn wherei‚àà{1;:::;I}andj‚àà{1;:::;J}index the row
and column within the image, k‚àà{1; 2;3}indexes the red, green, and blue inten-
sities, andn‚àà{1;:::;N}indexes the particular image within the data set These
higher-dimensional arrays are called tensors and include scalars, vectors, and matri-
ces as special cases We will see many examples of such tensors when we discuss
more sophisticated neural network architectures later in the book Massively parallel
processors such as GPUs are especially well suited to processing tensors Error Functions
In earlier chapters, we explored linear models for regression and classiÔ¨Åcation, and Chapter 4
Chapter 5 in the process we derived suitable forms for the error functions along with corre-
sponding choices for the output-unit activation function The same considerations
for choosing an error function apply for multilayer neural networks, and so for con-
venience, we will summarize the key points here 6.4.1 Regression
We start by discussing regression problems, and for the moment we consider
a single target variable tthat can take any real value Following the discussion of
regression in single-layer networks, we assume that thas a Gaussian distribution Section 2.3.4
with an x-dependent mean, which is given by the output of the neural network, so
that
p(t|x; w) =N/parenleftbig
t|y(x;w);2/parenrightbig
(6.23)
6.4 Error Functions 195
where2is the variance of the Gaussian noise Of course this is a somewhat restric-
tive assumption, and in some applications we will need to extend this approach to
allow for more general distributions For the conditional distribution given by (6.23), Section 6.5
it is sufÔ¨Åcient to take the output-unit activation function to be the identity, because
such a network can approximate any continuous function from xtoy Given a data
set ofNi.i.d observations X={x1;:::;xN}, along with corresponding target
values t={t1;:::;tN}, we can construct the corresponding likelihood function:
p(t|X; w;2) =N/productdisplay
n=1p(tn|y(xn;w);2): (6.24)
Note that in the machine learning literature, it is usual to consider the minimization
of an error function rather than the maximization of the likelihood, and so here we
will follow this convention

============================================================

=== CHUNK 197 ===
Palavras: 354
Caracteres: 2424
--------------------------------------------------
Taking the negative logarithm of the likelihood function
(6.24), we obtain the error function
1
22N/summationdisplay
n=1{y(xn;w)‚àítn}2+N
2ln2+N
2ln(2 ); (6.25)
which can be used to learn the parameters wand2 Consider Ô¨Årst the determination
ofw Maximizing the likelihood function is equivalent to minimizing the sum-of-
squares error function given by
E(w) =1
2N/summationdisplay
n=1{y(xn;w)‚àítn}2(6.26)
where we have discarded additive and multiplicative constants The value of w
found by minimizing E(w)will be denoted w Note that this will typically not
correspond to the global maximum of the likelihood function because the nonlin-
earity of the network function y(xn;w)causes the error E(w)to be non-convex,
and so Ô¨Ånding the global optimum is generally infeasible Moreover, regularization
terms may be added to the error function and other modiÔ¨Åcations may be made to Chapter 9
the training process, so that the resulting solution for the network parameters may
differ signiÔ¨Åcantly from the maximum likelihood solution Having found w?, the value of 2can be found by minimizing the error function
(6.25) to give Exercise 6.8
2?=1
NN/summationdisplay
n=1{y(xn;w?)‚àítn}2: (6.27)
Note that this can be evaluated once the iterative optimization required to Ô¨Ånd w?is
completed If we have multiple target variables, and we assume that they are independent,
conditional on xandw, with shared noise variance 2, then the conditional distri-
bution of the target values is given by
p(t|x; w) =N/parenleftbig
t|y(x;w);2I/parenrightbig
: (6.28)
196 6 DEEP NEURAL NETWORKS
Following the same argument as for a single target variable, we see that maximizing
the likelihood function with respect to the weights is equivalent to minimizing the
sum-of-squares error function: Exercise 6.9
E(w) =1
2N/summationdisplay
n=1/bardbly(xn;w)‚àítn/bardbl2: (6.29)
The noise variance is then given by
2?=1
NKN/summationdisplay
n=1/bardbly(xn;w?)‚àítn/bardbl2(6.30)
whereKis the dimensionality of the target variable The assumption of conditional
independence of the target variables can be dropped at the expense of a slightly more
complex optimization problem Exercise 6.10
Recall that there is a natural pairing of the error function (given by the negative
log likelihood) and the output-unit activation function In regression, we can view the Section 5.4.6
network as having an output activation function that is the identity, so that yk=ak

============================================================

=== CHUNK 198 ===
Palavras: 353
Caracteres: 2422
--------------------------------------------------
The corresponding sum-of-squares error function then has the property
@E
@ak=yk‚àítk: (6.31)
6.4.2 Binary classiÔ¨Åcation
Now consider binary classiÔ¨Åcation in which we have a single target variable
tsuch thatt= 1 denotes classC1andt= 0 denotes classC2 Following the
discussion of canonical link functions, we consider a network having a single output Section 5.4.6
whose activation function is a logistic sigmoid (6.13) so that 06y(x;w)61 We
can interpret y(x;w)as the conditional probability p(C1|x), withp(C2|x)given by
1‚àíy(x;w) The conditional distribution of targets given inputs is then a Bernoulli
distribution of the form
p(t|x; w) =y(x;w)t{1‚àíy(x;w)}1‚àít: (6.32)
If we consider a training set of independent observations, then the error function,
which is given by the negative log likelihood, is then a cross-entropy error of the
form
E(w) =‚àíN/summationdisplay
n=1{tnlnyn+ (1‚àítn) ln(1‚àíyn)} (6.33)
whereyndenotesy(xn;w) Simard, Steinkraus, and Platt (2003) found that using
the cross-entropy error function instead of the sum-of-squares for a classiÔ¨Åcation
problem leads to faster training as well as improved generalization Note that there is no analogue of the noise variance 2in (6.32) because the
target values are assumed to be correctly labelled However, the model is easily
extended to allow for labelling errors by introducing a probability that the target Exercise 6.11
6.4 Error Functions 197
valuethas been Ô¨Çipped to the wrong value (Opper and Winther, 2000) Here may
be set in advance, or it may be treated as a hyperparameter whose value is inferred
from the data If we haveKseparate binary classiÔ¨Åcations to perform, then we can use a net-
work having Koutputs each of which has a logistic-sigmoid activation function Associated with each output is a binary class label tk‚àà{0;1}, wherek= 1;:::;K If we assume that the class labels are independent, given the input vector, then the
conditional distribution of the targets is
p(t|x; w) =K/productdisplay
k=1yk(x;w)tk[1‚àíyk(x;w)]1‚àítk: (6.34)
Taking the negative logarithm of the corresponding likelihood function then gives
the following error function: Exercise 6.13
E(w) =‚àíN/summationdisplay
n=1K/summationdisplay
k=1{tnklnynk+ (1‚àítnk) ln(1‚àíynk)} (6.35)
whereynkdenotesyk(xn;w) Again, the derivative of the error function with re-
spect to the pre-activation for a particular output unit takes the form (6.31), just as in Exercise 6.14
the regression case

============================================================

=== CHUNK 199 ===
Palavras: 376
Caracteres: 2547
--------------------------------------------------
6.4.3 multiclass classiÔ¨Åcation
Finally, we consider the standard multiclass classiÔ¨Åcation problem in which each
input is assigned to one of Kmutually exclusive classes The binary target variables
tk‚àà {0; 1}have a 1-of- Kcoding scheme indicating the class, and the network Section 5.1.3
outputs are interpreted as yk(x;w) =p(tk= 1|x), leading to the error function
(5.80), which we reproduce here:
E(w) =‚àíN/summationdisplay
n=1K/summationdisplay
k=1tknlnyk(xn;w): (6.36)
The output-unit activation function, which corresponds to the canonical link, is given
by the softmax function: Section 5.4.4
yk(x;w) =exp(ak(x;w))/summationdisplay
jexp(aj(x;w)); (6.37)
which satisÔ¨Åes 06yk61and/summationtext
kyk= 1 Note that the yk(x;w)are unchanged
if a constant is added to all of the ak(x;w), causing the error function to be constant
for some directions in weight space This degeneracy is removed if an appropriate
regularization term is added to the error function Once again, the derivative of the Chapter 9
error function with respect to the pre-activation for a particular output unit takes the
familiar form (6.31) DEEP NEURAL NETWORKS
In summary, there is a natural choice of both output-unit activation function
and matching error function according to the type of problem being solved For
regression, we use linear outputs and a sum-of-squares error, for multiple indepen-
dent binary classiÔ¨Åcations, we use logistic sigmoid outputs and a cross-entropy error
function, and for multi-class classiÔ¨Åcation, we use softmax outputs with the corre-
sponding multi-class cross-entropy error function For classiÔ¨Åcation problems in-
volving two classes, we can use a single logistic sigmoid output, or alternatively, we
can use a network with two outputs having a softmax output activation function This procedure is quite general, and by considering other forms of conditional
distribution, we can derive the associated error functions as the corresponding neg-
ative log likelihood We will see an example of this in the next section when we
consider multimodal network outputs Mixture
Density Networks
So
far in this chapter we have discussed neural networks whose outputs represent
simple probability distributions comprising either a Gaussian for continuous vari-
ables or a binary distribution for discrete variables We close the chapter by showing
how a neural network can represent more general conditional probabilities by treat-
ing the outputs of the network as the parameters of a more complex distribution, in
this case a Gaussian mixture model

============================================================

=== CHUNK 200 ===
Palavras: 350
Caracteres: 2123
--------------------------------------------------
This is known as a mixture density network,
and we will see how to deÔ¨Åne the associated error function and the corresponding
output-unit activation functions 6.5.1 Robot kinematics example
The goal of supervised learning is to model a conditional distribution p(t|x),
which for many simple regression problems is chosen to be Gaussian However,
practical machine learning problems can often have signiÔ¨Åcantly non-Gaussian dis-
tributions These can arise, for example, with inverse problems in which the distri-
bution can be multimodal, in which case the Gaussian assumption can lead to very
poor predictions As a simple illustration of an inverse problem, consider the kinematics of a robot
arm, as illustrated in Figure 6.16 The forward problem involves Ô¨Ånding the end ef- Exercise 6.16
fector position given the joint angles and has a unique solution However, in practice
we wish to move the end effector of the robot to a speciÔ¨Åc position, and to do this we
must set appropriate joint angles We therefore need to solve the inverse problem,
which has two solutions, as seen in Figure 6.16 Forward problems often correspond to causality in a physical system and gen-
erally have a unique solution For instance, a speciÔ¨Åc pattern of symptoms in the
human body may be caused by the presence of a particular disease In machine
learning, however, we typically have to solve an inverse problem, such as trying to
predict the presence of a disease given a set of symptoms If the forward problem
involves a many-to-one mapping, then the inverse problem will have multiple solu-
tions For instance, several different diseases may result in the same symptoms Mixture Density Networks 199
Figure 6.16 (a) A two-link robot
arm, in which the Cartesian coor-
dinates (x1;x2)of the end effector
are determined uniquely by the two
joint angles 1and2and the (Ô¨Åxed)
lengthsL1andL2of the arms This
is known as the forward kinematics
of the arm (b) In practice, we have
to Ô¨Ånd the joint angles that will give
rise to a desired end effector posi-
tion This inverse kinematics has
two solutions corresponding to ‚Äòel-
bow up‚Äô and ‚Äòelbow down‚Äô

============================================================

=== CHUNK 201 ===
Palavras: 354
Caracteres: 2223
--------------------------------------------------
L1L2
12(x1;x2)(a)
(x1; x2)
elb
ow
downelbow
up (b)
In
the robotics example, the kinematics is deÔ¨Åned by geometrical equations, and
the multimodality is readily apparent However, in many machine learning problems
the presence of multimodality, particularly in problems involving spaces of high di-
mensionality, can be less obvious For tutorial purposes, however, we will consider
a simple toy problem for which we can easily visualize the multimodality The data
for this problem is generated by sampling a variable xuniformly over the interval
(0;1), to give a set of values {xn}, and the corresponding target values tnare ob-
tained by computing the function xn+0:3 sin(2xn)and then adding uniform noise
over the interval (‚àí0:1; 0:1) The inverse problem is then obtained by keeping the
same data points but exchanging the roles of xandt.Figure 6.17 shows the data sets
for the forward and inverse problems, along with the results of Ô¨Åtting two-layer neu-
ral networks having six hidden units and a single linear output unit by minimizing
a sum-of-squares error function Least squares corresponds to maximum likelihood
under a Gaussian assumption We see that this leads to a good model for the forward
problem but a very poor model for the highly non-Gaussian inverse problem 6.5.2 Conditional mixture distributions
We therefore seek a general framework for modelling conditional probability
distributions This can be achieved by using a mixture model for p(t|x) in which
Figure 6.17 On the left is the data
set for a simple forward problem in
which the red curve shows the result
of Ô¨Åtting a two-layer neural network
by minimizing the sum-of-squares
error function The corresponding
inverse problem, shown on the right,
is obtained by exchanging the roles
ofxandt Here the same network,
again trained by minimizing the sum-
of-squares error function, gives a
poor Ô¨Åt to the data due to the mul-
timodality of the data set DEEP NEURAL NETWORKS
Figure 6.18 Themixture density net-
work can represent general conditional
probability densities p(t|x) by consid-
ering a parametric mixture model for
the distribution of twhose parameters
are determined by the outputs of a neu-
ral network that takes xas its input
vector.xD

============================================================

=== CHUNK 202 ===
Palavras: 366
Caracteres: 2369
--------------------------------------------------
1
tp(t|x)
both
the mixing coefÔ¨Åcients as well as the component densities are Ô¨Çexible functions
of the input vector x, giving rise to a mixture density network For any given value of
x, the mixture model provides a general formalism for modelling an arbitrary condi-
tional density function p(t|x) Provided we consider a sufÔ¨Åciently Ô¨Çexible network,
we then have a framework for approximating arbitrary conditional distributions Here we will develop the model explicitly for Gaussian components, so that
p(t|x) =K/summationdisplay
k=1k(x)N/parenleftbig
t|k(x);2
k(x)/parenrightbig
: (6.38)
This is an example of a heteroscedastic model in which the noise variance on the
data is a function of the input vector x Instead of Gaussians, we can use other dis-
tributions for the components, such as Bernoulli distributions if the target variables
are binary rather than continuous We have also specialized to the case of isotropic
covariances for the components, although the mixture density network can readily
be extended to allow for general covariance matrices by representing the covariances
using a Cholesky factorization (Williams, 1996) Even with isotropic components,
the conditional distribution p(t|x) does not assume factorization with respect to the
components of t(in contrast to the standard sum-of-squares regression model) as a
consequence of the mixture distribution We now take the various parameters of the mixture model, namely the mix-
ing coefÔ¨Åcients k(x), the means k(x), and the variances 2
k(x), to be governed
by the outputs of a neural network that takes xas its input The structure of this
mixture density network is illustrated in Figure 6.18 The mixture density network
is closely related to the mixture-of-experts model (Jacobs et al., 1991) The prin-
cipal difference is that a mixture of experts has independent parameters for each
component model in the mixture, whereas in a mixture density network, the same
function is used to predict the parameters of all the component densities as well as
the mixing coefÔ¨Åcients, and so the nonlinear hidden units are shared amongst the
input-dependent functions The neural network in Figure 6.18 can, for example, be a two-layer network
having sigmoidal (tanh) hidden units If there are Kcomponents in the mixture
model (6.38), and if thasLcomponents, then the network will have Koutput-
6.5

============================================================

=== CHUNK 203 ===
Palavras: 355
Caracteres: 2428
--------------------------------------------------
Mixture Density Networks 201
unit pre-activations denoted by a
kthat determine the mixing coefÔ¨Åcients k(x),K
outputs denoted by a
kthat determine the Gaussian standard deviations k(x), and
K√óLoutputs denoted by a
kjthat determine the components kj(x)of the Gaussian
meansk(x) The total number of network outputs is given by (L+ 2)K, unlike
the usualLoutputs for a network that simply predicts the conditional means of the
target variables The mixing coefÔ¨Åcients must satisfy the constraints
K/summationdisplay
k=1k(x) = 1; 06k(x)61; (6.39)
which can be achieved using a set of softmax outputs:
k(x) =exp(a
k)/summationtextK
l=1exp(a
l): (6.40)
Similarly, the variances must satisfy 2
k(x)>0and so can be represented in terms
of the exponentials of the corresponding network pre-activations using
k(x) = exp(a
k): (6.41)
Finally, because the means k(x)have real components, they can be represented
directly by the network outputs:
kj(x) =a
kj(6.42)
in which the output-unit activation functions are given by the identity f(a) =a The learnable parameters of the mixture density network comprise the vector w
of weights and biases in the neural network, which can be set by maximum likelihood
or equivalently by minimizing an error function deÔ¨Åned to be the negative logarithm
of the likelihood For independent data, this error function takes the form
E(w) =‚àíN/summationdisplay
n=1ln/braceleftBiggK/summationdisplay
k=1k(xn;w)N/parenleftbig
tn|k(xn;w);2
k(xn;w)/parenrightbig/bracerightBigg
(6.43)
where we have made the dependencies on wexplicit 6.5.3 Gradient optimization
To minimize the error function, we need to calculate the derivatives of the error
E(w)with respect to the components of w We will see later how to compute these
derivatives automatically It is instructive, however, to derive suitable expressions for Chapter 8
the derivatives of the error with respect to the output-unit pre-activations explicitly as
this highlights the probabilistic interpretation of these quantities Because the error
function (6.43) is composed of a sum of terms, one for each training data point, we
can consider the derivatives for a particular input vector xnwith associated target
vector tn The derivatives of the total error Eare obtained by summing over all
202 6 DEEP NEURAL NETWORKS
data points, or the individual gradients for each data point can be used directly in
gradient-based optimization algorithms

============================================================

=== CHUNK 204 ===
Palavras: 377
Caracteres: 2468
--------------------------------------------------
Chapter 7
It is convenient to introduce the following variables:

nk=
k(tn|xn) =kNnk/summationtextK
l=1lNnl(6.44)
whereNnkdenotesN(
tn|k(xn);2
k(xn)) These quantities have a natural inter-
pretation as posterior probabilities for the components of the mixture in which the
mixing coefÔ¨Åcients k(x)are viewed as x-dependent prior probabilities Exercise 6.17
The derivatives of the error function with respect to the network output pre-
activations governing the mixing coefÔ¨Åcients are given by Exercise 6.18
@En
@
a
k=k‚àí
nk: (6.45)
Similarly, the derivatives with respect to the output pre-activations controlling the
component means are given by Exercise 6.19
@En
@
a
kl=
nk/braceleftbiggkl‚àítnl
2
k/bracerightbigg
: (6.46)
Finally
, the derivatives with respect to the output pre-activations controlling the com-
ponent variances are given by Exercise 6.20
@En
@
a
k=
nk/braceleftbigg
L‚àí/bardbltn‚àík/bardbl2
2
k/bracerightbigg
: (6.47)
6.5.4
Predictive distribution
We illustrate the use of a mixture density network by returning to the toy ex-
ample of an inverse problem shown in Figure 6.17 Plots of the mixing coefÔ¨Å-
cientsk(x), the means k(x), and the conditional density contours corresponding
top(t|x), are shown in Figure 6.19 The outputs of the neural network, and hence the
parameters in the mixture model, are necessarily continuous single-valued functions
of the input variables However, we see from Figure 6.19(c) that the model is able to
produce a conditional density that is unimodal for some values of xand trimodal for
other values by modulating the amplitudes of the mixing components k(x) Once a mixture density network has been trained, it can predict the conditional
density function of the target data for any given value of the input vector This
conditional density represents a complete description of the generator of the data, so
far as the problem of predicting the value of the output vector is concerned From this
density function, we can calculate more speciÔ¨Åc quantities that may be of interest in
different applications One of the simplest of these is the mean, corresponding to the
conditional average of the target data, and is given by
E[t|x] =/integraldisplay
tp(t|x) d t=K/summationdisplay
k=1k(x)k(x) (6.48)
6.5 Mixture Density Networks 203
Figure 6.19 (a) Plot of the mixing
coefÔ¨Åcients k(x)as a function of
xfor the three mixture components
in a mixture density network trained
on the data shown in Figure 6.17

============================================================

=== CHUNK 205 ===
Palavras: 370
Caracteres: 2430
--------------------------------------------------
The model has three Gaussian com-
ponents and uses a two-layer neu-
ral network with Ô¨Åve tanh sigmoidal
units in the hidden layer and nine
outputs (corresponding to the three
means and three variances of the
Gaussian components and the three
mixing coefÔ¨Åcients) At both small
and large values of x, where the
conditional probability density of the
target data is unimodal, only one
of the Gaussian components has
a high value for its prior probabil-
ity, whereas at intermediate values
ofx, where the conditional density
is trimodal, the three mixing coefÔ¨Å-
cients have comparable values (b)
Plots of the means k(x)using the
same colour coding as for the mix-
ing coefÔ¨Åcients (c) Plot of the con-
tours of the corresponding condi-
tional probability density of the tar-
get data for the same mixture den-
sity network (d) Plot of the ap-
proximate conditional mode, shown
by the red points, of the conditional
density (a)0 101
(b)0 101
(c)0 101
(d)0 101
where
we have used (6.38) Because a standard network trained by least squares
approximates the conditional mean, we see that a mixture density network can re-
produce the conventional least-squares result as a special case Of course, as we have
already noted, for a multimodal distribution the conditional mean is of limited value We can similarly evaluate the variance of the density function about the condi-
tional average, to give Exercise 6.21
s2(x) = E/bracketleftBig
/bardblt‚àíE[t|x]/bardbl2|x/bracketrightBig
(6.49)
=K/summationdisplay
k=1k(x)Ô£±
Ô£≤
Ô£≥2
k(x) +/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek(x)‚àíK/summationdisplay
l=1l(x)l(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2Ô£º
Ô£Ω
Ô£æ(6.50)
where we have used (6.38) and (6.48) This is more general than the corresponding
least-squares result because the variance is a function of x We have seen that for multimodal distributions, the conditional mean can give
a poor representation of the data For instance, in controlling the simple robot arm
shown in Figure 6.16, we need to pick one of the two possible joint angle settings
204 6 DEEP NEURAL NETWORKS
to achieve the desired end-effector location, but the average of the two solutions is
not itself a solution In such cases, the conditional mode may be of more value Because the conditional mode for the mixture density network does not have a sim-
ple analytical solution, a numerical iteration is required

============================================================

=== CHUNK 206 ===
Palavras: 384
Caracteres: 2342
--------------------------------------------------
A simple alternative is to
take the mean of the most probable component (i.e., the one with the largest mixing
coefÔ¨Åcient) at each value of x This is shown for the toy data set in Figure 6.19(d) Ex
ercises
6.1 (???) Use the result (2.126) to derive an expression for the surface area SDand
the volumeVDof a hypersphere of unit radius in Ddimensions To do this, con-
sider the following result, which is obtained by transforming from Cartesian to polar
coordinates:
D/productdisplay
i=1/integraldisplay‚àû
‚àí‚àûe‚àíx2
idxi=SD/integraldisplay‚àû
0e‚àír2rD‚àí1dr: (6.51)
Using the gamma function, deÔ¨Åned by
Œì(x) =/integraldisplay‚àû
0tx‚àí1e‚àítdt (6.52)
together with (2.126), evaluate both sides of this equation, and hence show that
SD=2D=2
Œì(
D=2): (6.53)
Next, by integrating with respect to the radius from 0to1, show that the volume of
the unit hypersphere in Ddimensions is given by
VD=SD
D: (6.54)
Finally
, use the results Œì(1) = 1 andŒì(3=2) =‚àö
=2to show that (6.53) and (6.54)
reduce to the usual expressions for D= 2andD= 3 6.2 (???) Consider a hypersphere of radius ainDdimensions together with the con-
centric hypercube of side 2a, so that the hypersphere touches the hypercube at the
centres of each of its sides By using the results of Exercise 6.1, show that the ratio
of the volume of the hypersphere to the volume of the cube is given by
volume of hypersphere
v
olume of cube=D=2
D2D‚àí
1Œì(D=2): (6.55)
Now make use of Stirling‚Äôs formula in the form
Œì(x+ 1)/similarequal(2)1=2e‚àíxxx+1=2; (6.56)
which is valid for x/greatermuch1, to show that, as D‚Üí‚àû, the ratio (6.55) goes to zero Show also that the distance from the centre of the hypercube to one of the corners
Exercises 205
divided by the perpendicular distance to one of the sides is‚àö
D, which therefore goes
to‚àûasD‚Üí‚àû From these results, we see that, in a space of high dimensionality,
most of the volume of a cube is concentrated in the large number of corners, which
themselves become very long ‚Äòspikes‚Äô 6.3 (???) In this exercise, we explore the behaviour of the Gaussian distribution in high-
dimensional spaces Consider a Gaussian distribution in Ddimensions given by
p(x) =1
(22)D=2exp/parenleftbigg
‚àí/bardblx/bardbl2
22/parenrightbigg
: (6.57)
We wish to Ô¨Ånd the density as a function of the radius in polar coordinates in which
the direction variables have been integrated out

============================================================

=== CHUNK 207 ===
Palavras: 356
Caracteres: 2322
--------------------------------------------------
To do this, show that the integral of
the probability density over a thin shell of radius rand thickness , where/lessmuch1, is
given byp(r)where
p(r) =SDrD‚àí1
(22)D=2exp/parenleftbigg
‚àír2
22/parenrightbigg
(6.58)
whereSDis the surface area of a unit hypersphere in Ddimensions Show that the
functionp(r)has a single stationary point located, for large D, at/hatwider/similarequal‚àö
D By
considering p(/hatwider+)where/lessmuch/hatwider, show that for large D,
p(/hatwider+) =p(/hatwider) exp/parenleftbigg
‚àí32
22/parenrightbigg
; (6.59)
which shows that /hatwideris a maximum of the radial probability density and also that p(r)
decays exponentially away from its maximum at /hatwiderwith length scale  We have
already seen that /lessmuch/hatwiderfor largeD, and so we see that most of the probability
mass is concentrated in a thin shell at large radius Finally, show that the probability
densityp(x) is larger at the origin than at the radius /hatwiderby a factor of exp(D=2) We therefore see that most of the probability mass in a high-dimensional Gaussian
distribution is located at a different radius from the region of high probability density 6.4 (??) Consider a two-layer network function of the form (6.11) in which the hidden-
unit nonlinear activation functions h(¬∑)are given by logistic sigmoid functions of the
form
(a) ={1 + exp(‚àía)}‚àí1: (6.60)
Show that there exists an equivalent network, which computes exactly the same func-
tion, but with hidden-unit activation functions given by tanh(a) where the tanh func-
tion is deÔ¨Åned by (6.14) Hint: Ô¨Årst Ô¨Ånd the relation between (a)andtanh(a), and
then show that the parameters of the two networks differ by linear transformations 6.5 (??) Theswish activation function (Ramachandran, Zoph, and Le, 2017) is deÔ¨Åned
by
h(x) =x(x) (6.61)
206 6 DEEP NEURAL NETWORKS
where(x)is the logistic-sigmoid activation function deÔ¨Åned by (6.13) When used
in a neural network, can be treated as a learnable parameter Either sketch or plot
using software graphs of the swish activation function as well as its Ô¨Årst derivative
for= 0:1,= 1:0, and= 10 Show that when ‚Üí‚àû, the swish function
becomes the ReLU function 6.6 (?)We saw in (5.72) that the derivative of the logistic-sigmoid activation function
can be expressed in terms of the function value itself

============================================================

=== CHUNK 208 ===
Palavras: 372
Caracteres: 2338
--------------------------------------------------
Derive the corresponding
result for the tanh activation function deÔ¨Åned by (6.14) 6.7 (??) Show that the softplus activation function (a)given by (6.16) satisÔ¨Åes the
properties:
(a)‚àí(‚àía) =a (6.62)
ln(a) =‚àí(‚àía) (6.63)
d(a)
da=(a) (6.64)
‚àí1(a) = ln (exp( a)‚àí1) (6.65)
where(a)is the logistic-sigmoid activation function given by (6.13) 6.8 (?)Show that minimization of the error function (6.25) with respect to the variance
2gives the result (6.27) 6.9 (?)Show that maximizing the likelihood function under the conditional distribu-
tion (6.28) for a multioutput neural network is equivalent to minimizing the sum-of-
squares error function (6.29) Also, show that the noise variance that minimizes this
error function is given by (6.30) 6.10 (??) Consider a regression problem involving multiple target variables in which it is
assumed that the distribution of the targets, conditioned on the input vector x, is a
Gaussian of the form
p(t|x; w) =N(t|y(x;w);) (6.66)
where y(x;w)is the output of a neural network with input vector xand weight
vector w, and is the covariance of the assumed Gaussian noise on the targets Given a set of independent observations of xandt, write down the error function
that must be minimized to Ô¨Ånd the maximum likelihood solution for w, if we assume
thatis Ô¨Åxed and known Now assume that is also to be determined from the data,
and write down an expression for the maximum likelihood solution for  Note that
the optimizations of wandare now coupled, in contrast to the case of independent
target variables discussed in Section 6.4.1 6.11 (??) Consider a binary classiÔ¨Åcation problem in which the target values are t‚àà
{0;1}, with a network output y(x;w)that represents p(t= 1|x), and suppose that
there is a probability that the class label on a training data point has been incorrectly
set data, write down the error function corresponding to the negative
log likelihood Verify that the error function (6.33) is obtained when = 0 Note that
Exer
cises 207
this error function makes the model robust to incorrectly labelled data, in contrast to
the usual cross-entropy error function 6.12 (??) The error function (6.33) for binary classiÔ¨Åcation problems was derived for a
network having a logistic-sigmoid output activation function, so that 06y(x;w)6
1, and data having target values t‚àà{0; 1}

============================================================

=== CHUNK 209 ===
Palavras: 366
Caracteres: 2394
--------------------------------------------------
Derive the corresponding error function
if we consider a network having an output ‚àí16y(x;w)61and target values
t= 1for classC1andt=‚àí1for classC2 What would be the appropriate choice of
output-unit activation function 6.13 (?)Show that maximizing the likelihood for a multi-class neural network model
in which the network outputs have the interpretation yk(x;w) =p(tk= 1|x) is
equivalent to minimizing the cross-entropy error function (6.36) 6.14 (?)Show that the derivative of the error function (6.33) with respect to the pre-
activationakfor an output unit having a logistic-sigmoid activation function yk=
(ak), where(a)is given by (6.13), satisÔ¨Åes (6.31) 6.15 (?)Show that the derivative of the error function (6.36) with respect to the pre-
activationakfor output units having a softmax activation function (6.37) satisÔ¨Åes
(6.31) 6.16 (??) Write down a pair of equations that express the Cartesian coordinates (x1;x2)
for the robot arm shown in Figure 6.16 in terms of the joint angles 1and2and
the lengthsL1andL2of the links Assume the origin of the coordinate system is
given by the attachment point of the lower arm These equations deÔ¨Åne the forward
kinematics of the robot arm 6.17 (??) Show that the variable 
nkdeÔ¨Åned by (6.44) can be viewed as the posterior
probabilities p(k|t)for the components of the mixture distribution (6.38) in which
the mixing coefÔ¨Åcients k(x)are viewed as x-dependent prior probabilities p(k) 6.18 (??) Derive the result (6.45) for the derivative of the error function with respect to
the network output pre-activations controlling the mixing coefÔ¨Åcients in the mixture
density network 6.19 (??) Derive the result (6.46) for the derivative of the error function with respect to
the network output pre-activations controlling the component means in the mixture
density network 6.20 (??) Derive the result (6.47) for the derivative of the error function with respect to the
network output pre-activations controlling the component variances in the mixture
density network 6.21 (???) Verify the results (6.48) and (6.50) for the conditional mean and variance of
the mixture density network model 7
Gradient
Descent
In the previous chapter we saw that neural networks are a very broad and Ô¨Çexible
class of functions and are able in principle to approximate any desired function to
arbitrarily high accuracy given a sufÔ¨Åciently large number of hidden units

============================================================

=== CHUNK 210 ===
Palavras: 370
Caracteres: 2295
--------------------------------------------------
More-
over, we saw that deep neural networks can encode inductive biases corresponding
to hierarchical representations, which prove valuable in a wide range of practical
applications We now turn to the task of Ô¨Ånding a suitable setting for the network
parameters (weights and biases), based on a set of training data As with the regression and classiÔ¨Åcation models discussed in earlier chapters,
we choose the model parameters by optimizing an error function We have seen how
to deÔ¨Åne a suitable error function for a particular application by using maximum
likelihood Although in principle the error function could be minimized numerically Section 6.4
through a series of direct error function evaluations, this turns out to be very inefÔ¨Å-
cient Instead, we turn to another core concept that is used in deep learning, which
209 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_7    
210 7 GRADIENT DESCENT
is that optimizing the error function can be done much more efÔ¨Åciently by making
use of gradient information, in other words by evaluating the derivatives of the error
function with respect to the network parameters This is why we took care to en-
sure that the function represented by the neural network is differentiable by design Likewise, the error function itself also needs to be differentiable The required derivatives of the error function with respect to each of the pa-
rameters in the network can be evaluated efÔ¨Åciently using a technique called back-
propagation, which involves successive computations that Ô¨Çow backwards through Chapter 8
the network in a way that is analogous to the forward Ô¨Çow of function computations
during the evaluation of the network outputs Although the likelihood is used to deÔ¨Åne an error function, the goal when op-
timizing the error function in a neural network is to achieve good generalization on
test data In classical statistics, maximum likelihood is used to Ô¨Åt a parametric model
to a Ô¨Ånite data set, in which the number of data points typically far exceeds the num-
ber of parameters in the model The optimal solution has the maximum value of the
likelihood function, and the values found for the Ô¨Åtted parameters are of direct inter-
est

============================================================

=== CHUNK 211 ===
Palavras: 357
Caracteres: 2093
--------------------------------------------------
By contrast, modern deep learning works with very rich models containing huge
numbers of learnable parameters, and the goal is never simply exact optimization Section 9.3.2
Instead, the properties and behaviour of the learning algorithm itself, along with var-
ious methods for regularization, are important in determining how well the solution Chapter 9
generalizes to new data Err
or Surfaces
Our
goal during training is to Ô¨Ånd values for the weights and biases in the neural
network that will allow it to make effective predictions For convenience we will
group these parameters into a single vector w, and we will optimize wby using a
chosen error function E(w) At this point, it is useful to have a geometrical picture
of the error function, which we can view as a surface sitting over ‚Äòweight space‚Äô, as
shown in Figure 7.1 First note that if we make a small step in weight space from wtow+wthen
the change in the error function is given by
E/similarequalwT‚àáE(w) (7.1)
where the vector‚àáE(w)points in the direction of the greatest rate of increase of
the error function Provided the error E(w)is a smooth, continuous function of w,
its smallest value will occur at a point in weight space such that the gradient of the
error function vanishes, so that
‚àáE(w) = 0 (7.2)
as otherwise we could make a small step in the direction of ‚àí‚àáE (w)and thereby
further reduce the error Points at which the gradient vanishes are called stationary
points and may be further classiÔ¨Åed into minima, maxima, and saddle points Error Surfaces 211
Figure 7.1 Geometrical view of the error function E(w)as a
surface sitting over weight space Point wAis a
local minimum and wBis the global minimum, so
thatE(wA)>E(wB) At any point wC, the local
gradient of the error surface is given by the vector
‚àáE w1
w2E(w)
wAwBwC
rE
W
e will aim to Ô¨Ånd a vector wsuch thatE(w)takes its smallest value How-
ever, the error function typically has a highly nonlinear dependence on the weights
and bias parameters, and so there will be many points in weight space at which the
gradient vanishes (or is numerically very small)

============================================================

=== CHUNK 212 ===
Palavras: 432
Caracteres: 2754
--------------------------------------------------
Indeed, for any point wthat is a
local minimum, there will generally be other points in weight space that are equiva-
lent minima For instance, in a two-layer network of the kind shown in Figure 6.9,
withMhidden units, each point in weight space is a member of a family of M Section 6.2.4
Furthermore, there may be multiple non-equivalent stationary points and in par-
ticular multiple non-equivalent minima A minimum that corresponds to the smallest
value of the error function across the whole of w-space is said to be a global min-
imum Any other minima corresponding to higher values of the error function are
said to be local minima The error surfaces for deep neural networks can be very
complex, and it was thought that gradient-based methods might become trapped in
poor local minima In practice, this seems not to be the case, and large networks can
reach solutions with similar performance under a variety of initial conditions Section 9.3.2
7.1.1 Local quadratic approximation
Insight into the optimization problem and into the various techniques for solving
it can be obtained by considering a local quadratic approximation to the error func-
tion The Taylor expansion of E(w)around some point /hatwidewin weight space is given
by
E(w)/similarequalE(/hatwidew) + (w‚àí/hatwidew)Tb+1
2(
w‚àí/hatwidew)TH(w‚àí/hatwidew) (7.3)
where cubic and higher terms have been omitted Here bis deÔ¨Åned to be the gradient
ofEevaluated at/hatwidew
b‚â°‚àáE|w=bw: (7.4)
TheHessian is deÔ¨Åned to be the corresponding matrix of second derivatives
H(/hatwidew) =‚àá‚àáE (w)|w=bw: (7.5)
212 7 GRADIENT DESCENT
If there is a total of Wweights and biases in the network, then wandbhave length
WandHhas dimensionality W√óW From (7.3), the corresponding local approx-
imation to the gradient is given by
‚àáE(w) =b+H(w‚àí/hatwidew): (7.6)
For points wthat are sufÔ¨Åciently close to /hatwidew, these expressions will give reasonable
approximations for the error and its gradient Consider the particular case of a local quadratic approximation around a point
w?that is a minimum of the error function In this case there is no linear term,
because‚àáE= 0atw?, and (7.3) becomes
E(w) =E(w?) +1
2(w‚àíw?)TH(w‚àíw?) (7.7)
where the Hessian His evaluated at w To interpret this geometrically, consider the
eigenvalue equation for the Hessian matrix:
Hui=iui (7.8)
where the eigenvectors uiform a complete orthonormal set so that Appendix A
uT
iuj=ij: (7.9)
We now expand (w‚àíw?)as a linear combination of the eigenvectors in the form
w‚àíw?=/summationdisplay
iiui: (7.10)
This can be regarded as a transformation of the coordinate system in which the origin
is translated to the point w?and the axes are rotated to align with the eigenvectors
through the orthogonal matrix whose columns are {u1;:::;uW}

============================================================

=== CHUNK 213 ===
Palavras: 368
Caracteres: 2248
--------------------------------------------------
By substituting Appendix A
(7.10) into (7.7) and using (7.8) and (7.9), the error function can be written in the
form Exercise 7.1
E(w) =E(w?) +1
2/summationdisplay
ii2
i: (7.11)
Suppose we set all i= 0 fori/negationslash=jand then vary j, corresponding to moving
waway from w?in the direction of uj We see from (7.11) that the error function
will increase if the corresponding eigenvalue jis positive and will decrease if it is
negative If all eigenvalues are positive then w?corresponds to a local minimum of
the error function, whereas if they are all negative then w?corresponds to a local
maximum If we have a mix of positive and negative eigenvalues then w?represents
a saddle point A matrix His said to be positive deÔ¨Ånite if, and only if,
vTHv>0; for all v: (7.12)
7.2 Gradient Descent Optimization 213
Figure 7.2 In the neighbourhood of a mini-
mum w?, the error function can
be approximated by a quadratic Contours of constant error are
then ellipses whose axes are
aligned with the eigenvectors
uiof the Hessian matrix, with
lengths that are inversely pro-
portional to the square roots of
the corresponding eigenvectors
i w1w2
‚àí1=2
1‚àí1=2
2u1
w?u2
Because
the eigenvectors{ui}form a complete set, an arbitrary vector vcan be
written in the form
v=/summationdisplay
iciui: (7.13)
From (7.8) and (7.9), we then have
vTHv=/summationdisplay
ic2
ii (7.14)
and so Hwill be positive deÔ¨Ånite if, and only if, all its eigenvalues are positive Exercise 7.2
Thus, a necessary and sufÔ¨Åcient condition for w?to be a local minimum is that the
gradient of the error function should vanish at w?and the Hessian matrix evaluated
atw?should be positive deÔ¨Ånite In the new coordinate system, whose basis vectors Exercise 7.3
are given by the eigenvectors {ui}, the contours of constant E(w)are axis-aligned
ellipses centred on the origin, as illustrated in Figure 7.2 Gradient
Descent Optimization
There
is little hope of Ô¨Ånding an analytical solution to the equation ‚àáE(w) = 0 for
an error function as complex as one deÔ¨Åned by a neural network, and so we resort to
iterative numerical procedures The optimization of continuous nonlinear functions
is a widely studied problem, and there exists an extensive literature on how to solve it
efÔ¨Åciently

============================================================

=== CHUNK 214 ===
Palavras: 369
Caracteres: 2272
--------------------------------------------------
Most techniques involve choosing some initial value w(0)for the weight
vector and then moving through weight space in a succession of steps of the form
w()=w(‚àí1)+ ‚àÜw(‚àí1)(7.15)
wherelabels the iteration step Different algorithms involve different choices for
the weight vector update ‚àÜw() Because of the complex shape of the error surface for all but the simplest neu-
ral networks, the solution found will depend, among other things, on the particular
choice of initial parameter values w(0) To Ô¨Ånd a sufÔ¨Åciently good solution, it may
214 7 GRADIENT DESCENT
be necessary to run a gradient-based algorithm multiple times, each time using a dif-
ferent randomly chosen starting point, and comparing the resulting performance on
an independent validation set 7.2.1 Use of gradient information
The gradient of an error function for a deep neural network can be evaluated
efÔ¨Åciently using the technique of error backpropagation, and applying this gradient Chapter 8
information can lead to signiÔ¨Åcant improvements in the speed of network training We can see why this is so, as follows In the quadratic approximation to the error function given by (7.3), the error
surface is speciÔ¨Åed by the quantities bandH, which contain a total of W(W+
3)=2 independent elements (because the matrix His symmetric), where Wis the Exercise 7.7
dimensionality of w(i.e., the total number of learnable parameters in the network) The location of the minimum of this quadratic approximation therefore depends on
O(W2)parameters, and we should not expect to be able to locate the minimum until
we have gathered O(W2)independent pieces of information If we do not make
use of gradient information, we would expect to have to perform O(W2)function
evaluations, each of which would require O(W)steps Thus, the computational
effort needed to Ô¨Ånd the minimum using such an approach would be O(W3) Now compare this with an algorithm that makes use of the gradient information Because‚àáE is a vector of length W, each evaluation of ‚àáE bringsWpieces of
information, and so we might hope to Ô¨Ånd the minimum of the function in O(W)
gradient evaluations As we shall see, by using error backpropagation, each such Chapter 8
evaluation takes only O(W)steps and so the minimum can now be found in O(W2)
steps

============================================================

=== CHUNK 215 ===
Palavras: 369
Caracteres: 2262
--------------------------------------------------
Although the quadratic approximation only holds in the neighbourhood of
a minimum, the efÔ¨Åciency gains are generic For this reason, the use of gradient
information forms the basis of all practical algorithms for training neural networks 7.2.2 Batch gradient descent
The simplest approach to using gradient information is to choose the weight up-
date in (7.15) such that there is a small step in the direction of the negative gradient,
so that
w()=w(‚àí1)‚àí‚àáE(w(‚àí1)) (7.16)
where the parameter >0is known as the learning rate After each such update, the
gradient is re-evaluated for the new weight vector w(+1)and the process repeated At each step, the weight vector is moved in the direction of the greatest rate of
decrease of the error function, and so this approach is known as gradient descent or
steepest descent Note that the error function is deÔ¨Åned with respect to a training set,
and so to evaluate ‚àáE, each step requires that the entire training set be processed Techniques that use the whole data set at once are called batch methods 7.2.3 Stochastic gradient descent
Deep learning methods beneÔ¨Åt greatly from very large data sets However, batch
methods can become extremely inefÔ¨Åcient if there are many data points in the train-
ing set because each error function or gradient evaluation requires the entire data set
7.2 Gradient Descent Optimization 215
Algorithm 7.1: Stochastic gradient descent
Input: Training set of data points indexed by n‚àà{1;:::;N}
Error function per data point En(w)
Learning rate parameter 
Initial weight vector w
Output: Final weight vector w
n‚Üê1
repeat
w‚Üêw‚àí‚àáEn(w)// update weight vector
n‚Üên+ 1(modN)// iterate over data
until convergence
return w
to be processed To Ô¨Ånd a more efÔ¨Åcient approach, note that error functions based on
maximum likelihood for a set of independent observations comprise a sum of terms,
one for each data point:
E(w) =N/summationdisplay
n=1En(w): (7.17)
The most widely used training algorithms for large data sets are based on a sequential
version of gradient descent known as stochastic gradient descent (Bottou, 2010), or
SGD, which updates the weight vector based on one data point at a time, so that
w()=w(‚àí1)‚àí‚àáEn(w(‚àí1)): (7.18)
This update is repeated by cycling through the data

============================================================

=== CHUNK 216 ===
Palavras: 356
Caracteres: 2131
--------------------------------------------------
A complete pass through the
whole training set is known as a training epoch This technique is also known as
online gradient descent, especially if the data arises from a continuous stream of
new data points Stochastic gradient descent is summarized in Algorithm 7.1 A further advantage of stochastic gradient descent, compared to batch gradient
descent, is that it handles redundancy in the data much more efÔ¨Åciently To see this,
consider an extreme example in which we take a data set and double its size by
duplicating every data point Note that this simply multiplies the error function by
a factor of 2 and so is equivalent to using the original error function, if the value of
the learning rate is adjusted to compensate Batch methods will require double the
computational effort to evaluate the batch error function gradient, whereas stochastic
gradient descent will be unaffected Another property of stochastic gradient descent
is the possibility of escaping from local minima, since a stationary point with respect
to the error function for the whole data set will generally not be a stationary point
for each data point individually GRADIENT DESCENT
7.2.4 Mini-batches
A downside of stochastic gradient descent is that the gradient of the error func-
tion computed from a single data point provides a very noisy estimate of the gradient
of the error function computed on the full data set We can consider an interme-
diate approach in which a small subset of data points, called a mini-batch, is used
to evaluate the gradient at each iteration In determining the optimum size for the
mini-batch, note that the error in computing the mean from Nsamples is given by
=‚àö
Nwhereis the standard deviation of the distribution generating the data This Exercise 7.8
indicates that there are diminishing returns in estimating the true gradient from in-
creasing the batch size If we increase the size of the mini-batch by a factor of 100
then the error only reduces by a factor of 10 Another consideration in choosing the
mini-batch size is the desire to make efÔ¨Åcient use of the hardware architecture on
which the code is running

============================================================

=== CHUNK 217 ===
Palavras: 358
Caracteres: 2260
--------------------------------------------------
For example, on some hardware platforms, mini-batch
sizes that are powers of 2 (for example, 64, 128, 256, One important consideration when using mini-batches is that the constituent data
points should be chosen randomly from the data set, since in raw data sets there
may be correlations between successive data points arising from the way the data
was collected (for example, if the data points have been ordered alphabetically or
by date) This is often handled by randomly shufÔ¨Çing the entire data set and then
subsequently drawing mini-batches as successive blocks of data The data set can
also be reshufÔ¨Çed between iterations through the data set, so that each mini-batch is
unlikely to have been used before, which can help escape local minima The variant
of stochastic gradient descent with mini-batches is summarized in Algorithm 7.2 Note that the learning algorithm is often still called ‚Äòstochastic gradient descent‚Äô
even when mini-batches are used 7.2.5 Parameter initialization
Iterative algorithms such as gradient descent require that we choose some ini-
tial setting for the parameters being learned The speciÔ¨Åc initialization can have a
signiÔ¨Åcant effect on how long it takes to reach a solution and on the generalization
performance of the resulting trained network Unfortunately, there is relatively little
theory to guide the initialization strategy One key consideration, however, is symmetry breaking Consider a set of hidden
units or output units that take the same inputs If the parameters were all initialized
with the same value, for example if they were all set to zero, the parameters of these
units would all be updated in unison and the units would each compute the same
function and hence be redundant This problem can be addressed by initializing
parameters randomly from some distribution to break symmetry If computational
resources permit, the network might be trained multiple times starting from different
random initializations and the results compared on held-out data The distribution used to initialize the weights is typically either a uniform distri-
bution in the range [‚àí; ]or a zero-mean Gaussian of the form N(0;2) The choice
of the value of is important, and various heuristics to select it have been proposed

============================================================

=== CHUNK 218 ===
Palavras: 369
Caracteres: 2334
--------------------------------------------------
One widely used approach is called He initialization (Heet al., 2015b) Gradient Descent Optimization 217
Algorithm 7.2: Mini-batch stochastic gradient descent
Input: Training set of data points indexed by n‚àà{1;:::;N}
Batch sizeB
Error function per mini-batch En:n+B‚àí1(w)
Learning rate parameter 
Initial weight vector w
Output: Final weight vector w
n‚Üê1
repeat
w‚Üêw‚àí‚àáEn:n+B‚àí1(w)// weight vector update
n‚Üên+B
ifn>N then
shufÔ¨Çe data
n‚Üê1
end if
until convergence
return w
network in which layer levaluates the following transformations
a(l)
i=M/summationdisplay
j=1wijz(l‚àí1)
j (7.19)
z(l)
i= ReLU(a(l)
i) (7.20)
whereMis the number of units that send connections to unit i, and the ReLU activa-
tion function is given by (6.17) Suppose we initialize the weights using a Gaussian
N(0;2), and suppose that the outputs z(l‚àí1)
j of the units in layer l‚àí1have variance
2 Then we can easily show that Exercise 7.9
E[a(l)
i] = 0 (7.21)
var[z(l)
j] =M
222(7.22)
where the factor of 1=2arises from the ReLU activation function Ideally we want
to ensure that the variance of the pre-activations neither decays to zero nor grows
signiÔ¨Åcantly as we propagate from one layer to the next If we therefore require that
the units at layer lalso have variance 2then we arrive at the following choice for
the standard deviation of the Gaussian used to initialize the weights that feed into a
218 7 GRADIENT DESCENT
Figure 7.3 Schematic illustration of Ô¨Åxed-step gradient
descent for an error function that has substantially differ-
ent curvatures along different directions The error sur-
faceEhas the form of a long valley, as depicted by the
ellipses Note that, for most points in weight space, the lo-
cal negative gradient vector ‚àí‚àáE does not point towards
the minimum of the error function Successive steps of
gradient descent can therefore oscillate across the valley,
leading to very slow progress along the valley towards the
minimum The vectors u1andu2are the eigenvectors of
the Hessian matrix u1u2
unit
withMinputs:
=/radicalbigg
2
M: (7.23)
It
is also possible to treat the scale of the initialization distribution as a hyper-
parameter and to explore different values across multiple training runs The bias pa-
rameters are typically set to small positive values to ensure that most pre-activations
are initially active during learning

============================================================

=== CHUNK 219 ===
Palavras: 380
Caracteres: 2366
--------------------------------------------------
This is particularly helpful with ReLU units,
where we want the pre-activations to be positive so that there is a non-zero gradient
to drive learning Another important class of techniques for initializing the parameters of a neural
network is by using the values that result from training the network on a different
task or by exploiting various forms of unsupervised training These techniques fall
into the broad class of transfer learning techniques Con
vergence
When
applying gradient descent in practice, we need to choose a value for the learn-
ing rate parameter  Consider the simple error surface depicted in Figure 7.3 for a
hypothetical two-dimensional weight space in which the curvature of Evaries sig-
niÔ¨Åcantly with direction, creating a ‚Äòvalley‚Äô At most points on the error surface, the
local gradient vector for batch gradient descent, which is perpendicular to the local
contour, does not point directly towards the minimum Intuitively we might expect
that increasing the value of should lead to bigger steps through weight space and
hence faster convergence However, the successive steps oscillate back and forth
across the valley, and if we increase too much, those oscillations will become di-
vergent Because must be kept sufÔ¨Åciently small to avoid divergent oscillations
across the valley, progress along the valley is very slow Gradient descent then takes
many small steps to reach the minimum and is a very inefÔ¨Åcient procedure We can gain deeper insight into the nature of this problem by considering the
quadratic approximation to the error function in the neighbourhood of the minimum Section 7.1.1
From (7.7), (7.8), and (7.10), the gradient of the error function in this approximation
7.3 Convergence 219
can be written as
‚àáE=/summationdisplay
iiiui: (7.24)
Again using (7.10) we can express the change in the weight vector in terms of corre-
sponding changes in the coefÔ¨Åcients {i}:
‚àÜw=/summationdisplay
i‚àÜiui: (7.25)
Combining (7.24) with (7.25) and the gradient descent formula (7.16) and using the
orthonormality relation (7.9) for the eigenvectors of the Hessian, we obtain the fol-
lowing expression for the change in iat each step of the gradient descent algorithm:
‚àÜi=‚àíii (7.26)
from which it follows that Exercise 7.10
new
i= (1‚àíi)old
i (7.27)
where ‚Äòold‚Äô and ‚Äònew‚Äô denote values before and after a weight update

============================================================

=== CHUNK 220 ===
Palavras: 368
Caracteres: 2284
--------------------------------------------------
Using the
orthonormality relation (7.9) for the eigenvectors together with (7.10), we have
uT
i(w‚àíw?) =i (7.28)
and soican be interpreted as the distance to the minimum along the direction ui From (7.27) we see that these distances evolve independently such that, at each step,
the distance along the direction of uiis multiplied by a factor (1‚àíi) After a total
ofTsteps we have
(T)
i= (1‚àíi)T(0)
i: (7.29)
It follows that, provided |1‚àíi|<1, the limitT‚Üí‚àû leads toi= 0, which
from (7.28) shows that w=w?and so the weight vector has reached the minimum
of the error Note that (7.29) demonstrates that gradient descent leads to linear convergence
in the neighbourhood of a minimum Also, convergence to the stationary point re-
quires that all the ibe positive, which in turn implies that the stationary point is
indeed a minimum By making larger we can make the factor (1‚àíi)smaller
and hence improve the speed of convergence There is a limit to how large can
be made, however We can permit (1‚àíi)to go negative (which gives oscillating
values ofi), but we must ensure that |1‚àíi|<1otherwise the ivalues will
diverge This limits the value of to <2=maxwheremaxis the largest of the
eigenvalues The rate of convergence, however, is dominated by the smallest eigen-
value, so with set to its largest permitted value, the convergence along the direction
corresponding to the smallest eigenvalue (the long axis of the ellipse in Figure 7.3)
will be governed by/parenleftbigg
1‚àí2min
max/parenrightbigg
(7.30)
220 7 GRADIENT DESCENT
wE‚àÜw(1)
‚àÜw(2)
‚àÜw(3)
Figure
7.4 With a Ô¨Åxed learning rate parameter, gradient descent down a surface with low curvature
leads to successively smaller steps corresponding to linear convergence In such a sit-
uation, the effect of a momentum term is like an increase in the effective learning rate
parameter whereminis the smallest eigenvalue If the ratio min=max(whose reciprocal
is known as the condition number of the Hessian) is very small, corresponding to
highly elongated elliptical error contours as in Figure 7.3, then progress towards the
minimum will be extremely slow 7.3.1 Momentum
One simple technique for dealing with the problem of widely differing eigenval-
ues is to add a momentum term to the gradient descent formula

============================================================

=== CHUNK 221 ===
Palavras: 383
Caracteres: 2365
--------------------------------------------------
This effectively adds
inertia to the motion through weight space and smooths out the oscillations depicted
inFigure 7.3 The modiÔ¨Åed gradient descent formula is given by
‚àÜw(‚àí1)=‚àí‚àáE/parenleftbig
w(‚àí1)/parenrightbig
+‚àÜw(‚àí2)(7.31)
whereis called the momentum parameter The weight vector is then updated using
(7.15) To understand the effect of the momentum term, consider Ô¨Årst the motion through
a region of weight space for which the error surface has relatively low curvature, as
indicated in Figure 7.4 If we make the approximation that the gradient is unchang-
ing, then we can apply (7.31) iteratively to a long series of weight updates, and then
sum the resulting arithmetic series to give
‚àÜw=‚àí‚àáE{1 ++2+:::} (7.32)
=‚àí
1‚àí‚àá
E (7.33)
and we see that the result of the momentum term is to increase the effective learning
rate fromto=(1‚àí) By contrast, in a region of high curvature in which gradient descent is oscillatory,
as indicated in Figure 7.5, successive contributions from the momentum term will
7.3.Convergence 221
Figure 7.5 For a situation in which successive
steps of gradient descent are oscilla-
tory, a momentum term has little in-
Ô¨Çuence on the effective value of the
learning rate parameter ‚àÜw(1)
‚àÜw(2)
‚àÜw(3)
tend to cancel and the effective learning rate will be close to  Thus, the momentum
term can lead to faster convergence towards the minimum without causing divergent
oscillations A schematic illustration of the effect of a momentum term is shown in
Figure 7.6 Although the inclusion of momentum can lead to an improvement in the per-
formance of gradient descent, it also introduces a second parameter whose value
needs to be chosen, in addition to that of the learning rate parameter  From (7.33)
we see thatshould be in the range 0661 A typical value used in practice
is= 0:9 Stochastic gradient descent with momentum is summarized in Algo-
rithm 7.3 The convergence can be further accelerated using a modiÔ¨Åed version of momen-
tum called Nesterov momentum (Nesterov, 2004; Sutskever et al In con-
ventional stochastic gradient descent with momentum, we Ô¨Årst compute the gradient
Figure 7.6 Illustration of the effect of adding
a momentum term to the gradient
descent algorithm, showing the
more rapid progress along the val-
ley of the error function, compared
with the unmodiÔ¨Åed gradient de-
scent shown in Figure 7.3

============================================================

=== CHUNK 222 ===
Palavras: 384
Caracteres: 2352
--------------------------------------------------
u1u2at the current location then take a step that is ampliÔ¨Åed by adding momentum from
the previous step With the Nesterov method, we change the order of these and Ô¨Årst
compute a step based on the previous momentum, then calculate the gradient at this
222 7 GRADIENT DESCENT
Algorithm 7.3: Stochastic gradient descent with momentum
Input: Training set of data points indexed by n‚àà{1;:::;N}
Batch sizeB
Error function per mini-batch En:n+B‚àí1(w)
Learning rate parameter 
Momentum parameter 
Initial weight vector w
Output: Final weight vector w
n‚Üê1
‚àÜw‚Üê0
repeat
‚àÜw‚Üê‚àí‚àáEn:n+B‚àí1(w) +‚àÜw // calculate update term
w‚Üêw+ ‚àÜw // weight vector update
n‚Üên+B
ifn>N then
shufÔ¨Çe data
n‚Üê1
end if
until convergence
return w
new location to Ô¨Ånd the update, so that
‚àÜw(‚àí1)=‚àí‚àáE/parenleftbig
w(‚àí1)+‚àÜw(‚àí2)/parenrightbig
+‚àÜw(‚àí2): (7.34)
For batch gradient descent, Nesterov momentum can improve the rate of conver-
gence, although for stochastic gradient descent it can be less effective 7.3.2 Learning rate schedule
In the stochastic gradient descent learning algorithm (7.18), we need to specify
a value for the learning rate parameter  Ifis very small then learning will proceed
slowly However, if is increased too much it can lead to instability Although some Section 7.3.1
oscillation can be tolerated, it should not be divergent In practice, the best results
are obtained by using a larger value for at the start of training and then reducing the
learning rate over time, so that the value of becomes a function of the step index :
w()=w(‚àí1)‚àí(‚àí1)‚àáEn(w(‚àí1)): (7.35)
7.3 Convergence 223
Examples of learning rate schedules include linear, power law, and exponential de-
cay:
()= (1‚àí=K)(0)+ (=K )(K)(7.36)
()=(0)(1 +=s)c(7.37)
()=(0)c=s(7.38)
where in (7.36) the value of reduces linearly over Ksteps, after which its value is
held constant at (K) Good values for the hyperparameters (0),(K),K,S, andc
must be found empirically It can be very helpful in practice to monitor the learning
curve showing how the error function evolves during the gradient descent iteration
to ensure that it is decreasing at a suitable rate 7.3.3 RMSProp and Adam
We saw that the optimal learning rate depends on the local curvature of the er-
ror surface, and moreover that this curvature can vary according to the direction in Section 7.3
parameter space

============================================================

=== CHUNK 223 ===
Palavras: 427
Caracteres: 2758
--------------------------------------------------
This motivates several algorithms that use different learning rates
for each parameter in the network The values of these learning rates are adjusted
automatically during training Here we review some of the most widely used exam-
ples Note, however, that this intuition really applies only if the principal curvature
directions are aligned with the axes in weight space, corresponding to a locally diag-
onal Hessian matrix, which is unlikely to be the case in practice Nevertheless, these
types of algorithms can be effective and are widely used The key idea behind AdaGrad, short for ‚Äòadaptive gradient‚Äô, is to reduce each
learning rate parameter over time by using the accumulated sum of squares of all the
derivatives calculated for that parameter (Duchi, Hazan, and Singer, 2011) Thus,
parameters associated with high curvature are reduced most rapidly SpeciÔ¨Åcally,
r()
i=r(‚àí1)
i +/parenleftbigg@E(w)
@wi/parenrightbigg2
(7.39)
w()
i=w(‚àí1)
i‚àí/radicalbigr
i+/parenleftbigg@E(w)
@wi/parenrightbigg
(7.40)
whereis the learning rate parameter, and is a small constant, say 10‚àí8, that
ensures numerical stability in the event that riis close to zero The algorithm is
initialized with r(0)
i= 0 HereE(w)is the error function for a particular mini-batch,
and the update (7.40) is standard stochastic gradient descent but with a modiÔ¨Åed
learning rate that is speciÔ¨Åc to each parameter One problem with AdaGrad is that it accumulates the squared gradients from the
very start of training, and so the associated weight updates can become very small,
which can slow down training too much in the later phases The idea behind the
RMSProp algorithm, which is short for ‚Äòroot mean square propagation‚Äô, is to replace
the sum of squared gradients of AdaGrad with an exponentially weighted average
224 7 GRADIENT DESCENT
(Hinton, 2012), giving
r()
i=r(‚àí1)
i + (1‚àí)/parenleftbigg@E(w)
@wi/parenrightbigg2
(7.41)
w()
i=w(‚àí1)
i‚àí/radicalbigr
i+/parenleftbigg@E(w)
@wi/parenrightbigg
(7.42)
where 0< < 1and a typical value is = 0:9 If we combine RMSProp with momentum, we obtain the Adam optimization
method (Kingma and Ba, 2014) where the name is derived from ‚Äòadaptive moments‚Äô Adam stores the momentum for each parameter separately using update equations
that consist of exponentially weighted moving averages for both the gradients and
the squared gradients in the form
s()
i=1s(‚àí1)
i + (1‚àí1)/parenleftbigg@E(w)
@wi/parenrightbigg
(7.43)
r()
i=2r(‚àí1)
i + (1‚àí2)/parenleftbigg@E(w)
@wi/parenrightbigg2
(7.44)
/hatwidesi()=s()
i
1‚àí
1(7.45)
/hatwideri=r
i
1‚àí
2(7.46)
w()
i=w(‚àí1)
i‚àí/hatwidesi
/radicalbig
/hatwider
i+: (7.47)
Here the factors 1=(1‚àí
1)and1=(1‚àí
2)correct for a bias introduced by initializing
s(0)
iandr(0)
ito zero

============================================================

=== CHUNK 224 ===
Palavras: 363
Caracteres: 2351
--------------------------------------------------
Note that the bias goes to zero as becomes large, since i<1, Exercise 7.12
and so in practice this bias correction is sometimes omitted Typical values for the
weighting parameters are 1= 0:9 and2= 0:99 Adam is the most widely adopted
learning algorithm in deep learning and is summarized in Algorithm 7.4 Normalization
Normalization of the variables computed during the forward pass through a neural
network removes the need for the network to deal with extremely large or extremely
small values Although in principle the weights and biases in a neural network can
adapt to whatever values the input and hidden variables take, in practice normaliza-
tion can be crucial for ensuring effective training Here we consider three kinds of
normalization according to whether we are normalizing across the input data, across
mini-batches, or across layers Normalization 225
Algorithm 7.4: Adam optimization
Input: Training set of data points indexed by n‚àà{1;:::;N}
Batch sizeB
Error function per mini-batch En:n+B‚àí1(w)
Learning rate parameter 
Decay parameters 1and2
Stabilization parameter 
Output: Final weight vector w
n‚Üê1
s‚Üê0
r‚Üê0
repeat
Choose a mini-batch at random from D
g=‚àí‚àáEn:n+B‚àí1(w)// evaluate gradient vector
s‚Üê1s+ (1‚àí1)g
r‚Üê2r+ (1‚àí2)g‚äôg// element-wise multiply
/hatwides‚Üês=(1‚àí
1)// bias correction
/hatwider‚Üêr=(1‚àí
2)// bias correction
‚àÜw‚Üê‚àí/hatwides‚àö
/hatwider+// element-wise operations
w‚Üêw+ ‚àÜw // weight vector update
n‚Üên+B
ifn+B >N then
shufÔ¨Çe data
n‚Üê1
end if
until convergence
return w
226 7 GRADIENT DESCENT
7.4.1 Data normalization
Sometimes we encounter data sets in which different input variables span very
different ranges For example, in health data, a patient‚Äôs height might be measured
in meters, such as 1.8m, whereas their blood platelet count might be measured in
platelets per microliter, such as 300,000 platelets per L Such variations can make
gradient descent training much more challenging Consider a single-layer regression
network with two weights in which the two corresponding input variables have very
different ranges Changes in the value of one of the weights produce much larger
changes in the output, and hence in the error function, than would similar changes in
the other weight This corresponds to an error surface with very different curvatures
along different axes as illustrated in Figure 7.3

============================================================

=== CHUNK 225 ===
Palavras: 392
Caracteres: 2410
--------------------------------------------------
For continuous input variables, it can therefore be very beneÔ¨Åcial to re-scale the
input values so that they span similar ranges This is easily done by Ô¨Årst evaluating
the mean and variance of each input:
i=1
NN/summationdisplay
n
=1xni (7.48)
2
i=1
NN/summationdisplay
n
=1(xni‚àíi)2; (7.49)
which is a calculation that is performed once, before any training is started The
input values are then re-scaled using
/tildewidexni=xni‚àíi
i(7.50)
so
that the re-scaled values {/tildewidexni}have zero mean and unit variance Note that the Exercise 7.14
same values of iandimust be used to pre-process any development, validation, or
test data to ensure that all inputs are scaled in the same way Input data normalization
is illustrated in Figure 7.7 Figure 7.7 Illustration of the effect of input data normal-
ization The red circles show the original data
points for a data set with two variables The
blue crosses show the data set after normal-
ization such that each variable now has zero
mean and unit variance across the data set ‚àí10 0 10x1‚àí10010
x2

7.4 Normalization 227
7.4.2 Batch normalization
We have seen the importance of normalizing the input data, and we can apply
similar reasoning to the variables in each hidden layer of a deep network If there is
wide variation in the range of activation values in a particular hidden layer, then nor-
malizing those values to have zero mean and unit variance should make the learning
problem easier for the next layer However, unlike normalization of the input values,
which can be done once prior to the start of training, normalization of the hidden-
unit values will need to be repeated during training every time the weight values are
updated This is called batch normalization (Ioffe and Szegedy, 2015) A further motivation for batch normalization arises from the phenomena of van-
ishing gradients andexploding gradients, which occur when we try to train very deep
neural networks From the chain rule of calculus, the gradient of an error function E
with respect to a parameter in the Ô¨Årst layer of the network is given by Section 8.1.5
@E
@wi=/summationdisplay
m¬∑¬∑¬∑/summationdisplay
l/summationdisplay
j@z(1)
m
@wi¬∑¬∑¬∑@z(K)
j
@z(K‚àí1)
l@E
@z(K)
j(7.51)
wherez(k)
jdenotes the activation of node jin layerk, and each of the partial deriva-
tives on the right-hand side of (7.51) represents the elements of the Jacobian matrix Section 8.1.5
for that layer

============================================================

=== CHUNK 226 ===
Palavras: 372
Caracteres: 2400
--------------------------------------------------
The product of a large number of such terms will tend towards 0if
most of them have a magnitude <1and will tend towards ‚àûif most of them have
a magnitude >1 Consequently, as the depth of a network increases, error function
gradients can tend to become either very large or very small Batch normalization
largely resolves this issue To see how batch normalization is deÔ¨Åned, consider a speciÔ¨Åc layer within a
multi-layer network Each hidden unit in that layer computes a nonlinear function of
its input pre-activation zi=h(ai), and so we have a choice of whether to normalize
the pre-activation values aior the activation values zi In practice, either approach
may be used, and here we illustrate the procedure by normalizing the pre-activations Because weight values are updated after each mini-batch of examples, we apply the
normalization to each mini-batch SpeciÔ¨Åcally, for a mini-batch of size K, we deÔ¨Åne
i=1
KK/summationdisplay
n=1ani (7.52)
2
i=1
KK/summationdisplay
n=1(ani‚àíi)2(7.53)
/hatwideani=ani‚àíi/radicalbig
2
i+(7.54)
where the summations over n= 1;:::;K are taken over the elements of the mini-
batch Here is a small constant, introduced to avoid numerical issues in situations
where2
iis small By normalizing the pre-activations in a given layer of the network, we reduce
the number of degrees of freedom in the parameters of that layer and hence we
228 7.GRADIENT DESCENT

Hidden unitsMini-batch
(a)
Hidden unitsMini-batch
(b)
Figure 7.8 Illustration of batch normalization and layer normalization in a neural network In batch normaliza-
tion, shown in (a), the mean and variance are computed across the mini-batch separately for each hidden unit In layer normalization, shown in (b), the mean and variance are computed across the hidden units separately for
each data point reduce its representational capability We can compensate for this by re-scaling the
pre-activations of the batch to have mean iand standard deviation 
iusing
/tildewideani=
i/hatwideani+i (7.55)
whereiand
iare adaptive parameters that are learned by gradient descent jointly
with the weights and biases of the network These learnable parameters represent a
key difference compared to input data normalization Section 7.4.1
It might appear that the transformation (7.55) has simply undone the effect of the
batch normalization since the mean and variance can now adapt to arbitrary values
again

============================================================

=== CHUNK 227 ===
Palavras: 373
Caracteres: 2232
--------------------------------------------------
However, the crucial difference is in the way the parameters evolve during
training For the original network, the mean and variance across a mini-batch are
determined by a complex function of all the weights and biases in the layer, whereas
in the representation given by (7.55), they are determined directly by independent
parametersiand
i, which turn out to be much easier to learn during gradient
descent Equations (7.52) ‚Äì (7.55) describe a transformation of the variables that is dif-
ferentiable with respect to the learnable parameters iand
i This can be viewed
as an additional layer in the neural network, and so each standard hidden layer can
be followed by a batch normalization layer The structure of the batch-normalization
process is illustrated in Figure 7.8 Once the network is trained and we want to make predictions on new data, we
7.4 Normalization 229
no longer have the training mini-batches available, and we cannot determine a mean
and variance from just one data example To solve this, we could in principle eval-
uateiand2
ifor each layer across the whole training set after we have made the
Ô¨Ånal update to the weights and biases However, this would involve processing the
whole data set just to evaluate these quantities and is therefore usually too expensive Instead, we compute moving averages throughout the training phase:
()
i=(‚àí1)
i + (1‚àí)i (7.56)
()
i=(‚àí1)
i + (1‚àí)i (7.57)
where 0661 These moving averages play no role during training but are used
to process new data points during the inference phase Although batch normalization is very effective in practice, there is uncertainty
as to why it works so well Batch normalization was originally motivated by noting
that updates to weights in earlier layers of the network change the distribution of
values seen by later layers, a phenomenon called internal covariate shift However,
later studies (Santurkar et al , 2018) suggest that covariate shift is not a signiÔ¨Åcant
factor and that the improved training results from an improvement in the smoothness
of the error function landscape 7.4.3 Layer normalization
With batch normalization, if the batch size is too small then the estimates of the
mean and variance become too noisy

============================================================

=== CHUNK 228 ===
Palavras: 374
Caracteres: 2434
--------------------------------------------------
Also, for very large training sets, the mini-
batches may be split across different GPUs, making global normalization across the
mini-batch inefÔ¨Åcient An alternative to normalizing across examples within a mini-
batch for each hidden unit separately is to normalize across the hidden-unit values
for each data point separately This is known as layer normalization (Ba, Kiros, and
Hinton, 2016) It was introduced in the context of recurrent neural networks where Section 12.2.5
the distributions change after each time step making batch normalization infeasible However, it is useful in other architectures such as transformer networks Chapter 12
By analogy with batch normalization, we therefore make the following transfor-
mation:
n=1
MM/summationdisplay
i=1ani (7.58)
2
n=1
MM/summationdisplay
i=1(ani‚àíi)2(7.59)
/hatwideani=ani‚àín/radicalbig
2n+(7.60)
where the sums i= 1;:::;M are taken over all hidden units in the layer As with
batch normalization, additional learnable mean and standard deviation parameters
are introduced for each hidden unit separately in the form (7.55) Note that the same
normalization function can be employed during training and during inference, and
230 7 GRADIENT DESCENT
so there is no need to store moving averages Layer normalization is compared with
batch normalization in Figure 7.8 Ex
ercises
7.1 (?)By substituting (7.10) into (7.7) and using (7.8) and (7.9), show that the error
function (7.7) can be written in the form (7.11) 7.2 (?)Consider a Hessian matrix Hwith eigenvector equation (7.8) By setting the
vector vin (7.14) equal to each of the eigenvectors uiin turn, show that His positive
deÔ¨Ånite if, and only if, all its eigenvalues are positive 7.3 (??) By considering the local Taylor expansion (7.7) of an error function about a
stationary point w?, show that the necessary and sufÔ¨Åcient condition for the station-
ary point to be a local minimum of the error function is that the Hessian matrix H,
deÔ¨Åned by (7.5) with /hatwidew=w?, is positive deÔ¨Ånite 7.4 (??) Consider a linear regression model with a single input variable xand a single
output variable yof the form
y(x;w;b) =wx+b (7.61)
together with a sum-of-squares error function given by
E(w;b) =1
2N/summationdisplay
n
=1{y(xn;w;b)‚àítn}2: (7.62)
Derive expressions for the elements of the 2√ó2Hessian matrix given by the second
derivatives of the error function with respect to the weight parameter wand bias pa-
rameterb

============================================================

=== CHUNK 229 ===
Palavras: 350
Caracteres: 2170
--------------------------------------------------
Show that the trace and the determinant of this Hessian are both positive Since the trace represents the sum of the eigenvalue and the determinant corresponds
to the product of the eigenvalues, then both eigenvalues are positive and hence the Appendix A
stationary point of the error function is a minimum 7.5 (??) Consider a single-layer classiÔ¨Åcation model with a single input variable xand
a single output variable yof the form
y(x;w;b) =(wx+b) (7.63)
where(¬∑)is the logistic sigmoid function deÔ¨Åned by (5.42) together with a cross-
entropy error function given by
E(w;b) =N/summationdisplay
n=1{tnlny(xn;w;b) + (1‚àítn) ln(1‚àíy(xn;w;b))}: (7.64)
Derive expressions for the elements of the 2√ó2Hessian matrix given by the second
derivatives of the error function with respect to the weight parameter wand bias pa-
rameterb Show that the trace and the determinant of this Hessian are both positive Exercises 231
Since the trace represents the sum of the eigenvalue and the determinant corresponds
to the product of the eigenvalues, then both eigenvalues are positive and hence the Appendix A
stationary point of the error function is a minimum 7.6 (??) Consider a quadratic error function deÔ¨Åned by (7.7) in which the Hessian matrix
Hhas an eigenvalue equation given by (7.8) Show that the contours of constant
error are ellipses whose axes are aligned with the eigenvectors uiwith lengths that
are inversely proportional to the square roots of the corresponding eigenvalues i 7.7 (?)Show that, as a consequence of the symmetry of the Hessian matrix H, the
number of independent elements in the quadratic error function (7.3) is given by
W(W+ 3)=2 7.8 (?)Consider a set of values x1;:::;xNdrawn from a distribution with mean and
variance2, and deÔ¨Åne the sample mean to be
x=1
NN/summationdisplay
n=1xn: (7.65)
Show that the expectation of the squared error (x‚àí)2with respect to the distri-
bution from which the data is drawn is given by 2=N This shows that the RMS
error in the sample mean is given by =‚àö
N, which decreases relatively slowly as
the sample size Nincreases 7.9 (??) Consider a layered network that computes the functions (7.19) and (7.20) in
layerl

============================================================

=== CHUNK 230 ===
Palavras: 362
Caracteres: 2159
--------------------------------------------------
Suppose we initialize the weights using a Gaussian N(0;2), and suppose
that the outputs z(l‚àí1)
j of the units in layer l‚àí1have variance 2 By using the form
of the ReLU activation function, show that the mean and variance of the outputs in
layerlare given by (7.21) and (7.22), respectively Hence, show that if we want
the units in layer lalso to have pre-activations with variance 2then the value of 
should be given by (7.23) 7.10 (??) By making use of (7.7), (7.8), and (7.10), derive the results (7.24) and (7.25),
which express the gradient vector and a general weight update, as expansions in the
eigenvectors of the Hessian matrix Use these results, together with the eigenvector
orthonormality relation (7.9) and the batch gradient descent formula (7.16), to de-
rive the result (7.26) for the batch gradient descent update expressed in terms of the
coefÔ¨Åcients{i} 7.11 (?)Consider a smoothly varying error surface with low curvature such that the gra-
dient varies only slowly with position Show that, for small values of the learning
rate and momentum parameters, the Nesterov momentum gradient update deÔ¨Åned
by (7.34) is equivalent to the standard gradient descent with momentum deÔ¨Åned by
(7.31) 7.12 (??) Consider a sequence of values {x1;:::;xN}of some variable x, and suppose
we compute an exponentially weighted moving average using the formula
n=n‚àí1+ (1‚àí)xn (7.66)
232 7 GRADIENT DESCENT
where 0661 By making use of the following result for the sum of a Ô¨Ånite
geometric series
n/summationdisplay
k=1k‚àí1=1‚àín
1‚àí(7.67)
show that if the sequence of averages is initialized using 0= 0, then the estimators
are biased and that the bias can be corrected using
/hatwiden=n
1‚àín: (7.68)
7.13 (?)In gradient descent, the weight vector wis updated by taking a step in weight
space in the direction of the negative gradient governed by a learning rate parameter
 Suppose instead that we choose a direction din weight space along which we
minimize the error function, given the current weight vector w() This involves
minimizing the quantity
E(w()+d) (7.69)
as a function of to give a value ?corresponding to a new weight vector w(+1)

============================================================

=== CHUNK 231 ===
Palavras: 369
Caracteres: 2416
--------------------------------------------------
Show that the gradient of E(w)atw(+1)is orthogonal to the vector d This is
known as a ‚Äòline search‚Äô method and it forms the basis for a variety of numerical
optimization algorithms (Bishop, 1995b) 7.14 (?)Show that the renormalized input variables deÔ¨Åned by (7.50), where iis deÔ¨Åned
by (7.48) and 2
iis deÔ¨Åned by (7.49), have zero mean and unit variance 8
Backpropagation
Our goal in this chapter is to Ô¨Ånd an efÔ¨Åcient technique for evaluating the gradient
of an error function E(w)for a feed-forward neural network We will see that this
can be achieved using a local message-passing scheme in which information is sent
backwards through the network and is known as error backpropagation, or some-
times simply as backprop Historically, the backpropagation equations would have been derived by hand
and then implemented in software alongside the forward propagation equations, with
both steps taking time and being prone to mistakes Modern neural network software
environments, however, allow virtually any derivatives of interest to be calculated
efÔ¨Åciently with only minimal effort beyond that of coding up the original network
function This idea, called automatic differentiation, plays a key role in modern deep Section 8.2
learning However, it is valuable to understand how the calculations are performed
so that we are not relying on ‚Äòblack box‚Äô software solutions In this chapter we
233 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_8    
234 8 BACKPROPAGATION
therefore explain the key concepts of backpropagation, and explore the framework
of automatic differentiation in detail Note that the term ‚Äòbackpropagation‚Äô is used in the neural computing literature
in a variety of different ways For instance, a feed-forward architecture may be
called a backpropagation network Also the term ‚Äòbackpropagation‚Äô is sometimes
used to describe the end-to-end training procedure for a neural network including
the gradient descent parameter updates In this book we will use ‚Äòbackpropagation‚Äô
speciÔ¨Åcally to describe the computational procedure used in the numerical evaluation
of derivatives such as the gradient of the error function with respect to the weights
and biases of a network This procedure can also be applied to the evaluation of other
important derivatives such as the Jacobian and Hessian matrices

============================================================

=== CHUNK 232 ===
Palavras: 358
Caracteres: 2337
--------------------------------------------------
Evaluation of Gradients
We now derive the backpropagation algorithm for a general network having arbitrary
feed-forward topology, arbitrary differentiable nonlinear activation functions, and a
broad class of error function The resulting formulae will then be illustrated using
a simple layered network structure having a single layer of sigmoidal hidden units
together with a sum-of-squares error Many error functions of practical interest, for instance those deÔ¨Åned by maxi-
mum likelihood for a set of i.i.d data, comprise a sum of terms, one for each data
point in the training set, so that
E(w) =N/summationdisplay
n=1En(w): (8.1)
Here we will consider the problem of evaluating ‚àáEn(w)for one such term in the
error function This may be used directly for stochastic gradient descent, or the
results could be accumulated over a set of training data points for batch or mini-
batch methods 8.1.1 Single-layer networks
Consider Ô¨Årst a simple linear model in which the outputs ykare linear combina-
tions of the input variables xiso that
yk=/summationdisplay
iwkixi (8.2)
together with a sum-of-squares error function that, for a particular input data point
n, takes the form
En=1
2/summationdisplay
k(ynk‚àítnk)2(8.3)
8.1 Evaluation of Gradients 235
whereynk=yk(xn;w), andtnkis the associated target value The gradient of this
error function with respect to a weight wjiis given by
@En
@wji= (ynj‚àítnj)xni: (8.4)
This can be interpreted as a ‚Äòlocal‚Äô computation involving the product of an ‚Äòerror
signal‚Äôynj‚àítnjassociated with the output end of the link wjiand the variable xni
associated with the input end of the link In Section 5.4.3, we saw how a similar
formula arises with the logistic-sigmoid activation function together with the cross-
entropy error function and similarly for the softmax activation function together with
its matching multivariate cross-entropy error function We will now see how this sim-
ple result extends to the more complex setting of multilayer feed-forward networks 8.1.2 General feed-forward networks
In general, a feed-forward network consists of a set of units each of which com-
putes a weighted sum of its inputs:
aj=/summationdisplay
iwjizi (8.5)
whereziis either the activation of another unit or an input unit that sends a connec-
tion to unitj, andwjiis the weight associated with that connection

============================================================

=== CHUNK 233 ===
Palavras: 363
Caracteres: 2111
--------------------------------------------------
Biases can be
included in this sum by introducing an extra unit, or input, with activation Ô¨Åxed at
+1, and so we do not need to deal with biases explicitly The sum in (8.5), known Section 6.2
as a pre-activation, is transformed by a nonlinear activation function h(¬∑) to give the
activationzjof unitjin the form
zj=h(aj): (8.6)
Note that one or more of the variables ziin the sum in (8.5) could be an input, and
similarly, the unit jin (8.6) could be an output For each data point in the training set, we will suppose that we have supplied the
corresponding input vector to the network and calculated the activations of all the
hidden and output units in the network by successive application of (8.5) and (8.6) This process is called forward propagation because it can be regarded as a forward
Ô¨Çow of information through the network Now consider the evaluation of the derivative of Enwith respect to a weight
wji The outputs of the various units will depend on the particular input data point
n However, to keep the notation uncluttered, we will omit the subscript nfrom the
network variables First note that Endepends on the weight wjionly via the summed
inputajto unitj We can therefore apply the chain rule for partial derivatives to give
@En
@wji=@En
@aj@aj
@wji: (8.7)
We now introduce a useful notation:
j‚â°@En
@aj(8.8)
236 8 BACKPROPAGATION
Figure 8.1 Illustration of the calculation of jfor hidden
unitjby backpropagation of the ‚Äôs from those
unitskto which unit jsends connections The
black arrows denote the direction of information
Ô¨Çow during forward propagation, and the red ar-
rows indicate the backward propagation of error
information.zi
zjjk
1...wj
i wk
j
where
the‚Äôs are often referred to as errors for reasons we will see shortly Using
(8.5), we can write
@aj
@
wji=zi: (8.9)
Substituting (8.8) and (8.9) into (8.7), we then obtain
@En
@
wji=jzi: (8.10)
Equation (8.10) tells us that the required derivative is obtained simply by multiplying
the value of for the unit at the output end of the weight by the value of zfor the
unit at the input end of the weight (where z= 1for a bias)

============================================================

=== CHUNK 234 ===
Palavras: 385
Caracteres: 2443
--------------------------------------------------
Note that this takes the
same form as that found for the simple linear model in (8.4) Thus, to evaluate the
derivatives, we need calculate only the value of jfor each hidden and output unit in
the network and then apply (8.10) As we have seen already, for the output units, we have
k=yk‚àítk (8.11)
provided we are using the canonical link as the output-unit activation function To Section 5.4.6
evaluate the ‚Äôs for hidden units, we again make use of the chain rule for partial
derivatives:
j‚â°@En
@
aj=/summationdisplay
k@En
@
ak@ak
@
aj(8.12)
where the sum runs over all units kto which unit jsends connections The arrange-
ment of units and weights is illustrated in Figure 8.1 Note that the units labelled k
include other hidden units and/or output units In writing down (8.12), we are mak-
ing use of the fact that variations in ajgive rise to variations in the error function
only through variations in the variables ak If we now substitute the deÔ¨Ånition of jgiven by (8.8) into (8.12) and make use
of (8.5) and (8.6), we obtain the following backpropagation formula: Exercise 8.1
j=h/prime(aj)/summationdisplay
kwkjk; (8.13)
which tells us that the value of for a particular hidden unit can be obtained by
propagating the ‚Äôs backwards from units higher up in the network, as illustrated
8.1 Evaluation of Gradients 237
Algorithm
8.1:Backpropagation
Input: Input vector xn
Network parameters w
Error function En(w)for inputxn
Activation function h(a)
Output: Error function derivatives {@En=@wji}
//
Forward propagation
forj‚ààall hidden and output units do
aj‚Üê/summationtext
iwj
izi//{zi}includes inputs {xi}
zj‚Üêh(aj)// activation function
end for
// Error evaluation
fork‚ààall output units do
k‚Üê@
En
@
ak// compute errors
end for
// Backward propagation, in reverse order
forj‚ààall hidden units do
j‚Üêh/prime(
aj)/summationtext
kwkjk// recursive backward evaluation
@En
@
wji‚Üêjzi// evaluate derivatives
end for
return/braceleftbigg@En
@
wji/bracerightbigg
inFigure 8.1 Note that the summation in (8.13) is taken over the Ô¨Årst index on
wkj(corresponding to backward propagation of information through the network),
whereas in the forward propagation equation (8.5), it is taken over the second index Exercise 8.2
Because we already know the values of the ‚Äôs for the output units, it follows that
by recursively applying (8.13), we can evaluate the ‚Äôs for all the hidden units in a
feed-forward network, regardless of its topology

============================================================

=== CHUNK 235 ===
Palavras: 383
Caracteres: 2455
--------------------------------------------------
The backpropagation procedure is
summarized in Algorithm 8.1 For batch methods, the derivative of the total error Ecan then be obtained by
repeating the above steps for each data point in the training set and then summing
over all data points in the batch or mini-batch:
@E
@
wji=/summationdisplay
n@En
@
wji: (8.14)
238 8 BACKPROPAGATION
In the above derivation we have implicitly assumed that each hidden or output unit in
the network has the same activation function h(¬∑) However, the derivation is easily
generalized to allow different units to have individual activation functions, simply
by keeping track of which form of h(¬∑) goes with which unit 8.1.3 A simple example
The above derivation of the backpropagation procedure allowed for general
forms for the error function, the activation functions, and the network topology To
illustrate the application of this algorithm, we consider a two-layer network of the
form illustrated in Figure 6.9, together with a sum-of-squares error The output units
have linear activation functions, so that yk=ak, and the hidden units have sigmoidal
activation functions given by
h(a)‚â°tanh(a) (8.15)
where tanh(a) is deÔ¨Åned by (6.14) A useful feature of this function is that its
derivative can be expressed in a particularly simple form:
h/prime(a) = 1‚àíh(a)2: (8.16)
We also consider a sum-of-squares error function, so that for data point nthe error
is given by
En=1
2K/summationdisplay
k=1(
yk‚àítk)2(8.17)
whereykis the activation of output unit k, andtkis the corresponding target value
for a particular input vector xn For each data point in the training set in turn, we Ô¨Årst perform a forward propa-
gation using
aj=D/summationdisplay
i=0w(1)
jixi (8.18)
zj= tanh(a j) (8.19)
yk=M/summationdisplay
j=0w(2)
kjzj (8.20)
whereDis the dimensionality of the input vector xandMis the total number of
hidden units Also we have used x0=z0= 1to allow bias parameters to be included
in the weights Next we compute the ‚Äôs for each output unit using
k=yk‚àítk: (8.21)
Then, we backpropagate these errors to obtain ‚Äôs for the hidden units using
j= (1‚àíz2
j)K/summationdisplay
k=1w(2)
kjk; (8.22)
8.1 Evaluation of Gradients 239
which follows from (8.13) and (8.16) Finally, the derivatives with respect to the
Ô¨Årst-layer and second-layer weights are given by
@En
@w(1)
ji=jxi;@En
@w(2)
kj=kzj: (8.23)
8.1.4 Numerical differentiation
One of the most important aspects of backpropagation is its computational efÔ¨Å-
ciency

============================================================

=== CHUNK 236 ===
Palavras: 367
Caracteres: 2480
--------------------------------------------------
To understand this, let us examine how the number of compute operations
required to evaluate the derivatives of the error function scales with the total number
Wof weights and biases in the network A single evaluation of the error function (for a given input data point) would
requireO(W)operations, for sufÔ¨Åciently large W This follows because, except for
a network with very sparse connections, the number of weights is typically much
greater than the number of units, and so the bulk of the computational effort in for-
ward propagation arises from evaluation of the sums in (8.5), with the evaluation of
the activation functions representing a small overhead Each term in the sum in (8.5)
requires one multiplication and one addition, leading to an overall computational
cost that isO(W) An alternative approach to backpropagation for computing the derivatives of the
error function is to use Ô¨Ånite differences This can be done by perturbing each weight
in turn and approximating the derivatives by using the expression
@En
@wji=En(wji+)‚àíEn(wji)
+O() (8.24)
where/lessmuch1 In a software simulation, the accuracy of the approximation to the
derivatives can be improved by making smaller, until numerical round-off problems
arise The accuracy of the Ô¨Ånite differences method can be improved signiÔ¨Åcantly
by using symmetrical central differences of the form
@En
@wji=En(wji+)‚àíEn(wji‚àí)
2+O(2): (8.25)
In this case, theO()corrections cancel, as can be veriÔ¨Åed by a Taylor expansion Exercise 8.3
of the right-hand side of (8.25), and so the residual corrections are O(2) Note,
however, that the number of computational steps is roughly doubled compared with
(8.24).Figure8.2showsaplotoftheerrorbetweenanumericalevaluationofa
gradient using both Ô¨Ånite differences (8.24) and central differences (8.25) versus the
analytical result, as a function of the value of the step size  The main problem with numerical differentiation is that the highly desirable
O(W)scaling has been lost Each forward propagation requires O(W)steps, and
there areWweights in the network each of which must be perturbed individually, so
that the overall computational cost is O(W2) However, numerical differentiation can play a useful role in practice, because
a comparison of the derivatives calculated from a direct implementation of back-
propagation, or from automatic differentiation, with those obtained using central
differences provides a powerful check on the correctness of the software

============================================================

=== CHUNK 237 ===
Palavras: 394
Caracteres: 2599
--------------------------------------------------
BACKPROPAGATION
Figure 8.2 The red curve shows a
plot of the error between the numer-
ical evaluation of a gradient using Ô¨Å-
nite differences (8.24) and the analyti-
cal result, as a function of  Asde-
creases, the plot initially shows a lin-
ear decrease in error, and this repre-
sents a power law behaviour since the
axes are logarithmic The slope of this
line is 1which shows that this error be-
haves likeO() At some point the eval-
uated gradient reaches the limit of nu-
merical round-off and further reduction
inleads to a noisy line, which again
follows a power law but where the error
now increases with decreasing  The
blue curve shows the corresponding re-
sult for central differences (8.25) We
see a much smaller error compared to
Ô¨Ånite differences, and the slope of the
line is 2which shows that the error is
O(2) 10‚àí1310‚àí1110‚àí910‚àí7
/epsilon110‚àí2010‚àí1810‚àí1610‚àí1410‚àí12ErrorÔ¨Ånite differences
central differences
8.1.5 The Jacobian matrix
We have seen how the derivatives of an error function with respect to the weights
can be obtained by propagating errors backwards through the network Backprop-
agation can also be used to calculate other derivatives Here we consider the eval-
uation of the Jacobian matrix, whose elements are given by the derivatives of the
network outputs with respect to the inputs:
Jki‚â°@yk
@xi(8.26)
where each such derivative is evaluated with all other inputs held Ô¨Åxed Jacobian
matrices play a useful role in systems built from a number of distinct modules, as
illustratedinFigure8.3.EachmodulecancompriseaÔ¨Åxedorlearnablefunction,
which can be linear or nonlinear, so long as it is differentiable Suppose we wish to minimize an error function Ewith respect to the parameter
winFigure8.3.Thederivativeoftheerrorfunctionisgivenby
@E
@w=/summationdisplay
k;j@E
@yk@yk
@zj@zj
@w(8.27)
inwhichtheJacobianmatrixfortheredmoduleinFigure8.3appearsasthemiddle
term on the right-hand side Because the Jacobian matrix provides a measure of the local sensitivity of the
outputs to changes in each of the input variables, it also allows any known errors ‚àÜxi
8.1 Evaluation of Gradients 241
Figure 8.3 Illustration of a modular
deep learning architecture in which the
Jacobian matrix can be used to back-
propagate error signals from the out-
puts through to earlier modules in the
system.u
w x@yk
@zjyv
z@E
@yk
@E
@zj
associated with the inputs to be propagated through the trained network to estimate
their contribution ‚àÜykto the errors at the outputs, through the relation
‚àÜyk/similarequal/summationdisplay
i@yk
@xi‚àÜxi; (8.28)
which assumes that the |‚àÜxi|are small

============================================================

=== CHUNK 238 ===
Palavras: 361
Caracteres: 2279
--------------------------------------------------
In general, the network mapping represented
by a trained neural network will be nonlinear, and so the elements of the Jacobian
matrix will not be constants but will depend on the particular input vector used Thus,
(8.28) is valid only for small perturbations of the inputs, and the Jacobian itself must
be re-evaluated for each new input vector The Jacobian matrix can be evaluated using a backpropagation procedure that is
like the one derived earlier for evaluating the derivatives of an error function with
respect to the weights We start by writing the element Jkiin the form
Jki=@yk
@xi=/summationdisplay
j@yk
@aj@aj
@xi
=/summationdisplay
jwji@yk
@aj(8.29)
where we have made use of (8.5) The sum in (8.29) runs over all units jto which
the input unit isends connections (for example, over all units in the Ô¨Årst hidden
layer in the layered topology considered earlier) We now write down a recursive
backpropagation formula for the derivatives @yk=@aj:
@yk
@aj=/summationdisplay
l@yk
@al@al
@aj
=h/prime(aj)/summationdisplay
lwlj@yk
@al(8.30)
where the sum runs over all units lto which unit jsends connections (corresponding
to the Ô¨Årst index of wlj) Again, we have made use of (8.5) and (8.6) This back-
propagation starts at the output units, for which the required derivatives can be found
directly from the functional form of the output-unit activation function For linear
output units, we have
@yk
@al=kl (8.31)
242 8 BACKPROPAGATION
whereklare the elements of the identity matrix and are deÔ¨Åned by
kl=/braceleftBigg
1;ifk=l;
0;otherwise:(8.32)
If we have individual logistic sigmoid activation functions at each output unit, then Section 3.4
@yk
@al=kl/prime(al) (8.33)
whereas for softmax outputs, we have Section 3.4
@yk
@al=klyk‚àíykyl: (8.34)
We can summarize the procedure for calculating the Jacobian matrix as follows Apply the input vector corresponding to the point in input space at which the Jaco-
bian matrix is to be evaluated, and forward propagate in the usual way to obtain the
states of all the hidden and output units in the network Next, for each row kof the
Jacobian matrix, corresponding to the output unit k, backpropagate using the recur-
sive relation (8.30), starting with (8.31), (8.33) or (8.34), for all the hidden units in
the network

============================================================

=== CHUNK 239 ===
Palavras: 355
Caracteres: 2275
--------------------------------------------------
Finally, use (8.29) for the backpropagation to the inputs The Jacobian
can also be evaluated using an alternative forward propagation formalism, which can
be derived in an analogous way to the backpropagation approach given here Exercise 8.5
Again, the implementation of such algorithms can be checked using numerical
differentiation in the form
@yk
@xi=yk(xi+)‚àíyk(xi‚àí)
2+O(2); (8.35)
which involves 2Dforward propagation passes for a network having Dinputs and
therefore requiresO(DW )steps in total 8.1.6 The Hessian matrix
We have shown how backpropagation can be used to obtain the Ô¨Årst derivatives
of an error function with respect to the weights in the network Backpropagation can
also be used to evaluate the second derivatives of the error, which are given by
@2E
@wji@wlk: (8.36)
It is often convenient to consider all the weight and bias parameters as elements wi
of a single vector, denoted w, in which case the second derivatives form the elements
Hijof the Hessian matrix H:
Hij=@2E
@wi@wj(8.37)
8.1 Evaluation of Gradients 243
wherei;j‚àà{1;:::;W}andWis the total number of weights and biases The
Hessian matrix arises in several nonlinear optimization algorithms used for training
neural networks based on considerations of the second-order properties of the error
surface (Bishop, 2006) It also plays a role in some Bayesian treatments of neural
networks (MacKay, 1992; Bishop, 2006) and has been used to reduce the precision
of the weights in large language models to lessen their memory footprint (Shen et al.,
2019) An important consideration for many applications of the Hessian is the efÔ¨Åciency
with which it can be evaluated If there are Wparameters (weights and biases) in
the network, then the Hessian matrix has dimensions W√óWand so the compu-
tational effort needed to evaluate the Hessian will scale like O(W2)for each point
in the data set Extension of the backpropagation procedure (Bishop, 1992) allows
the Hessian matrix to be evaluated efÔ¨Åciently with a scaling that is indeed O(W2) Exercise 8.6
Sometimes, we do not need the Hessian matrix explicitly but only the product vTH
of the Hessian with some vector v, and this product can be calculated efÔ¨Åciently
inO(W)steps using an extension of backpropagation (M√∏ller, 1993; Pearlmutter,
1994)

============================================================

=== CHUNK 240 ===
Palavras: 398
Caracteres: 2626
--------------------------------------------------
Since neural networks may contain millions or even billions of parameters, eval-
uating, or even just storing, the full Hessian matrix for many models is infeasible Evaluating the inverse of the Hessian is even more demanding as this has O(W3)
computational scaling Consequently there is interest in Ô¨Ånding effective approxi-
mations to the full Hessian One approximation involves simply evaluating only the diagonal elements of
the Hessian and implicitly setting the off-diagonal elements to zero This requires
O(W)storage and allows the inverse to be evaluated in O(W)steps but still requires
O(W2)computation (Ricotti, Ragazzini, and Martinelli, 1988), although with fur-
ther approximation this can be reduced to O(W)steps (Becker and LeCun, 1989;
LeCun, Denker, and Solla, 1990) In practice, however, the Hessian generally has
signiÔ¨Åcant off-diagonal terms, and so this approximation must be treated with care A more convincing approach, known as the outer product approximation, is
obtained as follows Consider a regression application using a sum-of-squares error
function of the form
E=1
2N/summationdisplay
n=1(yn‚àítn)2(8.38)
where we have considered a single output to keep the notation simple (the extension
to several outputs is straightforward) We can then write the Hessian matrix in the Exercise 8.8
form
H=‚àá‚àáE =N/summationdisplay
n=1‚àáyn(‚àáyn)T+N/summationdisplay
n=1(yn‚àítn)‚àá‚àáyn (8.39)
where‚àádenotes the gradient with respect to w If the network has been trained
on the data set and its outputs ynare very close to the target values tn, then the
Ô¨Ånal term in (8.39) will be small and can be neglected More generally, however,
it may be appropriate to neglect this term based on the following argument Recall
from Section 4.2 that the optimal function that minimizes a sum-of-squares loss is
244 8 BACKPROPAGATION
the conditional average of the target data The quantity (yn‚àítn)is then a random
variable with zero mean If we assume that its value is uncorrelated with the value
of the second derivative term on the right-hand side of (8.39), then the whole term
will average to zero in the summation over n Exercise 8.9
By neglecting the second term in (8.39), we arrive at the Levenberg‚ÄìMarquardt
approximation, also known as the outer product approximation because the Hessian
matrix is built up from a sum of outer products of vectors, given by
H/similarequalN/summationdisplay
n=1‚àáan‚àáaT
n: (8.40)
Evaluating the outer product approximation for the Hessian is straightforward as it
involves only Ô¨Årst derivatives of the error function, which can be evaluated efÔ¨Åciently
inO(W)steps using standard backpropagation

============================================================

=== CHUNK 241 ===
Palavras: 372
Caracteres: 2442
--------------------------------------------------
The elements of the matrix can then
be found inO(W2)steps by simple multiplication It is important to emphasize
that this approximation is likely to be valid only for a network that has been trained
appropriately, and that for a general network mapping, the second derivative terms
on the right-hand side of (8.39) will typically not be negligible For a cross-entropy error function for a network with logistic-sigmoid output-
unit activation functions, the corresponding approximation is given by Exercise 8.10
H/similarequalN/summationdisplay
n=1yn(1‚àíyn)‚àáan‚àáaT
n: (8.41)
An analogous result can be obtained for multi-class networks having softmax output-
unit activation functions The outer product approximation can also be used to de- Exercise 8.11
velop an efÔ¨Åcient sequential procedure for approximating the inverse of a Hessian Exercise 8.12
(Hassibi and Stork, 1993) Automatic Differentiation
We have seen the importance of using gradient information to train neural networks
efÔ¨Åciently There are essentially four ways in which the gradient of a neural network Section 7.2.1
error function can be evaluated The Ô¨Årst approach, which formed the mainstay of neural networks for many
years, is to derive the backpropagation equations by hand and then to implement
them explicitly in software If this is done carefully it results in efÔ¨Åcient code that
gives precise results that are accurate to numerical precision However, the process
of deriving the equations as well as the process of coding them both take time and are
prone to errors It also results in some redundancy in the code because the forward
propagation equations are coded separately from the backpropagation equations As
these often involve duplicated calculations, then if the model is altered, both the
forward and backward implementations need to be changed in unison Automatic Differentiation 245
can easily become a limitation on how quickly and effectively different architectures
can be explored empirically A second approach is to evaluate the gradients numerically using Ô¨Ånite differ-
ences This requires only a software implementation of the forward propagation Section 8.1.4
equations One problem with numerical differentiation is that it has limited com-
putational accuracy, although this is unlikely to be an issue for network training as
we may be using stochastic gradient descent in which each evaluation is only a very
noisy estimate of the local gradient

============================================================

=== CHUNK 242 ===
Palavras: 388
Caracteres: 2534
--------------------------------------------------
The main drawback of this approach is that it
scales poorly with the size of the network However, the technique is useful for de-
bugging other approaches, because the gradients are evaluated using only the forward
propagation code and so can be used to conÔ¨Årm the correctness of backpropagation
or other code used to evaluate gradients A third approach is called symbolic differentiation and makes use of specialist
software to automate the analytical manipulations that are done by hand in the Ô¨Årst
approach This process is an example of computer algebra orsymbolic computation
and involves the automatic application of the rules of calculus, such as the chain
rule, in a completely mechanistic process The resulting expressions are then imple-
mented in standard software An obvious advantage of this approach is that it avoids
human error in the manual derivation of the backpropagation equations Moreover,
the gradients are again calculated to machine precision, and the poor scaling seen
with numerical differentiation is avoided The major downside of symbolic differ-
entiation, however, is that the resulting expressions for derivatives can become ex-
ponentially longer than the original function, with correspondingly long evaluation
times Consider a function f(x)given by the product of u(x)andv(x) The function
and its derivative are given by
f(x) =u(x)v(x) (8.42)
f/prime(x) =u/prime(x)v(x) +u(x)v/prime(x): (8.43)
We see that there is redundant computation in that u(x)andv(x)must be evalu-
ated both for the calculation of f(x)and forf/prime(x) If the factors u(x)andv(x)
themselves involve factors, then we end up with a nested duplication of expressions,
which rapidly grow in complexity This problem is called expression swell As a further illustration, consider a function that is structured like two layers of
a neural network (Grosse, 2018) with a single input x, a hidden unit with activation
z, and an output yin which
z=h(w 1x+b1) (8.44)
y=h(w 2z+b2) (8.45)
whereh(a) is the soft ReLU:
(a) = ln (1 + exp( a)): (8.46)
The overall function is therefore given by
y(x) =h(w2h(w 1x+b1) +b2) (8.47)
246 8 BACKPROPAGATION
and the derivative of the network output with respect to w1, evaluated symbolically,
is given by Exercise 8.13
@y
@w1=w2xexp/parenleftbig
w1x+b1+b2+w2ln[1 +ew1x+b 1]/parenrightbig
(1 +ew1x+b 1) (1 + exp(b2+w2ln[1 +ew1x+b 1])): (8.48)
As well as being signiÔ¨Åcantly more complex than the original function, we also see
redundant computation where expressions such as w1x+b1occur in several places

============================================================

=== CHUNK 243 ===
Palavras: 390
Caracteres: 2689
--------------------------------------------------
A further major drawback with symbolic differentiation is that it requires that
the expression to be differentiated is expressed in closed form It therefore excludes
important control Ô¨Çow operations such as loops, recursions, conditional execution,
and procedure calls, which are valuable constructs that we might wish to use when
deÔ¨Åning the network function We therefore turn to the fourth technique for evaluating derivatives in neural
networks called automatic differentiation, also known as ‚Äòautodiff‚Äô or ‚Äòalgorithmic
differentiation‚Äô (Baydin et al., 2018) Unlike symbolic differentiation, the goal of
automatic differentiation is not to Ô¨Ånd a mathematical expression for the derivatives
but to have the computer automatically generate the code that implements the gra-
dient calculations given only the code for the forward propagation equations It
is accurate to machine precision, just as with symbolic differentiation, but is more
efÔ¨Åcient because it is able to exploit intermediate variables used in the deÔ¨Ånition
of the forward propagation equations and thereby avoid redundant evaluations It
is important to note that not only can automatic differentiation handle conventional
closed-form mathematical expressions but it can also deal with Ô¨Çow control elements
such as branches, loops, recursion, and procedure calls, and is therefore signiÔ¨Åcantly
more powerful than symbolic differentiation Automatic differentiation is a well-
established Ô¨Åeld with broad applicability that was developed largely outside of the
machine learning community Modern deep learning is a largely empirical process,
involving evaluating and comparing different architectures, and automatic differenti-
ation therefore plays a key role in enabling this experimentation to be done accurately
and efÔ¨Åciently The key idea of automatic differentiation is to take the code that evaluates a func-
tion, for example the forward propagation equations that evaluate the error function
for a neural network, and augment the code with additional variables whose values
are accumulated during code execution to obtain the required derivatives There are
two principal forms of automatic differentiation, known as forward mode and re-
verse mode We start by looking at forward mode, which is conceptually somewhat
simpler 8.2.1 Forward-mode automatic differentiation
In forward-mode automatic differentiation, we augment each intermediate vari-
ablezi, known as a ‚Äòprimal‚Äô variable, involved in the evaluation of a function, such
as the error function of a neural network, with an additional variable representing
the value of some derivative of that variable, which we can denote Àôzi, known as a
‚Äòtangent‚Äô variable

============================================================

=== CHUNK 244 ===
Palavras: 363
Caracteres: 2584
--------------------------------------------------
The tangent variables and their associated code are generated
8.2 Automatic Differentiation 247
Figure 8.4 Evaluation trace diagram showing
the steps involved in the numerical evaluation
of the function (8.49) using the primal equations
(8.50) to (8.56).x1v1x1
v3v1v2
v5exp(v3)
v7v5+v6
f
x2v2
x2v4
sin(v2)v6
v3‚àív4
automatically by the software environment Instead of simply doing forward prop-
agation to compute {zi}, the code now propagates tuples (zi;Àôzi)so that variables
and derivatives are evaluated in parallel The original function is generally deÔ¨Åned
in terms of elementary operators consisting of arithmetic operations and negation
as well as transcendental functions such as exponential, logarithm, and trigonomet-
ric functions, all of which have simple formulae for their derivatives Using these
derivatives in combination with the chain rule of calculus allows the code used to
evaluate gradients to be constructed automatically As an example, consider the following function, which has two input variables:
f(x1;x2) =x1x2+ exp(x1x2)‚àísin(x2): (8.49)
When implemented in software, the code consists of a sequence of operations that
can be expressed as an evaluation trace of the underlying elementary operations Thistracecanbevisualizedintheformofagraph,asshowninFigure8.4.Herewe
have deÔ¨Åned the following primal variables
v1=x1 (8.50)
v2=x2 (8.51)
v3=v1v2 (8.52)
v4= sin(v2) (8.53)
v5= exp(v3) (8.54)
v6=v3‚àív4 (8.55)
v7=v5+v6: (8.56)
Now suppose we wish to evaluate the derivative @f=@x 1 We deÔ¨Åne the tangent
variables by Àôvi=@vi=@x 1 Expressions for evaluating these can be constructed
automatically using the chain rule of calculus:
Àôvi=@vi
@x1=/summationdisplay
j‚ààpa(i)@vj
@x1@vi
@vj=/summationdisplay
j‚ààpa(i)Àôvj@vi
@vj: (8.57)
where pa(i)denotes the set of parents of the nodeiin the evaluation trace diagram,
that is the set of variables with arrows pointing to node i.Forexample,inFigure8.4
the parents of node v3are nodesv1andv2 Applying (8.57) to the evaluation trace
248 8 BACKPROPAGATION
Figure 8.5 Extension of the example shown in
Figure8.4toafunctionwithtwooutputsf1and
f2 x1v1x1
v3v1v2
v5exp(v3)
v7v5+v6
f1
x2v2
x2v4
sin(v2)v6
v3‚àív4v8
v5v6f2
equations (8.50) to (8.56), we obtain the following evaluation trace equations for the
tangent variables
Àôv1= 1 (8.58)
Àôv2= 0 (8.59)
Àôv3=v1Àôv2+ Àôv1v2 (8.60)
Àôv4= Àôv2cos(v2) (8.61)
Àôv5= Àôv3exp(v3) (8.62)
Àôv6= Àôv3‚àíÀôv4 (8.63)
Àôv7= Àôv5+ Àôv6: (8.64)
We can summarize automatic differentiation for this example as follows We
Ô¨Årst write code to implement the evaluation of the primal variables, given by (8.50) to
(8.56)

============================================================

=== CHUNK 245 ===
Palavras: 374
Caracteres: 2483
--------------------------------------------------
The associated equations and corresponding code for evaluating the tangent
variables (8.58) to (8.64) are generated automatically To evaluate the derivative
@f=@x 1, we input speciÔ¨Åc values of x1andx2and the code then executes the primal
and tangent equations, numerically evaluating the tuples (vi;Àôvi)in sequence until
we obtain Àôv5, which is the required derivative Exercise 8.17
Now consider an example with two outputs f1(x1;x2)andf2(x1;x2)where
f1(x1;x2)is deÔ¨Åned by (8.49) and
f2(x1;x2) = (x1x2‚àísin(x2)) exp(x1x2) (8.65)
asillustratedbytheevaluationtracediagraminFigure8.5.Weseethatthisin-
volves only a small extension to the evaluation equations for the primal and tangent
variables, and so both @f1=@x 1and@f2=@x 1can be evaluated together in a single
forward pass The downside, however, is that if we wish to evaluate derivatives with
respect to a different input variable x2then we have to run a separate forward pass In general, if we have a function with Dinputs andKoutputs then a single pass
of forward-mode automatic differentiation produces a single column of the K√óD
Jacobian matrix:
J=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞@f1
@x1:::@f1
@xD @fK
@x1:::@fK
@xDÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª: (8.66)
8.2 Automatic Differentiation 249
To compute column jof the Jacobian, we need to initialize the forward pass of the
tangent equations by setting Àôxj= 1andÀôxi= 0fori/negationslash=j We can write this in vector
form as Àôx=eiwhere eiis theith unit vector To compute the full Jacobian matrix
we needDforward-mode passes However, if we wish to evaluate the product of the
Jacobian with a vector r= (r1;:::;rD)T:
J=Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞@f1
@x1:::@f1
@xD @fK
@x1:::@fK
@xDÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞r1 rDÔ£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª(8.67)
then this can be done in single forward pass by setting Àôx=r Exercise 8.18
We see that forward-mode automatic differentiation can evaluate the full K√óD
Jacobian matrix of derivatives using Dforward passes This is very efÔ¨Åcient for net-
works with a few inputs and many outputs, such that K/greatermuchD However, we often
operate in a regime where we often have just one function, namely the error function
used for training, and large numbers of variables that we want to differentiate with
respect to, comprising the weights and biases in the network, of which there may
be millions or billions In such situations, forward-mode automatic differentiation is
extremely inefÔ¨Åcient We therefore turn to an alternative version of automatic dif-
ferentiation based on the a backwards Ô¨Çow of derivative data through the evaluation
trace graph

============================================================

=== CHUNK 246 ===
Palavras: 373
Caracteres: 2402
--------------------------------------------------
8.2.2 Reverse-mode automatic differentiation
We can think of reverse-mode automatic differentiation as a generalization of
the error backpropagation procedure As with forward mode, we augment each in-
termediate variable viwith additional variables, in this case called adjoint variables,
denotedvi Consider again a situation with a single output function ffor which the
adjoint variables are deÔ¨Åned by
vi=@f
@vi: (8.68)
These can be evaluated sequentially starting with the output and working backwards
by using the chain rule of calculus:
vi=@f
@vi=/summationdisplay
j‚ààch(i)@f
@vj@vj
@vi=/summationdisplay
j‚ààch(i)vj@vj
@vi: (8.69)
Here ch(i) denotes the children of nodeiin the evaluation trace graph, in other
words the set of nodes that have arrows pointing into them from node i The succes-
sive evaluation of the adjoint variables represents a Ô¨Çow of information backwards
through the graph, as we saw previously Figure 8.1
If we again consider the speciÔ¨Åc example function given by (8.50) to (8.56), we
obtain the following evaluation equations for the evaluation of the adjoint variables Exercise 8.16
250 8 BACKPROPAGATION
v7= 1 (8.70)
v6=v7 (8.71)
v5=v7 (8.72)
v4=‚àív6 (8.73)
v3=v5v5+v6 (8.74)
v2=v2v1+v4cos(v 2) (8.75)
v1=v3v2: (8.76)
Note that these start at the output and then Ô¨Çow backwards through the graph to the
inputs Even with multiple inputs, only a single backward pass is required to evaluate
the derivatives For a neural network error function, the derivatives of Ewith respect
to the weight and biases are obtained as the corresponding adjoint variables How-
ever, if we now have more than one output then we need to run a separate backward Figure 8.5
pass for each output variable Reverse mode is often more memory intensive than forward mode because all
of the intermediate primal variables must be stored so that they will be available as
needed when evaluating the adjoint variables during the backward pass By contrast,
with forward mode, the primal and tangent variables are computed together during
the forward pass, and therefore variables can be discarded once they have been used It is therefore also generally easier to implement forward mode compared to reverse
mode For both forward-mode and reverse-mode automatic differentiation, a single
pass through the network is guaranteed to take no more than 6 times the compu-
tational cost of a single function evaluation

============================================================

=== CHUNK 247 ===
Palavras: 395
Caracteres: 2446
--------------------------------------------------
In practice, the overhead is typically
closer to a factor of 2 or 3 (Griewank and Walther, 2008) Hybrids of forward and
reverse modes are also of interest One situation in which this arises is in the eval-
uation of the product of a Hessian matrix with a vector, which can be calculated
without explicit evaluation of the full Hessian (Pearlmutter, 1994) Here we can use
reverse mode to calculate the gradient of code, which itself has been generated by the
forward model We start from a vector band a point xat which the Hessian‚Äìvector
product is to be evaluated By setting Àôx=vand using forward mode, we obtain
the directional derivative vT‚àáf This is then differentiated using reverse mode to
obtain‚àá2fv=Hv IfWis the number of parameters in the neural network then
this evaluation has O(W)complexity even though the Hessian is of size W√óW The Hessian itself can also be evaluated explicitly using automatic differentiation
but this hasO(W2)complexity Exercises
8.1 (?)By making use of (8.5), (8.6), (8.8), and (8.12), verify the backpropagation for-
mula (8.13) for evaluating the derivatives of an error function 8.2 (??) Consider a network that consists of layers and rewrite the backpropagation
formula (8.13) in matrix notation by starting with the forward propagation equation
(6.19) Note that the result involves multiplication by the transposes of the matrices Exer
cises 251
8.3 (?)By using a Taylor expansion, verify that the terms that are O()cancel on the
right-hand side of (8.25) 8.4 (??) Consider a two-layer network of the form shown in Figure 6.9 with the addition
of extra parameters corresponding to skip-layer connections that go directly from the
inputs to the outputs By extending the discussion of Section 8.1.3, write down the
equations for the derivatives of the error function with respect to these additional
parameters 8.5 (???) In Section 8.1.5, we derived a procedure for evaluating the Jacobian matrix of
a neural network using a backpropagation procedure Derive an alternative formal-
ism for Ô¨Ånding the Jacobian based on forward propagation equations 8.6 (???) Consider a two-layer neural network, and deÔ¨Åne the quantities
k=@En
@
ak; M kk/prime‚â°@2En
@
ak@ak/prime: (8.77)
Derive expressions for the elements of the Hessian matrix expressed in terms of k
andMkk/primefor elements in which (i) both weights are in the second layer, (ii) both
weights are in the Ô¨Årst layer, and (iii) one weight is in each layer

============================================================

=== CHUNK 248 ===
Palavras: 356
Caracteres: 2280
--------------------------------------------------
8.7 (???) Extend the results of Exercise 8.6 for the exact Hessian of a two-layer network
to include skip-layer connections that go directly from inputs to outputs 8.8 (??) The outer product approximation to the Hessian matrix for a neural network us-
ing a sum-of-squares error function is given by (8.40) Extend this result for multiple
outputs 8.9 (??) Consider a squared-loss function of the form
E(w) =1
2/integraldisplay
/integraldisplay
{y(x;w)‚àít}2p(x;t) d xdt (8.78)
wherey(x;w)is a parametric function such as a neural network The result (4.37)
shows that the function y(x;w)that minimizes this error is given by the conditional
expectation of tgiven x Use this result to show that the second derivative of Ewith
respect to two elements wrandwsof the vector w, is given by
@2E
@
wr@ws=/integraldisplay@y
@
wr@y
@
wsp(x) d x: (8.79)
Note that, for a Ô¨Ånite sample from p(x), we obtain (8.40) 8.10 (??) Derive the expression (8.41) for the outer product approximation of a Hessian
matrix for a network having a single output with a logistic-sigmoid output-unit acti-
vation function and a cross-entropy error function, corresponding to the result (8.40)
for the sum-of-squares error function BACKPROPAGATION
8.11 (??) Derive an expression for the outer product approximation of a Hessian matrix
for a network having Koutputs with a softmax output-unit activation function and
a cross-entropy error function, corresponding to the result (8.40) for the sum-of-
squares error function 8.12 (???) Consider the matrix identity
/parenleftbig
M+vvT/parenrightbig‚àí1=M‚àí1‚àí(M‚àí1v)/parenleftbig
vTM‚àí1/parenrightbig
1
+vTM‚àí1v; (8.80)
which is simply a special case of the Woodbury identity (A.7) By applying (8.80)
to the outer product approximation (8.40) for a Hessian matrix, derive a formula that
allows the inverse of the Hessian matrix to be computed by making one pass through
the training data and updating the inverse Hessian with each data point Note that
the algorithm can initialized using H=Iwhereis a small constant, and that the
results are not particularly sensitive to the precise value of 8.13 (??) Verify that the derivative of (8.47) is given by (8.48) 8.14 (??) The logistic map is a function deÔ¨Åned by the iterative relation Ln+1(x) =
4Ln(x) (1‚àíLn(x)) withL1(x) =x

============================================================

=== CHUNK 249 ===
Palavras: 387
Caracteres: 2540
--------------------------------------------------
Write down the evaluation trace equations for
L2(x),L3(x), andL4(x), and then write down expressions for the corresponding
derivativesL/prime
1(x),L/prime
2(x),L/prime
3(x), andL/prime
4(x) Do not simplify the expressions but
instead simply note how the complexity of the formulae for the derivatives grows
much more rapidly than the expressions for the functions themselves 8.15 (??) Starting from the evaluation trace equations (8.50) to (8.56) for the example
function (8.49), use (8.57) to derive the forward-mode tangent variable evaluation
equations (8.58) to (8.64) 8.16 (??) Starting from the evaluation trace equations (8.50) to (8.56) for the example
function (8.49), and referring to Figure 8.4, use (8.69) to derive the reverse-mode
adjoint variable evaluation equations (8.70) to (8.76) 8.17 (???) Consider the example function (8.49) Write down an expression for @f=@x 1
and evaluate this function for x1= 1andx2= 2 Now use the evaluation trace equa-
tions (8.50) to (8.56) to evaluate the variables v1tov7and then use the evaluation
trace equations of forward-mode automatic differentiation to evaluate the tangent
variables Àôv1toÀôv7and to conÔ¨Årm that the resulting value of @f=@x 1agrees with that
found directly Similarly, use the evaluation trace equations of reverse-mode auto-
matic differentiation (8.70) to (8.76) to evaluate the adjoint variables v7tov1and
ag
ain conÔ¨Årm that the resulting value of @f=@x 1agrees with that found directly 8.18 (??) By expressing an arbitrary vector r= (r1;:::;rD)Tas a linear combination of
the unit vectors ei, wherei= 1;:::;D , show that the product of the Jacobian of a
function with rin the form (8.67) can be evaluated using a single pass of forward-
mode automatic differentiation by setting Àôx=r 9
Regularization
We introduced the concept of regularization when discussing polynomial curve Ô¨Åt-
ting as a way to reduce over-Ô¨Åtting by discouraging the parameters of the model from Section 1.2
taking values with a large magnitude This involved adding a simple penalty term to
the error function to give a regularized error function in the form
/tildewideE(w) =E(w) +
2wTw (9.1)
where wis the vector of model parameters, E(w)is the unregularized error function,
and the regularization hyperparameter controls the strength of the regularization
effect An improvement in predictive accuracy with such a regularizer can be under-
stood in terms of the bias‚Äìvariance trade-off through the reduction in the variance of Section 4.3
the solution at the expense of some increase in bias

============================================================

=== CHUNK 250 ===
Palavras: 360
Caracteres: 2347
--------------------------------------------------
In this chapter we will explore
regularization in depth and will discuss several different approaches to regulariza-
253 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_9    
254 9 We will also look more broadly at the important role of bias in achieving good
generalization from Ô¨Ånite training data sets In a practical application, it is very unlikely that the process that generates the
data will correspond precisely to a particular neural network architecture, and so
any given neural network will only ever represent an approximation to the true data
generator Larger networks can provide closer approximations, but this comes at the
risk of over-Ô¨Åtting In practice, we Ô¨Ånd that the best generalization results are almost
always obtained by using a larger network combined with some form of regular-
ization In this chapter we explore several alternative regularization techniques in-
cluding early stopping, model averaging, dropout, data augmentation, and parameter
sharing Multiple forms of regularization can be used together if desired For exam-
ple, error function regularization of the form (9.1) is often used alongside dropout Inductive Bias
When we compared the predictive error of polynomials of various orders for the si-
nusoidal synthetic data problem, we saw that the smallest generalization error was Section 1.2.4
achieved using a polynomial of intermediate complexity, being neither too simple
nor too Ô¨Çexible A similar result was found when we used a regularization term of
the form (9.1) to control model complexity, as an intermediate value of the regular- Section 1.2.5
ization coefÔ¨Åcient gave the best predictions for new input values Insight into this
result came from the bias‚Äìvariance decomposition, where we saw that an appropriate Section 4.3
level of bias in the model was important to allow generalization from Ô¨Ånite data sets Simple models with high bias are unable to capture the variation in the underlying
data generation process, whereas highly Ô¨Çexible models with low bias are prone to
over-Ô¨Åtting leading to poor generalization As the size of the data set grows, we can
afford to use more Ô¨Çexible models having less bias without incurring excessive vari-
ance, thereby leading to improved generalization

============================================================

=== CHUNK 251 ===
Palavras: 362
Caracteres: 2262
--------------------------------------------------
Note that in a practical setting, our
choice of model might also be inÔ¨Çuenced by factors such as memory usage or speed
of execution Here we ignore such ancillary considerations and focus on the core
goal of achieving good predictive performance, in other words good generalization 9.1.1 Inverse problems
This issue of model choice lies at the heart of machine learning and can be
traced to the fact that most machine learning tasks are examples of inverse prob-
lems Given a conditional distribution p(t|x) along with a Ô¨Ånite set of input points
{x1;:::;xN}, it is straightforward, at least in principle, to sample corresponding
values{t1;:::;tN}from that distribution In machine learning, however, we have
to solve the inverse of this problem, namely to infer an entire distribution given only a
Ô¨Ånite number of samples This is intrinsically ill-posed, as there are inÔ¨Ånitely many
distributions which could potentially have been responsible for generating the ob-
served training data In fact any distribution that has a non-zero probability density
at the observed target values is a candidate Inductive Bias 255
For machine learning to be useful, however, we need to make predictions for
new values of x, and therefore we need a way to choose a speciÔ¨Åc distribution from
amongst the inÔ¨Ånitely many possibilities The preference for one choice over oth-
ers is called inductive bias, or prior knowledge, and plays a central role in machine
learning Prior knowledge may come from background information that helps con-
strain the space of solutions For many applications, small changes in the input
values should lead to small changes in the output values, and so we should bias our
solutions towards those with smoothly varying functions Regularization terms of
the form (9.1) encourage the model weights to have a smaller magnitude and hence
introduce a bias towards functions that vary more slowly with changes in the inputs Likewise, when detecting objects in images, we can introduce prior knowledge that
the identity of an object is generally independent of its location within the image Chapter 10
This is known as translation invariance, and incorporating this into our solution can
greatly simplify the task of building a system with good generalization

============================================================

=== CHUNK 252 ===
Palavras: 370
Caracteres: 2388
--------------------------------------------------
However,
care must be taken not to incorporate biases or constraints that are inconsistent with
the underlying process that generates the data For example, assuming that the re-
lationship between outputs and inputs is linear when in fact there are signiÔ¨Åcant
nonlinearities can lead to a system that produces inaccurate answers Techniques such as transfer learning and multi-task learning can also be viewed Section 6.3.4
from the perspective of regularization When training data for a particular task is
limited, additional data from a different, but related, task can be used to help de-
termine the learnable parameters in a neural network The assumption of similarity
between the tasks represents a more sophisticated form of inductive bias compared
to simple regularization, and this explains the improved performance resulting from
the use of the additional data 9.1.2 No free lunch theorem
The core focus of this book is on the important class of machine learning mod-
els called deep neural networks These are highly Ô¨Çexible models and have revo-
lutionized many Ô¨Åelds including computer vision, speech recognition, and natural
language processing In fact, they have become the framework of choice for the
great majority of machine learning applications It might appear, therefore, that they
represent a ‚Äòuniversal‚Äô learning algorithm able to solve all tasks However, even very
Ô¨Çexible neural networks contain important inductive biases For example, convolu-
tional neural networks encode speciÔ¨Åc forms of inductive bias, including translation Chapter 10
equivariance, that are especially useful in applications involving images Theno free lunch theorem (Wolpert, 1996), named from the expression ‚ÄòThere‚Äôs
no such thing as a free lunch,‚Äô states that every learning algorithm is as good as any
other when averaged over all possible problems If a particular model or algorithm
is better than average on some problems, it must be worse than average on others However, this is a rather theoretical notion as the space of possible problems here
includes relationships between input and output that would be very uncharacteristic
of any plausible practical application For example, we have already noted that most
examples of practical interest exhibit some degree of smoothness, in which small
changes in the input values are associated, for the most part, with small changes in
256 9

============================================================

=== CHUNK 253 ===
Palavras: 363
Caracteres: 2278
--------------------------------------------------
REGULARIZATION
the target values Models such as neural networks, and indeed most widely used
machine learning techniques, exhibit this form of inductive bias, and therefore to
some degree, they have broad applicability Although the no free lunch theorem is somewhat theoretical, it does highlight
the central importance of bias in determining the performance of a machine learning
algorithm It is not possible to learn ‚Äòpurely from data‚Äô in the absence of any bias In
practice, bias may be implicit For example, every neural network has a Ô¨Ånite number
of parameters, which therefore limits the functions that it can represent However,
bias may also be encoded explicitly as a reÔ¨Çection of prior knowledge relating to the
speciÔ¨Åc type of problem being solved In trying to Ô¨Ånd general-purpose learning algorithms, we are really seeking in-
ductive biases that are appropriate to the broad classes of applications that will be
encountered in practice For any given application, however, better results can be ob-
tained if it is possible to incorporate stronger inductive biases that are speciÔ¨Åc to that
application The perspective of model-based machine learning (Winn et al., 2023)
advocates making all the assumptions in machine learning models explicit so that
the appropriate choices can be made for inductive biases We have seen that inductive bias can be incorporated through the form of dis-
tribution, for example by specifying that the output is a linear function of a Ô¨Åxed
set of speciÔ¨Åc basis functions It can also be incorporated through the addition of
a regularization term to the error function used during training Yet another way to
control the complexity of a neural network is through the training process itself We Chapter 7
will see that deep neural networks can give good generalization even when the num-
ber of adjustable parameters exceeds the number of training data points, provided
the training process is set up correctly Part of the skill in applying deep learning to
real-world problems is in the careful design of inductive bias and the incorporation
of prior knowledge 9.1.3 Symmetry and invariance
In many applications of machine learning, the predictions should be unchanged,
orinvariant, under one or more transformations of the input variables

============================================================

=== CHUNK 254 ===
Palavras: 374
Caracteres: 2368
--------------------------------------------------
For example,
when classifying an object in two-dimensional images, such as a cat or dog, a par-
ticular object should be assigned the same classiÔ¨Åcation irrespective of its position
within the image This is known as translation invariance Likewise changes to the
size of the object within the image should also leave its classiÔ¨Åcation unchanged This is called scale invariance Exploiting such symmetries to create inductive bi-
ases can greatly improve the performance of machine learning models and forms the
subject of geometric deep learning (Bronstein et al., 2021) Transformations, such as a translation or scaling, that leave particular properties
unchanged, represent symmetries The set of all transformations corresponding to a
particular symmetry form a mathematical structure called a group A group consists
of a set of elements A;B;C;:::together with a binary operation for composing pairs
of elements together, which we denote using the notation A‚ó¶B The following four
axioms hold for a group:
9.1 Inductive Bias 257
1.Closed: For any two elements A;Bin the set,A‚ó¶B must also be in the set 2.Associative: For any three elements A;B;Cin the set, (A‚ó¶B )‚ó¶C=A‚ó¶(B‚ó¶C ) 3.Identity: There exists an element Iof the set, called the identity, with the
property:A‚ó¶I =I‚ó¶A =Afor every elementAin the set 4.Inverse: For each element Ain the set, there exists another element in the
set, which we denote by A‚àí1, called the inverse, which has the property: A‚ó¶
A‚àí1=A‚àí1‚ó¶A=I Simple examples of groups include the set of rotations of a square through multiples
of90‚ó¶or the set of continuous translations of an object in a two-dimensional plane Exercise 9.1
In principle, invariance of the predictions made by a neural network to trans-
formations of the input space could be learned from data, without any special mod-
iÔ¨Åcations to the network or the training procedure In practice, however, this can
prove to be extremely challenging because such transformations can produce sub-
stantial changes in the raw data For example, relatively small translations of an
object within an image, even by a few pixels, can cause pixel values to change sig-
niÔ¨Åcantly Furthermore, multiple invariances must often hold at the same time, for
example invariance to translations in two dimensions as well as scaling, rotation,
changes of intensity, changes of colour balance, and many others

============================================================

=== CHUNK 255 ===
Palavras: 350
Caracteres: 2334
--------------------------------------------------
There are expo-
nentially many possible combinations of such transformations, making the size of
the required training set needed to learn all of these invariances prohibitive We therefore seek more efÔ¨Åcient approaches for encouraging an adaptive model
to exhibit the required invariances These can broadly be divided into four categories:
1.Pre-processing Invariances are built into a pre-processing stage by computing
features of the data that are invariant under the required transformations Any
subsequent regression or classiÔ¨Åcation system that uses such features as inputs
will necessarily also respect these invariances 2.Regularized error function A regularization term is added to the error func-
tion to penalize changes in the model output when the input is subject to one
of the invariant transformations The training set is expanded using replicas of the training
data points, transformed according to the desired invariances and carrying the
same output target values as the untransformed examples 4.Network architecture The invariance properties are built into the structure
of a neural network through an appropriate choice of network architecture One challenge with approach 1 is to design features that exhibit the required
invariances without also discarding information that can be useful for determining
the network outputs We have already seen that Ô¨Åxed, hand-crafted features have
limited capabilities and have been superseded by learned representations obtained Chapter 6
using deep neural networks REGULARIZATION
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
Figure 9.1 Illustration of data set augmentation, showing (a) the original image, (b) horizontal inversion, (c)
scaling, (d) translation, (e) rotation, (f) brightness and contrast change, (g) additive noise, and (h) colour shift An example of approach 2 is the technique of tangent propagation (Simard et al.,
1992) in which a regularisation term is added to the error function during training This term directly penalizes changes in the output resulting from changes in the input
variables that correspond to one of the invariant transformations A limitation of this
technique, in addition to the extra complexity of training, is can only cope with small
transformations (e.g., translations by less than a pixel) Approach 3 is known as data set augmentation

============================================================

=== CHUNK 256 ===
Palavras: 377
Caracteres: 2366
--------------------------------------------------
It is often relatively easy to
implement and can prove to be very effective in practice It is often applied in the
context of image analysis as it straightforward to create the transformed training data Figure 9.1 shows examples of such transformations applied to an image of a cat For medical images of soft tissue, data augmentation could also include continuous
‚Äòrubber sheet‚Äô deformations (Ronneberger, Fischer, and Brox, 2015) For sequential training algorithms, such as stochastic gradient descent, the data
set can be augmented by transforming each input data point before it is presented
to the model so that, if the data points are being recycled, a different transformation
(drawn from an appropriate distribution) is applied each time For batch methods, a
similar effect can be achieved by replicating each data point a number of times and
transforming each copy independently We can analyse the effect of using augmented data by considering transforma-
tions that represent small changes to the original examples and then making a Taylor
expansion of the error function in powers of the magnitude of the transformation
(Bishop, 1995c; Leen, 1995; Bishop, 2006) This leads to a regularized error func-
tion in which the regularizer penalizes the gradient of the network output with respect
9.1 Inductive Bias 259
to the input variables projected onto the direction of transformation This is related
to the technique of tangent propagation discussed above A special case arises when
the transformation of the input variables consists simply of the addition of random
noise, in which case the regularizer penalizes the derivatives of the network outputs
with respect to the inputs Again, this is intuitively reasonable, since we are encour- Exercise 9.2
aging the network outputs to remain unchanged despite the addition of noise to the
input variables Finally, approach 4, in which we build invariances into the structure of the net-
work, has proven to be very powerful and effective and leads to other key beneÔ¨Åts We will discuss this approach at length in the context of convolutional neural net- Chapter 10
works for computer vision 9.1.4 Equivariance
An important generalization of invariance is called equivariance in which the
output of the network, instead of remaining constant when the input is transformed,
is itself transformed in a speciÔ¨Åc way

============================================================

=== CHUNK 257 ===
Palavras: 358
Caracteres: 2174
--------------------------------------------------
For example, consider a network that takes
an image as input and returns a segmentation of that image in which each pixel
is classiÔ¨Åed as belonging either to a foreground object or to the background In
this case, if the location of the object within the image is translated, we want the
corresponding segmentation of the object to be similarly translated Suppose we
denote the image by I, and the operation of the segmentation network by S, then for
a translation operation Twe have
S(T(I)) =T(S(I)); (9.2)
which says that the segmentation of the translated image is given by the translation
of the segmentation of the original image This is illustrated in Figure 9.2
More generally, equivariance can hold if the transformation applied to the output
is different to that applied to the input:
S(T(I)) =/tildewideT(S(I)): (9.3)
For example, if the segmented image has a lower resolution than the original image,
then ifTis a translation in the original image space, /tildewideTrepresents the corresponding
translation in the lower-dimensional segmentation space Similarly, if Sis an oper-
ator that measures the orientation of an object within an image, and Trepresents a
rotation (which is a complex nonlinear transformation of all of the pixel values in the
image) then/tildewideTwill increment or decrement the scalar orientation value generated by
S We also see that invariance is a special case of equivariance in which the output
transformation is simply the identity For example, if Cis a neural network that
classiÔ¨Åes an object within an image and Tis a translation operator then
C(T(I)) =C(I); (9.4)
which says that the class of the object does not depend on its position within the
image REGULARIZATION
Figure 9.2 Illustration of equivariance, cor-
responding to (9.2) If an image
(a) is Ô¨Årst translated to give (b)
and then segmented to give (d),
the result is the same as if the
image is Ô¨Årst segmented to give
(c) and then translated to give
(d) (a)
 (b)
(c)
 (d)S ST
T
9.2 Weight Decay
We introduced regularization in the context of linear regression to control model Section 1.2.5
complexity, as an alternative to limiting the number of parameters in the model

============================================================

=== CHUNK 258 ===
Palavras: 361
Caracteres: 2267
--------------------------------------------------
The
simplest regularizer comprises the sum of the squares of the model parameters to
give a regularized error function of the form (9.1), which penalizes parameter values
with large magnitude The effective model complexity is then determined by the
choice of the regularization coefÔ¨Åcient  We have also seen that this additive regularization term can be interpreted as
the negative logarithm of a zero-mean Gaussian prior distribution over the weight
vector w This provides a probabilistic perspective on the inclusion of prior knowl- Section 2.6.2
edge into the model training process Unfortunately, this prior is expressed over the
model parameters, whereas any domain knowledge we might possess regarding the
problem to be solved is more likely to be expressed in terms of the network function
from inputs to outputs The relationship between the parameters and the network
function is, however, extremely complex, and therefore only very limited kinds of
prior knowledge can easily be expressed directly as priors over model parameters The sum-of-squares regularizer in (9.1) is known in the machine learning litera-
ture as weight decay because in sequential learning algorithms, it encourages weight
values to decay towards zero, unless supported by the data One advantage of this Exercise 9.3
kind of regularizer is that it is trivial to evaluate its derivatives for use in gradient
descent training SpeciÔ¨Åcally for (9.1) the gradient is given by
‚àá/tildewideE(w) =‚àáE(w) +w: (9.5)
9.2 Weight Decay 261
u1u2
w1w2 E(w)
E(w) +Œª(w2
1+w2
2)
w2
1+w2
2w‚ãÜ
/hatwidew
Figure
9.3 Contours of the error function (red), the regularization term (green), and a linear combination of
the two (blue) for a quadratic error function and a sum-of-squares regularizer (w2
1+w2
2) Here the axes in
parameter space have been rotated to align with the axes of the elliptical contour of the unregularized error
function For = 0, the minimum error is indicated by w When >0, the minimum of the regularized error
functionE(w) +(w2
1+w2
2)is shifted towards the origin This shift is greater in the direction of w1because
the unregularized error is relatively insensitive to the parameter value, and less in direction w2where the error is
more strongly dependent on the parameter value

============================================================

=== CHUNK 259 ===
Palavras: 344
Caracteres: 2168
--------------------------------------------------
The regularization term is effectively suppressing parameters
that have only a small effect on the accuracy of the network predictions We see that the factor of 1=2in (9.1), which is often included by convention, disap-
pears when we take the derivative The general effect of a quadratic regularizer can be seen by considering a two-
dimensional parameter space along with an unregularized error function E(w)that
is a quadratic function of w(corresponding to a simple linear regression model with
a sum-of-squares error function), as illustrated in Figure 9.3 The axes in param- Section 4.1.2
eter space have been rotated to align with the eigenvectors of the Hessian matrix, Section 8.1.6
corresponding to the axes of the elliptical error function contours We see that the
effect of the regularization term is to shrink the magnitudes of the weight parameters However, the effect is much larger for parameter w1because the unregularized error
is much less sensitive to the value of w1compared to that of w2 Intuitively only
the parameter w2is ‚Äòactive‚Äô because the output is relatively insensitive to w1, and
hence the regularizer pushes w1close to zero The effective number of parameters
is the number that remain active after regularization, and this concept can be formal-
ized either from a Bayesian or from a frequentist perspective (Bishop, 2006; Hastie,
Tibshirani, and Friedman, 2009) For ‚Üí‚àû, all the parameters are driven to zero
and the effective number of parameters is then zero As is reduced, the number
of parameters increases until for = 0 it equals the total number of actual param-
262 9 REGULARIZATION
eters in the model We see that controlling model complexity by regularization has
similarities to controlling model complexity by limiting the number of parameters 9.2.1 Consistent regularizers
One of the limitations of simple weight decay in the form (9.1) is that it breaks
certain desirable transformation properties of network mappings To illustrate this,
consider a multilayer perceptron network having two layers of weights and linear
output units that performs a mapping from a set of input variables {xi}to a set of
output variables{yk}

============================================================

=== CHUNK 260 ===
Palavras: 368
Caracteres: 2576
--------------------------------------------------
The activations of the hidden units in the Ô¨Årst hidden layer
take the form
zj=h/parenleftBigg/summationdisplay
iwjixi+wj0/parenrightBigg
(9.6)
whereas the activations of the output units are given by
yk=/summationdisplay
jwkjzj+wk0: (9.7)
Suppose we perform a linear transformation of the input data:
xi‚Üí/tildewidexi=axi+b: (9.8)
Then we can arrange for the mapping performed by the network to be unchanged
by making a corresponding linear transformation of the weights and biases from the
inputs to the units in the hidden layer: Exercise 9.4
wji‚Üí/tildewidewji=1
awji (9.9)
wj0‚Üí/tildewidewj0=wj0‚àíb
a/summationdisplay
iwji: (9.10)
Similarly, a linear transformation of the output variables of the network of the form
yk‚Üí/tildewideyk=cyk+d (9.11)
can be achieved transforming the second-layer weights and biases using
wkj‚Üí/tildewidewkj=cwkj (9.12)
wk0‚Üí/tildewidewk0=cwk0+d: (9.13)
If we train one network using the original data and one network using data for which
the input and/or target variables have been transformed by one of the above linear
transformations, then consistency requires that we should obtain equivalent networks
that differ only by the linear transformation of the weights as given Any regularizer
should be consistent with this property, otherwise it would arbitrarily favour one
solution over another, equivalent one Clearly, simple weight decay (9.1), which
treats all weights and biases on an equal footing, does not satisfy this property Weight Decay 263
We therefore look for a regularizer that is invariant under the linear transforma-
tions (9.9), (9.10), (9.12), and (9.13) These require that the regularizer should be
invariant to re-scaling of the weights and to shifts of the biases Such a regularizer is
given by
1
2/summationdisplay
w‚ààW 1w2+2
2/summationdisplay
w‚ààW 2w2(9.14)
whereW1denotes
the set of weights in the Ô¨Årst layer, W2denotes the set of weights
in the second layer, and biases are excluded from the summations This regularizer
will remain unchanged under the weight transformations provided the regularization
parameters are re-scaled using 1‚Üía1=21and2‚Üíc‚àí1=22 The regularizer (9.14) corresponds to a prior distribution over the parameters of
the form:
p(w|1;2)‚àùexp/parenleftBigg
‚àí1
2/summationdisplay
w‚ààW 1w2‚àí2
2/summationdisplay
w‚ààW 2w2/parenrightBigg
: (9.15)
Note
that priors of this form are improper (they cannot be normalized) because the
bias parameters are unconstrained Using improper priors can lead to difÔ¨Åculties in
selecting regularization coefÔ¨Åcients and in model comparison within the Bayesian
framework

============================================================

=== CHUNK 261 ===
Palavras: 356
Caracteres: 2170
--------------------------------------------------
It is therefore common to include separate priors for the biases (which
then break the shift invariance) that have their own hyperparameters We can illustrate the effect of the resulting four hyperparameters by drawing
samples from the prior and plotting the corresponding network functions, as shown
inFigure 9.4 The priors are governed by four hyperparameters, b
1,w
1,b
2, andw
2,
which represent the precisions of the Gaussian distributions of the Ô¨Årst-layer biases,
Ô¨Årst-layer weights, second-layer biases, and second-layer weights, respectively We
see that the parameter w
2governs the vertical scale of the functions (note the dif-
ferent vertical axis ranges on the top two diagrams), w
1governs the horizontal scale
of variations in the function values, and b
1governs the horizontal range over which
variations occur The parameter b
2, whose effect is not illustrated here, governs the
range of the vertical offsets of the functions
More generally, we can consider regularizers in which the weights are divided
into any number of groups Wkso that
‚Ñ¶(w) =1
2/summationdisplay
kk/bardbl
w/bardbl2
k (9.16)
where
/bardblw/bardbl2
k=/summationdisplay
j‚ààWkw2
j: (9.17)
For example, we could use a different regularizer for each layer in a multilayer net-
work REGULARIZATION
Œ±w
1= 1,Œ±b
1= 1,Œ±w
2= 1,Œ±b
2= 1
‚àí1 ‚àí0.5 0 0.5 1‚àí6‚àí4‚àí2024
Œ±w
1= 1,Œ±b
1= 1,Œ±w
2= 10, Œ±b
2= 1
‚àí1 ‚àí0.5 0 0.5 1‚àí60‚àí40‚àí2002040
Œ±w
1= 1000, Œ±b
1= 100,Œ±w
2= 1,Œ±b
2= 1
‚àí1 ‚àí0.5 0 0.5 1‚àí10‚àí505
Œ±w
1= 1000, Œ±b
1= 1000, Œ±w
2= 1,Œ±b
2= 1
‚àí1 ‚àí0.5 0 0.5 1‚àí10‚àí505
Figure
9.4 Illustration of the effect of the hyperparameters governing the prior distribution over weights and
biases in a two-layer network having a single input, a single linear output, and 12hidden units with tanh activation
functions 9.2.2 Generalized weight decay
A generalization of the simple quadratic regularizer is sometimes used:
‚Ñ¶(w) =
2M/summationdisplay
j=1|
wj|q(9.18)
whereq= 2 corresponds to the quadratic regularizer in (9.1) Figure 9.5 shows
contours of the regularization function for different values of q A regularizer of the form (9.18) with q= 1is known as a lasso in the statistics
literature (Tibshirani, 1996)

============================================================

=== CHUNK 262 ===
Palavras: 366
Caracteres: 2309
--------------------------------------------------
For quadratic error functions, it has the property that
ifis sufÔ¨Åciently large, some of the coefÔ¨Åcients wjare driven to zero, leading to a
sparse model in which the corresponding basis functions play no role Weight Decay 265
Figure 9.5 Contours of the regu-
larization term in (9.18) for various
values of the parameter q w1w2
q= 0.5
w1w2
q= 1
w1w2
q= 2
w1w2
q= 4
we
Ô¨Årst note that minimizing the regularized error function given by
E(w) +
2M/summationdisplay
j=1|
wj|q(9.19)
is equivalent to minimizing the unregularized error function E(w)subject to the
constraint Exercise 9.5
M/summationdisplay
j=1|wj|q6 (9.20)
for an appropriate value of the parameter , where the two approaches can be related
using Lagrange multipliers The origin of the sparsity can be seen in Figure 9.6, Appendix C
which shows the minimum of the error function, subject to the constraint (9.20) As
is increased, more parameters will be driven to zero By comparison, a quadratic
regularizer leaves both weight parameters with non-zero values Regularization allows complex models to be trained on data sets of limited size
without severe over-Ô¨Åtting, essentially by limiting the effective model complexity However, the problem of determining the optimal model complexity is then shifted
from one of Ô¨Ånding the appropriate number of learnable parameters to one of deter-
mining a suitable value of the regularization coefÔ¨Åcient  We will discuss the issue
of model complexity in the next section REGULARIZATION
w1w2
/hatwidew
|w1|+|w2|/lessorequalslantŒ∑E(w)
w1w2
/hatwidew
w2
1+w2
2/lessorequalslantŒ∑E(w)
Figure 9.6 Plot of the contours of the unregularized error function (red) along with the constraint region (9.20)
for the lasso regularizer q= 1on the left, and the quadratic regularizer q= 2on the right, in which the optimum
value for the parameter vector wis denoted by bw The lasso gives a sparse solution in which bw1= 0, whereas
the quadratic regularizer simply reduces w1to a smaller value Learning Curves
We have already explored how the generalization performance of a model varies as
we change the number of parameters in the model, the size of the data set, and the
coefÔ¨Åcient of a weight-decay regularization term Each of these allows for a trade-
off between bias and variance to minimize the generalization error

============================================================

=== CHUNK 263 ===
Palavras: 370
Caracteres: 2320
--------------------------------------------------
Another factor
that inÔ¨Çuences this trade-off is the learning process itself During optimization of
the error function through gradient descent, the training error typically decreases
as the model parameters are updated, whereas the error for hold-out data may be
non-monotonic This behaviour can be visualized using learning curves, which plot
performance measures such as training set and validation set error as a function of
iteration number during an iterative learning process such as stochastic gradient de-
scent These curves provide insight into the progress of training and also offer a
practical methodology for controlling the effective model complexity 9.3.1 Early stopping
An alternative to regularization as a way of controlling the effective complexity
of a network is early stopping The training of deep learning models involves an
iterative reduction of the error function deÔ¨Åned with respect to a set of training data Although the error function evaluated using the training set often shows a broadly
monotonic decrease as a function of the iteration number, the error measured with
respect to held-out data, generally called a validation set, often shows a decrease at
9.3 Learning Curves 267
0 10 20 30 40 500.150.20.25
0 10 20 30 40 500.350.40.45
Figure
9.7 An illustration of the behaviour of training set error (left) and validation set error (right) during a
typical training session, as a function of the iteration step, for the sinusoidal data set To achieve the best
generalization performance , the training should be stopped at the point shown by the vertical dashed lines,
corresponding to the minimum of the validation set error Ô¨Årst, followed by an increase as the network starts to over-Ô¨Åt Therefore, to obtain
a network with good generalization performance, training should be stopped at the
point of smallest error with respect to the validation data set, as indicated in Fig-
ure 9.7 This behaviour of the learning curves is sometimes explained qualitatively in
terms of the effective number of parameters in the network This number starts out
small and then grows during training, corresponding to a steady increase in the ef-
fective complexity of the model Stopping training before a minimum of the training
error has been reached is a way to limit the effective network complexity

============================================================

=== CHUNK 264 ===
Palavras: 374
Caracteres: 2315
--------------------------------------------------
We can verify this insight for a quadratic error function and show that early stop-
ping should exhibit similar behaviour to regularization using a simple weight-decay
term (Bishop, 1995a) This can be understood from Figure 9.8, in which the axes
in weight space have been rotated to be parallel to the eigenvectors of the Hessian
matrix If, in the absence of weight decay, the weight vector starts at the origin and Section 7.1.1
proceeds during training along a path that follows the local negative gradient vec-
tor, then the weight vector will move initially parallel to the w2axis through a point
corresponding roughly to /hatwidewand then move towards the minimum of the error func-
tionwML This follows from the shape of the error surface and the widely differing
eigenvalues of the Hessian Stopping at a point near /hatwidewis therefore similar to weight
decay The relationship between early stopping and weight decay can be made quan-
titative, thereby showing that the quantity (whereis the iteration index and is Exercise 9.6
the learning rate parameter) acts like the reciprocal of the regularization parameter  The effective number of parameters in the network therefore grows during training REGULARIZATION
Figure 9.8 A schematic illustration of why
early stopping can give similar
results to weight decay for a
quadratic error function The
ellipses show contours of con-
stant error, and w?denotes the
maximum likelihood solution cor-
responding to the minimum of the
unregularized error function If the
weight vector starts at the origin
and moves according to the local
negative gradient direction, then it
will follow the path shown by the
curve By stopping training early,
a weight vector bwis found that is
qualitatively like that obtained with
a simple weight-decay regularizer
along with training to the minimum
of the regularized error, as can
be seen by comparing with Fig-
ure 9.3 w‚ãÜ
/hatwidew
w1w2
9.3.2
Double descent
The bias‚Äìvariance trade-off provides insight into the generalization performance Section 4.3
of a learnable model as the number of parameters in the model is varied Models with
too few parameters will have a high test set error due to the limited representational
capacity (high bias), and as the number of parameters increases, the test error is ex-
pected to fall

============================================================

=== CHUNK 265 ===
Palavras: 354
Caracteres: 2178
--------------------------------------------------
However, as the number of parameters is increased further, the test
error increases again due to over-Ô¨Åtting (high variance) This leads to the conven-
tional belief, widespread in classical statistics, that the number of parameters in the
model needs to be limited according to the size of the data set and that for a given
training data set, very large models are expected to have poor performance Contrary to this expectation, however, modern deep neural networks can have
excellent performance even when the number of parameters far exceeds that required
to achieve a perfect Ô¨Åt to the training data (Zhang et al , 2016), and the general
wisdom in the deep learning community is that bigger models are better Although
early stopping is sometimes used, models may also be trained to zero error and yet
still have good performance on test data These seemingly contradictory perspectives can be reconciled by examining
learning curves and other plots of generalization performance versus model com-
plexity, which reveal a more subtle phenomenon called double descent (Belkin et
al., 2019) This is illustrated in Figure 9.9, which shows training set and test set er-
rors versus model complexity, as determined by the number of learnable parameters,
for a large neural network called ResNet18 (He et al , 2015a), which has 18 layers
of parameters trained on an image classiÔ¨Åcation task The number of weights and
biases in the network is varied by changing the ‚Äòwidth parameter‚Äô, which governs
the number of hidden units in each layer We see that the training error decreases
monotonically with increasing complexity of the model, as expected Learning Curves 269
Figure 9.9 Plot of training set and test set errors for a large neural network model called ResNet18 trained on an
image classiÔ¨Åcation problem versus the complexity of a model The horizontal axis represents a hyperparameter
governing the number of hidden units and hence the overall number of weights and biases in the network The
vertical dashed line, labelled ‚Äòinterpolation threshold‚Äô indicates the level of model complexity at which the model
is capable, in principle, of achieving zero error on the training set

============================================================

=== CHUNK 266 ===
Palavras: 358
Caracteres: 2227
--------------------------------------------------
(2019) with permission.]
test set error decreases at Ô¨Årst then increases again and then Ô¨Ånally decreases again This reduction in test set error for very large models continues even after the training
set error has reached zero This surprising behaviour is more complex than we would expect from the usual
bias‚Äìvariance discussion of classical statistics and exhibits two different regimes of
model Ô¨Åtting, as shown schematically in Figure 9.9, corresponding to the classical
bias‚Äìvariance trade-off for small to medium complexity, followed by a further re-
duction in test set error as we enter a regime of very large models The transition
between the two regimes occurs roughly when the number of parameters in the model
is sufÔ¨Åciently large that the model is able to Ô¨Åt the training data exactly (Belkin et al.,
2019) (2019) deÔ¨Åne the effective model complexity to be the maxi-
mum size of training data set on which a model can achieve zero training error, and
so double descent arises when the effective model complexity exceeds the number
of data points in the training set We see similar behaviour if we control model complexity using early stopping,
as seen in Figure 9.10 Increasing the number of training epochs increases the ef-
fective model complexity, and for a sufÔ¨Åciently large model, double descent is again
observed For such models there are many possible solutions including those that
over-Ô¨Åt to the data It therefore seems to be a property of stochastic gradient descent
that the implicit biases that it introduces lead to good generalization performance Analogous results are also obtained when a regularization term in the error func-
270 9 REGULARIZATION
Figure 9.10 Plot of test set error versus number of epochs of gradient descent training for ResNet18 models
of various sizes The effective model complexity increases with the number of training epochs, and the double
descent phenomenon is observed for a sufÔ¨Åciently large model (2019) with permission.]
tion is used to control complexity Here the test set error of a large model trained
to convergence shows double descent with respect to 1=, the inverse regularization
parameter, since high corresponds to low complexity (Yilmaz and Heckel, 2022)

============================================================

=== CHUNK 267 ===
Palavras: 368
Caracteres: 2182
--------------------------------------------------
One ironic consequence of double descent is that it possible to operate in a
regime where increasing the size of the training data set could actually reduce per-
formance, contrary to the conventional view that more data is always a good thing For a model in the critical regime shown in Figure 9.9, an increase in the size of the
training set can push the interpolation threshold to the right, leading to a higher test
set error This is conÔ¨Årmed in Figure 9.11, which shows the test set error for a trans-
former model as a function of the dimensionality of the input space, known as the Chapter 12
embedding dimension Increasing the embedding dimension increases the number
of weights and biases in the model and hence increases the model complexity We
see that increasing the training set size from 4,000 to 18,000 data points leads to a
curve that is overall much lower However, for a range of embedding dimensions
that correspond to models in the critical complexity regime, increasing the size of
the data set can actually reduce generalization performance Parameter Sharing
Regularization terms, such as the L2regularizer/bardblw/bardbl2, help to reduce over-Ô¨Åtting by
encouraging weight values to be close to zero Another way to reduce network com-
plexity is to impose hard constraints on the weights by forming them into groups and
requiring that all weights within each group share the same value, in which the shared
9.4 Parameter Sharing 271
Figure 9.11 Plot of test set error for a large transformer model versus the embedding dimension, which controls
the number of parameters in the model Increasing the size of the training set from 4,000 to 18,000 samples
generally leads to a lower test set error, but for some intermediate values of model complexity, there can be an
increase in the error, as shown by the vertical red arrows (2019) with permission.]
value is learned from data This is known as weight sharing orparameter sharing or
parameter tying It means that the number of degrees of freedom is smaller than the
number of connections in the network Usually this is introduced as a way to encode
inductive bias into a network to express some known invariances

============================================================

=== CHUNK 268 ===
Palavras: 444
Caracteres: 2894
--------------------------------------------------
Evaluating the
error function gradients for such networks can be done using a small modiÔ¨Åcation
to backpropagation although in practice this is handled implicitly through automatic Exercise 9.7
differentiation We will make extensive use of parameter sharing when we discuss
convolutional neural networks Parameter sharing is applicable, however, only to Chapter 10
particular problems in which the form of the constraints can be speciÔ¨Åed in advance 9.4.1 Soft weight sharing
Instead of using a hard constraint that forces sets of model parameters to be
equal, Nowlan and Hinton (1992) introduced a form of soft weight sharing in which
a regularization term encourages groups of weights to have similar values Further-
more, the division of weights into groups, the mean weight value for each group,
and the spread of values within the groups are all determined as part of the learning
process Recall that the simple-weight decay regularizer in (9.1) can be viewed as the
negative log of a Gaussian prior distribution over the weights This encourages all
the weights to converge towards a single value of zero We can instead encourage the
weight values to form several groups, rather than just one group, by considering a
probability distribution that is a mixture of Gaussians The means {j}and variances Section 3.2.9
{2
j}of the Gaussian components, as well as the mixing coefÔ¨Åcients {j}, will be
considered as adjustable parameters to be determined as part of the learning process REGULARIZATION
Thus, we have a probability density of the form
p(w) =/productdisplay
iÔ£±
Ô£≤
Ô£≥K/summationdisplay
j=1jN(wi|j;2
j)Ô£º
Ô£Ω
Ô£æ(9.21)
whereKis the number of components in the mixture Taking the negative logarithm
then leads to a regularization function of the form
‚Ñ¶(w) =‚àí/summationdisplay
ilnÔ£´
Ô£≠K/summationdisplay
j=1jN(wi|j;2
j)Ô£∂
Ô£∏: (9.22)
The total error function is then given by
/tildewideE(w) =E(w) +‚Ñ¶(w ) (9.23)
whereis the regularization coefÔ¨Åcient This error is minimized jointly with respect to the weights {wi}and with respect
to the parameters{j;j;j}of the mixture model This can be done using gradient
descent, which requires that we evaluate the derivatives of ‚Ñ¶(w)with respect to all
the learnable parameters To do this, it is convenient to regard the {j}asprior
probabilities for each component to have generated a weight value, and to introduce
the corresponding posterior probabilities, which are given by Bayes‚Äô theorem: Exercise 9.8

j(wi) =jN(wi|j;2
j)/summationtext
kkN(wi|k;2
k): (9.24)
The derivatives of the total error function with respect to the weights are then given
by Exercise 9.9
@/tildewideE
@wi=@E
@wi+/summationdisplay
j
j(wi)(wi‚àíj)
2
j: (9.25)
The effect of the regularization term is therefore to pull each weight towards the
centre of the jth Gaussian, with a force proportional to the posterior probability of
that Gaussian for the given weight

============================================================

=== CHUNK 269 ===
Palavras: 379
Caracteres: 2531
--------------------------------------------------
This is precisely the kind of effect that we are
seeking Derivatives of the error with respect to the centres of the Gaussians are also
easily computed to give Exercise 9.10
@/tildewideE
@j=/summationdisplay
i
j(wi)(j‚àíwi)
2
j(9.26)
which has a simple intuitive interpretation, because it pushes jtowards an aver-
age of the weight values, weighted by the posterior probabilities that the respective
weight parameters were generated by component j Parameter Sharing 273
To ensure that the variances {2
j}remain positive, we introduce new variables
{j}deÔ¨Åned by
2
j= exp(j) (9.27)
and an unconstrained minimization is performed with respect to the {j} The asso-
ciated derivatives are then given by Exercise 9.11
@/tildewideE
@=
2/summationdisplay
i
j(wi)/parenleftBigg
1‚àí(wi‚àíj)2
2
j/parenrightBigg
: (9.28)
This process drives jtowards a weighted average of the squared deviations of the
weights around the corresponding centre j, where the weighting coefÔ¨Åcients are
again given by the posterior probability that each weight is generated by component
j For the derivatives with respect to the mixing coefÔ¨Åcients j, we need to take
account of the constraints
/summationdisplay
jj= 1; 06i61; (9.29)
which follow from the interpretation of the jas prior probabilities This can be
done by expressing the mixing coefÔ¨Åcients in terms of a set of auxiliary variables
{j}using the softmax function given by
j=exp(j)/summationtextK
k=1exp(k): (9.30)
The derivatives of the regularized error function with respect to the {j}then take
the form Exercise 9.12
@/tildewideE
@j=/summationdisplay
i{j‚àí
j(wi)}: (9.31)
We see thatjis therefore driven towards the average posterior probability for mix-
ture component j A different application of soft weight sharing (Lasserre, Bishop, and Minka,
2006) introduces a principled approach that combines the unsupervised training of
a generative model with the supervised training of a corresponding discriminative
model This is useful in situations where we have a signiÔ¨Åcant amount of unlabelled
data but where labelled data is in short supply The generative model has the advan-
tage that all of the data can be used to determine its parameters, whereas only the
labelled examples directly inform the parameters of the discriminative model How-
ever, a discriminative model can achieve better generalization when there is model
mis-speciÔ¨Åcation, in other words when the model does not exactly describe the true
distribution that generates the data, as is typically the case

============================================================

=== CHUNK 270 ===
Palavras: 371
Caracteres: 2390
--------------------------------------------------
By introducing a soft
tying of the parameters of the two models, we obtain a well-deÔ¨Åned hybrid of gen-
erative and discriminative approaches that can be robust to model mis-speciÔ¨Åcation
while also beneÔ¨Åting from being trained on unlabelled data REGULARIZATION
inputgradient
inputgradient
inputgradient
(a) (b) (c)
Figure 9.12 Plots of the Jacobian for networks with a single input and a single output, showing (a) a network
with two layers of weights, (b) a network with 25 layers of weights, and (c) a network with 51 layers of weights
together with residual connections (2017) with permission.]
9.5 Residual Connections
The representational power of deep neural networks stems in large part from the
use of multiple layers of processing, and it has been observed that increasing the
number of layers in a network can increase generalization performance signiÔ¨Åcantly We have also seen how batch normalization, along with careful initialization of the Section 7.4.2
weights and biases, can help address the problem of vanishing or exploding gradients Section 7.2.5
in deep networks However, even with batch normalization, it becomes increasingly
difÔ¨Åcult to train networks with a large number of layers One explanation for this phenomenon is called shattered gradients (Balduzzi et
al., 2017) We have seen that the representational capabilities of neural networks
increase exponentially with depth With ReLU activation functions, there is an ex-
ponential increase in the number of linear regions that the network can represent Section 6.3
However, a consequence of this is a proliferation of discontinuities in the gradient
of the error function This is illustrated for networks with a single input variable
and a single output variable in Figure 9.12 Here the derivative of the output vari-
able with respect to the input variable (the Jacobian of the network) is plotted as
a function of the input variable From the chain rule of calculus, these derivatives
determine the gradients of the error function surface We see that for deep networks,
extremely small changes in the weight parameters in the early layers of the network
can produce signiÔ¨Åcant changes in the gradient Iterative gradient-based optimiza-
tion algorithms assume that the gradient varies smoothly across parameter space,
and hence this ‚Äòshattered gradient‚Äô effect can render training ineffective in very deep
networks

============================================================

=== CHUNK 271 ===
Palavras: 360
Caracteres: 2250
--------------------------------------------------
An important modiÔ¨Åcation to the architecture of neural networks that greatly as-
sists in training very deep networks is that of residual connections (Heet al., 2015a),
which are a particular form of skip-layer connections Consider a neural network
9.5 Residual Connections 275
x F1 + F2 + F3 + y
z1 z2
Figure
9.13 A residual network consisting of three residual blocks, corresponding to the sequence of transfor-
mations (9.35) to (9.37) that consists of a sequence of three layers of the form
z1=F1(x) (9.32)
z2=F2(z1) (9.33)
y=F3(z2): (9.34)
Here the functions Fl(¬∑)might simply consist of a linear transformation followed
by a ReLU activation function or they might be more complex with multiple linear,
activation function, and normalization layers A residual connection consists simply
of adding the input to each function back onto the output to give
z1=F1(x) + x (9.35)
z2=F2(z1) +z1 (9.36)
y=F3(z2) +z2: (9.37)
Each combination of a function and a residual connection, such as F1(x) + x, is
called a residual block A residual network, also known as a ResNet, consists of
multiple layers of such blocks in sequence A modiÔ¨Åed network with residual con-
nections is illustrated in Figure 9.13 A residual block can easily generate the identity
transformation, if the parameters in the nonlinear function are small enough for the
function outputs to become close to zero The term ‚Äòresidual‚Äô refers to the fact that in each block the function learns the
residual between the identity map and the desired output, which we can see by rear-
ranging the residual transformation:
Fl(zl‚àí1) =zl‚àízl‚àí1: (9.38)
The gradients in a network with residual connections are much less sensitive to input
values compared to a standard deep network, as seen in Figure 9.12(c) (2017) developed a way to visualize error surfaces directly, which
showed that the effect of the residual connections is to create smoother error function
surfaces, as shown in Figure 9.14 It is usual to include batch normalization layers
in a residual network, as together they signiÔ¨Åcantly reduce the issue of vanishing
and exploding gradients (2015a) showed that including residual connec-
tions allows very deep networks, potentially having hundreds of layers, to be trained
effectively

============================================================

=== CHUNK 272 ===
Palavras: 362
Caracteres: 2163
--------------------------------------------------
Further insight into the way residual connections encourage smooth error sur-
faces can be obtained if we combine (9.35), (9.36), and (9.37) to give a single overall
equation for the whole network:
y=F3(F2(F1(x) + x) +z1) +z2: (9.39)
276 9 REGULARIZATION
(a) (b)
Figure 9.14 (a) A visualization of the error surface for a network with 56 layers (b) The same network with the
inclusion of residual connections, showing the smoothing effect that comes from the residual connections (2017) with permission.]
We can now substitute for the intermediate variables z1andz2to give an expression
for the network output as a function of the input x: Exercise 9.13
y=F3(F2(F1(x) +x) +F1(x) +x)
+F2(F1(x) +x))
+F1(x) +x: (9.40)
This expanded form of the residual network is depicted in Figure 9.15 We see that
the overall function consists of multiple networks acting in parallel and that these
include networks with fewer layers The network has the representational capability
of a deep network, since it contains such a network as a special case However, the
error surface is moderated by a combination of shallow and deep sub-networks Note that the skip-layer connections deÔ¨Åned by (9.40) require the input and all
the intermediate variables to have the same dimensionality so that they can be added We can change the dimensionality at some point in the network by including a non-
square matrix Wof learnable parameters in the form
zl=Fl(zl‚àí1) +Wzl‚àí1: (9.41)
So far we have not been speciÔ¨Åc about the form of the learnable nonlinear func-
tionsFl(¬∑) The simplest choice would be a standard neural network that alternates
between layers consisting of a learnable linear transformation and a Ô¨Åxed nonlinear
activation function such as ReLU This opens two possibilities for placing the resid-
ual connections, as shown in Figure 9.16 In version (a) the quantities being added
are always non-negative since they are given by the outputs of ReLU layers, and so
to allow for both positive and negative values, version (b) is more commonly used Model Averaging 277
F1 + F2 + F3
F1 +
F1 + F2 +
F1 ++ y x
Figure
9.15 The same network as in Figure 9.13, shown here in expanded form

============================================================

=== CHUNK 273 ===
Palavras: 364
Caracteres: 2270
--------------------------------------------------
Model
Averaging
If
we have several different models trained to solve the same problem then instead
of trying to select the single best model, we can often improve generalization by
averaging the predictions made by the individual models Such combinations of
models are sometimes called committees orensembles For models that produce
probabilistic outputs, the predicted distribution is the average of the predictions from
each model:
p(y|x) =1
LL/summationdisplay
l=1pl(
y|x) (9.42)
wherepl(y|x)is the output of model landLis the total number of models Linear ReLU + Linear ReLU +
(a)
ReLU Linear + ReLU ReLU +
(b)
Figure
9.16 Two alternative ways to include residual network connections into a standard feed-forward network
that alternates between learnable linear layers and nonlinear ReLU activation functions REGULARIZATION
This averaging process can be motivated by considering the trade-off between
bias and variance Recall from Figure 4.7 that when we trained multiple polynomials Section 4.3
using the sinusoidal data and then averaged the resulting functions, the contribution
arising from the variance term tended to cancel, leading to improved predictions In practice, of course, we have only a single data set, and so we have to Ô¨Ånd
a way to introduce variability between the different models within the committee One approach is to use bootstrap data sets, in which multiple data sets are created as
follows Suppose our original data set consists of Ndata points X={x1;:::;xN} We can create a new data set XBby drawing Npoints at random from X, with
replacement, so that some points in Xmay be replicated in XB, whereas other points
inXmay be absent from XB This process can be repeated Ltimes to generate L
data sets each of size Nand each obtained by sampling from the original data set X Each data set can then be used to train a model, and the predictions of the resulting
models are averaged This procedure is known as bootstrap aggregation or bagging
(Breiman, 1996) An alternative approach to forming an ensemble is to use the
original data set to train multiple different models having different architectures We can analyse the beneÔ¨Åts of ensemble predictions by considering a regression
problem with an input vector xand a single output variable y

============================================================

=== CHUNK 274 ===
Palavras: 371
Caracteres: 2420
--------------------------------------------------
Suppose we have a
set of trained models y1(x);:::;y M(x), and we form a committee prediction given
by
yCOM(x) =1
MM/summationdisplay
m
=1ym(x): (9.43)
If the true function that we are trying to predict is given by h(x), then the output of
each of the models can be written as the true value plus an error:
ym(x) =h(x) +m(x): (9.44)
The average sum-of-squares error then takes the form
Ex/bracketleftBig
{ym(x)‚àíh(x)}2/bracketrightBig
=Ex/bracketleftbig
m(x)2/bracketrightbig
(9.45)
where Ex[¬∑]denotes a frequentist expectation with respect to the distribution of the
input vector x The average error made by the models acting individually is therefore
EAV=1
MM/summationdisplay
m
=1Ex/bracketleftbig
m(x)2/bracketrightbig
: (9.46)
Similarly, the expected error from the committee (9.43) is given by
ECOM =ExÔ£Æ
Ô£∞/braceleftBigg
1
MM/summationdisplay
m
=1ym(x)‚àíh(x)/bracerightBigg2Ô£π
Ô£ª
=ExÔ£Æ
Ô£∞/braceleftBigg
1
MM/summationdisplay
m
=1m(x)/bracerightBigg2Ô£π
Ô£ª: (9.47)
9.6 Model Averaging 279
If we assume that the errors have zero mean and are uncorrelated, so that
Ex[m(x)] = 0 (9.48)
Ex[m(x)l(x)] = 0; m/negationslash=l (9.49)
then we obtain Exercise 9.14
ECOM =1
MEA
V: (9.50)
This apparently dramatic result suggests that the average error of a model can be
reduced by a factor of Msimply by averaging Mversions of the model Unfortu-
nately, it depends on the key assumption that the errors due to the individual models
are uncorrelated In practice, the errors are typically highly correlated, and the re-
duction in the overall error is generally much smaller It can, however, be shown that
the expected committee error will not exceed the expected error of the constituent
models, so that ECOM6EAV Exercise 9.15
A somewhat different approach to model combination, known as boosting (Fre-
und and Schapire, 1996), combines multiple ‚Äòbase‚Äô classiÔ¨Åers to produce a form of
committee whose performance can be signiÔ¨Åcantly better than that of any of the base
classiÔ¨Åers Boosting can give good results even if the base classiÔ¨Åers perform only
slightly better than random The principal difference between boosting and the com-
mittee methods, such as bagging as discussed above, is that the base classiÔ¨Åers are
trained in sequence and each base classiÔ¨Åer is trained using a weighted form of the
data set in which the weighting coefÔ¨Åcient associated with each data point depends
on the performance of the previous classiÔ¨Åers

============================================================

=== CHUNK 275 ===
Palavras: 380
Caracteres: 2258
--------------------------------------------------
In particular, points that are misclas-
siÔ¨Åed by one of the base classiÔ¨Åers are given a greater weight when used to train
the next classiÔ¨Åer in the sequence Once all the classiÔ¨Åers have been trained, their
predictions are then combined through a weighted majority voting scheme In practice, the major drawback of all model combination methods is that mul-
tiple models have to be trained and then predictions have to be evaluated for all the
models, thereby increasing the computational cost of both training and inference How signiÔ¨Åcant this depends on the speciÔ¨Åc application scenario 9.6.1 Dropout
A widely used and very effective form of regularization known as dropout (Sri-
vastava et al., 2014) can be viewed as an implicit way to perform approximate model
averaging over exponentially many models without having to train multiple models
individually It has broad applicability and is computationally cheap Dropout is one
of the most effective forms of regularization and is widely used in applications The central idea of dropout is to delete nodes from the network, including their
connections, at random during training Each time a data point is presented to the
network, a new random choice is made for which nodes to omit Figure 9.17 shows
a simple network along with examples of pruned networks in which subsets of nodes
have been omitted Dropout is applied to both hidden nodes and input nodes, but not outputs, and is
equivalent to setting the output of a dropped node to zero It can be implemented by
deÔ¨Åning a mask vector Ri‚àà{0; 1}which multiplies the activation of the non-output
280 9 REGULARIZATION
inputshidden units
outputs
Figure 9.17 A neural network on the left along with two examples of pruned networks in which a random subset
of nodes have been omitted nodeifor data point n, whose values are set to 1with probability  A value of
= 0:5 seems to work well for the hidden nodes, whereas for the inputs a value of
= 0:8 is typically used During training, as each data point is presented to the network, a new mask is
created, and the forward and backward propagation steps are applied on that pruned
network to create error function gradients, which are then used to update the weights,
for example by stochastic gradient descent

============================================================

=== CHUNK 276 ===
Palavras: 370
Caracteres: 2293
--------------------------------------------------
If the data points are grouped into mini-
batches then the gradients are averaged over the data points in each mini-batch before Section 7.2.4
applying the weight update For a network with Mnon-output nodes, there are 2M
pruned networks, and so only a small fraction of these networks will ever be con-
sidered during training This differs from conventional ensemble methods in which
each of the networks in the ensemble is independently trained to convergence An-
other difference is that the exponentially many networks that are implicitly being
trained with dropout are not independent but share their parameter values with the
full network, and hence with each other Note that training can take longer with
dropout since the individual parameter updates are very noisy Also, because the
error function is intrinsically noisy, it is harder to conÔ¨Årm that the optimization al-
gorithm is working correctly just by looking for a decreasing error function during
training Once training is complete, predictions can in principle be made by applying the
ensemble rule (9.42), which in this case takes the form
p(y|x) =/summationdisplay
Rp(R)p(y|x;R) (9.51)
where the sum is over the exponentially large space of masks, and p(y|x;R)is the
predictive distribution from the network with mask R Because this summation is
intractable, it can be approximated by sampling a small number of masks, and in
practice, as few as 10or20masks can be sufÔ¨Åcient to obtain good results This
procedure is known as Monte Carlo dropout Exercises 281
An even simpler approach is to make predictions using the trained network with
no nodes masked out, and to re-scale the weights in the network so that the expected
input to each node is roughly the same during testing as it would be during training,
compensating for the fact that in training a proportion of the nodes would be missing Thus, if a node is present with probability during training, then during testing the
output weights from that node would be multiplied by before using the network to
make predictions A different motivation for dropout comes from the Bayesian perspective In a
fully Bayesian treatment, we would make predictions by averaging over all possible Section 2.6
2Mnetwork models, with each network weighted by its posterior probability

============================================================

=== CHUNK 277 ===
Palavras: 364
Caracteres: 2369
--------------------------------------------------
Com-
putationally, this would be prohibitively expensive, both during training when eval-
uating the posterior probabilities and during testing when computing the weighted
predictions Dropout approximates this model averaging by giving an equal weight
to each possible model Further intuition behind dropout comes from its role in reducing over-Ô¨Åtting In
a standard network, the parameters can become tuned to noise on individual data
points, with hidden nodes becoming over-specialized Each node adjusts its weights
to minimize the error, given the outputs of other nodes, leading to co-adaptation of
nodes in a way that might not generalize to new data With dropout, each node
cannot rely on the presence of other speciÔ¨Åc nodes and must instead make useful
contributions in a broad range of contexts, thereby reducing co-adaptation and spe-
cialization For a simple linear regression model trained using least squares, dropout
regularization is equivalent to a modiÔ¨Åed form of quadratic regularization Exercise 9.18
Exercises
9.1 (?)By considering each of the four group axioms in turn, show that the set of all pos-
sible rotations of a square through (positive or negative) multiples of 90‚ó¶, together
with the binary operation of composing rotations, forms a group Similarly, show Section 9.1.3
that the set of all continuous translations of an object in a two-dimensional plane
also forms a group 9.2 (??) Consider a linear model of the form
y(x;w) =w0+D/summationdisplay
i=1wixi (9.52)
together with a sum-of-squares error function of the form
ED(w) =1
2N/summationdisplay
n=1{y(xn;w)‚àítn}2: (9.53)
Now suppose that Gaussian noise iwith zero mean and variance 2is added in-
dependently to each of the input variables xi By making use of E[i] = 0 and
E[ij] =ij2, show that minimizing EDaveraged over the noise distribution is
282 9 REGULARIZATION
equivalent to minimizing the sum-of-squares error for noise-free input variables with
the addition of a weight-decay regularization term, in which the bias parameter w0
is omitted from the regularizer 9.3 (??) Consider an error function that consists simply of the quadratic regularizer
‚Ñ¶(w) =‚àí1
2wTw (9.54)
together with the gradient descent update formula
w(+1)=w(+1)‚àí‚àá‚Ñ¶(w ): (9.55)
By considering the limit of inÔ¨Ånitesimal updates, write down a corresponding dif-
ferential equation for the evolution of w

============================================================

=== CHUNK 278 ===
Palavras: 365
Caracteres: 2417
--------------------------------------------------
Write down the solution of this equation
starting from an initial value w0, and show that the elements of wdecay exponen-
tially to zero 9.4 (?)Verify that the network function deÔ¨Åned by (9.6) and (9.7) is invariant under
the transformation (9.8) applied to the inputs, provided the weights and biases are
simultaneously transformed using (9.9) and (9.10) Similarly, show that the network
outputs can be transformed according to (9.11) by applying the transformation (9.12)
and (9.13) to the second-layer weights and biases 9.5 (??) By using Lagrange multipliers, show that minimizing the regularized error Appendix C
function given by (9.19) is equivalent to minimizing the unregularized error func-
tionE(w)subject to the constraint (9.20) Discuss the relationship between the
parametersand 9.6 (???) Consider a quadratic error function of the form
E=E0+1
2(w‚àíw?)TH(w‚àíw?) (9.56)
where w?represents the minimum, and the Hessian matrix His positive deÔ¨Ånite and
constant Suppose the initial weight vector w(0)is chosen to be at the origin and is
updated using simple gradient descent:
w()=w(‚àí1)‚àí‚àáE (9.57)
wheredenotes the step number, and is the learning rate (which is assumed to be
small) Show that, after steps, the components of the weight vector parallel to the
eigenvectors of Hcan be written
w()
j={1‚àí(1‚àíj)}w j (9.58)
wherewj=wTuj, andujandjare the eigenvectors and eigenvalues of H, re-
spectively, deÔ¨Åned by
Huj=juj: (9.59)
Exercises 283
Show that as ‚Üí‚àû, this gives w()‚Üíw?as expected, provided |1‚àíj|<1 Now suppose that training is halted after a Ô¨Ånite number of steps Show that the
components of the weight vector parallel to the eigenvectors of the Hessian satisfy
w()
j/similarequalw jwhenj/greatermuch()‚àí1(9.60)
|w()
j|/lessmuch|w j|whenj/lessmuch()‚àí1: (9.61)
This result shows that ()‚àí1plays an analogous role to the regularization parameter
in weight decay 9.7 (??) Consider a neural network in which multiple weights are constrained to have the
same value Discuss how the standard backpropagation algorithm must be modiÔ¨Åed
to ensure that such constraints are satisÔ¨Åed when evaluating the derivatives of an
error function with respect to the adjustable parameters in the network 9.8 (?)Consider a mixture distribution deÔ¨Åned by
p(w) =M/summationdisplay
j=1jN(w|j;2
j) (9.62)
in which{j}can be viewed as prior probabilities p(j)for the corresponding Gaus-
sian components

============================================================

=== CHUNK 279 ===
Palavras: 390
Caracteres: 2566
--------------------------------------------------
Using Bayes‚Äô theorem, show that the corresponding posterior
probabilities p(j|w)are given by (9.24) 9.9 (??) Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.25) 9.10 (??) Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.26) 9.11 (??) Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.28) 9.12 (??) Show that the derivatives of the mixing coefÔ¨Åcients {k}deÔ¨Åned by (9.30) with
respect to the auxiliary parameters {j}are given by
@k
@j=jkj‚àíjk: (9.63)
Hence, by making use of the constraint/summationtext
k
k(wi) = 1 for alli, derive the result
(9.31) 9.13 (?)Verify that combining (9.35), (9.36), and (9.37) gives a single overall equation
for the whole network in the form (9.40) 9.14 (??) The expected sum-of-squares error EAVfor a simple committee model can be
deÔ¨Åned by (9.46), and the expected error of the committee itself is given by (9.47) Assuming that the individual errors satisfy (9.48) and (9.49), derive the result (9.50) REGULARIZATION
9.15 (??) By making use of Jensen‚Äôs inequality (2.102) for the special case of the convex
functionf(x) =x2, show that the average expected sum-of-squares error EAVof
the members of a simple committee model, given by (9.46), and the expected error
ECOM of the committee itself, given by (9.47), satisfy
ECOM6EAV: (9.64)
9.16 (??) By making use of Jensen‚Äôs in equality (2.102), show that the result (9.64) de-
rived in the previous exercise holds for any error function E(y), not just sum-of-
squares, provided it is a convex function of y 9.17 (??) Consider a committee in which we allow unequal weighting of the constituent
models, so that
yCOM(x) =M/summationdisplay
m=1mym(x): (9.65)
To ensure that the predictions yCOM(x)remain within sensible limits, suppose that
we require that they be bounded at each value of xby the minimum and maximum
values given by any of the members of the committee, so that
ymin(x)6yCOM(x)6ymax(x): (9.66)
Show that a necessary and sufÔ¨Åcient condition for this constraint is that the coefÔ¨Å-
cientsmsatisfy
m>0;M/summationdisplay
m=1m= 1: (9.67)
9.18 (???) Here we explore the effect of dropout regularization on a simple linear regres-
sion model trained using least squares Consider a model of the form
yk=D/summationdisplay
i=1wkixi (9.68)
along with a sum-of-squares error function given by
E(W) =N/summationdisplay
n=1K/summationdisplay
k=1/braceleftBigg
tnk‚àíD/summationdisplay
i=1wkiRnixni/bracerightBigg2
(9.69)
where the elements Rni‚àà{0; 1}of the dropout matrix are chosen randomly from
a Bernoulli distribution with parameter 

============================================================

=== CHUNK 280 ===
Palavras: 362
Caracteres: 2442
--------------------------------------------------
We now take an expectation over the
distribution of random dropout parameters Show that
E[Rni] = (9.70)
E[RniRnj] =ij+ (1‚àíij)2: (9.71)
Exercises 285
Hence, show that the expected error function for this dropout model is given by
E[E(W)] =N/summationdisplay
n=1K/summationdisplay
k=1/braceleftBigg
ynk‚àíD/summationdisplay
i=1wkixni/bracerightBigg2
(9.72)
+(1‚àí)N/summationdisplay
n=1K/summationdisplay
k=1D/summationdisplay
i=1w2
kix2
ni: (9.73)
Thus, we see that the expected error function corresponds to a sum-of-squares error
with a quadratic regularizer in which the regularization coefÔ¨Åcient is scaled sepa-
rately for each input variable according to the data values seen by that input Finally,
write down a closed-form solution for the weight matrix that minimizes this regular-
ized error function 10
Convolutional
Networks
The simplest machine learning models assume that the observed data values are un-
structured, meaning that the elements of the data vectors x= (x1;:::;xD)are
treated as if we do not know anything in advance about how the individual elements
might relate to each other If we were to make a random permutation of the ordering
of these variables and apply this Ô¨Åxed permutation consistently on all training and
test data, there would be no difference in the performance for the models considered
so far Many applications of machine learning, however, involve structured data in
which there are additional relationships between input variables For example, the
words in natural language form a sequence, and if we were to model language as a Chapter 12
generative autoregressive process then we would expect each word to depend more
strongly on the immediately preceding words and less so on words much earlier in
the sequence Likewise, the pixels of an image have a well-deÔ¨Åned spatial relation-
287 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_10    
288 10 CONVOLUTIONAL NETWORKS
ship to each other in which the input variables are arranged in a two-dimensional
grid, and nearby pixels have highly correlated values We have already seen that our knowledge of the structure of speciÔ¨Åc data modal-
ities can be utilized through the addition of a regularization term to the error function Section 9.1
in the training objective, through data augmentation, or through modiÔ¨Åcations to the
model architecture

============================================================

=== CHUNK 281 ===
Palavras: 361
Caracteres: 2377
--------------------------------------------------
These approaches can help guide the model to respect certain
properties such as invariance and equivariance with respect to transformations of the Section 9.1.4
input data In this chapter we will take a look at an architectural approach called a
convolutional neural network (CNN), which we will see can be viewed as a sparsely
connected multilayer network with parameter sharing, and designed to encode in-
variances and equivariances speciÔ¨Åc to image data Computer Vision
The automatic analysis and interpretation of image data form the focus of the Ô¨Åeld
of computer vision and represent a major application area for machine learning
(Szeliski, 2022) Historically, computer vision was based largely on 3-dimensional
projective geometry Hand-crafted features were constructed and used as input to
simple learning algorithms (Hartley and Zisserman, 2004) However, it was one
of the Ô¨Årst Ô¨Åelds to be transformed by the deep learning revolution, predominantly
thanks to the CNN architecture Although the architecture was originally developed
in the context of image analysis, it has also been applied in other domains such as the
analysis of sequential data Recently alternative architectures based on transformers Chapter 12
have become competitive with convolutional networks in some applications There are many applications for machine learning in computer vision, of which
some of the most commonly encountered are the following:
1.ClassiÔ¨Åcation of images, for example classifying an image of a skin lesion as Figure 1.1
benign or malignant This is sometimes called ‚Äòimage recognition‚Äô 2.Detection of objects in an image and determining their locations within the Figure 10.19
image, for example detecting pedestrians from camera data collected by an
autonomous vehicle 3.Segmentation of images, in which each pixel is classiÔ¨Åed individually thereby Figure 10.26
dividing the image into regions sharing a common label For example, a nat-
ural scene might be segmented into sky, grass, trees, and buildings, whereas
a medical scan image could be segmented into cancerous tissue and normal
tissue 4.Caption generation in which a textual description is generated automatically Figure 12.27
from an image 5.Synthesis of new images, for example generating images of human faces Im- Figure 1.3
ages can also be synthesized based on a text input describing the desired image
content

============================================================

=== CHUNK 282 ===
Palavras: 374
Caracteres: 2338
--------------------------------------------------
Computer Vision 289
6.Inpainting in which a region of an image is replaced with synthesized pixels Figure 20.9
that are consistent with the rest of the image This is used, for example, to
remove unwanted objects during image editing 7.Style transfer in which an input image in one style, for example a photograph, Figure 10.32
is transformed into a corresponding image in a different style, for example an
oil painting 8.Super-resolution in which the resolution of an image is improved by increas- Figure 20.8
ing the number of pixels and synthesizing associated high-frequency informa-
tion 9.Depth prediction in which one or more views are used to predict the distance
of the scene from the camera at each pixel in a target image 10.Scene reconstruction in which one or more two-dimensional images of a
scene are used to reconstruct a three-dimensional representation 10.1.1 Image data
An image comprises a rectangular array of pixels, in which each pixel has either
a grey-scale intensity or more commonly a triplet of red, green, and blue channels
each with its own intensity value These intensities are non-negative numbers that
also have some maximum value corresponding to the limits of the camera or other
hardware device used to capture the image For the most part, we will view the in-
tensities as continuous variables, but in practice they are represented with Ô¨Ånite pre-
cision, for example as 8-bit numbers represented as integers in the range 0;:::; 255 Some images, such as the magnetic resonance imaging (MRI) scans used in medical
diagnosis, comprise three-dimensional grids of voxels Similarly, videos comprise
a sequence of two-dimensional images and therefore can also be viewed as three-
dimensional structures in which successive frames are stacked through time Now consider the challenge of applying neural networks to image data to ad-
dress some of the applications highlighted above Images generally have a high
dimensionality, with typical cameras capturing images comprising tens of megapix-
els Treating the image data as unstructured may therefore require a model with a
vast number of parameters that would be infeasible to train More signiÔ¨Åcantly, such
an approach fails to take account of the highly structured nature of image data, in
which the relative positions of different pixels play a crucial role

============================================================

=== CHUNK 283 ===
Palavras: 392
Caracteres: 2392
--------------------------------------------------
We can see this
because if we take the pixels of an image and randomly permute them, then the result
no longer looks like a natural image Similarly, if we generate a synthetic image by
drawing random values for the pixel intensities independently for each pixel, there
is essentially zero chance of generating something that looks like a natural image Figure 6.8
Local correlations are important, and in a natural image there is a much higher prob-
ability that two nearby pixels will have similar colours and intensities compared to
two pixels that are far apart This represents powerful prior knowledge and can be
used to encode strong inductive biases into a neural network, leading to models with
far fewer parameters and with much better generalization accuracy CONVOLUTIONAL NETWORKS
10.2 Con
volutional Filters
One
motivation for the introduction of convolutional networks is that for image data,
which is the modality for which CNNs were designed, a standard fully connected
architecture would require vast numbers of parameters due to the high-dimensional
nature of images To see this, consider a colour image with 103√ó103pixels, each
with three values corresponding to red, green, and blue intensities If the Ô¨Årst hidden
layer of the network has, say, 1,000 hidden units, then we already have 3√ó109
weights in the Ô¨Årst layer Furthermore, such a network would have to learn any
invariances and equivariances by example, which would require huge data sets By
designing an architecture that incorporates our inductive bias about the structure
of images, we can reduce the data set requirements dramatically and also improve
generalization with respect to symmetries in the image space To exploit the two-dimensional structure of image data to create inductive bi-
ases, we can use four interrelated concepts: hierarchy, locality, equivariance, and
invariance Consider the task of detecting faces in images There is a natural hier-
archical structure because one image may contain several faces, and each face in-
cludes elements such as eyes, and each eye has structure such as an iris, which itself
has structure such as edges At the lowest level of the hierarchy, a node in a neural
network could detect the presence of a feature such as an edge using information
that is local to a small region of an image, and therefore it would only need to see
a small subset of the image pixels

============================================================

=== CHUNK 284 ===
Palavras: 368
Caracteres: 2189
--------------------------------------------------
More complex structures further up the hierar-
chy can be detected by composing multiple features found at previous levels A key
point, however, is that although we want to build the general concept of hierarchy
into the model, we want the details of the hierarchy, including the type of features
extracted at each level, to be learned from data, not hand-coded Hierarchical models
Ô¨Åt naturally within the deep learning framework, which already allows very complex
concepts to be extracted from raw data through a succession of, possibly very many,
‚Äòlayers‚Äô of processing, in which the whole system is trained end-to-end 10.2.1 Feature detectors
For simplicity we will initially restrict our attention to grey-scale images (i.e.,
ones having a single channel) Consider a single unit in the Ô¨Årst layer of a neural
network that takes as input just the pixel values from a small rectangular region,
or patch, from the image, as illustrated in Figure 10.1(a) This patch is referred to
as the receptive Ô¨Åeld of that unit, and it captures the notion of locality We would
like weight values associated with this unit to learn to detect some useful low-level
feature The output of this unit is given by the usual functional form comprising a
weighted linear combination of the input values, which is subsequently transformed
using a nonlinear activation function:
z= ReLU(wTx+w0) (10.1)
where xis a vector of pixel values for the receptive Ô¨Åeld, and we have assumed a
ReLU activation function Because there is one weight associated with each input
10.2 Convolutional Filters 291
Figure 10.1 (a) Illustration of a re-
ceptive Ô¨Åeld, showing a unit in a hid-
den layer of a network that receives
input from pixels in a 3√ó3patch of
the image Pixels in this patch form
the receptive Ô¨Åeld for this unit (b)
The weight values associated with
this hidden unit can be visualized as
a small 3√ó3matrix, known as a ker-
nel There is also an additional bias
parameter that is not shown here.image
hidden units
(a)0:4 1:7 0:9
2:3‚àí2:14:0
‚àí1:40:7 2:1
(b)
pixel, the weights themselves form a small two-dimensional grid known as a Ô¨Ålter ,
sometimes also called a kernel , which itself can be visualized as an image

============================================================

=== CHUNK 285 ===
Palavras: 354
Caracteres: 2014
--------------------------------------------------
This is
illustrated in Figure 10.1(b) Suppose that wandw0in (10.1) are Ô¨Åxed and we ask for which value of the
input image patch xwill this hidden unit give the largest output response To answer
this we need to constrain xin some way, so let us suppose that its norm /bardblx/bardbl2is Ô¨Åxed Then the solution for xthat maximizes wTx, and hence maximizes the response of
the hidden unit, is of the form x=wfor some coefÔ¨Åcient This says that Exercise 10.1
the maximum output response from this hidden unit occurs when it detects a patch
of image that, up to an overall scaling, looks like the kernel image Note that the
ReLU generates a non-zero output only when wTxexceeds a threshold of ‚àíw0, and
therefore the unit acts as a feature detector that signals when it Ô¨Ånds a sufÔ¨Åciently
good match to its kernel 10.2.2 Translation equivariance
Next note that if a small patch in a face image represents, for example, an eye
at that location, then the same set of pixel values in a different part of the image
must represent an eye at the new location Our neural network needs to be able to
generalize what it has learned in one location to all possible locations in the image,
without needing to see examples in the training set of the corresponding feature at
every possible location To achieve this, we can simply replicate the same hidden-
unit weight values at multiple locations across the image, as illustrated for a one-
dimensional input space in Figure 10.2 The units of the hidden layer form a feature map in which all the units share
the same weights Consequently if a local patch of an image produces a particular
292 10 CONVOLUTIONAL NETWORKS
Figure 10.2 Illustration of convolution for a one-dimensional array of
input values and a kernel of width 2 The connections
are sparse and are shared by the hidden units, as shown
by the red and blue arrows in which links with the same
colour have the same weight values This network there-
fore has six connections but only two independent learn-
able parameters

============================================================

=== CHUNK 286 ===
Palavras: 367
Caracteres: 2235
--------------------------------------------------
response
in the unit connected to that patch, then the same set of pixel values at
a different location will produce the same response in the corresponding translated
location in the feature map This is an example of equivariance We see that the Section 9.1.3
connections in this network are sparse in that most connections are absent Also, the
values of the weights are shared by all the hidden units, as indicated by the colours
of the connections This transformation is an example of a convolution We can extend the idea of convolution to two-dimensional images as follows
(Dumoulin and Visin, 2016) For an image Iwith pixel intensities I(j;k), and a
Ô¨ÅlterKwith pixel values K(l;m), the feature map Chas activation values given by
C(j;k) =/summationdisplay
l/summationdisplay
mI(j+l;k+m)K (l;m) (10.2)
where we have omitted the nonlinear activation function for clarity This again is
an example of a convolution and is sometimes expressed as C=I‚àóK Note that
strictly speaking (10.2) is called a cross-correlation, which differs slightly from the
conventional mathematical deÔ¨Ånition of convolution, but here we will follow com- Exercise 10.4
mon practice in the machine learning literature and refer to (10.2) as a convolution The relationship (10.2) is illustrated in Figure 10.3 for a 3√ó3image and a 2√ó2
Ô¨Ålter Importantly, when using batch normalization in a convolutional network, the Section 7.4.2
same value of mean and variance must be used at every spatial location within a fea-
ture map when normalizing the states of the units to ensure that the statistics of the
feature map are independent of location As an example of the application of convolution, we consider the problem of
detecting edges in images using a Ô¨Åxed, hand-crafted convolutional Ô¨Ålter Intuitively,
we can think of a vertical edge as occurring when there is a signiÔ¨Åcant local change
in the intensity between pixels as we move horizontally across the image We can
measure this by convolving the image with a 3√ó3Ô¨Ålter of the form
‚àí
1 0 1
‚àí1 0 1
‚àí1 0 1(10.3)
10.2 Convolutional Filters 293
abc
def
ghi‚àójk
lm=aj+bk+
dl+embj+ck+
el+fm
dj+ek+
gl+hmej+fk+
hl+im
I K C
Figure 10.3 Example of a 3√ó3image convolved with a 2√ó2Ô¨Ålter to give a resulting 2√ó2feature map

============================================================

=== CHUNK 287 ===
Palavras: 350
Caracteres: 2207
--------------------------------------------------
Similarly we can detect horizontal edges by convolving with the transpose of this
Ô¨Ålter:
‚àí1‚àí1‚àí1
0 0 0
1 1 1(10.4)
Figure 10.4 shows the results of applying these two convolutional Ô¨Ålters to a sample
image Note that in Figure 10.4(b) if a vertical edge corresponds to an increase in
pixel intensity, the corresponding point on the feature map is positive (indicated by a
light colour), whereas if the vertical edge corresponds to a decrease in pixel intensity,
the corresponding point on the feature map is negative (indicated by a dark colour),
with analogous properties for Figure 10.4(c) for horizontal edges Comparing this convolutional structure with a standard fully connected net-
work, we see several advantages: (i) the connections are sparse, leading to far fewer
weights even with large images, (ii) the weight values are shared, greatly reducing
the number of independent parameters and consequently reducing the required size
(a)
 (b)
 (c)
Figure 10.4 Illustration of edge detection using convolutional Ô¨Ålters showing (a) the original image, (b) the result
of convolving with the Ô¨Ålter (10.3) that detects vertical edges, and (c) the result of convolving with the Ô¨Ålter (10.4)
that detects horizontal edges CONVOLUTIONAL NETWORKS
of the training set needed to learn those parameters, and (iii) the same network can
be applied to images of different sizes without the need for retraining We will re-
turn to this Ô¨Ånal point later, but for the moment, simply note that changing the size Section 10.4.3
of the input image simply changes the size of the feature map but does not change
the number of weights, or the number of independent learnable parameters, in the
model One Ô¨Ånal observation regarding convolutional networks is that, by exploiting
the massive parallelism of graphics processing units (GPUs) to achieve high compu-
tational throughput, convolutions can be implemented very efÔ¨Åciently 10.2.3 Padding
We see from Figure 10.3 that the convolution map is smaller than the original
image If the image has dimensionality J√óKpixels and we convolve with a kernel
of dimensionality M√óM(Ô¨Ålters are typically chosen to be square) the resulting
feature map has dimensionality (J‚àíM+ 1)√ó(K‚àíM+ 1)

============================================================

=== CHUNK 288 ===
Palavras: 367
Caracteres: 2152
--------------------------------------------------
In some cases
we want the feature map to have the same dimensions as the original image This
can be achieved by padding the original image with additional pixels around the
outside, as illustrated in Figure 10.5 If we pad with P pixels then the output map has
dimensionality (J+2P‚àíM+1)√ó(K+2P‚àíM+1) If there is no padding, so that
P= 0, this is called a valid convolution When the value of Pis chosen such that the
output array has the same size as the input, corresponding to P= (M‚àí1)=2, this Exercise 10.6
is called a same convolution, because the image and the feature map have the same
dimensions In computer vision, Ô¨Ålters generally use odd values of M, so that the
padding can be symmetric on all sides of the image and that there is a well-deÔ¨Åned
central pixel associated with the location of the Ô¨Ålter Finally, we have to choose a
suitable value for the intensities associated with the padding pixels A typical choice
is to set the padding values to zero, after Ô¨Årst subtracting the mean from each image
so that zero represents the average value of the pixel intensity Padding can also be
applied to feature maps for processing by subsequent convolutional layers 10.2.4 Strided convolutions
In typical image processing applications, the images can have very large num-
bers of pixels, and since the kernels are often relatively small, so that M/lessmuchJ;K , the
convolutional feature map will be of a similar size to the original image and will be
the same size if same padding is used Sometimes we wish to use feature maps that
are signiÔ¨Åcantly smaller than the original image to provide Ô¨Çexibility in the design
of convolutional network architectures One way to achieve this is to use strided
convolutions in which, instead of stepping the Ô¨Ålter over the image one pixel at a
time, it is moved in larger steps of size S, called the stride If we use the same stride
horizontally and vertically, then the number of elements in the feature map will be Exercise 10.7
/floorleftbiggJ+ 2P‚àíM
S‚àí1/floorrightbigg
√ó/floorleftbiggK+
2P‚àíM
S‚àí1/floorrightbigg
(10.5)
where‚åä
x‚åãdenotes the ‚ÄòÔ¨Çoor‚Äô of x, i.e., the largest integer that is less than or equal
tox

============================================================

=== CHUNK 289 ===
Palavras: 377
Caracteres: 2274
--------------------------------------------------
For large images and small Ô¨Ålter sizes, the image map will be roughly a factor
of1=S smaller than the original image Convolutional Filters 295
Figure 10.5 Illustration of a 4√ó4image that has been padded
with additional pixels to create a 6√ó6image.0 0 0 0 0 0
0X11X12X13X140
0X21X22X23X240
0X31X32X33X340
0X41X42X43X440
0 0 0 0 0 0
10.2.5 Multi-dimensional convolutions
So far we have considered convolutions over a single grey-scale image For a
colour image there will be three channels corresponding to the red, green, and blue
colours We can easily extend convolutions to cover multiple channels by extending
the dimensionality of the Ô¨Ålter An image with J√óKpixels andCchannels will
be described by a tensor of dimensionality J√óK√óC We can introduce a Ô¨Ålter Section 6.3.7
described by a tensor of dimensionality M√óM√óCcomprising a separate M√óM
Ô¨Ålter for each of the Cchannels Assuming no padding and a stride of 1, this again
gives a feature map of size (J‚àíM+1)√ó(K‚àíM+1), as is illustrated in Figure 10.6 image
hidden units
(a)1:2 0:8‚àí3:7
‚àí3:6‚àí2:14:0
2:4 0:7 2:1‚àí3:20:7 1:3
‚àí1:4‚àí2:14:0
4:2 0:7 2:10:4 1:7 0:9
2:3‚àí2:14:0
‚àí1:00:7 2:1
(b)
Figure 10.6 (a) Illustration of a multi-dimensional Ô¨Ålter that takes input from across the R, G, and B channels (b) The kernel here has 27 weights (plus a bias parameter not shown) and can be visualized as a 3√ó3√ó3
tensor 296 10.CONVOLUTIONAL NETWORKS
Figure 10.7 The multi-dimensional convolu-
tional Ô¨Ålter layer shown in Figure 10.6 can be
extended to include multiple independent Ô¨Ålter
channels We now make a further important extension to convolutions Up to now we
have created a single feature map in which all the points in the feature map share the
same set of learnable parameters For a Ô¨Ålter of dimensionality M√óM√óC, this
will haveM2Cweight parameters, irrespective of the size of the image In addition
there will be a bias parameter associated with this unit Such a Ô¨Ålter is analogous
to a single hidden node in a fully connected network, and it can learn to detect only
one kind of feature and is therefore very limited To build more Ô¨Çexible models,
we simply include multiple such Ô¨Ålters, in which each Ô¨Ålter has its own independent
set of parameters giving rise to its own independent feature map, as illustrated in
Figure 10.7

============================================================

=== CHUNK 290 ===
Palavras: 379
Caracteres: 2222
--------------------------------------------------
We will again refer to these separate feature maps as channels The
Ô¨Ålter tensor now has dimensionality M√óM√óC√óCOUT whereCis the number
of input channels and COUT is the number of output channels Each output channel
will have its own associated bias parameter, so the total number of parameters will
be(M2C+ 1)COUT A useful concept in designing convolutional networks is the 1√ó1convolution
(Lin, Chen, and Yan, 2013), which is simply a convolutional layer in which the Ô¨Ålter
size is a single pixel The Ô¨Ålters have Cweights, one for each input channel, plus
a bias One application for 1√ó1convolutions is simply to change the number of
channels (typically to reduce the number of channels) without changing the size of
the feature maps, by setting the number of output channels to be different to the
number of input channels It is therefore complementary to strided convolutions or
pooling in that it reduces the number of channels rather than the dimensionality of
the channels 10.2.6 Pooling
A convolutional layer encodes translation equivariance , whereby if a small patch
of pixels, representing the receptive Ô¨Åeld of a hidden unit, is moved to a different
10.2 Convolutional Filters 297
Figure 10.8 Illustration of max-pooling
in which blocks of 2√ó2pixels in a fea-
ture map are combined using the ‚Äòmax‚Äô
operator to generate a new feature map
of smaller dimensionality.3 5 4 6
1 1 9 4
7 10 9 5
12 2 9 45 9
12 9
location in the image, the associated outputs of the feature map will move to the
corresponding location in the feature map This is valuable for applications such as
Ô¨Ånding the location of an object within an image For other applications, such as
classifying an image, we want the output to be invariant to translations of the input In all cases, however, we want the network to be able to learn hierarchical structure
in which complex features at a particular level are built up from simpler features
at the previous level In many cases the spatial relationship between those simpler
features will be important For example, it is the relative positions of the eyes, nose,
and mouth that help determine the presence of a face and not just the presence of
these features in arbitrary locations within the image

============================================================

=== CHUNK 291 ===
Palavras: 356
Caracteres: 2121
--------------------------------------------------
However, small changes in the
relative locations do not affect the classiÔ¨Åcation, and we want to be invariant to such
small translations of individual features This can be achieved using pooling applied
to the output of the convolutional layer Pooling has similarities to using a convolutional layer in that an array of units is
arranged in a grid, with each unit taking input from a receptive Ô¨Åeld in the previous
feature map layer Again, there is a choice of Ô¨Ålter size and of stride length The
difference, however, is that the output of a pooling unit is a simple, Ô¨Åxed function of
its inputs, and so there are no learnable parameters in pooling A common example
of a pooling function is max-pooling (Zhou and Chellappa, 1988) in which each unit
simply outputs the max function applied to the input values This is illustrated with
a simple example in Figure 10.8 Here the stride length is equal to the Ô¨Ålter width,
and so there is no overlap of the receptive Ô¨Åelds As well as building in some local translation invariance, pooling can also be
used to reduce the dimensionality of the representation by down-sampling the feature
map Note that using strides greater than 1in a convolutional layer also has the effect
of down-sampling the feature maps We can interpret the activation of a unit in a feature map as a measure of the
strength of detection of a corresponding feature, so that the max-pooling preserves
information on whether the feature is present and with what strength but discards
some positional information There are many other choices of pooling function, for
example average pooling in which the pooling function computes the average of the
values from the corresponding receptive Ô¨Åeld in the feature map These all introduce
some degree of local translation invariance Pooling is usually applied to each channel of a feature map independently CONVOLUTIONAL NETWORKS
example, if we have a feature map with 8channels, each of dimensionality 64√ó64,
and we apply max-pooling with a receptive Ô¨Åeld of size 2√ó2and a stride of 2, the
output of the pooling operation will be a tensor of dimensionality 32√ó32√ó8

============================================================

=== CHUNK 292 ===
Palavras: 359
Caracteres: 2190
--------------------------------------------------
We can also apply pooling across multiple channels of a feature map, which
gives the network the potential to learn other invariances beyond simple translation
invariance For example, if several channels in a convolutional layer learn to detect
the same feature but at different orientations, then max-pooling across those feature
maps will be approximately invariant to rotations Pooling also allows a convolutional network to process images of varying sizes Ultimately, the output, and generally some of the intermediate layers, of a convolu-
tional network must have a Ô¨Åxed size Variable-sized inputs can be accommodated
by varying the stride length of the pooling according to the size of the image such
that the number of pooled outputs remains constant 10.2.7 Multilayer convolutions
The convolutional network structure described so far is analogous to a single
layer in a standard fully connected neural network To allow the network to discover
and represent hierarchical structure in the data, we now extend the architecture by
considering multiple layers of the kind described above Each convolutional layer is
described by a Ô¨Ålter tensor of dimensionality M√óM√óCIN√óCOUT in which the
number of independent weight and bias parameters is (M2CIN+1)C OUT Each such
convolutional layer can optionally be followed by a pooling layer We can now apply
multiple such layers of convolution and pooling in succession, in which the COUT
output channels of a particular layer, analogous to the RGB channels of the input
image, form the input channels of the next layer Note that the number of channels
in a feature map is sometimes called the ‚Äòdepth‚Äô of the feature map, but we prefer to
reserve the term depth to mean the number of layers in a multilayer network A key property that we built into the convolutional framework is that of locality,
in which a given unit in a feature map takes information only from a small patch, the
receptive Ô¨Åeld, in the previous layer When we construct a deep neural network in
which each layer is convolutional then the effective receptive Ô¨Åeld of a unit in later
layers in the network becomes much larger than those in earlier layers, as seen in
Figure 10.9

============================================================

=== CHUNK 293 ===
Palavras: 363
Caracteres: 2196
--------------------------------------------------
In many applications, the output units of the network need to make predictions
about the image as a whole, for example in a classiÔ¨Åcation task, and so they need
to combine information from across the whole of the input image This is typically
achieved by introducing one or two standard fully connected layers as the Ô¨Ånal stages
of the network, in which each unit is connected to every unit in the previous layer The number of parameters in such an architecture can be manageable because the Ô¨Å-
nal convolutional layer generally has much lower dimensionality than the input layer
due to the intermediate pooling layers Nevertheless, the Ô¨Ånal fully connected layers
may contain the majority of the independent degrees of freedom in the network even
if the number of (shared) connections in the network is larger in the convolutional
layers A complete CNN therefore comprises multiple layers of convolutions inter-
10.2 Convolutional Filters 299
Figure 10.9 Illustration of how the effective re-
ceptive Ô¨Åeld grows with depth in
a multilayer convolutional network Here we see that the red unit at
the top of the output layer takes
inputs from a receptive Ô¨Åeld in
the middle layer of units, each of
which has a receptive Ô¨Åeld in the
Ô¨Årst layer of units Thus, the ac-
tivation of the red unit in the out-
put layer depends on the outputs
of 3 units in the middle layer and 5
units in the input layer receptive
Ô¨Åeld
spersed with
pooling operations, and often with conventional fully connected layers
in the Ô¨Ånal stages of the network There are many choices to be made in designing
such an architecture including the number of layers, the number of channels in each
layer, the Ô¨Ålter sizes, the stride widths, and multiple other such hyperparameters A
wide variety of different architectures have been explored, although in practice it is
difÔ¨Åcult to make a systematic comparison of hyperparameter values using hold-out
data due to the high computational cost of training each candidate conÔ¨Åguration 10.2.8 Example network architectures
Convolutional networks were the Ô¨Årst deep neural networks (i.e., ones with
more than two learnable layers of parameters) to be successfully deployed in ap-
plications

============================================================

=== CHUNK 294 ===
Palavras: 367
Caracteres: 2269
--------------------------------------------------
An early example was LeNet, which was used to classify low-resolution
monochrome images of handwritten digits (LeCun et al., 1989; LeCun et al., 1998) The development of more powerful convolutional networks was accelerated through
the introduction of a large-scale benchmark data set called ImageNet (Deng et al.,
2009) comprising some 14 million natural images each of which has been hand la-
belled into one of nearly 22,000 categories This was a much larger data set than had
been used previously, and the advances in the Ô¨Åeld driven by ImageNet served to em-
phasize the importance of large-scale data, alongside well-designed models having
appropriate inductive biases, in building successful deep learning solutions A subset of images comprising 1,000 non-overlapping categories formed the ba-
sis for the annual ImageNet Large Scale Visual Recognition Challenge Again, this
was a much larger number of categories than the typically few dozen classes previ-
ously considered Having so many categories made the problem much more chal-
lenging because, if the classes were distributed uniformly, random guessing would
300 10 CONVOLUTIONAL NETWORKS
have an error rate of 99.9% The data set has just over 1.28 million training images,
50,000 validation images, and 100,000 test images The classiÔ¨Åers are designed to
produce a ranked list of predicted output classes on test images, and results are re-
ported in terms of top-1 and top-5 error rates, meaning an image is deemed to be
correctly classiÔ¨Åed if the true class appears at the top of the list or if it is in one of the
Ô¨Åve highest-ranked class predictions Early results with this data set achieved a top-5
error rate of around 25.5% An important advance was made by the AlexNet convo-
lutional network architecture (Krizhevsky, Sutskever, and Hinton, 2012), which won
the 2012 competition and reduced the top-5 error rate to a new record of 15.3% Key
aspects of this model were the use of the ReLU activation function, the application of
GPUs to train the network, and the use of dropout regularization Subsequent years Section 9.6.1
saw further advances, leading to error rates of around 3%, which is somewhat better
than human-level performance for the same data, which is around 5% (Dodge and
Karam, 2017)

============================================================

=== CHUNK 295 ===
Palavras: 371
Caracteres: 2273
--------------------------------------------------
This can be attributed to the difÔ¨Åculty humans have in distinguishing
subtly different classes (for example multiple varieties of mushrooms) As an example of a typical convolutional network architecture, we look in detail
at the VGG-16 model (Simonyan and Zisserman, 2014), where VGG stands for the
Visual Geometry Group, who developed the model, and 16 refers to the number of
learnable layers in the model VGG-16 has some simple design principles leading to
a relatively uniform architecture, shown in Figure 10.10, that minimizes the number
of hyperparameter choices that need to be made It takes an input image having
224√ó224pixels and three colour channels, followed by sets of convolutional layers
interspersed with down-sampling All convolutional layers have Ô¨Ålters of size 3√ó3
with a stride of 1, same padding, and a ReLU activation function, whereas the max-
pooling operations all use stride 2 and Ô¨Ålter size 2√ó2thereby down-sampling the
number of units by a factor of 4 The Ô¨Årst learnable layer is a convolutional layer in
which each unit takes input from a 3√ó3√ó3‚Äòcube‚Äô from the stack of input channels
and so has 28 parameters including the bias These parameters are shared across
all units in the feature map for that channel There are 64 such feature channels in
the Ô¨Årst layer, giving an output tensor of size 224√ó224√ó64 The second layer
is also convolutional and again has 64 channels This is followed by max-pooling
giving feature maps of size 112√ó112 Layers 3 and 4 are again convolutional, of
dimensionality 112√ó112, and each was chosen to have 128 channels This increase
in the number of channels offsets to some extent the down-sampling in the max-
pooling layer to ensure that the number of variables in the representation at each
layer does not decrease too rapidly through the network Again, this is followed by a
max-pooling operation to give a feature map size of 56√ó56 Next come three more
convolutional layers each with 256 channels, thereby again doubling the number of
channels in association with the down-sampling This is followed by another max-
pooling to give feature maps of size 28√ó28followed by three more convolutional
layers each having 512 channels, followed by another max-pooling, which down-
samples to feature maps of size 14√ó14

============================================================

=== CHUNK 296 ===
Palavras: 360
Caracteres: 2332
--------------------------------------------------
This is followed by three more convolutional
layers, although the number of feature maps in these layers is kept at 512, followed
by another max-pooling, which brings the size of the feature maps down to 7√ó7 Finally there are three more layers that are fully connected meaning that they are
10.2 Convolutional Filters 301
convolution ‚ÜíReLU
max pooling
fully connected ‚ÜíReLU
softmax activationinput image
224√ó224√ó3
‚Üê224√ó224√ó64
‚Üê112√ó112√ó128
‚Üê56√ó56√ó256
‚Üë
28√ó28√ó512‚Üì14√ó14√ó512
‚Üë
7√ó7√ó512‚Üì1√ó1√ó4096
‚Üì1√ó1√ó1000
Figure 10.10 The architecture of a typical convolutional network, in this case a model called VGG-16 standard neural network layers with full connectivity and no sharing of parameters The Ô¨Ånal max-pooling layer has 512 channels each of size 7√ó7giving 25,088 units
in total The Ô¨Årst fully connected layer has 4,096 units, each of which is connected
to each of the max-pooling units This is followed by a second fully connected layer
again with 4,096 units, and Ô¨Ånally there is a third fully connected layer with 1,000
units so that the network can be applied to a classiÔ¨Åcation problem involving 1,000
classes All the learnable layers in the network have nonlinear ReLU activation
functions, except for the output layer, which has a softmax activation function In
total there are roughly 138 million independently learnable parameters in VGG-16,
the majority of which (nearly 103 million) are in the Ô¨Årst fully connected layer,
whereas most of the connections are in the Ô¨Årst convolutional layer Exercise 10.8
Earlier CNNs typically had fewer convolutional layers, as they had larger re-
ceptive Ô¨Åelds For example, Alexnet (Krizhevsky, Sutskever, and Hinton, 2012) has
11√ó11receptive Ô¨Åelds with a stride of 4 We saw in Figure 10.9 that larger receptive
Ô¨Åelds can also be achieved implicitly by using multiple layers each having smaller
receptive Ô¨Åelds The advantage of the latter approach is that it requires signiÔ¨Åcantly
fewer parameters, effectively imposing an inductive bias on the larger Ô¨Ålters as they
must be composed of convolutional sub-Ô¨Ålters Although this is a highly complex
architecture, only the network function itself needs to be coded explicitly since the
derivatives of the cost function can be evaluated using automatic differentiation and Section 8.2
the cost function optimized using stochastic gradient descent

============================================================

=== CHUNK 297 ===
Palavras: 362
Caracteres: 2332
--------------------------------------------------
CONVOLUTIONAL NETWORKS
10.3 Visualizing
Trained CNNs
W
e turn now to an exploration of the features learned by modern deep CNNs, and
we will see some remarkable similarities to the properties of the mammalian visual
cortex 10.3.1 Visual cortex
Historically, much of the motivation for CNNs came from pioneering research
in neuroscience, which gave insights into the nature of visual processing in mam-
mals including humans Electrical signals from the retina are transformed through
a series of processing layers in the visual cortex, which is at the back of the brain,
where the neurons are organized into two-dimensional sheets each of which forms a
map of the two-dimensional visual Ô¨Åeld In their pioneering work, Hubel and Wiesel
(1959) measured the electrical responses of individual neurons in the visual cortex
of cats while presenting visual stimuli to the cats‚Äô eyes They discovered that some
neurons, called ‚Äòsimple cells‚Äô, have a strong response to visual inputs with a simple
edge oriented at a particular angle and located at a particular position within the vi-
sual Ô¨Åeld, whereas other stimuli generated relatively little response in those neurons More detailed studies showed that the response of these simple cells can be modelled
using Gabor Ô¨Ålters, which are two-dimensional functions deÔ¨Åned by
G(x;y ) =Aexp/parenleftbig
‚àí/tildewidex2‚àí/tildewidey2/parenrightbig
sin (!/tildewidex+) (10.6)
where
/tildewidex= (x‚àíx0) cos() + (y‚àíy0) sin() (10.7)
/tildewidey=‚àí(x‚àíx0) sin() + (y‚àíy0) cos(): (10.8)
Equations (10.7) and (10.8) represent a rotation of the coordinate system through
an angleand therefore the sin(¬∑) term in (10.6) represents a sinusoidal spatial
oscillation oriented in a direction deÔ¨Åned by the polar angle , with frequency The exponential factor in (10.6) creates a decay envelope that
localizes the Ô¨Ålter in the neighbourhood of position (x0;y0)and with decay rates
governed by and Example Gabor Ô¨Ålters are shown in Figure 10.11 Hubel and Wiesel also discovered the presence of ‚Äòcomplex cells‚Äô, which re-
spond to more complex stimuli and which seem to be derived by combining and
processing the output of simple cells These responses exhibit some degree of invari-
ance to small changes in the input such as shifts in location, analogous to the pooling
units in a convolutional deep network

============================================================

=== CHUNK 298 ===
Palavras: 352
Caracteres: 2213
--------------------------------------------------
Deeper levels of the mammalian visual pro-
cessing system have even more speciÔ¨Åc responses and even greater invariance to
transformations of the visual input Such cells have been termed ‚Äògrandmother cells‚Äô
because such a cell could notionally respond if, and only if, the visual input corre-
sponds to a person‚Äôs grandmother, irrespective of location, scale, lighting, or other
transformations of the scene This work directly inspired an early form of deep neu-
ral network called the neocognitron (Fukushima, 1980), which was the forerunner of
10.3 Visualizing Trained CNNs 303
Figure 10.11 Examples of Gabor Ô¨Ålters deÔ¨Åned
by (10.6) The orientation angle
varies from 0in the top row to
=2in the bottom row, whereas
the frequency varies from != 1
in the left column to != 10 in the
right column convolutional neural networks The neocognitron had multiple layers of processing
comprising local receptive Ô¨Åelds with shared weights followed by local averaging or
max-pooling to confer positional invariance However, it lacked an end-to-end train-
ing procedure since it predated the development of backpropagation, relying instead
on greedy layer-wise learning through an unsupervised clustering algorithm 10.3.2 Visualizing trained Ô¨Ålters
Suppose we have a trained deep CNN and we wish to explore what the hidden
units have learned to detect For the Ô¨Ålters in the Ô¨Årst convolutional layer this is
relatively straightforward, as they correspond to small patches in the original input
image space, and so we can visualize the network weights associated with these
Ô¨Ålters directly as small images The Ô¨Årst convolutional layer computes inner products
between the Ô¨Ålters and the corresponding image patches, and so the unit will have a
large activation when the inner product has a large magnitude Figure 10.12 shows some example Ô¨Ålters from the Ô¨Årst layer of a CNN trained
on the ImageNet data set We see a remarkable similarity between these Ô¨Ålters and
the Gabor Ô¨Ålters of Figure 10.11 However, this does not imply that a convolutional
neural network is a good model of how the brain works, because very similar results
can be obtained from a wide variety of statistical methods (Hyv ¬®arinen, Hurri, and
Hoyer, 2009)

============================================================

=== CHUNK 299 ===
Palavras: 369
Caracteres: 2231
--------------------------------------------------
This is because these characteristic Ô¨Ålters are a general property of
the statistics of natural images and therefore prove useful for image understanding
in both natural and artiÔ¨Åcial systems Although we can visualize the Ô¨Ålters in the Ô¨Årst layer directly, the subsequent
layers in the network are harder to interpret because their inputs are not patches
of images but groups of Ô¨Ålter responses One approach, analogous to that used by
Hubel and Wiesel, is to present a large number of image patches to the network and
304 10 CONVOLUTIONAL NETWORKS
Figure 10.12 Examples of learned Ô¨Ålters from
the Ô¨Årst layer of AlexNet Note
the remarkable similarity of many
of the learned Ô¨Ålters to the Gabor
Ô¨Ålters in Figure 10.11, which cor-
respond to features detected by
living neurons in the visual cortex
of mammals see which produce the highest activation value in any particular hidden unit Fig-
ure 10.13 shows examples obtained using a network with Ô¨Åve convolutional layers,
followed by two fully connected layers, trained on 1.3 million ImageNet data points
spanning 1;000classes We see a natural hierarchical structure, with the Ô¨Årst layer re-
sponding to edges, the second layer responding to textures and simple shapes, layer 3
showing components of objects (such as wheels), and layer 5 showing entire objects We can extend this technique to go beyond simply selecting image patches from
the validation set and instead perform a numerical optimization over the input vari-
ables to maximize the activation of a particular unit (Zeiler and Fergus, 2013; Si-
monyan, Vedaldi, and Zisserman, 2013; Yosinski et al., 2015) If we chose the unit
to be one of the outputs then we can look for an image that is most representative
of the corresponding class label Because the output units generally have a softmax
activation function, it is better to maximise the pre-activation value that feeds into
the softmax rather than the class probability directly, as this ensures the optimization
depends on only one class For example, if we seek the image that produces the
strongest response to the class ‚Äòdog‚Äô, then if we optimize the softmax output it could
drive the image to be, say, less like a cat because of the denominator in the softmax

============================================================

=== CHUNK 300 ===
Palavras: 374
Caracteres: 2354
--------------------------------------------------
This approach is related to adversarial training Unconstrained optimization of the Chapter 17
output-unit activation, however, leads to individual pixel values being driven to inÔ¨Ån-
ity and also creates high-frequency structure that is difÔ¨Åcult to interpret, and so some
form of regularization is required to Ô¨Ånd solutions that are closer to natural images (2015) used a regularization function comprising the sum of squares
of the pixel values along with a procedure that alternates gradient-based updates to
the image pixel values with a blurring operation to remove high-frequency structure
and a clipping operation that sets to zero those pixel values that make only small
contributions to the class label Example results are shown in Figure 10.14 Visualizing Trained CNNs 305
Layer 1 Layer 2 Layer 3 Layer 5
Figure 10.13 Examples of image patches (taken from a validation set) that produce the strongest activation in
the hidden units in a network having Ô¨Åve convolutional layers trained on ImageNet data The top nine activations
in each feature map are arranged as a 3√ó3grid for four randomly chosen channels in each of the corresponding
layers We see a steady progression in complexity with depth, from simple edges in layer 1 to complete objects
in layer 5 [From Zeiler and Fergus (2013) with permission.]
10.3.3 Saliency maps
Another way to gain insight into the features used by a convolutional network
is through saliency maps , which aim to identify those regions of an image that are
most signiÔ¨Åcant in determining the class label This is best done by investigating the
Ô¨Ånal convolutional layer because this still retains spatial localization, which becomes
lost in the subsequent fully connected layers, and yet it has the highest level of se-
mantic representation The Grad-CAM (gradient class activation mapping) method
(Selvaraju et al , 2016) Ô¨Årst computes, for a given input image, the derivatives of the
output-unit pre-activation a(c)for a given class c, before the softmax, with respect to
the pre-activations a(k)
ijof all the units in the Ô¨Ånal convolutional layer for channel k For each channel in that layer, the average of those derivatives is evaluated to give
k=1
Mk/summationdisplay
i/summationdisplay
j@a(c)
@a(k)
ij(10.9)
whereiandjindex the rows and columns of channel k, andMkis the total number
of units in that channel

============================================================

=== CHUNK 301 ===
Palavras: 356
Caracteres: 2245
--------------------------------------------------
These averages are then used to form a weighted sum of the
form:
L=/summationdisplay
kkA(k)(10.10)
in which A(k)is a matrix with elements a(k)
ij The resulting array has the same
dimensionality as the Ô¨Ånal convolutional layer, for example 14√ó14for the VGG
network shown in Figure 10.10, and can be superimposed on the original image in
the form of a ‚Äòheat map‚Äô as seen in Figure 10.15 CONVOLUTIONAL NETWORKS
Figure 10.14 Examples of synthetic images generated by maximizing the class probability with respect to the
image pixel channel values for a trained convolutional classiÔ¨Åer Four different solutions, obtained with different
settings of the regularization parameters, are shown for each of four object classes [From Y osinski et al (2015)
with permission.]
10.3.4 Adversarial attacks
Gradients with respect to changes in the input image pixel values can also be
used to create adversarial attacks against convolutional networks (Szegedy et al These attacks involve making very small modiÔ¨Åcations to an image, at a level
that is imperceptible to a human, which cause the image to be misclassiÔ¨Åed by the
neural network One simple approach to creating adversarial images is called the
fast gradient sign method (Goodfellow, Shlens, and Szegedy, 2014) This involves
changing each pixel value in an image xby a Ô¨Åxed amount with a sign determined
by the gradient of an error function E(x;t)with respect to the pixel values This
gives a modiÔ¨Åed image deÔ¨Åned by
x/prime=x+sign(‚àáxE(x;t)): (10.11)
Heretis the true label of x, and the error E(x;t)could, for example, be the neg-
ative log likelihood of x The required gradient can be computed efÔ¨Åciently using
backpropagation During conventional training of a neural network, the network pa- Chapter 8
rameters are adjusted to minimize this error, whereas the modiÔ¨Åcation deÔ¨Åned by
(10.11) alters the image (while keeping the trained network parameters Ô¨Åxed) so as
Figure 10.15 Saliency maps for the
VGG-16 network with respect to the
‚Äòdog‚Äô and ‚Äòcat‚Äô categories [From Sel-
varaju et al (2016) with permission.]
Original image Saliency map for ‚Äòdog‚Äô Saliency map for ‚Äòcat‚Äô

10.3 Visualizing Trained CNNs 307
Figure 10.16 Example of an adver-
sarial attack against a trained convo-
lutional network

============================================================

=== CHUNK 302 ===
Palavras: 356
Caracteres: 2238
--------------------------------------------------
The image on the
left is classiÔ¨Åed as a panda with conÔ¨Å-
dence 57:7% The addition of a small
level of a random-looking perturba-
tion (that itself is classiÔ¨Åed as a ne-
matode with conÔ¨Ådence 8:2%) results
in the image on the right, which is
classiÔ¨Åed as a gibbon with conÔ¨Ådence
99:3% [From Goodfellow, Shlens, and
Szegedy (2014) with permission.]
panda 57.7% gibbon 99.3%
to increase the error By keeping small, we ensure that the changes to the image are
undetectable to the human eye Remarkably, this can give images that are misclassi-
Ô¨Åed by the network with high conÔ¨Ådence, as seen in the example in Figure 10.16 The ability to fool neural networks in this way raises potential security concerns
as it creates opportunities for attacking trained classiÔ¨Åers It might appear that this
issue arises from over-Ô¨Åtting, in which a high-capacity model has adapted precisely
to the speciÔ¨Åc image such that small changes in the input produce large changes
in the predicted class probabilities However, it turns out that an image that has
been adapted to give a spurious output for a particular trained network can give
similarly spurious outputs when fed to other networks (Goodfellow, Shlens, and
Szegedy, 2014) Moreover, a similar adversarial result can be obtained with much
less Ô¨Çexible linear models It is even possible to create physical artefacts such that
a regular, uncorrupted image of the artefact will give erroneous predictions when
presented to a trained neural network, as seen in Figure 10.17 Although these basic
kinds of adversarial attacks can be addressed by simple modiÔ¨Åcations to the network
training process, more sophisticated approaches are harder to defeat Understanding
the implications of these results and mitigating their potential pitfalls remain open
areas of research Figure 10.17 Two examples of physical stop
signs that have been modiÔ¨Åed Images of these
objects are robustly classiÔ¨Åed as 45 mph speed-
limit signs by CNNs (2018)
with permission.]

308 10 CONVOLUTIONAL NETWORKS
10.3.5 Synthetic images
As a Ô¨Ånal example of image modiÔ¨Åcation that provides additional insights into
the operation of a trained convolutional network, we consider a technique called
DeepDream (Mordvintsev, Olah, and Tyka, 2015)

============================================================

=== CHUNK 303 ===
Palavras: 376
Caracteres: 2250
--------------------------------------------------
The goal is to generate a synthetic
image with exaggerated characteristics We do this by determining which nodes in
a particular hidden layer of the network respond strongly to a particular image and
then modifying the image to amplify those responses For example, if we present
an image of some clouds to a network trained on object recognition and a particular
node detects a cat-like pattern at a particular region of the image, then we modify
the image to be more ‚Äòcat like‚Äô in that region To do this, we apply an image to the
input of the network and forward propagate through to some particular layer We
then set the backpropagation variables for that layer equal to the pre-activations of Section 8.1.2
the nodes and run backpropagation to the input layer to get a gradient vector over
the pixels of the image Finally, we modify the image by taking a small step in the
direction of the gradient vector This procedure can be viewed as a gradient-based
method for increasing the function Exercise 10.10
F(I) =/summationdisplay
i;j;kaijk(I)2(10.12)
whereaijk(I)is the pre-activation of the unit in row iand column jof channelkin
the chosen layer when the input image is I, and the sum is over all units and over
all channels in that layer To generate smooth-looking images, some regularization
is applied in the form of spatial smoothing and pixel clipping This process can
then be repeated multiple times if stronger enhancements are desired Examples
of the resulting image are shown in Figure 10.18 It is interesting that even though
convolutional networks are trained to discriminate between object classes, they seem
able to capture at least some of the information needed to generate images from those
classes This technique can be applied to a photograph, or we can start with inputs con-
sisting of random noise to obtain an image generated entirely from the trained net-
work Although DeepDream provides some insights into the operation of the trained
network, it has primarily been used to generate interesting looking images as a form
of artwork Object
Detection
W
e have motivated the design of CNNs primarily by the image classiÔ¨Åcation prob-
lem, in which an entire image is assigned to a single class, for example ‚Äòcat‚Äô or
‚Äòbicycle‚Äô

============================================================

=== CHUNK 304 ===
Palavras: 360
Caracteres: 2136
--------------------------------------------------
This is reasonable for data sets such as ImageNet where, by design, each
image is dominated by a single object However, there are many other applications
for CNNs that are able to exploit the inbuilt inductive biases More generally, the
convolutional layers of a CNN trained on a large image data base for a particular
task can learn internal representations that have broad applicability, and therefore a
10.4 Object Detection 309
5 iterations
 30 iterations
layer 7
 layer 10
Figure 10.18 Examples of DeepDream applied to an image The top row shows outputs when the algorithm
is applied using the activations from the 7th convolutional layer of the VGG-16 network after Ô¨Åve iterations and
after 30 iterations Similarly, the bottom row shows examples using the 10th layer, again after Ô¨Åve iterations and
after 30 iterations CNN can be Ô¨Åne-tuned for a wide range of speciÔ¨Åc tasks We have already seen Section 6.3.4
an example of a convolutional network trained on ImageNet data, which through
transfer learning was able to achieve human-level performance on skin lesion classi-
Ô¨Åcation Section 1.1.1
10.4.1 Bounding boxes
Many images have multiple objects belonging to one or more classes, and we
may wish to detect the presence and class of each object Moreover, in many appli-
cations of computer vision we also need to determine the locations within the image
of any objects that are detected For example, an autonomous vehicle that uses RGB
cameras may need to detect the presence and location of pedestrians and also identify
road signs, other vehicles, etc Consider the problem of specifying the location of an object in an image A
widely used approach is to deÔ¨Åne a bounding box, which consists of a rectangle that
Ô¨Åts closely to the boundary of the object, as illustrated in Figure 10.19 The bounding
box can be deÔ¨Åned by the coordinates of its centre along with its width and height in
the form of a vector b= (bx;by;bW;bH) Here the elements of bcan be speciÔ¨Åed
in terms of pixels or as continuous numbers where, by convention, the top left of the
image is given coordinates (0;0)and the bottom right is given coordinates (1;1)

============================================================

=== CHUNK 305 ===
Palavras: 366
Caracteres: 2193
--------------------------------------------------
CONVOLUTIONAL NETWORKS
Figure 10.19 An image containing several objects from different classes in which the location of each
object is labelled by a close-Ô¨Åtting rectangle known as a bounding box Here blue boxes
correspond to the class ‚Äòcar‚Äô, red boxes to the class ‚Äòpedestrian‚Äô, and orange boxes to the
class ‚ÄòtrafÔ¨Åc light‚Äô [Original image courtesy of Wayve Technologies Ltd.]
When images are assumed to contain one, and only one, object drawn from
a predeÔ¨Åned set of Cclasses, a CNN will generally have Coutput units whose
activation functions are deÔ¨Åned by the softmax function An object can be localized Section 5.3
by using an additional four outputs, with linear activation functions trained to predict
the bounding box coordinates (bx;by;bW;bH) Since these quantities are continuous,
a sum-of-squares error function over the corresponding outputs may be appropriate This is used for example by Redmon et al (2015), who Ô¨Årst divide the image into a
7√ó7grid For each grid cell, they use a convolutional network to output the class
and bounding box coordinates of any object associated with that grid cell, based on
features taken from the whole image 10.4.2 Intersection-over-union
We need a meaningful way to measure the performance of a trained network that
can predict bounding boxes In image classiÔ¨Åcation the output of the network is a
probability distribution over class labels, and we can measure performance by look-
ing at the log likelihood for the true class labels on a test set For object localization,
however, we need some way to measure the accuracy of a bounding box relative to
some ground truth, where the latter could, for example, be obtained by human la-
belling The extent to which the predicted and target boxes overlap can be used as
the basis for such a measure, but the area of the overlap will depend on the size of
the object within the image Also a predicted bounding box should be be penalized
for the region of the prediction that lies outside the ground truth bounding box A
better metric that addresses both of these issues is called intersection-over-union , or
IoU, and is simply the ratio of the area of the intersection of the two bounding boxes
10.4

============================================================

=== CHUNK 306 ===
Palavras: 352
Caracteres: 2052
--------------------------------------------------
Object Detection 311
Figure 10.20 Illustration of the
intersection-over-union metric for
quantifying the accuracy of a bound-
ing box prediction If the predicted
bounding box is shown by the blue
rectangle and the ground truth by the
red rectangle, then the intersection-
over-union is deÔ¨Åned as the ratio of
the area of the intersection of the
boxes, shown in green on the left,
divided by the area of their union,
shown in green on the right.area
of intersection area
of union
to
that of their union, as illustrated in Figure 10.20 Note that the IoU measure lies
in the range 0to1 Predictions can be labelled as correct if the IoU measure exceeds
a threshold, which is typically set at 0:5 Note that IoU is not generally used directly
as a loss function for training as it is hard to optimize by gradient descent, and so
training is typically performed using centred objects, and the IoU score is mainly
used an evaluation metric 10.4.3 Sliding windows
One approach to object detection and object localization starts by creating a
training set consisting of tightly cropped examples of the object to be detected, as
well as examples of similarly cropped sections of images that do not contain any
object (the ‚Äòbackground‚Äô class) This data set is used to train a classiÔ¨Åer, such as
a deep CNN, whose outputs represent the probability of there being an object of
each particular class in the input window The trained model is then used to detect
objects in a new image by ‚Äòscanning‚Äô an input window across the image and, for
each location, taking the resulting subset of the image as input to the classiÔ¨Åer This
is called a sliding window When an object is detected with high probability, the
associated window location then deÔ¨Ånes the corresponding bounding box One obvious drawback of this approach is that it can be computationally very
costly due to the large number of potential window positions in the image Further-
more, the process may have to be repeated using windows of various scales to allow
for different sizes of object within the image

============================================================

=== CHUNK 307 ===
Palavras: 354
Caracteres: 2219
--------------------------------------------------
A cost saving can be made by moving
the input window in strides across the image, both horizontally and vertically, which
are larger than one pixel However, there is a trade-off between precision of location
using a small stride and reducing the computational cost by using a larger stride The
computational cost of a sliding window approach may be reasonable for simple clas-
siÔ¨Åers, but for deep neural networks potentially containing millions of parameters,
the cost of a naive implementation can be prohibitive Fortunately, the convolutional structure of the neural network allows for a dra-
matic improvement in efÔ¨Åciency (Sermanet et al., 2013) We note that a convo-
lutional layer within such a network itself involves sliding a feature detector, with
shared weights, across the input image in strides Consequently, when a sliding win-
dow is used to generate multiple forward passes through a convolutional network
312 10 CONVOLUTIONAL NETWORKS
Figure 10.21 Illustration of replicated calculations
when a CNN is used to process data
from a sliding input window, in which
the red and blue boxes show two
overlapping locations for the input
window The green box represents
one of the locations for the receptive
Ô¨Åeld of a hidden unit in the Ô¨Årst con-
volutional layer, and the evaluation of
the corresponding hidden-unit activa-
tion is shared across the two window
positions there is substantial redundancy in the computation, as illustrated in Figure 10.21 Because the computational structure of sliding windows mirrors that of convolu-
tions, it turns out to be remarkably simple to implement sliding windows efÔ¨Åciently
in a convolutional network Consider the simpliÔ¨Åed convolutional network in Fig-
ure 10.22 , which consists of a convolutional layer followed by a max-pooling layer
followed by a fully connected layer For simplicity we have shown only a single
channel in each layer, but the extension to multiple channels is straightforward The
input image to the network has size 6√ó6, the Ô¨Ålters in the convolutional layer have
size3√ó3with stride 1, and the max-pooling layer has non-overlapping receptive
Ô¨Åelds of size 2√ó2with stride 1 This is followed by a fully connected layer with
a single output unit

============================================================

=== CHUNK 308 ===
Palavras: 386
Caracteres: 2271
--------------------------------------------------
Note that we can also view this Ô¨Ånal layer as another convolu-
tional layer with a Ô¨Ålter size that is 2√ó2, so that there is only a single position for
the Ô¨Ålter and hence a single output Now suppose this network is trained on centred images of objects and then ap-
plied to a larger image of size 8√ó8, as shown in Figure 10.23 in which we simply
enlarge the network by increasing the size of the convolutional and max-pooling
layers The convolution layer now has size 6√ó6and the pooling layer has size
Figure 10.22 Example of a simple
convolutional network having a sin-
gle channel at each layer used to il-
lustrate the concept of a sliding win-
dow for detecting objects in images.6√ó6input image
3√ó3convolution
2√ó2pooling
fully connected
10.4 Object Detection 313
8√ó8input image
3√ó3convolution
2√ó2pooling
fully connected
Figure 10.23 Application of the network shown in Figure 10.22 to a larger image in which the additional
computation required corresponds to the blue regions There are now four output units each of which has its own softmax function The weights into this unit are shared across the four units We see that the calcula-
tions needed to process the input corresponding to a window position in the top left
corner of the input image are the same as those used to process the original 6√ó6
inputs used in training For the remaining window positions, only a small amount
of additional computation is needed, as indicated by the blue squares, leading to a
signiÔ¨Åcant increase in efÔ¨Åciency compared to a naive repeated application of the full
convolutional network Note that the fully connected layers themselves now have a Exercise 10.12
convolutional structure 10.4.4 Detection across scales
As well as looking for objects in different positions in the image, we also need
to look for objects at different scales and at different aspect ratios For example, a
tight bounding box drawn around a cat will have a different aspect ratio when the
cat is sitting upright compared to when it is lying down Instead of using multiple
detectors with different sizes and shapes of input window, it is simpler but equivalent
to use a Ô¨Åxed input window and to make multiple copies of the input image each with
a different pair of horizontal and vertical scaling factors

============================================================

=== CHUNK 309 ===
Palavras: 362
Caracteres: 2232
--------------------------------------------------
The input window is then
scanned over each of the image copies to detect objects, and the associated scaling
factors are then used to transform the bounding box coordinates back into the original
image space, as illustrated in Figure 10.24 CONVOLUTIONAL NETWORKS
(a) (b) (c)
Figure 10.24 Illustration of the detection and localization of objects at multiple scales and aspect ratios using
a Ô¨Åxed input window The original image (a) is replicated multiple times and each copy is scaled in the horizontal
and/or vertical directions, as illustrated for a horizontal scaling in (b) A Ô¨Åxed-sized window is then scanned
over the scaled images When an object is detected with high probability, as illustrated by the red box in (b),
the corresponding window coordinates can be projected back into the original image space to determine the
corresponding bounding box as shown in (c) 10.4.5 Non-max suppression
By scanning a trained convolutional network over an image, it is possible to
detect multiple instances of the same class of object within the image as well as in-
stances of objects from other classes However, this also tends to produce multiple
detections of the same object at similar locations, as illustrated in Figure 10.25 This
can be addressed using non-max suppression, which, for each object class in turn,
works as follows It Ô¨Årst runs the sliding window over the whole image and evaluates
the probability of an object of that class being present at each location Next it elim-
inates all the associated bounding boxes whose probability is below some threshold,
say0:7, giving a result of the kind illustrated in Figure 10.25 The box with the
highest probability is considered to be a successful detection, and the corresponding
bounding box is recorded as a prediction Next, any other boxes whose IoU with
the successful detection box exceeds some threshold, say 0:5, is discarded This is
intended to eliminate multiple nearby detections of the same object Then of the
remaining boxes, the one with the highest probability is declared to be another suc-
cessful detection, and the elimination step is repeated The process continues until
all bounding boxes have either been discarded or declared as successful detections

============================================================

=== CHUNK 310 ===
Palavras: 378
Caracteres: 2318
--------------------------------------------------
10.4.6 Fast region CNNs
Another way to speed up object detection and localization is to note that a scan-
ning window approach applies the full power of a deep convolutional network to
all areas of the image, even though some areas may be unlikely to contain an ob-
ject Instead, we can apply some form of computationally cheaper technique, for
example a segmentation algorithm, to identify parts of the image where there is a
higher probability of Ô¨Ånding an object, and then apply the full network only to these
areas, leading to techniques such as fast region proposals with CNN orfast R-CNN
10.5 Image Segmentation 315
Figure 10.25 Schematic illustration of multiple de-
tections of the same object at nearby
locations, along with their associ-
ated probabilities The red bounding
box corresponds to the highest over-
all probability Non-max suppres-
sion eliminates the other overlapping
candidate bounding boxes shown in
blue, while preserving the detection
of another instance of the same ob-
ject class shown by the bounding box
in green 0.81
0.910.75
0.95
(Girshick,
2015) It is also possible to use a region proposal convolutional network
to identify the most promising regions, leading to faster R-CNN (Ren et al., 2015),
which allows end-to-end training of both the region proposal network and the detec-
tion and localization network A disadvantage of the sliding window approach is that if we want a very precise
localization the objects then we need to consider large numbers of Ô¨Ånely spaced win-
dow positions, which becomes computationally costly A more efÔ¨Åcient approach is
to combine sliding windows with the direct bounding box predictions that we dis-
cussed at the start of this section (Sermanet et al., 2013) In this case, the continuous
outputs predict the position of the bounding box relative to the window position and
therefore provide some Ô¨Åne-tuning to the predicted position Ima
ge Segmentation
In
an image classiÔ¨Åcation problem, an entire image is assigned to a single class la-
bel We have seen that more detailed information is provided if multiple objects are
detected and their positions recorded using bounding boxes An even more detailed Section 10.4
analysis is obtained with semantic segmentation in which every pixel of an image is
assigned to one of a predeÔ¨Åned set of classes

============================================================

=== CHUNK 311 ===
Palavras: 381
Caracteres: 2339
--------------------------------------------------
This means that the output space will
have the same dimensionality as the input image and can therefore be conveniently
represented as an image with the same number of pixels Although the input image
will generally have three channels for R, G, and B, the output array will have C
channels, if there are Cclasses, representing the probability for each class If we as-
sociate a different (arbitrarily chosen) colour with each class, then the prediction of a
segmentation network can be represented as an image in which each pixel is coloured
according to the class having the highest probability, as illustrated in Figure 10.26 10.5.1 Convolutional segmentation
A simple way to approach a semantic segmentation problem would be to con-
struct a convolutional network that takes as input a rectangular section of the image
316 10 CONVOLUTIONAL NETWORKS
Figure 10.26 Example of an image and its corresponding semantic segmentation in which each pixel is
coloured according to its class For example, blue pixels correspond to the class ‚Äòcar‚Äô, red pixels to the class
‚Äòpedestrian‚Äô, and orange pixels to the class ‚ÄòtrafÔ¨Åc light‚Äô [Courtesy of Wayve Technologies Ltd.]
centred on a pixel and that has a single softmax output that classiÔ¨Åes that pixel By
applying such a network to each pixel in turn, the entire image can be segmented
(this would require edge padding around the image depending on the size of the
input window) However, this approach would be extremely inefÔ¨Åcient due to redun-
dant calculations caused by overlapping patches As we have seen, we can remove Figure 10.21
this inefÔ¨Åciency by grouping together the forward-pass calculations for different in-
put locations into a single network, which results in a model in which the Ô¨Ånal fully
connected layers are also convolutional We could therefore create a CNN in which Figure 10.23
each layer has the same dimensionality as the input image, by having stride 1at each
layer with same padding and no pooling Each output unit has a softmax activation
function with weights that are shared across all outputs Although this could work,
such a network would still need many layers, with multiple channels in each layer,
to learn the complex internal representations needed to achieve high accuracy, and
overall this would be prohibitively costly for images of reasonable resolution

============================================================

=== CHUNK 312 ===
Palavras: 384
Caracteres: 2448
--------------------------------------------------
10.5.2 Up-sampling
As we have already seen, most convolutional networks use several levels of
down-sampling so that as the number of channels increases, the size of the feature
maps decreases, keeping the overall size and cost of the network tractable, while
allowing the network to extract semantically meaningful high-order features from the
image We can use this concept to create a more efÔ¨Åcient architecture for semantic
segmentation by taking a standard deep convolutional network and adding additional
learnable layers that take the low-dimensional internal representation and transform
it back up to the original image resolution (Long, Shelhamer, and Darrell, 2014; Noh,
Hong, and Han, 2015; Badrinarayanan, Kendall, and Cipolla, 2015), as illustrated in
Figure 10.27 To do this we need a way to reverse the down-sampling effects of strided convo-
lutions and pooling operations Consider Ô¨Årst the up-sampling analogue of pooling,
where the output layer has a larger number of units than the input layer, for example
10.5 Image Segmentation 317
high resolutionmid resolution
low resolution
Figure 10.27 Illustration of a convolutional neural network used for semantic image segmentation, showing
the reduction in the dimensionality of the feature maps through a series of strided convolutions and/or pooling
operations, followed by a series of transpose convolutions and/or unpooling which increase the dimensionality
back up to that of the original image with each input unit corresponding to a 2√ó2block of output units The question is
then what values to use for the outputs To Ô¨Ånd an up-sampling analogue of average
pooling, we can simply copy over each input value into all the corresponding out-
put units, as shown in Figure 10.28(a) We see that applying average pooling to the
output of this operation regenerates the input For max-pooling, we can consider the operation shown in Figure 10.28(b) in
which each input value is copied into the Ô¨Årst unit of the corresponding output block,
and the remaining values in each block are set to zero Again we see that apply-
ing a max-pooling operation to the output layer regenerates the input layer This
is sometimes called max-unpooling Assigning the non-zero value to the Ô¨Årst ele-
ment of the output block seems arbitrary, and so a modiÔ¨Åed approach can be used
that also preserves more of the spatial information from the down-sampling layers
(Badrinarayanan, Kendall, and Cipolla, 2015)

============================================================

=== CHUNK 313 ===
Palavras: 360
Caracteres: 1911
--------------------------------------------------
This is done by choosing a network
architecture in which each max-pooling down-sampling layer has a corresponding
1 2
3 41 1 2 2
1 1 2 2
3 3 4 4
3 3 4 4
(a)1 2
3 41 0 2 0
0 0 0 0
3 0 4 0
0 0 0 0
(b)
Figure 10.28 Illustration of unpooling operations showing (a) an analogue of average pooling and (b) an ana-
logue of max-pooling 318 10.CONVOLUTIONAL NETWORKS
5 2 4 2
7 1 0 3
3 8 9 6
4 7 8 17 4
8 9intermediate layers3 7
9 40 0 7 0
3 0 0 0
0 9 4 0
0 0 0 0
Figure 10.29 Some of the spatial information from a max-pooling layer, shown on the left, can be preserved by
noting the location of the maximum value for each 2√ó2block in the input array, and then in the corresponding
up-sampling layer, placing the non-zero entry at the corresponding location in the output array up-sampling layer later in the network Then during down-sampling, a record is kept
of which element in each block had the maximum value, and then in the correspond-
ing up-sampling layer, the non-zero element is chosen to have the same location, as
illustrated for 2√ó2max-pooling in Figure 10.29 10.5.3 Fully convolutional networks
The up-sampling methods considered above are Ô¨Åxed functions, much like the
average-pooling and max-pooling down-sampling operations We can also use a
learned up-sampling that is analogous to strided convolution for down-sampling In
strided convolution, each unit on the output map is connected via shared learnable
weights to a small patch on the input map, and as we move one step through the
output array, the Ô¨Ålter is moved two or more steps across the input array, and hence
the output array has lower dimensionality than the input array For up-sampling, we
use a Ô¨Ålter that connects one pixel in the input array to a patch in the output array,
and then chose the architecture so that as we move one step across the input array, we
move two or more steps across the output array (Dumoulin and Visin, 2016)

============================================================

=== CHUNK 314 ===
Palavras: 350
Caracteres: 2164
--------------------------------------------------
This is
illustrated for 3√ó3Ô¨Ålters, and an output stride of 2, inFigure 10.30 Note that there
are output cells for which multiple Ô¨Ålter positions overlap, and the corresponding
output values can be found either by summing or by averaging the contributions
from the individual Ô¨Ålter positions This up-sampling is called transpose convolution because, if the down-sampling
convolution is expressed in matrix form, the corresponding up-sampling is given by
the transpose matrix It is also called ‚Äòfractionally strided convolution‚Äô because the Exercise 10.13
stride of a standard convolution is the ratio of the step size in the output layer to the
step size in the input layer In Figure 10.30 , for example, this ratio is 1=2 Note
that this is sometimes also referred to as ‚Äòdeconvolution‚Äô, but it is better to avoid this
term since deconvolution is widely used in mathematics to mean the inverse of the
operation of convolution used in functional analysis, which is a different concept If
we have a network architecture with no pooling layers, so that the down-sampling
and up-sampling are done purely using convolutions, then the architecture is known
as a fully convolutional network (Long, Shelhamer, and Darrell, 2014) It can take
an arbitrarily sized image and will output a segmentation map of the same size Image Segmentation 319
Figure 10.30 Illustration of transpose convolution for a 3√ó
3Ô¨Ålter with an output stride of 2 This can
be thought of as the inverse operation to a
3√ó3convolution The red output patch is
given by multiplying the kernel by the acti-
vation of the red unit in the input layer, and
similarly for the blue output patch The ac-
tivations of cells for which patches overlap
are calculated by summing or averaging the
contributions from the individual patches.input 2√ó2output 5√ó5
10.5.4 The U-net architecture
We have seen that the down-sampling associated with strided convolutions and
pooling allows the number of channels to be increased without the size of the net-
work becoming prohibitive This also has the effect of reducing the spatial resolution
and hence discarding positional information as the signals Ô¨Çow through the network

============================================================

=== CHUNK 315 ===
Palavras: 392
Caracteres: 2351
--------------------------------------------------
Although this is Ô¨Åne for image classiÔ¨Åcation, the loss of spatial information is a
problem for semantic segmentation as we want to classify each pixel One approach
for addressing this is the U-net architecture (Ronneberger, Fischer, and Brox, 2015)
illustrated in Figure 10.31 , where the name comes from the U-shape of the diagram Figure 10.31 The U-net architecture has a symmetrical arrangement of down-sampling and up-sampling lay-
ers, and the output from each down-sampling layer is concatenated with the corresponding up-sampling layer CONVOLUTIONAL NETWORKS
Figure 10.32 An example of neural style transfer showing a photograph of a canal scene (left) that has been
rendered in the style of The Wreck of a Transport Ship by J Turner (centre) and in the style of The Starry
Night by Vincent van Gogh (right) In each case the image used to provide the style is shown in the inset [From
Gatys, Ecker, and Bethge (2015) with permission.]
The core concept is that for each down-sampling layer there is a corresponding up-
sampling layer, and the Ô¨Ånal set of channel activations at each down-sampling layer
is concatenated with the corresponding Ô¨Årst set of channels in the up-sampling layer,
thereby giving those layers access to higher-resolution spatial information Note that
1√ó1convolutions may be used in the Ô¨Ånal layer of a U-net to reduce the number
of channels down to the number of classes, which is then followed by a softmax
activation function Style Transfer
As we have seen, early layers in a deep convolutional network learn to detect simple
features such as edges and textures whereas later layers learn to detect more complex
entities such as objects We can exploit this property to re-render an image in the Section 10.3
style of a different image using a process called neural style transfer (Gatys, Ecker,
and Bethge, 2015) This is illustrated in Figure 10.32 Our goal is to generate a synthetic image Gwhose ‚Äòcontent‚Äô is deÔ¨Åned by an
image Cand whose ‚Äòstyle‚Äô is taken from some other image S This is achieved
by deÔ¨Åning an error function E(G)given by the sum of two terms, one of which
encourages Gto have a similar content to Cwhereas the other encourages Gto
have a similar style to S:
E(G) =Econtent (G;C) +Estyle(G;S): (10.13)
The concepts of content and style are deÔ¨Åned implicitly by the functional forms of
these two terms

============================================================

=== CHUNK 316 ===
Palavras: 384
Caracteres: 2567
--------------------------------------------------
We can then Ô¨Ånd Gby starting from a randomly initialized image
and using gradient descent to minimize E(G) To deÔ¨ÅneEcontent (G;C), we can pick a particular convolutional layer in the
network and measure the activations of the units in that layer when image Gis
used as input and also when image Cis used as input We can then encourage the
10.6 Style Transfer 321
corresponding pre-activations to be similar by using a sum-of-squares error function
of the form
Econtent (G;C) =/summationdisplay
i;j;k{aijk(G)‚àíaijk(C)}2(10.14)
whereaijk(G) denotes the pre-activation of the unit at position (i;j)in channelk
of that layer when the input image is G, and similarly for aijk(C) The choice of
which layer to use in deÔ¨Åning the pre-activations will inÔ¨Çuence the Ô¨Ånal result, with
earlier layers aiming to match low-level features like edges and later layers matching
more complex structures or even entire objects In deÔ¨Åning Estyle(G;C), the intuition is that style is determined by the co-
occurrence of features from different channels within a convolutional layer For
example, if the style image Sis such that vertical edges are generally associated
with orange blobs, then we would like the same to be true for the generated image
G However, although Econtent (G;C)tries to match features in Gat the same lo-
cations as corresponding features in C, for the style error Estyle(G;S)we want G
to have characteristics that match those of Sbut taken from any location, and so
we take an average over locations in a feature map Again, consider a particular
convolutional layer We can measure the extent to which a feature in channel kco-
occurs with the corresponding feature in channel k/primefor input image Gby forming
the cross-correlation matrix
Fkk/prime(G) =I/summationdisplay
i=1J/summationdisplay
j=1aijk(G)aijk/prime(G) (10.15)
whereIandJare the dimensions of the feature maps in this particular convolutional
layer, and the product aijkaijk/primewill be large if both features are activated If there
areKchannels in this layer, then Fkk/primeform the elements of a K√óKmatrix, called
thestyle matrix We can measure the extent to which the two images GandShave
the same style by comparing their style matrices using
Estyle(G;S) =1
(2IJK )2K/summationdisplay
k=1K/summationdisplay
k/prime=1{Fkk/prime(G)‚àíFkk/prime(S)}2: (10.16)
Although we could again make use of a single layer, more pleasing results are ob-
tained by using contributions from multiple layers in the form
Estyle(G;S) =/summationdisplay
llE(l)
style(G;S) (10.17)
whereldenotes the convolutional layer

============================================================

=== CHUNK 317 ===
Palavras: 374
Caracteres: 2354
--------------------------------------------------
The coefÔ¨Åcients ldetermine the relative
weighting between the different layers and also the weighting relative to the content
error term These weighting coefÔ¨Åcients are adjusted empirically using subjective
judgement CONVOLUTIONAL NETWORKS
Ex
ercises
10.1 (?)Consider a Ô¨Åxed weight vector wand show that the input vector xthat maximizes
the scalar product wTx, subject to the constraint that /bardblx/bardbl2is constant, is given by
x=wfor some scalar This can most easily be done using a Lagrange multiplier Appendix C
10.2 (??) Consider a convolutional network layer with a one-dimensional input array and
a one-dimensional feature map as shown in Figure 10.2, in which the input array
has dimensionality 5and the Ô¨Ålters have width 3with a stride of 1 Show that this
can be expressed as a special case of a fully connected layer by writing down the
weight matrix in which missing connections are replaced by zeros and where shared
parameters are indicated by using replicated entries Ignore any bias parameters 10.3 (?)Explicitly calculate the output of the following convolution of a 4√ó4input matrix
with a 2√ó2Ô¨Ålter:
2
5‚àí3 0
0 6 0‚àí4
‚àí1‚àí3 0 2
5 0 0 3*‚àí
2 0
4 6= (10.18)
10.4 (??) If an image IhasJ√óKpixels and a Ô¨Ålter KhasL√óMelements, write
down the limits for the two summations in (10.2) In the mathematics literature, the
operation (10.2) would be called a cross-correlation, whereas a convolution would
be deÔ¨Åned by
C(j;k) =/summationdisplay
l/summationdisplay
mI(j‚àíl;k‚àím)K (l;m): (10.19)
Write down the limits for the summations in (10.19) Show that (10.19) can be
written in the equivalent ‚ÄòÔ¨Çipped‚Äô form
C(j;k) =/summationdisplay
l/summationdisplay
mI(j+l;k+m)K (l;m) (10.20)
and again write down the limits for the summations 10.5 (?)In mathematics, a convolution for a continuous variable xis deÔ¨Åned by
F(x) =/integraldisplay‚àû
‚àí‚àûG(y)k(x‚àíy) dy (10.21)
wherek(x‚àíy)is the kernel function By considering a discrete approximation to
the integral, explain the relationship to a convolutional layer, deÔ¨Åned by (10.19), in
a CNN Exer
cises 323
10.6 (?)Consider an image of size J√óKthat is padded with an additional Ppixels on
all sides and which is then convolved using a kernel of size M√óMwhereMis an
odd number Show that if we choose P= (M‚àí1)=2, then the resulting feature map
will have size J√óKand hence will be the same size as the original image

============================================================

=== CHUNK 318 ===
Palavras: 372
Caracteres: 2279
--------------------------------------------------
10.7 (?)Show that if a kernel of size M√óMis convolved with an image of size J√óKwith
padding of depth Pand strides of length Sthen the dimensionality of the resulting
feature map is given by (10.5)
10.8 (??) For each of the 16layers in the VGG-16 CNN shown in Figure 10.10, evaluate
(i) the number of weights (i.e., connections) including biases and (ii) the number
of independently learnable parameters ConÔ¨Årm that the total number of learnable
parameters in the network is approximately 138million 10.9 (??) Consider a convolution of the form (10.2) and suppose that the kernel is sepa-
rable so that
K(l;m) =F(l)G(m) (10.22)
for some functions F(¬∑)andG(¬∑) Show that instead of performing a single two-
dimensional convolution it is now possible to compute the same answer using two
one-dimensional convolutions thereby resulting in a signiÔ¨Åcant improvement in efÔ¨Å-
ciency 10.10 (?)The DeepDream update procedure involves setting the variables for backprop-
agation equal to the pre-activations of the nodes in the chosen layer and then running
backpropagation to the input layer to get a gradient vector over the pixels of the im-
age Show that this can be derived as a gradient optimization with respect to the
pixels of an image Iapplied to the function (10.12) 10.11 (??) When designing a neural network to detect objects from Cdifferent classes in
an image, we can use a 1-of-(C +1) class label with one variable for each object class
and one additional variable representing a ‚Äòbackground‚Äô class, i.e., an input image
region that does not contain an object belonging to any of the deÔ¨Åned classes The
network will then output a vector of probabilities of length (C+ 1) Alternatively,
we can use a single binary variable to denote the presence or absence of an object
and then use a separate 1-of-C vector to denote the speciÔ¨Åc object class In this
case, the network outputs a single probability representing the presence of an object
and a separate set of probabilities over the class label Write down the relationship
between these two sets of probabilities 10.12 (??) Calculate the number of computational steps required to make one forward
pass through the convolutional network shown in Figure 10.22, ignoring biases and
ignoring the evaluation of activation functions

============================================================

=== CHUNK 319 ===
Palavras: 365
Caracteres: 2429
--------------------------------------------------
Similarly, calculate the total num-
ber of computational steps for a single forward pass through the expanded network
shown in Figure 10.23 Finally, evaluate ratio of nine repeated naive applications of
the network in Figure 10.22 to an 8√ó8image compared to a single application of
the network in Figure 10.23 This ratio indicates the improvement in efÔ¨Åciency from
using a convolutional implementation of the sliding window technique CONVOLUTIONAL NETWORKS
10.13 (??) In this exercise we use one-dimensional vectors to demonstrate why a con-
volutional up-sampling is sometimes called a transpose convolution Consider a
one-dimensional strided convolutional layer with an input having four units with ac-
tivations (x1;x2;x3;x4), which is padded with zeros to give (0;x 1;x2;x3;x4;0),
and a Ô¨Ålter with parameters (w1;w2;w3) Write down the one-dimensional activa-
tion vector of the output layer assuming a stride of 2 Express this output in the
form of a matrix Amultiplied by the vector (0;x 1;x2;x3;x4;0) Now consider
an up-sampling convolution in which the input layer has activations (z1;z2)with
a Ô¨Ålter having values (w1;w2;w3)and an output stride of 2 Write down the six-
dimensional output vector assuming that overlapping Ô¨Ålter values are summed and
that the activation function is just the identity Show that this can be expressed as a
matrix multiplication using the transpose matrix AT 11
Structured
Distributions
We have seen that probability forms one of the most important foundational concepts
for deep learning For example, a neural network used for binary classiÔ¨Åcation is
described by a conditional probability distribution of the form
p(t|x;w) =y(x;w)t{1‚àíy(x;w)}(1‚àít)(11.1)
wherey(x;w)represents a neural network function that takes a vector xas input
and is governed by a vector wof learnable parameters The corresponding cross-
entropy likelihood forms the basis for deÔ¨Åning an error function used to train the
neural network Although the network function might be extremely complex, the
conditional distribution in (11.1) has a simple form However, there are many im-
portant deep learning models that have a much richer probabilistic structure, such as
large language models, normalizing Ô¨Çows, variational autoencoders, diffusion mod-
els, and many others To describe and exploit this structure, we introduce a powerful
325 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C

============================================================

=== CHUNK 320 ===
Palavras: 378
Caracteres: 2598
--------------------------------------------------
Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_11    
326 11 STRUCTURED DISTRIBUTIONS
framework called probabilistic graphical models, or simply graphical models, which
allows structured probability distributions to be expressed in graphical form When
combined with neural networks to deÔ¨Åne associated probability distributions, graph-
ical models offer huge Ô¨Çexibility when creating sophisticated models that can be
trained end to end using stochastic gradient descent in which gradients are evaluated
efÔ¨Åciently using auto-differentiation In this chapter, we will focus on the core con-
cepts of graphical models needed for applications in deep learning, whereas a more
comprehensive treatment of graphical models for machine learning can be found in
Bishop (2006) Graphical Models
Probability theory can be expressed in terms of two simple equations known as the
sum rule and the product rule All of the probabilistic manipulations discussed in Section 2.1
this book, no matter how complex, amount to repeated application of these two
equations In principle, we could therefore formulate and use complex probabilistic
models purely by using algebraic manipulation However, we will Ô¨Ånd it advan-
tageous to augment the analysis using diagrammatic representations of probability
distributions, as these offer several useful properties:
1 They provide a simple way to visualize the structure of a probabilistic model
and can be used to design and motivate new models Insights into the properties of the model, including conditional independence
properties, can be obtained by inspecting the graph The complex computations required to perform inference and learning in so-
phisticated models can be expressed in terms of graphical operations, such as
message-passing, in which the underlying mathematical operations are carried
out implicitly Although such graphical models have nodes and edges much like neural network
diagrams, their interpretation is speciÔ¨Åcally probabilistic and carries a richer seman-
tics To help avoid confusion, in this book we denote neural network diagrams in
blue and probabilistic graphical models in red 11.1.1 Directed graphs
A graph comprises nodes, also called vertices, connected by links, also known
asedges In a probabilistic graphical model, each node represents a random variable,
and the links express probabilistic relationships between these variables The graph
then captures the way in which the joint distribution over all the random variables
can be decomposed into a product of factors each depending only on a subset of the
variables

============================================================

=== CHUNK 321 ===
Palavras: 368
Caracteres: 2345
--------------------------------------------------
In this chapter we will focus on graphical models in which the links of the
graphs have a particular direction indicated by arrows These are known as directed
graphical models and are also called Bayesian networks orBayes nets Graphical Models 327
Figure 11.1 A directed graphical model representing the joint probability distri-
bution over three variables a,b, andc, corresponding to the decom-
position on the right-hand side of (11.3).ab
c
The
other major class of graphical models are Markov random Ô¨Åelds, also known
asundirected graphical models, in which the links do not carry arrows and have no
directional signiÔ¨Åcance Directed graphs are useful for expressing causal relation-
ships between random variables, whereas undirected graphs are better suited to ex-
pressing soft constraints between random variables Both directed and undirected
graphs can be viewed as special cases of a representation called factor graphs From
now on we focus our attention on directed graphical models Note, however, that
undirected graphs, without the probabilistic interpretation, will also arise in our dis-
cussion of graph neural networks in which the nodes represent deterministic vari- Chapter 13
ables as in standard neural networks 11.1.2 Factorization
To motivate the use of directed graphs to describe probability distributions, con-
sider Ô¨Årst an arbitrary joint distribution p(a;b;c) over three variables a,b, andc Note that at this stage, we do not need to specify anything further about these vari-
ables, such as whether they are discrete or continuous Indeed, one of the powerful
aspects of graphical models is that a speciÔ¨Åc graph can make probabilistic statements
for a broad class of distributions By application of the product rule of probability
(2.9), we can write the joint distribution in the form
p(a;b;c) = p(c|a;b)p(a;b): (11.2)
A second application of the product rule, this time to the second term on the right-
hand side of (11.2), gives
p(a;b;c) = p(c|a;b)p(b|a)p(a): (11.3)
Note that this decomposition holds for any choice of the joint distribution We now
represent the right-hand side of (11.3) in terms of a simple graphical model as fol-
lows First we introduce a node for each of the random variables a,b, andcand as-
sociate each node with the corresponding conditional distribution on the right-hand
side of (11.3)

============================================================

=== CHUNK 322 ===
Palavras: 352
Caracteres: 2159
--------------------------------------------------
Then, for each conditional distribution we add directed links (depicted
as arrows) from the nodes corresponding to the variables on which the distribution is
conditioned Thus, for the factor p(c|a;b), there will be links from nodes aandbto
nodec, whereas for the factor p(a), there will be no incoming links The result is the
graph shown in Figure 11.1 If there is a link going from node ato nodeb, then we
say that node ais the parent of nodeb, and we say that node bis the child of nodea Note that we will not make any formal distinction between a node and the variable
to which it corresponds but will simply use the same symbol to refer to both STRUCTURED DISTRIBUTIONS
Figure 11.2 Example of a directed graph describing the joint distri-
bution over variables x1;:::;x 7 The corresponding de-
composition of the joint distribution is given by (11.5).x1x2x3
x4x5
x6x7
An
important point to note about (11.3) is that the left-hand side is symmetrical
with respect to the three variables a,b, andc, whereas the right-hand side is not In
making the decomposition in (11.3), we have implicitly chosen a particular ordering,
namelya;b;c, and had we chosen a different ordering we would have obtained a
different decomposition and hence a different graphical representation For the moment let us extend the example of Figure 11.1 by considering the
joint distribution over Kvariables given by p(x1;:::;xK) By repeated application
of the product rule of probability, this joint distribution can be written as a product
of conditional distributions, one for each of the variables:
p(x1;:::;xK) =p(xK|x1;:::;xK‚àí1):::p(x2|x1)p(x 1): (11.4)
For a given choice of K, we can again represent this as a directed graph having K
nodes, one for each conditional distribution on the right-hand side of (11.4), with
each node having incoming links from all lower numbered nodes We say that this
graph is fully connected because there is a link between every pair of nodes So far, we have worked with completely general joint distributions, and so their
factorization, and associated representation as fully connected graphs, will be appli-
cable to any choice of distribution

============================================================

=== CHUNK 323 ===
Palavras: 375
Caracteres: 2308
--------------------------------------------------
As we will see shortly, it is the absence of links
in the graph that conveys interesting information about the properties of the class
of distributions that the graph represents Consider the graph shown in Figure 11.2 Note that it is not a fully connected graph because, for instance, there is no link from
x1tox2or fromx3tox7 We take this graph and extract the corresponding repre-
sentation of the joint probability distribution written in terms of the product of a set
of conditional distributions, one for each node in the graph Each such conditional
distribution will be conditioned only on the parents of the corresponding node in the
graph For instance, x5will be conditioned on x1andx3 The joint distribution of
all seven variables is therefore given by
p(x1)p(x 2)p(x 3)p(x 4|x1;x2;x3)p(x 5|x1;x3)p(x 6|x4)p(x 7|x4;x5): (11.5)
The reader should take a moment to study carefully the correspondence between
(11.5) and Figure 11.2 We can now state in general terms the relationship between a given directed
graph and the corresponding distribution over the variables The joint distribution
deÔ¨Åned by a graph is given by the product, over all of the nodes of the graph, of
11.1 Graphical Models 329
a conditional distribution for each node conditioned on the variables corresponding
to the parents of that node in the graph Thus, for a graph with Knodes, the joint
distribution is given by
p(x1;:::;xK) =K/productdisplay
k=1p(xk|pa(k)) (11.6)
where pa(k)denotes the set of parents of xk This key equation expresses the factor-
ization properties of the joint distribution for a directed graphical model Although
we have considered each node to correspond to a single variable, we can equally
well associate sets of variables and vector-valued or tensor-valued variables with the
nodes of a graph It is easy to show that the representation on the right-hand side of
(11.6) is always correctly normalized provided the individual conditional distribu-
tions are normalized Exercise 11.1
The directed graphs that we are considering are subject to an important restric-
tion, namely that there must be no directed cycles In other words, there are no closed
paths within the graph such that we can move from node to node along links follow-
ing the direction of the arrows and end up back at the starting node

============================================================

=== CHUNK 324 ===
Palavras: 376
Caracteres: 2485
--------------------------------------------------
Such graphs are
also called directed acyclic graphs , orDAGs This is equivalent to the statement that Exercise 11.2
there exists an ordering of the nodes such that there are no links that go from any
node to any lower-numbered node 11.1.3 Discrete variables
We have discussed the importance of probability distributions that are mem-
bers of the exponential family, and we have seen that this family includes many Section 3.4
well-known distributions as special cases Although such distributions are relatively
simple, they form useful building blocks for constructing more complex probability
distributions, and the framework of graphical models is very useful in expressing
the way in which these building blocks are linked together There are two particu-
lar choices for the component distributions that are widely used, corresponding to
discrete variables and to Gaussian variables We begin by examining the discrete
case The probability distribution p(x|) for a single discrete variable xhavingK
possible states (using the 1-of-K representation) is given by
p(x|) =K/productdisplay
k=1xk
k(11.7)
and is governed by the parameters = ( 1;:::;K)T Due to the constraint/summationtext
kk= 1, onlyK‚àí1values forkneed to be speciÔ¨Åed to deÔ¨Åne the distribution Now suppose that we have two discrete variables, x1andx2, each of which has
Kstates, and we wish to model their joint distribution We denote the probability of
observing both x1k= 1 andx2l= 1 by the parameter kl, wherex1kdenotes the
330 11 STRUCTURED DISTRIBUTIONS
Figure 11.3 (a) This fully connected
graph describes a general
distribution over two K-state
discrete variables having a
total ofK2‚àí1parameters (b) By dropping the link
between the nodes, the
number of parameters is
reduced to 2(K‚àí1).x1 x2
(a)x1 x212
(b)
kth
component of x1,
and similarly for x2l The joint distribution can be written
p(x1;x2|) =K/productdisplay
k=1K/productdisplay
l=1x1kx2l
kl:
Because the parameters klare subject to the constraint/summationtext
k/summationtext
lkl= 1, this distri-
bution is governed by K2‚àí1parameters It is easily seen that the total number of
parameters that must be speciÔ¨Åed for an arbitrary joint distribution over Mvariables
isKM‚àí1and therefore grows exponentially with the number Mof variables Using the product rule, we can factor the joint distribution p(x1;x2)in the form
p(x2|x1)p(x 1), which corresponds to a two-node graph with a link going from the
x1node to the x2node as shown in Figure 11.3(a)

============================================================

=== CHUNK 325 ===
Palavras: 350
Caracteres: 2364
--------------------------------------------------
The marginal distribution p(x1)
is governed by K‚àí1parameters, as before Similarly, the conditional distribution
p(x2|x1)requires the speciÔ¨Åcation of K‚àí1parameters for each of the Kpossible
values of x1 The total number of parameters that must be speciÔ¨Åed in the joint
distribution is therefore (K‚àí1) +K(K‚àí1) =K2‚àí1as before Now suppose that the variables x1andx2are independent, corresponding to
the graphical model shown in Figure 11.3(b) Each variable is then described by a
separate discrete distribution, and the total number of parameters would be 2(K‚àí1) For a distribution over Mindependent discrete variables, each having Kstates, the
total number of parameters would be M(K‚àí1), which therefore grows linearly
with the number of variables From a graphical perspective, we have reduced the
number of parameters by dropping links in the graph, at the expense of having a
more restricted class of distributions More generally, if we have Mdiscrete variables x1;:::;xM, we can model the
joint distribution using a directed graph with one variable for each node The condi-
tional distribution at each node is given by a set of non-negative parameters subject
to the usual normalization constraint If the graph is fully connected, then we have a
completely general distribution having KM‚àí1parameters, whereas if there are no
links in the graph, the joint distribution factorizes into the product of the marginal
distributions, and the total number of parameters is M(K‚àí1) Graphs having in-
termediate levels of connectivity allow for more general distributions than the fully
factorized one while requiring fewer parameters than the general joint distribution As an illustration, consider the chain of nodes shown in Figure 11.4 The marginal
distribution p(x1)requiresK‚àí1parameters, whereas each of the M‚àí1condi-
tional distributions p(xi|xi‚àí1), fori= 2;:::;M , requiresK(K‚àí1)parameters Graphical Models 331
Figure 11.4 This chain of Mdiscrete nodes, each hav-
ingKstates, requires the speciÔ¨Åcation of
K‚àí1 + (M‚àí1)K(K‚àí1)parameters, which
grows linearly with the length Mof the chain In contrast, a fully connected graph of M
nodes would have KM‚àí1parameters, which
grows exponentially with M.x1 x2 xM12M
This
gives a total parameter count of K‚àí1+(M‚àí1)K(K‚àí1), which is quadratic
inKand which grows linearly (rather than exponentially) with the length Mof the
chain

============================================================

=== CHUNK 326 ===
Palavras: 374
Caracteres: 2494
--------------------------------------------------
An alternative way to reduce the number of independent parameters in a model
is by sharing parameters (also known as tying of parameters) For instance, in the
chain example of Figure 11.4, we can arrange that all the conditional distributions
p(xi|xi‚àí1), fori= 2;:::;M , are governed by the same set of K(K‚àí1)param-
eters, giving the model shown in Figure 11.5 Together with the K‚àí1parameters
governing the distribution of x1, this gives a total of K2‚àí1parameters that must be
speciÔ¨Åed to deÔ¨Åne the joint distribution Another way of controlling the exponential growth of the number of parameters
in models of discrete variables is to use parameterized representations for the condi-
tional distributions instead of complete tables of conditional probability values To
illustrate this idea, consider the graph in Figure 11.6 in which all the nodes represent
binary variables Each of the parent variables xiis governed by a single parame-
terirepresenting the probability p(xi= 1), giving Mparameters in total for the
parent nodes The conditional distribution p(y|x1;:::;xM), however, would require
2Mparameters representing the probability p(y= 1) for each of the 2Mpossible
settings of the parent variables Thus, in general the number of parameters required
to specify this conditional distribution will grow exponentially with M We can ob-
tain a more parsimonious form for the conditional distribution by using a logistic
sigmoid function acting on a linear combination of the parent variables, giving Section 3.4
p(y= 1|x 1;:::;xM) =/parenleftBigg
w0+M/summationdisplay
i=1wixi/parenrightBigg
=(wTx) (11.8)
where(a) = (1+exp(‚àía))‚àí1is the logistic sigmoid, x= (x0;x1;:::;xM)Tis an
(M+ 1)-dimensional vector of parent states augmented with an additional variable
x0whose value is clamped to 1, and w= (w 0;w1;:::;wM)Tis a vector of M+ 1
parameters This is a more restricted form of conditional distribution than the general
case but is now governed by a number of parameters that grows linearly with M In
Figure 11.5 As in Figure 11.4 but with a single set of
parametersshared amongst all the condi-
tional distributions p(xi|xi‚àí1) STRUCTURED DISTRIBUTIONS
Figure 11.6 A graph comprising Mparentsx1;:::;xMand a single
childy, used to illustrate the idea of parameterized con-
ditional distributions for discrete variables.x1x2xM
y this sense, it is analogous to the choice of a restrictive form of covariance matrix (for
example, a diagonal matrix) in a multivariate Gaussian distribution

============================================================

=== CHUNK 327 ===
Palavras: 370
Caracteres: 2513
--------------------------------------------------
11.1.4 Gaussian variables
We now turn to graphical models in which the nodes represent continuous vari-
ables having Gaussian distributions Each distribution is conditioned on the state
of its parents in the graph That dependence could take many forms, and here we
focus on a speciÔ¨Åc choice in which the mean of each Gaussian is some linear func-
tion of the states of the Gaussian parent variables This leads to a class of models
called linear-Gaussian models, which include many cases of practical interest such
as probabilistic principal component analysis, factor analysis, and linear dynamical Section 16.2
systems (Roweis and Ghahramani, 1999) Consider an arbitrary directed acyclic graph over Dvariables in which node i
represents a single continuous random variable xihaving a Gaussian distribution The mean of this distribution is taken to be a linear combination of the states of its
parent nodes pa(i)of nodei:
p(xi|pa(i)) =NÔ£´
Ô£≠xi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j‚ààpa(i)wijxj+bi;viÔ£∂
Ô£∏ (11.9)
wherewijandbiare parameters governing the mean and viis the variance of the
conditional distribution for xi The log of the joint distribution is then the log of the
product of these conditionals over all nodes in the graph and hence takes the form
lnp(x) =D/summationdisplay
i=1lnp(xi|pa(i)) (11.10)
=‚àíD/summationdisplay
i=11
2viÔ£´
Ô£≠xi‚àí/summationdisplay
j‚ààpa(i)wijxj‚àíbiÔ£∂
Ô£∏2
+ const (11.11)
where x= (x1;:::;xD)Tand ‚Äòconst‚Äô denotes terms independent of x We see that
this is a quadratic function of the components of x, and hence the joint distribution
p(x) is a multivariate Gaussian We can Ô¨Ånd the mean and covariance of this joint distribution as follows The
mean of each variable is given by the recursion relation Exercise 11.6
E[xi] =/summationdisplay
j‚ààpa(i)wijE[xj] +bi: (11.12)
11.1 Graphical Models 333
Figure 11.7 A directed graph over three Gaussian variables with one
missing link.x1x2x3
and
so we can Ô¨Ånd the components of E[x] = ( E[x1];:::;E[xD])Tby starting at
the lowest numbered node and working recursively through the graph, where we
assume that the nodes are numbered such that each node has a higher number than
its parents Similarly, the elements of the covariance matrix of the joint distribution
satisfy a recursion relation of the form Exercise 11.7
cov[xi;xj] =/summationdisplay
k‚ààpa(j )wjkcov[xi;xk] +Iijvj (11.13)
and so the covariance can similarly be evaluated recursively starting from the lowest
numbered node

============================================================

=== CHUNK 328 ===
Palavras: 363
Caracteres: 2569
--------------------------------------------------
We now consider two extreme cases of possible graph structures First, suppose
that there are no links in the graph, which therefore comprises Disolated nodes In this case, there are no parameters wijand so there are just Dparametersbiand
Dparametersvi From the recursion relations (11.12) and (11.13), we see that the
mean ofp(x) is given by (b1;:::;bD)Tand the covariance matrix is diagonal of
the form diag(v 1;:::;vD) The joint distribution has a total of 2Dparameters and
represents a set of Dindependent univariate Gaussian distributions Now consider a fully connected graph in which each node has all lower num-
bered nodes as parents In this case the total number of independent parameters
{wij}and{vi}in the covariance matrix is D(D+ 1)=2corresponding to a general Exercise 11.8
symmetric covariance Graphs having some intermediate level of complexity correspond to joint Gaus-
sian distributions with partially constrained covariance matrices Consider for exam-
ple the graph shown in Figure 11.7, which has a link missing between variables x1
andx3 Using the recursion relations (11.12) and (11.13), we see that the mean and
covariance of the joint distribution are given by Exercise 11.9
= (b1;b2+w21b1;b3+w32b2+w32w21b1)T(11.14)
=Ô£´
Ô£≠v1 w21v1 w32w21v1
w21v1v2+w2
21v1w32(v2+w2
21v1)
w32w21v1w32(v2+w2
21v1)v3+w2
32(v2+w2
21v1)Ô£∂
Ô£∏: (11.15)
We can readily extend the linear-Gaussian graphical model to a situation in
which the nodes of the graph represent multivariate Gaussian variables In this case,
we can write the conditional distribution for node iin the form
p(xi|pa(i)) =NÔ£´
Ô£≠xi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j‚ààpa(i)Wijxj+bi;iÔ£∂
Ô£∏ (11.16)
where now Wijis a matrix (which is non-square if xiandxjhave different dimen-
sionality) Again it is easy to verify that the joint distribution over all variables is
Gaussian Exercise 11.10
334 11 STRUCTURED DISTRIBUTIONS
Figure 11.8 Directed graphical model representing the binary classiÔ¨Åer
model described by the joint distribution (11.17) showing only
the stochastic variables {t1;:::;tN}andw.w
t1 tN 11.1.5
Binary classiÔ¨Åer
We can illustrate the use of directed graphs to describe probability distributions
using a two-class classiÔ¨Åer model with Gaussian prior over the learnable parameters Section 2.6.2
We can write this in the form
p(t;w|X;) =p(w|)N/productdisplay
n=1p(tn|w;xn) (11.17)
where t= (t1;:::;tN)Tis the vector of target values, Xis the data matrix with
rowsxT
1;:::xT
N, and the distribution p(t|x; w)is given by (11.1)

============================================================

=== CHUNK 329 ===
Palavras: 364
Caracteres: 2206
--------------------------------------------------
We also assume a
Gaussian prior over the parameter vector wgiven by
p(w|) =N(w|0;I): (11.18)
The stochastic variables in this model are {t1;:::;tN}andw In addition, this
model contains the noise variance 2and the hyperparameter , both of which are
parameters of the model rather than stochastic variables If we consider for a mo-
ment only the stochastic variables, then the distribution given by (11.17) can be
represented by the graphical model shown in Figure 11.8 When we start to deal with more complex models, it becomes inconvenient to
have to write out multiple nodes of the form t1;:::;tNexplicitly as in Figure 11.8 We therefore introduce a graphical notation that allows such multiple nodes to be ex-
pressed more compactly We draw a single representative node tnand then surround
this with a box, called a plate, labelled with Nto indicate that there are Nnodes of
this kind Rewriting the graph of Figure 11.8 in this way, we obtain the graph shown
inFigure 11.9 11.1.6 Parameters and observations
We will sometimes Ô¨Ånd it helpful to make the parameters of a model, as well
as its stochastic variables, explicit in the graphical representation To do this, we
will adopt the convention that random variables are denoted by open circles and
deterministic parameters are denoted by Ô¨Çoating variables If we take the graph of
Figure 11.9 An alternative, more compact, representation of the graph shown
inFigure 11.8 in which we have introduced a plate (the box la-
belledN) that represents Nnodes of which only a single exam-
pletnis shown explicitly.wtn
N
11.1 Graphical Models 335
Figure 11.10 The same model as in Figure 11.9 but with the determin-
istic parameters shown explicitly by the Ô¨Çoating variables w
tnxn
2
N
Figure
11.9 and include the deterministic parameters, we obtain the graph shown in
Figure 11.10 When we apply a graphical model to a problem in machine learning, we will
typically set some of the random variables to speciÔ¨Åc observed values For example,
the stochastic variables {tn}in the linear regression model will be set equal to the
speciÔ¨Åc values given in the training set In a graphical model, we denote such ob-
served variables by shading the corresponding nodes

============================================================

=== CHUNK 330 ===
Palavras: 361
Caracteres: 2402
--------------------------------------------------
Thus, the graph corresponding
toFigure 11.10 in which the variables {tn}are observed is shown in Figure 11.11 Note that the value of wis not observed, and so wis an example of a latent
variable, also known as a hidden variable Such variables play a crucial role in many
of the models discussed in this book We therefore have three kinds of variables in a
directed graphical model First, there are unobserved (also called latent, or hidden)
stochastic variables, which are denoted by open red circles Second, when stochastic
variables are observed, so that that they are set to speciÔ¨Åc values, they are denoted
by red circles shaded with blue Finally, non-stochastic parameters are denoted by
Ô¨Çoating variables, as seen in Figure 11.11 Note that model parameters such as ware generally of little direct interest in
themselves, because our ultimate goal is to make predictions for new input values Suppose we are given a new input value /hatwidexand we wish to Ô¨Ånd the corresponding
probability distribution for /hatwidetconditioned on the observed data The joint distribution
of all the random variables in this model, conditioned on the deterministic parame-
ters, is given by
p(/hatwidet;t;w|/hatwidex;X;) =p(w|)p(/hatwidet|w;/hatwidex)N/productdisplay
n=1p(tn|w;xn) (11.19)
and the corresponding graphical model is shown in Figure 11.12 Figure 11.11 As in Figure 11.10 but with the nodes {tn}shaded to indi-
cate that the corresponding random variables have been
set to their observed values given by the training set STRUCTURED DISTRIBUTIONS
Figure 11.12 The classiÔ¨Åcation model, corresponding to Fig-
ure 11.11, showing a new input value bxtogether with
the corresponding model prediction bt w
tnxn
^t^x
N
The
required predictive distribution for /hatwidetis then obtained from the sum rule
of probability by integrating out the model parameters w This integration over
parameters represents a fully Bayesian treatment, which is rarely used in practice,
especially with deep neural networks Instead, we approximate this integral by Ô¨Årst
Ô¨Ånding the most probable value wMAP that maximizes the posterior distribution and
then using just this single value to make predictions using p(/hatwidet|wMAP;/hatwidex) 11.1.7 Bayes‚Äô theorem
When stochastic variables in a probabilistic model are set equal to observed val-
ues, the distributions over other unobserved stochastic variables change accordingly

============================================================

=== CHUNK 331 ===
Palavras: 357
Caracteres: 2302
--------------------------------------------------
The process of calculating these updated distributions is known as inference We can
illustrate this by considering the graphical interpretation of Bayes‚Äô theorem Suppose
we decompose the joint distribution p(x;y )over two variables xandyinto a product
of factors in the form p(x;y ) =p(x)p(y|x) This can be represented by the directed
graph shown in Figure 11.13(a) Now suppose we observe the value of y, as indi-
cated by the shaded node in Figure 11.13(b) We can view the marginal distribution
p(x)as a prior over the latent variable x, and our goal is to infer the corresponding
posterior Using the sum and product rules of probability we can evaluate
p(y) =/summationdisplay
x/primep(y|x/prime)p(x/prime); (11.20)
which can then be used in Bayes‚Äô theorem to calculate
p(x|y ) =p(y|x)p(x)
p
(y): (11.21)
Thus, the joint distribution is now expressed in terms of p(x|y )andp(y) From
a graphical perspective, the joint distribution p(x;y )is represented by the graph
shown in Figure 11.13(c), in which the direction of the arrow is reversed This is the
simplest example of an inference problem for a graphical model For complex graphical models that capture rich probabilistic structure, the pro-
cess of calculating posterior distributions once some of the stochastic variables are
observed can be complex and subtle Conceptually, it simply involves the systematic
application of the sum and product rules of probability, or equivalently Bayes‚Äô theo-
rem In practice, however, managing these calculations efÔ¨Åciently can beneÔ¨Åt greatly
from an exploitation of the graphical structure These calculations can be expressed
11.2 Conditional Independence 337
Figure 11.13 A graphical representation
of Bayes‚Äô theorem showing
(a) a joint distribution over
two variables xandyex-
pressed in factorized form,
(b) the case with yset to an
observed value, and (c) the
resulting posterior distribu-
tion overx, given by Bayes‚Äô
theorem.x
y
(a)x
y
(b)x
y
(c)
in terms of elegant calculations on the graph that involve sending local messages be-
tween nodes Such methods give exact answers for tree-structured graphs and give
approximate iterative algorithms for graphs with loops Since we will not discuss
these further here, see Bishop (2006) for a more comprehensive discussion in the
context of machine learning

============================================================

=== CHUNK 332 ===
Palavras: 351
Caracteres: 2320
--------------------------------------------------
Conditional Independence
An important concept for probability distributions over multiple variables is that of
conditional independence (Dawid, 1980) Consider three variables a,b, andc, and
suppose that the conditional distribution of agivenbandcis such that it does not
depend on the value of b, so that
p(a|b;c ) =p(a|c): (11.22)
We say that ais conditionally independent of bgivenc This can be expressed in a
slightly different way if we consider the joint distribution of aandbconditioned on
c, which we can write in the form
p(a;b|c) = p(a|b;c )p(b|c)
=p(a|c)p(b|c) (11.23)
where we have used the product rule of probability together with (11.22) We see
that, conditioned on c, the joint distribution of aandbfactorizes into the product of
the marginal distribution of aand the marginal distribution of b(again both condi-
tioned onc) This says that the variables aandbare statistically independent, given
c Note that our deÔ¨Ånition of conditional independence will require that (11.22), or
equivalently (11.23), must hold for every possible value of c, and not just for some
values We will sometimes use a shorthand notation for conditional independence
(Dawid, 1979) in which
a‚ä• ‚ä•b|c (11.24)
denotes that ais conditionally independent of bgivenc Conditional independence
properties play an important role in probabilistic models for machine learning be-
cause they simplify both the structure of a model and the computations needed to
perform inference and learning under that model STRUCTURED DISTRIBUTIONS
Figure 11.14 The Ô¨Årst of three examples of graphs over three variables a,b,
andcused to discuss conditional independence properties of
directed graphical models ac
b
If
we are given an expression for the joint distribution over a set of variables
in terms of a product of conditional distributions (i.e., the mathematical representa-
tion underlying a directed graph), then we could in principle test whether any po-
tential conditional independence property holds by repeated application of the sum
and product rules of probability In practice, such an approach would be very time-
consuming An important and elegant feature of graphical models is that conditional
independence properties of the joint distribution can be read directly from the graph
without having to perform any analytical manipulations

============================================================

=== CHUNK 333 ===
Palavras: 367
Caracteres: 2326
--------------------------------------------------
The general framework
for achieving this is called d-separation, where the ‚Äòd‚Äô stands for ‚Äòdirected‚Äô (Pearl,
1988) Here we will motivate the concept of d-separation and give a general state-
ment of the d-separation criterion A formal proof can be found in Lauritzen (1996) 11.2.1 Three example graphs
We begin our discussion of the conditional independence properties of directed
graphs by considering three simple examples each involving graphs having just three
nodes Together, these will motivate and illustrate the key concepts of d-separation The Ô¨Årst of the three examples is shown in Figure 11.14, and the joint distribution
corresponding to this graph is easily written down using the general result (11.6) to
give
p(a;b;c) = p(a|c)p(b|c)p(c): (11.25)
If none of the variables are observed, then we can investigate whether aandbare
independent by marginalizing both sides of (11.25) with respect to cto give
p(a;b) =/summationdisplay
cp(a|c)p(b|c)p(c): (11.26)
In general, this does not factorize into the product p(a)p(b), and so
a/negationslash‚ä• ‚ä•b|‚àÖ (11.27)
where‚àÖdenotes the empty set, and the symbol /negationslash‚ä• ‚ä•means that the conditional inde-
pendence property does not hold in general Of course, it may hold for a particular
distribution by virtue of the speciÔ¨Åc numerical values associated with the various
conditional probabilities, but it does not follow in general from the structure of the
graph Now suppose we condition on the variable c, as represented by the graph of
Figure 11.15 From (11.25), we can easily write down the conditional distribution of
11.2 Conditional Independence 339
Figure 11.15 As in Figure 11.14 but where we have conditioned on the value
of variablec ac
b
aandb
, givenc, in the form
p(a;b|c) =p(a;b;c)
p
(c)
=p(a|c)p(b|c)
and so we obtain the conditional independence property
a‚ä• ‚ä•b|c:
We can provide a simple graphical interpretation of this result by considering
the path from node ato nodebviac The node cis said to be tail-to-tail with respect
to this path because the node is connected to the tails of the two arrows, and the
presence of such a path connecting nodes aandbcauses these nodes to be depen-
dent However, when we condition on node c, as in Figure 11.15, the conditioned
node ‚Äòblocks‚Äô the path from atoband causesaandbto become (conditionally)
independent

============================================================

=== CHUNK 334 ===
Palavras: 353
Caracteres: 2226
--------------------------------------------------
We can similarly consider the graph shown in Figure 11.16 The joint distribu-
tion corresponding to this graph is again obtained from our general formula (11.6)
to give
p(a;b;c) = p(a)p(c|a)p(b|c): (11.28)
First, suppose that none of the variables are observed Again, we can test to see if a
andbare independent by marginalizing over cto give
p(a;b) =p(a)/summationdisplay
cp(c|a)p(b|c) = p(a)p(b|a)
which in general does not factorize into p(a)p(b), and so
a/negationslash‚ä• ‚ä•b|‚àÖ (11.29)
as before Now suppose we condition on node c, as shown in Figure 11.17 Using Bayes‚Äô
Figure 11.16 The second of our three examples of three-node graphs
used to motivate the conditional independence frame-
work for directed graphical models.acb
340 11 STRUCTURED DISTRIBUTIONS
Figure 11.17 As in Figure 11.16 but now conditioning on node c.acb
theorem
together with (11.28), we obtain
p(a;b|c) =p(a;b;c)
p
(c)
=p(a)p(c|a)p(b|c)
p
(c)
=p(a|c)p(b|c)
and so again we obtain the conditional independence property
a‚ä• ‚ä•b|c:
As before, we can interpret these results graphically The node cis said to be
head-to-tail with respect to the path from node ato nodeb Such a path connects
nodesaandband renders them dependent If we now observe c, as in Figure 11.17,
then this observation ‚Äòblocks‚Äô the path from atoband so we obtain the conditional
independence property a‚ä• ‚ä•b|c Finally, we consider the third of our three-node examples, shown by the graph in
Figure 11.18 As we will see, this has a more subtle behaviour than the two previous
graphs The joint distribution can again be written down using our general result
(11.6) to give
p(a;b;c) = p(a)p(b)p(c|a;b): (11.30)
Consider Ô¨Årst the case where none of the variables are observed Marginalizing both
sides of (11.30) over cwe obtain
p(a;b) =p(a)p(b)
and soaandbare independent with no variables observed, in contrast to the two
previous examples We can write this result as
a‚ä• ‚ä•b|‚àÖ: (11.31)
Now suppose we condition on c, as indicated in Figure 11.19 The conditional dis-
tribution ofaandbis then given by
p(a;b|c) =p(a;b;c)
p
(c)
=p(a)p(b)p(c|a;b)
p
(c);
Figure 11.18 The last of our three examples of three-node graphs used to ex-
plore conditional independence properties in graphical models

============================================================

=== CHUNK 335 ===
Palavras: 370
Caracteres: 2109
--------------------------------------------------
This graph has rather different properties from the two previous
examples.a
cb
11.2 Conditional Independence 341
Figure 11.19 As in Figure 11.18 but conditioning on the value of node c In
this graph, the act of conditioning induces a dependence be-
tweenaandb.a
cb
which
in general does not factorize into the product p(a|c)p(b|c), and so
a/negationslash‚ä• ‚ä•b|c:
Thus, our third example has the opposite behaviour from the Ô¨Årst two Graphically,
we say that node cishead-to-head with respect to the path from atobbecause it
connects to the heads of the two arrows The node cis sometimes called a collider
node When node cis unobserved, it ‚Äòblocks‚Äô the path, and the variables aandbare
independent However, conditioning on c‚Äòunblocks‚Äô the path and renders aandb
dependent There is one more subtlety associated with this third example that we need to
consider First we introduce some more terminology We say that node yis ade-
scendant of nodexif there is a path from xtoyin which each step of the path
follows the directions of the arrows Then it can be shown that a head-to-head path
will become unblocked if either the node, or any of its descendants, is observed Exercise 11.13
In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked
unless it is observed, in which case it blocks the path By contrast, a head-to-head
node blocks a path if it is unobserved, but once the node and/or at least one of its
descendants is observed the path becomes unblocked 11.2.2 Explaining away
It is worth spending a moment to understand further the unusual behaviour of the
graph in Figure 11.19 Consider a particular instance of such a graph corresponding
to a problem with three binary random variables relating to the fuel system on a car,
as shown in Figure 11.20 The variables are B, which represents the state of a battery
that is either charged (B = 1) or Ô¨Çat (B = 0),Fwhich represents the state of the
fuel tank that is either full of fuel (F = 1) or empty (F = 0), andG, which is the
state of an electric fuel gauge and which indicates that the fuel tank is either full
(G= 1) or empty (G = 0)

============================================================

=== CHUNK 336 ===
Palavras: 351
Caracteres: 1897
--------------------------------------------------
The battery is either charged or Ô¨Çat, and independently,
B
GFB
GFB
GF
Figure
11.20 An example of a three-node graph used to illustrate ‚Äòexplaining away‚Äô The three nodes
represent the state of the battery (B ), the state of the fuel tank (F ), and the reading on
the electric fuel gauge (G) See the text for details STRUCTURED DISTRIBUTIONS
the fuel tank is either full or empty, with prior probabilities
p(B= 1) = 0:9
p(F= 1) = 0:9:
Given the state of the fuel tank and the battery, the fuel gauge reads full with proba-
bilities given by
p(G= 1|B = 1;F = 1) = 0:8
p(G= 1|B = 1;F = 0) = 0:2
p(G= 1|B = 0;F = 1) = 0:2
p(G= 1|B = 0;F = 0) = 0:1
so this is a rather unreliable fuel gauge All remaining probabilities are determined
by the requirement that probabilities sum to one, and so we have a complete speciÔ¨Å-
cation of the probabilistic model Before we observe any data, the prior probability of the fuel tank being empty
isp(F= 0) = 0:1 Now suppose that we observe the fuel gauge and discover that
it reads empty, i.e., G= 0, corresponding to the middle graph in Figure 11.20 We
can use Bayes‚Äô theorem to evaluate the posterior probability of the fuel tank being
empty First we evaluate the denominator for Bayes‚Äô theorem:
p(G= 0) =/summationdisplay
B‚àà{0;1}/summationdisplay
F‚àà{0;1}p(G= 0|B;F )p(B )p(F) = 0:315 (11.32)
and similarly we evaluate
p(G= 0|F = 0) =/summationdisplay
B‚àà{0;1}p(G= 0|B;F = 0)p(B) = 0:81 (11.33)
and using these results, we have
p(F= 0|G = 0) =p(G= 0|F = 0)p(F= 0)
p
(G= 0)/similarequal0:257 (11.34)
and sop(F= 0|G = 0)> p(F = 0) Thus, observing that the gauge reads empty
makes it more likely that the tank is indeed empty, as we would intuitively expect Next suppose that we also check the state of the battery and Ô¨Ånd that it is Ô¨Çat, i.e.,
B= 0 We have now observed the states of both the fuel gauge and the battery, as
shown by the right-hand graph in Figure 11.20

============================================================

=== CHUNK 337 ===
Palavras: 384
Caracteres: 2163
--------------------------------------------------
The posterior probability that the
fuel tank is empty given the observations of both the fuel gauge and the battery state
is then given by
p(F= 0|G = 0;B = 0) =p(G= 0|B = 0;F = 0)p(F= 0)/summationtext
F‚àà{
0;1}p(G= 0|B = 0;F )p(F)/similarequal0:111 (11.35)
where the prior probability p(B = 0) has cancelled between the numerator and
denominator Thus, the probability that the tank is empty has decreased (from 0:257
11.2 Conditional Independence 343
Figure 11.21 Illustration of d-separation See the text for details.a
ef
b
ca
ef
b
c
(a) (b)
to0
:111) as a result of the observation of the state of the battery This accords with
our intuition that Ô¨Ånding that the battery is Ô¨Çat explains away the observation that the
fuel gauge reads empty We see that the state of the fuel tank and that of the battery
have indeed become dependent on each other as a result of observing the reading on
the fuel gauge In fact, this would also be the case if, instead of observing the fuel
gauge directly, we observed the state of some descendant of G, for example a rather
unreliable witness who reports seeing that the gauge was reading empty Note that Exercise 11.14
the probability p(F= 0|G = 0;B = 0)/similarequal0:111 is greater than the prior probability
p(F= 0) = 0:1 because the observation that the fuel gauge reads zero still provides
some evidence in favour of an empty fuel tank 11.2.3 D-separation
We now give a general statement of the d-separation property (Pearl, 1988) for
directed graphs Consider a general directed graph in which A,B, andCare arbi-
trary non-intersecting sets of nodes (whose union may be smaller than the complete
set of nodes in the graph) We wish to ascertain whether a particular conditional
independence statement A‚ä• ‚ä•B|Cis implied by a given directed acyclic graph To
do so, we consider all possible paths from any node in Ato any node in B Any such
path is said to be blocked if it includes a node such that either
(a)the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the
node is in the set C, or
(b)the arrows meet head-to-head at the node and neither the node, nor any of its
descendants is in the set C

============================================================

=== CHUNK 338 ===
Palavras: 383
Caracteres: 2456
--------------------------------------------------
If all paths are blocked, then Ais said to be d-separated from BbyC, and the joint
distribution over all the variables in the graph will satisfy A‚ä• ‚ä•B|C D-separation is illustrated in Figure 11.21 In graph (a), the path from atobis
not blocked by node fbecause it is a tail-to-tail node for this path and is not observed,
nor is it blocked by node ebecause, although the latter is a head-to-head node, it has
a descendant cin the conditioning set Thus, the conditional independence statement
a‚ä• ‚ä•b|cdoes notfollow from this graph In graph (b), the path from atobis blocked
by nodefbecause this is a tail-to-tail node that is observed, and so the conditional
independence property a‚ä• ‚ä•b|fwill be satisÔ¨Åed by any distribution that factorizes
344 11 STRUCTURED DISTRIBUTIONS
Figure 11.22 A graphical representation of the naive Bayes model for clas-
siÔ¨Åcation Conditioned on the class label Ck, the elements of
the observed vector x= (x(1);:::;x(L))are assumed to be
independent according
to this graph Note that this path is also blocked by node ebecauseeis
a head-to-head node and neither it nor its descendant are in the conditioning set In
d-separation, parameters such as inFigure 11.12, which are indicated by Ô¨Çoating
variables, behave in the same way as observed nodes However, there are no marginal
distributions associated with such nodes, and consequently parameter nodes never
themselves have parents and so all paths through these nodes will always be tail-to-
tail and hence blocked Consequently they play no role in d-separation Another example of conditional independence and d-separation is provided by
i.i.d (independent and identically distributed) data Consider the binary classiÔ¨Åca- Section 2.3.2
tion model shown in Figure 11.12 Here the stochastic nodes correspond to {tn},
w, and/hatwidet We see that the node for wis tail-to-tail with respect to the path from /hatwidet
to any one of the nodes tn, and so we have the following conditional independence
property:
/hatwidet‚ä• ‚ä•tn|w: (11.36)
Thus, conditioned on the network parameters w, the predictive distribution for /hatwidetis
independent of the training data {t1;:::;tN} We can therefore Ô¨Årst use the training
data to determine the posterior distribution (or some approximation to the posterior
distribution) over the coefÔ¨Åcients wand then we can discard the training data and use
the posterior distribution for wto make predictions of /hatwidetfor new input observations
/hatwidex

============================================================

=== CHUNK 339 ===
Palavras: 399
Caracteres: 2590
--------------------------------------------------
11.2.4 Naive Bayes
A related graphical structure arises in an approach to classiÔ¨Åcation called the
naive Bayes model, in which we use conditional independence assumptions to sim-
plify the model structure Suppose our data consists of observations of a vector x,
and we wish to assign values of xto one ofKclasses We can deÔ¨Åne a class-
conditional density p(x|Ck)for each of the classes, along with prior class probabili-
tiesp(Ck) The key assumption of the naive Bayes model is that, conditioned on the
classCk, the distribution of the input variable factorizes into the product of two or
more densities Suppose we partition xintoLelements x= (x(1);:::;x(L)) Naive
Bayes then takes the form
p(x|Ck) =L/productdisplay
l=1p(x(l)|Ck) (11.37)
where it is assumed that (11.37) holds for each of the classes Ckseparately The
graphical representation of this model is shown in Figure 11.22 We see that an
observation ofCkwould block the path between x(i)andx(j)forj/negationslash=ibecause such
11.2 Conditional Independence 345
x1x2
p(x|C1)p(x|C2)
(a)
x1x2
p(x) (b)
Figure
11.23 Illustration of a naive Bayes classiÔ¨Åer for a two-dimensional data space, showing (a) the condi-
tional distributions p(x|Ck)for each of the two classes and (b) the marginal distribution p(x) in which we have
assumed equal class priors p(C1) =p(C2) = 0:5 Note that the conditional distributions factorize with respect to
x1andx2, whereas the marginal distribution does not paths are tail-to-tail at the node Ck, and so x(i)andx(j)are conditionally independent
givenCk If, however, we marginalize out Ck, the tail-to-tail path from x(i)tox(j)is
no longer blocked, which tells us that in general the marginal density p(x) will not
factorize with respect to the elements x(1);:::;x(L) If we are given a labelled training set, comprising observations {x1;:::;xN}
together with their class labels, then we can Ô¨Åt the naive Bayes model to the training
data using maximum likelihood by assuming that the data are drawn independently
from the model The solution is obtained by Ô¨Åtting the model for each class sepa- Exercise 11.15
rately using the corresponding labelled data and then setting the class priors p(Ck)
equal to the fraction of training data points in each class The probability that a
vector xbelongs to classCkis then given by Bayes‚Äô theorem in the form
p(Ck|x) =p(x|Ck)p(Ck)
p
(x)(11.38)
wherep(x|Ck)is given by (11.37), and p(x) can be evaluated using
p(x) =K/summationdisplay
k=1p(x|Ck)p(Ck): (11.39)
The naive Bayes model is illustrated for a two-dimensional data space in Fig-
ure 11.23 in which x= (x 1;x2)

============================================================

=== CHUNK 340 ===
Palavras: 358
Caracteres: 2284
--------------------------------------------------
Here we assume that the conditional densities
346 11 STRUCTURED DISTRIBUTIONS
p(x|Ck)for each of the two classes are axis-aligned Gaussians, and hence that they
each factorize with respect to x1andx2so that
p(x|Ck) =p(x1|Ck)p(x 2|Ck): (11.40)
However, the marginal density p(x) given by
p(x) =K/summationdisplay
k=1p(x|Ck)p(Ck) (11.41)
is now a mixture of Gaussians and does not factorize with respect to x1andx2 We have already encountered a simple application of the naive Bayes model in the
context of fusing data from different sources, such as blood tests and skin images for
medical diagnosis Section 5.2.4
The naive Bayes assumption is helpful when the dimensionality Dof the input
space is high, making density estimation in the full D-dimensional space more chal-
lenging It is also useful if the input vector contains both discrete and continuous
variables, since each can be represented separately using appropriate models (e.g.,
Bernoulli distributions for binary observations or Gaussians for real-valued vari-
ables) The conditional independence assumption of this model is clearly a strong
one that may lead to rather poor representations of the class-conditional densities Nevertheless, even if this assumption is not precisely satisÔ¨Åed, the model may still
give good classiÔ¨Åcation performance in practice because the decision boundaries can
be insensitive to some of the details in the class-conditional densities, as illustrated
inFigure 5.8 11.2.5 Generative models
Many applications of machine learning can be viewed as examples of inverse
problems in which there is an underlying, often physical, process that generates data,
and the goal is to learn now to invert this process For example, an image of an
object can be viewed as the output of a generative process in which the type of
object is selected from some distribution of possible object classes The position and
orientation of the object are also chosen from some prior distributions, and then the
resulting image is created Given a large data set of images labelled with the type,
position, and scale of the objects they contain, the goal is to train a machine learning
model that can take new, unlabelled images and detect the presence of an object
including its location within the image and its size

============================================================

=== CHUNK 341 ===
Palavras: 354
Caracteres: 2222
--------------------------------------------------
The machine learning solution
therefore represents the inverse of the process that generated the data One approach would be to train a deep neural network, such as a convolutional
network, to take an image as input and to generate outputs that describe the object‚Äôs
type, position, and scale This approach therefore tries to solve the inverse problem
directly and is an example of a discriminative model It can achieve high accuracy
provided ample examples of labelled images are available In practice, unlabelled
images are often plentiful, and much of the effort in obtaining a training set goes
into proving the labels, which may be done by hand Our simple discriminative
model cannot directly make use of unlabelled images during training Conditional Independence 347
Figure 11.24 A graphical model representing the process by which
images of objects are created The identity of an object
(a discrete variable) and the position and orientation
of that object (continuous variables) have independent
prior probabilities The image (an array of pixel intensi-
ties) has a probability distribution that is dependent on
the identity of the object as well as on its position and
orientation.class position scale
image
An
alternative approach is to model the generative process and then subse-
quently to invert it computationally In our image example, if we assume that the
object‚Äôs class, position, and scale are all chosen independently, then we can represent
the generative process using a directed graphical model as shown in Figure 11.24 Note that the directions of the arrows correspond to the sequence of generative steps,
and so the model represents the causal process (Pearl, 1988) by which the observed
data is generated This is an example of a generative model because once it is trained,
it can be used to generate synthetic images by Ô¨Årst selecting values for object‚Äôs class,
position, and scale from the learned prior distributions and then subsequently sam-
pling an image from the learned conditional distribution We will later see how diffu-
sion models and other generative models can synthesize impressive high-resolution
images based on a textual description of the desired content and style of the image

============================================================

=== CHUNK 342 ===
Palavras: 375
Caracteres: 2560
--------------------------------------------------
Chapter 20
The graph in Figure 11.24 assumes that, when no image is observed, the class,
position, and scale variables are independent This follows because every path be-
tween any two of these variables is head-to-head with respect to the image variable,
which is unobserved However, when we observe an image, those paths become
unblocked, and the class, position, and scale variables are no longer independent Intuitively this is reasonable because being told the identity of the object within the
image provides us with very relevant information to assist us with determining its
location The hidden variables in a probabilistic model need not, however, have any ex-
plicit physical interpretation but may be introduced simply to allow a more complex
joint distribution to be constructed from simpler components For example, models
such as normalizing Ô¨Çows, variational autoencoders, and diffusion models all use
deep neural networks to create complex distributions in the data space by transform-
ing hidden variables having a simple Gaussian distribution 11.2.6 Markov blanket
A conditional independence property that is helpful when discussing more com-
plex directed graphs is called the Markov blanket orMarkov boundary Consider
a joint distribution p(x1;:::;xD)represented by a directed graph having Dnodes,
and consider the conditional distribution of a particular node with variables xicon-
ditioned on all the remaining variables xj/negationslash=i Using the factorization property (11.6),
348 11 STRUCTURED DISTRIBUTIONS
Figure 11.25 The Markov blanket of a node xicomprises the
set of parents, children, and co-parents of the
node It has the property that the conditional
distribution of xi, conditioned on all the remain-
ing variables in the graph, is dependent only on
the variables in the Markov blanket.xi
we
can express this conditional distribution in the form
p(xi|x{j/negationslash=i}) =p(x1;:::;xD)/integraldisplay
p
(x1;:::;xD) dxi
=/productdisplay
kp(xk|pa(k))
/integraldisplay/productdisplay
kp
(xk|pa(k)) dxi
in which the integral is replaced by a summation for discrete variables We now
observe that any factor p(xk|pa(k))that does not have any functional dependence
onxican be taken outside the integral over xiand will therefore cancel between
numerator and denominator The only factors that remain will be the conditional
distributionp(xi|pa(i))for node xiitself, together with the conditional distributions
for any nodes xksuch that node xiis in the conditioning set of p(xk|pa(k)), in other
words for which xiis a parent of xk

============================================================

=== CHUNK 343 ===
Palavras: 357
Caracteres: 2168
--------------------------------------------------
The conditional p(xi|pa(i))will depend on
the parents of node xi, whereas the conditionals p(xk|pa(k))will depend on the
children of xias well as on the co-parents, in other words variables corresponding
to parents of node xkother than node xi The set of nodes comprising the parents,
the children, and the co-parents is called the Markov blanket and is illustrated in
Figure 11.25 We can think of the Markov blanket of a node xias being the minimal set of
nodes that isolates xifrom the rest of the graph Note that it is not sufÔ¨Åcient to
include only the parents and children of node xibecause explaining away means
that observations of the child nodes will not block paths to the co-parents We must
therefore observe the co-parent nodes as well 11.2.7 Graphs as Ô¨Ålters
We have seen that a particular directed graph represents a speciÔ¨Åc decomposi-
tion of a joint probability distribution into a product of conditional probabilities, and
it also expresses a set of conditional independence statements obtained through the
d-separation criterion The d-separation theorem is really an expression of the equiv-
alence of these two properties To make this clear, it is helpful to think of a directed
graph as a Ô¨Ålter Suppose we consider a particular joint probability distribution p(x)
over the variables xcorresponding to the (unobserved) nodes of the graph The Ô¨Ål-
ter will allow this distribution to pass through if, and only if, it can be expressed in
11.3 Sequence Models 349
p
(x) D
F
Figure
11.26 We can view a graphical model (in this case a directed graph) as a Ô¨Ålter in which a prob-
ability distribution p(x) is allowed through the Ô¨Ålter if, and only if, it satisÔ¨Åes the directed
factorization property (11.6) The set of all possible probability distributions p(x) that pass
through the Ô¨Ålter is denoted DF We can alternatively use the graph to Ô¨Ålter distributions
according to whether they respect all the conditional independence properties implied by
the d-separation properties of the graph The d-separation theorem says the same set of
distributionsDFwill be allowed through this second kind of Ô¨Ålter terms of the factorization (11.6) implied by the graph

============================================================

=== CHUNK 344 ===
Palavras: 365
Caracteres: 2292
--------------------------------------------------
If we present to the Ô¨Ålter the
set of all possible distributions p(x) over the set of variables x, then the subset of
distributions that are passed by the Ô¨Ålter is denoted DF, for directed factorization This is illustrated in Figure 11.26 Alternatively, we can use the graph as a different kind of Ô¨Ålter by Ô¨Årst listing
all the conditional independence properties obtained by applying the d-separation
criterion to the graph and then allowing a distribution to pass only if it satisÔ¨Åes all of
these properties If we present all possible distributions p(x) to this second kind of
Ô¨Ålter, then the d-separation theorem tells us that the set of distributions that will be
allowed through is precisely the set DF It should be emphasized that the conditional independence properties obtained
from d-separation apply to any probabilistic model described by that particular di-
rected graph This will be true, for instance, whether the variables are discrete or
continuous or a combination of these Again, we see that a particular graph describes
a whole family of probability distributions At one extreme, we have a fully connected graph that exhibits no conditional
independence properties at all and which can represent any possible joint probability
distribution over the given variables The set DF will contain all possible distri-
butionsp(x) At the other extreme, we have a fully disconnected graph, i.e., one
having no links at all This corresponds to joint distributions that factorize into the
product of the marginal distributions over the variables comprising the nodes of the
graph Note that for any given graph, the set of distributions DF will include any
distributions that have additional independence properties beyond those described by
the graph For instance, a fully factorized distribution will always be passed through
the Ô¨Ålter implied by any graph over the corresponding set of variables Sequence
Models
There
are many important applications of machine learning in which the data consists
of asequence of values For example, text comprises a sequence of words, whereas
a protein comprises a sequence of amino acids Many sequences are ordered by
350 11 STRUCTURED DISTRIBUTIONS
Figure 11.27 An illustration of a general autoregressive
model of the form (11.42) with four nodes

============================================================

=== CHUNK 345 ===
Palavras: 356
Caracteres: 2338
--------------------------------------------------
x1 x2 x3 x4
time,
such as the audio signals from a microphone or daily rainfall measurements
at a particular location Sometimes the terminology of ‚Äòtime‚Äô as well as ‚Äòpast‚Äô and
‚Äòfuture‚Äô are used when referring to other types of sequential data, not just temporal
sequences Applications involving sequences include speech recognition, automatic
translation between languages, detecting genes in DNA, synthesizing music, writ-
ing computer code, holding a conversation with a modern search engine, and many
others We will denote a data sequence by x1;:::;xNwhere each element xnof the
sequence comprises a vector of values Note that we might have several such se-
quences drawn independently from the same distribution, in which case the joint
distribution over all the sequences factorizes into the product of the distributions
over each sequence individually From now on, we focus on modelling just one of
those sequences We have already seen in (11.4) that by repeated application of the product rule
of probability, a general distribution over Nvariables can be written as the product
of conditional distributions, and that the form of this decomposition depends on a
speciÔ¨Åc of ordering for the variables For vector-valued variables, and if we chose
an ordering that corresponds to the order of the variables in the sequence, then we
can write
p(x1;:::;xN) =N/productdisplay
n=1p(xn|x1;:::;xn‚àí1): (11.42)
This corresponds to a directed graph in which each node receives a link from every
previous node in the sequence, as illustrated using four variables in Figure 11.27 This is known as an autoregressive model This representation has complete generality and therefore from a modelling per-
spective adds no value since it encodes no assumptions We can constrain the space
of models by introducing conditional independence properties by removing links
from the graph, or equivalently by removing variables from the conditioning set of
the factors on the right-hand-side of (11.42) The strongest assumption would be to remove all conditioning variables, giving
a joint distribution of the form
p(x1;:::;xN) =N/productdisplay
n=1p(xn); (11.43)
which treats the variables as independent and therefore completely ignores the order-
ing information This corresponds to a probabilistic graphical model without links,
as shown in Figure 11.28

============================================================

=== CHUNK 346 ===
Palavras: 374
Caracteres: 2462
--------------------------------------------------
Sequence Models 351
Figure 11.28 The simplest approach to mod-
elling a sequence of observations
is to treat them as independent,
corresponding to a probabilistic
graphical model without links.x1 x2 x3 x4 Interesting
models that capture sequential properties while introducing mod-
elling assumptions lie between these two extremes One strong assumption would
be to assume that each conditional distribution depends only on the immediately
preceding variable in the sequence, giving a joint distribution of the form
p(x1;:::;xN) =p(x1)N/productdisplay
n=2p(xn|xn‚àí1): (11.44)
Note that the Ô¨Årst variable in the sequence is treated slightly differently since it has
no conditioning variable The functional form (11.44) is known as a Markov model,
orMarkov chain, and is represented by a graph consisting of a simple chain of nodes,
as seen in Figure 11.29 Using d-separation, we see that the conditional distribution Section 11.2.3
for observation xn, given all of the observations up to time n, is given by
p(xn|x1;:::;xn‚àí1) =p(xn|xn‚àí1); (11.45)
which is easily veriÔ¨Åed by direct evaluation starting from (11.44) and using the prod-
uct rule of probability Thus, if we use such a model to predict the next observation Exercise 11.16
in a sequence, the distribution of predictions will depend only on the value of the im-
mediately preceding observation and will be independent of all earlier observations More speciÔ¨Åcally, (11.44) is known as a Ô¨Årst-order Markov model because only
one conditioning variable appears in each conditional distribution We can extend
the model by allowing each conditional distribution to depend on the two preceding
variables, giving a second-order Markov model of the form
p(x1;:::;xN) =p(x1)p(x 2|x1)N/productdisplay
n=3p(xn|xn‚àí1;xn‚àí2): (11.46)
Note that the Ô¨Årst two variables are treated differently as they have fewer than two
conditioning variables This model is shown as a directed graph in Figure 11.30 By using d-separation (or by direct evaluation using the rules of probability), Exercise 11.17
we see that in the second-order Markov model, the conditional distribution of xn
given all previous observations x1;:::;xn‚àí1 is independent of the observations
x1;:::xn‚àí3 We can similarly consider extensions to an Mthorder Markov chain in
Figure 11.29 A Ô¨Årst-order Markov chain of ob-
servations in which the distribu-
tion of a particular observation xn
is conditioned on the value of the
previous observation xn‚àí1.x1 x2 x3 x4

============================================================

=== CHUNK 347 ===
Palavras: 384
Caracteres: 2535
--------------------------------------------------
STRUCTURED DISTRIBUTIONS
Figure 11.30 A second-order Markov chain in
which the conditional distribution
of a particular observation xnde-
pends on the values of the two
previous observations xn‚àí1 and
xn‚àí2.x1 x2 x3 x4 which
the conditional distribution for a particular variable depends on the previous
Mvariables However, we have paid a price for this increased Ô¨Çexibility because the
number of parameters in the model is now much larger Suppose the observations
are discrete variables having Kstates Then the conditional distribution p(xn|xn‚àí1)
in a Ô¨Årst-order Markov chain will be speciÔ¨Åed by a set of K‚àí1parameters for
each of theKstates of xn‚àí1giving a total of K(K‚àí1)parameters Now suppose
we extend the model to an Mthorder Markov chain, so that the joint distribution is
built up from conditionals p(xn|xn‚àíM;:::;xn‚àí1) If the variables are discrete and
if the conditional distributions are represented by general conditional probability ta-
bles, then such a model will have KM‚àí1(K‚àí1)parameters Thus, the number of
parameters grows exponentially with M, which will generally render this approach
impractical for larger values of M 11.3.1 Hidden variables
Suppose we wish to build a model for sequences that is not limited by the
Markov assumption to any order and yet can be speciÔ¨Åed using a limited number of
free parameters We can achieve this by introducing additional latent variables, thus
permitting a rich class of models to be constructed out of simple components For
each observation xn, we introduce a corresponding latent variable zn(which may be
of different type or dimensionality to the observed variable) We now assume that it
is the latent variables that form a Markov chain, giving rise to the graphical structure
known as a state-space model, which is shown in Figure 11.31 It satisÔ¨Åes the key
conditional independence property that zn‚àí1andzn+1are independent given zn, so
that
zn+1‚ä• ‚ä•zn‚àí1|zn: (11.47)
The joint distribution for this model is given by
p(x1;:::;xN;z1;:::;zN) =p(z1)/bracketleftBiggN/productdisplay
n=2p(zn|zn‚àí1)/bracketrightBiggN/productdisplay
n=1p(xn|zn): (11.48)
Figure 11.31 A state-space model expresses the joint
probability distribution over a sequence
of observed states x1;:::;xNin terms
of a Markov chain of hidden states
z1;:::;zNin the form (11.48).z1 z2 zN
x1 x2 xN
Exer
cises 353
Using the d-separation criterion, we see that in the state-space model there
is always a path connecting any two observed variables xnandxmvia the la-
tent variables and that this path is never blocked

============================================================

=== CHUNK 348 ===
Palavras: 355
Caracteres: 2359
--------------------------------------------------
Thus, the predictive distribution
p(xn+1|x1;:::;xn)for observation xn+1given all previous observations does not
exhibit any conditional independence properties, and so our predictions for xn+1de-
pend on all previous observations The observed variables, therefore, do not satisfy
the Markov property at any order There are two important models for sequential data that are described by this
graph If the latent variables are discrete, then we obtain a hidden Markov model
(Elliott, Aggoun, and Moore, 1995) Note that the observed variables in a hidden
Markov model may be discrete or continuous, and a variety of different conditional
distributions can be used to model them If both the latent and the observed variables
are Gaussian (with a linear-Gaussian dependence of the conditional distributions on
their parents), then we obtain a linear dynamical system, also known as a Kalman
Ô¨Ålter (Zarchan and Musoff, 2005) Both hidden Markov models and Kalman Ô¨Ålters
are discussed at length, along with algorithms for training them, in Bishop (2006) Such models can be made considerably more Ô¨Çexible by replacing the simple discrete
probability tables, or linear-Gaussian distributions, used to deÔ¨Åne p(xn|zn)with
deep neural networks Ex
ercises
11.1 (?)By marginalizing out the variables in order, show that the representation (11.6)
for the joint distribution of a directed graph is correctly normalized, provided each
of the conditional distributions is normalized 11.2 (?)Show that the property of there being no directed cycles in a directed graph
follows from the statement that there exists an ordered numbering of the nodes such
that for each node there are no links going to a lower-numbered node 11.3 (??) Consider three binary variables a;b;c‚àà{0; 1}having the joint distribution
given in Table 11.1 Show by direct evaluation that this distribution has the property
thataandbare marginally dependent, so that p(a;b)/negationslash=p(a)p(b), but that they
become independent when conditioned on c, so thatp(a;b|c) = p(a|c)p(b|c) for
bothc= 0andc= 1 Table 11.1 The joint distribution over three binary variables abcp
(a;b;c)
000 0.192
001 0.144
010 0.048
011 0.216
100 0.192
101 0.064
110 0.048
111 0.096
354 11 STRUCTURED DISTRIBUTIONS
11.4 (??) Evaluate the distributions p(a),p(b|c), andp(c|a) corresponding to the joint
distribution given in Table 11.1

============================================================

=== CHUNK 349 ===
Palavras: 424
Caracteres: 2705
--------------------------------------------------
Hence, show by direct evaluation that p(a;b;c) =
p(a)p(c|a)p(b|c) Draw the corresponding directed graph 11.5 (?)For the model shown in Figure 11.6, we have seen that the number of parameters
required to specify the conditional distribution p(y|x1;:::;xM), wherexi‚àà{0; 1},
could be reduced from 2MtoM+ 1by making use of the logistic sigmoid represen-
tation (11.8) An alternative representation (Pearl, 1988) is given by
p(y= 1|x 1;:::;xM) = 1‚àí(1‚àí0)M/productdisplay
i=1(1‚àíi)xi(11.49)
where the parameters irepresent the probabilities p(xi= 1) and0is an additional
parameter satisfying 06061 The conditional distribution (11.49) is known as
thenoisy-OR Show that this can be interpreted as a ‚Äòsoft‚Äô (probabilistic) form of the
logical OR function (i.e., the function that gives y= 1whenever at least one of the
xi= 1) Discuss the interpretation of 0 11.6 (??) Starting from the deÔ¨Ånition (11.9) for the conditional distributions, derive the
recursion relation (11.12) for the mean of the joint distribution for a linear-Gaussian
model 11.7 (??) Starting from the deÔ¨Ånition (11.9) for the conditional distributions, derive the
recursion relation (11.13) for the covariance matrix of the joint distribution for a
linear-Gaussian model 11.8 (??) Show that the number of parameters in the covariance matrix of a fully con-
nected linear-Gaussian graphical model over Dvariables deÔ¨Åned by (11.9) is D(D+
1)=2 11.9 (??) Using the recursion relations (11.12) and (11.13), show that the mean and co-
variance of the joint distribution for the graph shown in Figure 11.7 are given by
(11.14) and (11.15), respectively 11.10 (?)Verify that the joint distribution over a set of vector-valued variables deÔ¨Åned by a
linear-Gaussian model in which each node corresponds to a distribution of the form
(11.16) is itself a Gaussian 11.11 (?)Show thata‚ä• ‚ä•b;c|dimpliesa‚ä• ‚ä•b|d 11.12 (?)Using the d-separation criterion, show that the conditional distribution for a node
xin a directed graph, conditioned on all the nodes in the Markov blanket, is inde-
pendent of the remaining variables in the graph 11.13 (?)Consider the directed graph shown in Figure 11.32 in which none of the variables
is observed Suppose we now observe the variable d Show
that in general a/negationslash‚ä• ‚ä•b|d Exer
cises 355
Figure 11.32 Example of a graphical model used to explore the conditional
independence properties of the head-to-head path a‚Äìc‚Äìb when
a descendant of c, namely the node d, is observed.a
cb
d
11.14 (
??)Consider the example of the car fuel system shown in Figure 11.20, and suppose
that instead of observing the state of the fuel gauge Gdirectly, the gauge is seen by
the driverD, who reports to us the reading on the gauge

============================================================

=== CHUNK 350 ===
Palavras: 354
Caracteres: 2287
--------------------------------------------------
This report says that the
gauge shows either that the tank is full D= 1or that it is empty D= 0 Our driver
is a bit unreliable, as expressed through the following probabilities:
p(D= 1|G = 1) = 0:9 (11.50)
p(D= 0|G = 0) = 0:9: (11.51)
Suppose that the driver tells us that the fuel gauge shows empty, in other words
that we observe D= 0 Evaluate the probability that the tank is empty given only
this observation Similarly, evaluate the corresponding probability given also the
observation that the battery is Ô¨Çat, and note that this second probability is lower Discuss the intuition behind this result, and relate the result to Figure 11.32 11.15 (??) Suppose we train a naive Bayes model, with the assumption (11.37), using
maximum likelihood Assume that each of the class-conditional densities p(x(l)|Ck)
is governed by its own independent parameters w(l) Show that the maximum like-
lihood solution involves Ô¨Åtting each of the class-conditional densities using the cor-
responding observed data vectors x(l)
1;:::;x(l)
Nby maximizing the likelihood with
respect to the corresponding class label data, and then setting the class priors p(Ck)
to the fraction of training data points in each class 11.16 (??) Consider the joint probability distribution (11.44) corresponding to the directed
graph of Figure 11.29 Using the sum and product rules of probability, verify that
this joint distribution satisÔ¨Åes the conditional independence property (11.45) for n=
2;:::;N Similarly, show that the second-order Markov model described by the joint
distribution (11.46) satisÔ¨Åes the conditional independence property
p(xn|x1;:::;xn‚àí1) =p(xn|xn‚àí1;xn‚àí2) (11.52)
forn= 3;:::;N 11.17 (?)Use d-separation, as discussed in Section 11.2, to verify that the Markov model
shown in Figure 11.29 havingNnodes in total satisÔ¨Åes the conditional independence
properties (11.45) for n= 2;:::;N Similarly, show that a model described by the
graph in Figure 11.30 in which there are Nnodes in total satisÔ¨Åes the conditional
independence properties (11.52) for n= 3;:::;N STRUCTURED DISTRIBUTIONS
11.18 (?)Consider a second-order Markov process described by the graph in Figure 11.30 By combining adjacent pairs of variables, show that this can be expressed as a Ô¨Årst-
order Markov process over the new variables

============================================================

=== CHUNK 351 ===
Palavras: 385
Caracteres: 2575
--------------------------------------------------
11.19 (?)By using d-separation, show that the distribution p(x1;:::;xN)of the observed
data for the state-space model represented by the directed graph in Figure 11.31 does
not satisfy any conditional independence properties and hence does not exhibit the
Markov property at any Ô¨Ånite order 12
Transformers
Transformers represent one of the most important developments in deep learning They are based on a processing concept called attention, which allows a network to
give different weights to different inputs, with weighting coefÔ¨Åcients that themselves
depend on the input values, thereby capturing powerful inductive biases related to
sequential and other forms of data These models are known as transformers because they transform a set of vec-
tors in some representation space into a corresponding set of vectors, having the
same dimensionality, in some new space The goal of the transformation is that the
new space will have a richer internal representation that is better suited to solving
downstream tasks Inputs to a transformer can take the form of unstructured sets
of vectors, ordered sequences, or more general representations, giving transformers
broad applicability Transformers were originally introduced in the context of natural language pro-
357 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_12    
358 12 TRANSFORMERS
cessing, or NLP (where a ‚Äònatural‚Äô language is one such as English or Mandarin) and
have greatly surpassed the previous state-of-the-art approaches based on recurrent
neural networks (RNNs) Transformers have subsequently been found to achieve
excellent results in many other domains For example, vision transformers often
outperform CNNs in image processing tasks, whereas multimodal transformers that
combine multiple types of data, such as text, images, audio, and video, are amongst
the most powerful deep learning models One major advantage of transformers is that transfer learning is very effective, so
that a transformer model can be trained on a large body of data and then the trained
model can be applied to many downstream tasks using some form of Ô¨Åne-tuning A
large-scale model that can subsequently be adapted to solve multiple different tasks
is known as a foundation model Furthermore, transformers can be trained in a self-
supervised way using unlabelled data, which is especially effective with language
models since transformers can exploit vast quantities of text available from the inter-
net and other sources

============================================================

=== CHUNK 352 ===
Palavras: 350
Caracteres: 2266
--------------------------------------------------
The scaling hypothesis asserts that simply by increasing the
scale of the model, as measured by the number of learnable parameters, and train-
ing on a commensurately large data set, signiÔ¨Åcant improvements in performance
can be achieved, even with no architectural changes Moreover, the transformer is
especially well suited to massively parallel processing hardware such as graphical
processing units, or GPUs, allowing exceptionally large neural network language
models having of the order of a trillion (1012) parameters to be trained in reason-
able time Such models have extraordinary capabilities and show clear indications
of emergent properties that have been described as the early signs of artiÔ¨Åcial general
intelligence (Bubeck et al., 2023) The architecture of a transformer can seem complex, or even daunting, to a
newcomer as it involves multiple different components working together, in which
the various design choices can seem arbitrary In this chapter we therefore aim to give
a comprehensive step-by-step introduction to all the key ideas behind transformers
and to provide clear intuition to motivate the design of the various elements We Ô¨Årst
describe the transformer architecture and then focus on natural language processing,
before exploring other application domains Attention
The fundamental concept that underpins a transformer is attention This was orig-
inally developed as an enhancement to RNNs for machine translation (Bahdanau, Section 12.2.5
Cho, and Bengio, 2014) However, Vaswani et al (2017) later showed that signiÔ¨Å-
cantly improved performance could be obtained by eliminating the recurrence struc-
ture and instead focusing exclusively on the attention mechanism Today, transform-
ers based on attention have completely superseded RNNs in almost all applications We will motivate the use of attention using natural language as an example,
12.1 Attention 359
I s
wam across the r
iver to get to the other bank
I s
wam across the r
iver to get to the other bank
Figure
12.1 Schematic illustration of attention in which the interpretation of the word ‚Äòbank‚Äô is inÔ¨Çuenced by the
words ‚Äòriver‚Äô and ‚Äòswam‚Äô, with the thickness of each line being indicative of the strength of its inÔ¨Çuence although it has much broader applicability

============================================================

=== CHUNK 353 ===
Palavras: 357
Caracteres: 2144
--------------------------------------------------
Consider the following two sentences:
I swam across the river to get to the other bank I walked across the road to get cash from the bank Here the word ‚Äòbank‚Äô has different meanings in the two sentences However, this
can be detected only by looking at the context provided by other words in the se-
quence We also see that some words are more important than others in determining
the interpretation of ‚Äòbank‚Äô In the Ô¨Årst sentence, the words ‚Äòswam‚Äô and ‚Äòriver‚Äô most
strongly indicate that ‚Äòbank‚Äô refers to the side of a river, whereas in the second sen-
tence, the word ‚Äòcash‚Äô is a strong indicator that ‚Äòbank‚Äô refers to a Ô¨Ånancial institution We see that to determine the appropriate interpretation of ‚Äòbank‚Äô, a neural network
processing such a sentence should attend to, in other words rely more heavily on,
speciÔ¨Åc words from the rest of the sequence This concept of attention is illustrated
inFigure 12.1 Moreover, we also see that the particular locations that should receive more
attention depend on the input sequence itself: in the Ô¨Årst sentence it is the second and
Ô¨Åfth words that are important whereas in the second sentence it is the eighth word In a standard neural network, different inputs will inÔ¨Çuence the output to different
extents according to the values of the weights that multiply those inputs Once the
network is trained, however, those weights, and their associated inputs, are Ô¨Åxed By contrast, attention uses weighting factors whose values depend on the speciÔ¨Åc
input data Figure 12.2 shows the attention weights from a section of a transformer
network trained on natural language When we discuss natural language processing, we will see how word embed-
ding can be used to map words into vectors in an embedding space These vectors
can then be used as inputs for subsequent neural network processing These embed-
dings capture elementary semantic properties, for example by mapping words with
similar meanings to nearby locations in the embedding space One characteristic of
such embeddings is that a given word always maps to the same embedding vector TRANSFORMERS
Figure 12.2 An example of learned attention weights

============================================================

=== CHUNK 354 ===
Palavras: 361
Caracteres: 2131
--------------------------------------------------
(2017) with permission.]
A transformer can be viewed as a richer form of embedding in which a given vector
is mapped to a location that depends on the other vectors in the sequence Thus,
the vector representing ‚Äòbank‚Äô in our example above could map to different places
in a new embedding space for the two different sentences For example, in the Ô¨Årst
sentence the transformed representation might put ‚Äòbank‚Äô close to ‚Äòwater‚Äô in the em-
bedding space, whereas in the second sentence the transformed representation might
put it close to ‚Äòmoney‚Äô As an example of attention, consider the modelling of proteins We can view
a protein as a one-dimensional sequence of molecular units called amino acids A
protein can comprise potentially hundreds or thousands of such units, each of which
is given by one of 22 possibilities In a living cell, a protein folds up into a three-
dimensional structure in which amino acids that are widely separated in the one-
dimensional sequence can become physically close in three-dimensional space and
thereby interact Transformer models allows these distant amino acids to ‚Äòattend‚Äô to Figure 1.2
each other thereby greatly improving the accuracy with which their 3-dimensional
structure can be modelled (Vig et al 12.1.1 Transformer processing
The input data to a transformer is a set of vectors {xn}of dimensionality D,
wheren= 1;:::;N We refer to these data vectors as tokens , where a token might,
for example, correspond to a word within a sentence, a patch within an image, or
an amino acid within a protein The elements xniof the tokens are called features Later we will see how to construct these token vectors for natural language data and
for images A powerful property of transformers is that we do not have to design a
new neural network architecture to handle a mix of different data types but instead
can simply combine the data variables into a joint set of tokens Before we can gain a clear understanding of the operation of a transformer, it
12.1 Attention 361
Figure 12.3 The structure of the data matrix X, of di-
mensionN√óD, in which row nrepre-
sents the transposed data vector xT
n

============================================================

=== CHUNK 355 ===
Palavras: 359
Caracteres: 2196
--------------------------------------------------
XN(tokens)
D(features)xT
n
is
important to be precise about notation We will follow the standard convention
and combine the data vectors into a matrix Xof dimensions N√óDin which the
nth row comprises the token vector xT
n, and where n= 1;:::;N labels the rows,
as illustrated in Figure 12.3 Note that this matrix represents one set of input tokens,
and that for most applications, we will require a data set containing many sets of
tokens, such as independent passages of text where each word is represented as one
token The fundamental building block of a transformer is a function that takes a
data matrix as input and creates a transformed matrix /tildewideXof the same dimensionality
as the output We can write this function in the form
/tildewideX= TransformerLayer [X] : (12.1)
We can then apply multiple transformer layers in succession to construct deep net-
works capable of learning rich internal representations Each transformer layer con-
tains its own weights and biases, which can be learned using gradient descent using
an appropriate cost function, as we will discuss in detail later in the chapter Section 12.3
A single transformer layer itself comprises two stages The Ô¨Årst stage, which im-
plements the attention mechanism, mixes together the corresponding features from
different token vectors across the columns of the data matrix, whereas the second
stage then acts on each row independently and transforms the features within each
token vector We start by looking at the attention mechanism 12.1.2 Attention coefÔ¨Åcients
Suppose that we have a set of input tokens x1;:::;xNin an embedding space
and we want to map this to another set y1;:::;yNhaving the same number of tokens
but in a new embedding space that captures a richer semantic structure Consider a
particular output vector yn The value of ynshould depend not just on the corre-
sponding input vector xnbut on all the vectors x1;:::;xNin the set With attention,
this dependence should be stronger for those inputs xmthat are particularly impor-
tant for determining the modiÔ¨Åed representation of yn A simple way to achieve this
is to deÔ¨Åne each output vector ynto be a linear combination of the input vectors
362 12

============================================================

=== CHUNK 356 ===
Palavras: 353
Caracteres: 2182
--------------------------------------------------
TRANSFORMERS
x1;:::;xNwith weighting coefÔ¨Åcients anm:
yn=N/summationdisplay
m=1anmxm (12.2)
whereanmare called attention weights The coefÔ¨Åcients should be close to zero for
input tokens that have little inÔ¨Çuence on the output ynand largest for inputs that
have most inÔ¨Çuence We therefore constrain the coefÔ¨Åcients to be non-negative to
avoid situations in which one coefÔ¨Åcient can become large and positive while another
coefÔ¨Åcient compensates by becoming large and negative We also want to ensure that
if an output pays more attention to a particular input, this will be at the expense of
paying less attention to the other inputs, and so we constrain the coefÔ¨Åcients to sum
to unity Thus, the weighting coefÔ¨Åcients must satisfy the following two constraints:
anm>0 (12.3)
N/summationdisplay
m=1anm= 1: (12.4)
Together these imply that each coefÔ¨Åcient lies in the range 06anm61and so the Exercise 12.1
coefÔ¨Åcients deÔ¨Åne a ‚Äòpartition of unity‚Äô For the special case amm= 1, it follows that
anm= 0forn/negationslash=m, and therefore ym=xmso that the input vector is unchanged
by the transformation More generally, the output ymis a blend of the input vectors
with some inputs given more weight than others Note that we have a different set of coefÔ¨Åcients for each output vector yn;and
the constraints (12.3) and (12.4) apply separately for each value of n These co-
efÔ¨Åcientsanmdepend on the input data, and we will shortly see how to calculate
them 12.1.3 Self-attention
The next question is how to determine the coefÔ¨Åcients anm Before we discuss
this in detail, it is useful to Ô¨Årst introduce some terminology taken from the Ô¨Åeld of
information retrieval Consider the problem of choosing which movie to watch in
an online movie streaming service One approach would be to associate each movie
with a list of attributes describing things such as the genre (comedy, action, etc.), the
names of the leading actors, the length of the movie, and so on The user could then
search through a catalogue to Ô¨Ånd a movie that matches their preferences We could
automate this by encoding the attributes of each movie in a vector called the key The corresponding movie Ô¨Åle itself is called a value

============================================================

=== CHUNK 357 ===
Palavras: 381
Caracteres: 2253
--------------------------------------------------
Similarly, the user could then
provide their own personal vector of values for the desired attributes, which we call
thequery The movie service could then compare the query vector with all the key
vectors to Ô¨Ånd the best match and send the corresponding movie to the user in the
form of the value Ô¨Åle We can think of the user ‚Äòattending‚Äô to the particular movie
whose key most closely matches their query This would be considered a form of
hard attention in which a single value vector is returned For the transformer, we
generalize this to soft attention in which we use continuous variables to measure
12.1 Attention 363
the degree of match between queries and keys and we then use these variables to
weight the inÔ¨Çuence of the value vectors on the outputs This will also ensure that
the transformer function is differentiable and can therefore be trained by gradient
descent Following the analogy with information retrieval, we can view each of the input
vectors xnas a value vector that will be used to create the output tokens We also use
the vector xndirectly as the key vector for input token n That would be analogous
to using the movie itself to summarize the characteristics of the movie Finally, we
can use xmas the query vector for output ym, which can then be compared to each
of the key vectors To see how much the token represented by xnshould attend to
the token represented by xm, we need to work out how similar these vectors are One simple measure of similarity is to take their dot product xT
nxm To impose the
constraints (12.3) and (12.4), we can deÔ¨Åne the weighting coefÔ¨Åcients anmby using
thesoftmax function to transform the dot products: Section 5.3
anm=exp(xT
nxm)/summationtextN
m/prime=1exp(xTnxm/prime): (12.5)
Note that in this case there is no probabilistic interpretation of the softmax function
and it is simply being used to normalize the attention weights appropriately So in summary, each input vector xnis transformed to a corresponding output
vector ynby taking a linear combination of input vectors of the form (12.2) in which
the weightanmapplied to input vector xmis given by the softmax function (12.5)
deÔ¨Åned in terms of the dot product xT
nxmbetween the query xnfor inputnand the
keyxmassociated with input m

============================================================

=== CHUNK 358 ===
Palavras: 381
Caracteres: 2385
--------------------------------------------------
Note that, if all the input vectors are orthogonal,
then each output vector is simply equal to the corresponding input vector so that
ym=xmform= 1;:::;N Exercise 12.3
We can write (12.2) in matrix notation by using the data matrix X, along with
the analogous N√óDoutput matrix Y, whose rows are given by ym, so that
Y= Softmax/bracketleftbig
XXT/bracketrightbig
X (12.6)
where Softmax[L] is an operator that takes the exponential of every element of a
matrix Land then normalizes each row independently to sum to one From now on,
we will focus on matrix notation for clarity This process is called self-attention because we are using the same sequence to
determine the queries, keys, and values We will encounter variants of this attention
mechanism later in this chapter Also, because the measure of similarity between
query and key vectors is given by a dot product, this is known as dot-product self-
attention 12.1.4 Network parameters
As it stands, the transformation from input vectors {xn}to output vectors{yn}
is Ô¨Åxed and has no capacity to learn from data because it has no adjustable parame-
ters Furthermore, each of the feature values within a token vector xnplays an equal
role in determining the attention coefÔ¨Åcients, whereas we would like the network to
364 12 TRANSFORMERS
have the Ô¨Çexibility to focus more on some features than others when determining
token similarity We can address both issues if we deÔ¨Åne modiÔ¨Åed feature vectors
given by a linear transformation of the original vectors in the form
/tildewideX=XU (12.7)
where Uis aD√óDmatrix of learnable weight parameters, analogous to a ‚Äòlayer‚Äô
in a standard neural network This gives a modiÔ¨Åed transformation of the form
Y= Softmax/bracketleftbig
XUUTXT/bracketrightbig
XU: (12.8)
Although this has much more Ô¨Çexibility, it has the property that the matrix
XUUTXT(12.9)
is symmetric, whereas we would like the attention mechanism to support signiÔ¨Åcant
asymmetry For example, we might expect that ‚Äòchisel‚Äô should be strongly associ-
ated with ‚Äòtool‚Äô since every chisel is a tool, whereas ‚Äòtool‚Äô should only be weakly
associated with ‚Äòchisel‚Äô because there are many other kinds of tools besides chis-
els Although the softmax function means the resulting matrix of attention weights
is not itself symmetric, we can create a much more Ô¨Çexible model by allowing the
queries and the keys to have independent parameters

============================================================

=== CHUNK 359 ===
Palavras: 359
Caracteres: 2345
--------------------------------------------------
Furthermore, the form (12.8)
uses the same parameter matrix Uto deÔ¨Åne both the value vectors and the attention
coefÔ¨Åcients, which again seems like an undesirable restriction We can overcome these limitations by deÔ¨Åning separate query, key, and value
matrices each having their own independent linear transformations:
Q=XW(q)(12.10)
K=XW(k)(12.11)
V=XW(v)(12.12)
where the weight matrices W(q),W(k), and W(v)represent parameters that will
be learned during the training of the Ô¨Ånal transformer architecture Here the matrix
W(k)has dimensionality D√óDkwhereDkis the length of the key vector The
matrix W(q)must have the same dimensionality D√óDkasW(k)so that we can
form dot products between the query and key vectors A typical choice is Dk=D Similarly, W(v)is a matrix of size D√óDv, whereDvgoverns the dimensionality of
the output vectors If we set Dv=D, so that the output representation has the same
dimensionality as the input, this will facilitate the inclusion of residual connections,
which we discuss later Also, multiple transformer layers can be stacked on top of Section 12.1.7
each other if each layer has the same dimensionality We can then generalize (12.6)
to give
Y= Softmax/bracketleftbig
QKT/bracketrightbig
V (12.13)
where QKThas dimension N√óN, and the matrix Yhas dimension N√óDv The
calculation of the matrix QKTis illustrated in Figure 12.4, whereas the evaluation
of the matrix Yis illustrated in Figure 12.5 Attention 365
√ó W(q)
D√óD= Q
N√óD
W(k)
D√óD= K
N√óD√óQKT
N√óNX
N√óD
Figure 12.4 Illustration of the evaluation of the matrix QKT, which determines the attention coefÔ¨Å-
cients in a transformer The input Xis separately transformed using (12.10) and (12.11)
to give the query matrix Qand key matrix K, respectively, which are then multiplied to-
gether In practice we can also include bias parameters in these linear transformations However, the bias parameters can be absorbed into the weight matrices, as we did
with standard neural networks, by augmenting the data matrix Xwith an additional Section 6.2.1
column of 1‚Äôs and by augmenting the weight matrices with an additional row of
parameters to represent the biases From now on we will treat the bias parameters as
implicit to avoid cluttering the notation Compared to a conventional neural network, the signal paths have multiplicative
relations between activation values

============================================================

=== CHUNK 360 ===
Palavras: 355
Caracteres: 2291
--------------------------------------------------
Whereas standard networks multiply activations
by Ô¨Åxed weights, here the activations are multiplied by the data-dependent attention
coefÔ¨Åcients This means, for example, that if one of the attention coefÔ¨Åcients is
close to zero for a particular choice of input vector, the resulting signal path will
ignore the corresponding incoming signal, which will therefore have no inÔ¨Çuence
Figure 12.5 Illustration of the evaluation
of the output from an attention layer given
the query, key, and value matrices Q,
K, and V, respectively The entry at
the position highlighted in the output ma-
trixYis obtained from the dot prod-
uct of the highlighted row and column
of the Softmax
QKT
andVmatrices,
respectively.Y
N√óDv= Softmax QKT
N√óN√ó
N√óDvV
366 12.TRANSFORMERS
Figure 12.6 Information Ô¨Çow in a scaled dot-
product self-attention neural network
layer Here ‚Äòmat mul‚Äô denotes matrix
multiplication, and ‚Äòscale‚Äô refers to the
normalization of the argument to the
softmax using‚àöDk.This structure
constitutes a single attention ‚Äòhead‚Äô XW(k)W(q)W(v)K Q VmatmulscalesoftmaxmatmulY
onthe network outputs By contrast, if a standard neural network learns to ignore a
particular input or hidden-unit variable, it does so for all input vectors 12.1.5 Scaled self-attention
There is one Ô¨Ånal reÔ¨Ånement we can make to the self-attention layer Recall that
the gradients of the softmax function become exponentially small for inputs of high
magnitude, just as happens with tanh or logistic-sigmoid activation functions To
help prevent this from happening, we can re-scale the product of the query and key
vectors before applying the softmax function To derive a suitable scaling, note that
if the elements of the query and key vectors were all independent random numbers
with zero mean and unit variance, then the variance of the dot product would be Dk Exercise 12.4
We therefore normalize the argument to the softmax using the standard deviation
given by the square root of Dk, so that the output of the attention layer takes the
form
Y= Attention( Q;K;V)‚â°Softmax/bracketleftbiggQKT
‚àöDk/bracketrightbigg
V: (12.14)
This is called scaled dot-product self-attention , and is the Ô¨Ånal form of our self-
attention neural network layer The structure of this layer is summarized in Fig-
ure 12.6 and in Algorithm 12.1

============================================================

=== CHUNK 361 ===
Palavras: 375
Caracteres: 2444
--------------------------------------------------
12.1.6 Multi-head attention
The attention layer described so far allows the output vectors to attend to data-
dependent patterns of input vectors and is called an attention head Attention 367
Algorithm
12.1: Scaled dot-product self-attention
Input: Set of tokens X‚ààRN√óD:{x1;:::;xN}
Weight matrices{W(q);W(k)}‚ààRD√óDkandW(v)‚ààRD√óDv
Output: Attention(Q; K;V)‚ààRN√óDv:{y1;:::;yN}
Q=XW(q)//
compute queries Q‚ààRN√óDk
K=XW(k)// compute keys K‚ààRN√óDk
V=XW(v)// compute values V‚ààRN√óD
return Attention(Q; K;V) = Softmax/bracketleftbiggQKT
‚àöDk/bracketrightbigg
V
might
be multiple patterns of attention that are relevant at the same time In natu-
ral language, for example, some patterns might be relevant to tense whereas others
might be associated with vocabulary Using a single attention head can lead to av-
eraging over these effects Instead we can use multiple attention heads in parallel These consist of identically structured copies of the single head, with independent
learnable parameters that govern the calculation of the query, key, and value matri-
ces This is analogous to using multiple different Ô¨Ålters in each layer of a convolu-
tional network Suppose we have Hheads indexed by h= 1;:::;H of the form
Hh= Attention(Q h;Kh;Vh) (12.15)
where Attention(¬∑;¬∑;¬∑)is given by (12.14), and we have deÔ¨Åned separate query, key,
and value matrices for each head using
Qh=XW(q)
h(12.16)
Kh=XW(k)
h(12.17)
Vh=XW(v)
h: (12.18)
The heads are Ô¨Årst concatenated into a single matrix, and the result is then linearly
transformed using a matrix W(o)to give a combined output in the form
Y(X) = Concat [H 1;:::;HH]W(o): (12.19)
This is illustrated in Figure 12.7 Each matrix Hhhas dimension N√óDv, and so the concatenated matrix has
dimensionN√óHD v This is transformed by the linear matrix W(o)of dimension
HD v√óDto give the Ô¨Ånal output matrix Yof dimension N√óD, which is the same
as the original input matrix X The elements of the matrix W(o)are learned during
the training phase along with the query, key, and value matrices Typically Dvis
368 12 TRANSFORMERS
Figure 12.7 Network architecture for multi-
head attention Each head com-
prises the structure shown in Fig-
ure 12.6, and has its own key,
query, and value parameters The
outputs of the heads are con-
catenated and then linearly pro-
jected back to the input data
dimensionality.N√óH
DvH1H2...HH√ó W(o)
H
Dv√óD= Y
N√óD
chosen
to be equal to D=H so that the resulting concatenated matrix has dimension
N√óD

============================================================

=== CHUNK 362 ===
Palavras: 365
Caracteres: 2469
--------------------------------------------------
Multi-head attention is summarized in Algorithm 12.2, and the information
Ô¨Çow in a multi-head attention layer is illustrated in Figure 12.8 Note that the formulation of multi-head attention given above, which follows
that used in the research literature, includes some redundancy in the successive mul-
tiplication of the W(v)matrix for each head and the output matrix W(o) Removing
this redundancy allows a multi-head self-attention layer to be written as a sum over
contributions from each of the heads separately Exercise 12.5
12.1.7 Transformer layers
Multi-head self-attention forms the core architectural element in a transformer
network We know that neural networks beneÔ¨Åt greatly from depth, and so we would
like to stack multiple self-attention layers on top of each other To improve training
Algorithm
12.2: Multi-head attention
Input: Set of tokens X‚ààRN√óD:{x1;:::;xN}
Query weight matrices {W(q)
1;:::;W(q)
H}‚ààRD√óD
Key weight matrices {W(k)
1;:::;W(k)
H}‚ààRD√óD
Value weight matrices {W(v)
1;:::;W(v)
H}‚ààRD√óDv
Output weight matrix W(o)‚ààRHD v√óD
Output: Y‚ààRN√óD:{y1;:::;xN}
//
compute self-attention for each head (Algorithm 12.1)
forh= 1;:::;H do
Qh=XW(q)
h;Kh=XW(k)
h;Vh=XW(v)
h
Hh=
Attention ( Qh;Kh;Vh)//Hh‚ààRN√óDv
end for
H= Concat [H 1;:::;HN]// concatenate heads
return Y(X) = HW(o)
12.1 Attention 369
Xself-attention self-attention self-attentionconcatlinearY
Figure
12.8 Information Ô¨Çow in a multi-head attention layer The associated computation, given by
Algorithm 12.2, is illustrated in Figure 12.7 efÔ¨Åciency, we can introduce residual connections that bypass the multi-head struc- Section 9.5
ture To do this we require that the output dimensionality is the same as the input
dimensionality, namely N√óD This is then followed by layer normalization (Ba, Section 7.4.3
Kiros, and Hinton, 2016), which improves training efÔ¨Åciency The resulting trans-
formation can be written as
Z= LayerNorm [ Y(X) + X] (12.20)
where Yis deÔ¨Åned by (12.19) Sometimes the layer normalization is replaced by
pre-norm in which the normalization layer is applied before the multi-head self-
attention instead of after, as this can result in more effective optimization, in which
case we have
Z=Y(X/prime) +X; where X/prime= LayerNorm [ X]: (12.21)
In each case, Zagain has the same dimensionality N√óDas the input matrix X We have seen that the attention mechanism creates linear combinations of the
value vectors, which are then linearly combined to produce the output vectors

============================================================

=== CHUNK 363 ===
Palavras: 356
Caracteres: 2247
--------------------------------------------------
Also,
the values are linear functions of the input vectors, and so we see that the outputs
of an attention layer are constrained to be linear combinations of the inputs Non-
linearity does enter through the attention weights, and so the outputs will depend
nonlinearly on the inputs via the softmax function, but the output vectors are still
constrained to lie in the subspace spanned by the input vectors and this limits the
expressive capabilities of the attention layer We can enhance the Ô¨Çexibility of the
transformer by post-processing the output of each layer using a standard nonlinear
neural network with Dinputs andDoutputs, denoted MLP[¬∑] for ‚Äòmultilayer per-
ceptron‚Äô For example, this might consist of a two-layer fully connected network
with ReLU hidden units This needs to be done in a way that preserves the ability
370 12.TRANSFORMERS
Figure 12.9 One layer of the transformer architecture that
implements the transformation (12.1) Here
‚ÄòMLP‚Äô stands for multilayer perceptron, while
‚Äòadd and norm‚Äô denotes a residual connection
followed by layer normalization Xmulti-head
self-attentionadd& normMLPadd& normeX
Z
ofthe transformer to process sequences of variable length To achieve this, the same
shared network is applied to each of the output vectors, corresponding to the rows of
Z Again, this neural network layer can be improved by using a residual connection It also includes layer normalization so that the Ô¨Ånal output from the transformer layer
has the form
/tildewideX= LayerNorm [MLP [ Z] +Z]: (12.22)
This leads to an overall architecture for a transformer layer shown in Figure 12.9 and
summarized in Algorithm 12.3 Again, we can use a pre-norm instead, in which case
the Ô¨Ånal output is given by
/tildewideX= MLP( Z/prime) +Z;where Z/prime= LayerNorm [ Z]: (12.23)
In a typical transformer there are multiple such layers stacked on top of each other The layers generally have identical structures, although there is no sharing of weights
and biases between different layers 12.1.8 Computational complexity
The attention layer discussed so far takes a set of Nvectors each of length
Dand maps them into another set of Nvectors having the same dimensionality Thus, the inputs and outputs each have overall dimensionality ND

============================================================

=== CHUNK 364 ===
Palavras: 359
Caracteres: 2399
--------------------------------------------------
If we had used a
standard fully connected neural network to map the input values to the output values,
it would haveO(N2D2)independent parameters Likewise the computational cost
of evaluating one forward pass through such a network would also be O(N2D2) In the attention layer, the matrices W(q),W(k), andW(v)are shared across in-
put tokens, and therefore the number of independent parameters is O(D2), assuming
Dk/similarequalDv/similarequalD Since there are Ninput tokens, the number of computational steps
12.1 Attention 371
Algorithm 12.3: Transformer layer
Input: Set of tokens X‚ààRN√óD:{x1;:::;xN}
Multi-head self-attention layer parameters
Feed-forward network parameters
Output:/tildewideX‚ààRN√óD:{/tildewidex1;:::;/tildewidexN}
Z= LayerNorm [ Y(X) + X]//Y(X) from Algorithm 12.2
/tildewideX= LayerNorm [MLP [Z] + Z]// shared neural network
return/tildewideX
in evaluating the dot products in a self-attention layer is O(N2D) We can think
of a self-attention layer as a sparse matrix in which parameters are shared between
speciÔ¨Åc blocks of the matrix The subsequent neural network layer, which has D Exercise 12.6
inputs andDoutputs, has a cost that is O(D2) Since it is shared across tokens, it
has a complexity that is linear in N, and therefore overall this layer has a cost that is
O(ND2) Depending on the relative sizes of NandD, either the transformer layer
or the MLP layer may dominate the computational cost Compared to a fully con-
nected network, a transformer layer is computationally more efÔ¨Åcient Many vari-
ants of the transformer architecture have been proposed (Lin et al., 2021; Phuong
and Hutter, 2022) including modiÔ¨Åcations aimed at improving efÔ¨Åciency (Tay et al.,
2020) 12.1.9 Positional encoding
In the transformer architecture, the matrices W(q)
h,W(k)
h, andW(v)
hare shared
across the input tokens, as is the subsequent neural network As a consequence, the
transformer has the property that permuting the order of the input tokens, i.e., the
rows of X, results in the same permutation of the rows of the output matrix /tildewideX In Exercise 12.7
other words a transformer is equivariant with respect to input permutations The Section 10.2
sharing of parameters in the network architecture facilitates the massively parallel
processing of the transformer, and also allows the network to learn long-range de-
pendencies just as effectively as short-range dependencies

============================================================

=== CHUNK 365 ===
Palavras: 367
Caracteres: 2282
--------------------------------------------------
However, the lack of
dependence on token order becomes a major limitation when we consider sequential
data, such as the words in a natural language, because the representation learned by
a transformer will be independent of the input token ordering The two sentences
‚ÄòThe food was bad, not good at all.‚Äô and ‚ÄòThe food was good, not bad at all.‚Äô con-
tain the same tokens but they have very different meanings because of the different
token ordering Clearly token order is crucial for most sequential processing tasks
including natural language processing, and so we need to Ô¨Ånd a way to inject token
order information into the network Since we wish to retain the powerful properties of the attention layers that we
have carefully constructed, we aim to encode the token order in the data itself in-
372 12 TRANSFORMERS
stead of having to be represented in the network architecture We will therefore
construct a position encoding vector rnassociated with each input position nand
then combine this with the associated input token embedding xn One obvious way
to combine these vectors would be to concatenate them, but this would increase the
dimensionality of the input space and hence of all subsequent attention spaces, cre-
ating a signiÔ¨Åcant increase in computational cost Instead, we can simply add the
position vectors onto the token vectors to give
/tildewidexn=xn+rn: (12.24)
This requires that the positional encoding vectors have the same dimensionality as
the token-embedding vectors At Ô¨Årst it might seem that adding position information onto the token vector
would corrupt the input vectors and make the task of the network much more difÔ¨Å-
cult However, some intuition as to why this can work well comes from noting that
two randomly chosen uncorrelated vectors tend to be nearly orthogonal in spaces of
high dimensionality, indicating that the network is able to process the token identity Exercise 12.8
information and the position information relatively separately Note also that, be-
cause of the residual connections across every layer, the position information does
not get lost in going from one transformer layer to the next Moreover, due to the
linear processing layers in the transformer, a concatenated representation has similar
properties to an additive one

============================================================

=== CHUNK 366 ===
Palavras: 361
Caracteres: 2329
--------------------------------------------------
Exercise 12.9
The next task is to construct the embedding vectors {rn} A simple approach
would be to associate an integer 1;2;3;::: with each position However, this has the
problem that the magnitude of the value increases without bound and therefore may
start to corrupt the embedding vector signiÔ¨Åcantly Also it may not generalize well
to new input sequences that are longer than those used in training, since these will
involve coding values that lie outside the range of those used in training Alterna-
tively we could assign a number in the range (0;1)to each token in the sequence,
which keeps the representation bounded However, this representation is not unique
for a given position as it depends on the overall sequence length An ideal positional encoding should provide a unique representation for each
position, it should be bounded, it should generalize to longer sequences, and it should
have a consistent way to express the number of steps between any two input vectors
irrespective of their absolute position because the relative position of tokens is often
more important than the absolute position There are many approaches to positional encoding (Dufter, Schmitt, and Sch ¬®utze,
2021) Here we describe a technique based on sinusoidal functions introduced by
Vaswani et al For a given position nthe associated position-encoding vec-
tor has components rnigiven by
rni=Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥sin/parenleftBign
Li=D/parenrightBig
; ifiis
even;
cos/parenleftBign
L(
i‚àí1)=D/parenrightBig
;ifiis odd.(12.25)
We see that the elements of the embedding vector rnare given by a series of sine and
cosine functions of steadily increasing wavelength, as illustrated in Figure 12.10(a) Attention 373
r6r5r4r3r2r1
embedding dimensionnmposition
(a)
embedding dimensionposition
‚àí101
 (b)
Figure 12.10 Illustrations of the functions deÔ¨Åned by (12.25) and used to construct position-encoding vectors (a) A plot in which the horizontal axis shows the different components of the embedding vector rwhereas the
vertical axis shows the position in the sequence The values of the vector elements for two positions nandmare
shown by the intersections of the sine and cosine curves with the horizontal grey lines (b) A heat map illustration
of the position-encoding vectors deÔ¨Åned by (12.25) for dimension D= 100 withL= 30 for the Ô¨Årst N= 200
positions

============================================================

=== CHUNK 367 ===
Palavras: 351
Caracteres: 2198
--------------------------------------------------
TRANSFORMERS
This encoding has the property that the elements of the vector rnall lie in the
range (‚àí1; 1) It is reminiscent of the way binary numbers are represented, with the
lowest order bit alternating with high frequency, and subsequent bits alternating with
steadily decreasing frequencies:
1:0001
2:0010
3:0011
4:0100
5:0101
6:0110
7:0111
8:1000
9:1001
For the encoding given by (12.25), however, the vector elements are continuous
variables rather than binary A plot of the position-encoding vectors is shown in
Figure 12.10(b) One nice property of the sinusoidal representation given by (12.25) is that, for
any Ô¨Åxed offset k, the encoding at position n+kcan be represented as a linear
combination of the encoding at position n, in which the coefÔ¨Åcients do not depend Exercise 12.10
on the absolute position but only on the value of k The network should therefore be
able to learn to attend to relative positions Note that this property requires that the
encoding makes use of both sine and cosine functions Another popular approach to positional representation is to use learned position
encodings This is done by having a vector of weights at each token position that
can be learned jointly with the rest of the model parameters during training, and
avoids using hand-crafted representations Because the parameters are not shared
between the token positions, the tokens are no longer invariant under a permutation,
which is the purpose of a positional encoding However, this approach does not
meet the criteria we mentioned earlier of generalizing to longer input sequences,
as the encoding will be untrained for positional encodings not seen during training Therefore, this approach is generally most suitable when the input length is relatively
constant during both training and inference Natural
Language
No
w that we have studied the architecture of the transformer, we will explore how
this can be used to process language data consisting of words, sentences, and para-
graphs Although this is the modality that transformers were originally developed to
operate on, they have proved to be a very general class of models and have become
the state-of-the-art for most input data types

============================================================

=== CHUNK 368 ===
Palavras: 350
Caracteres: 2087
--------------------------------------------------
Later in this chapter we will look at
their use in other domains Section 12.4
Many languages, including English, comprise a series of words separated by
white space, along with punctuation symbols, and therefore represent an example of
12.2 Natural Language 375
sequential data For the moment we will focus on the words, and we will return to Section 11.3
punctuation later Section 12.2.2
The Ô¨Årst challenge is to convert the words into a numerical representation that
is suitable for use as the input to a deep neural network One simple approach is to
deÔ¨Åne a Ô¨Åxed dictionary of words and then introduce vectors of length equal to the
size of the dictionary along with a ‚Äòone hot‚Äô representation for each word, in which
thekth word in the dictionary is encoded with a vector having a 1in positionkand
0in all other positions For example if ‚Äòaardwolf‚Äô is the third word in our dictionary
then its vector representation would be (0;0;1;0;:::; 0) An obvious problem with a one-hot representation is that a realistic dictionary
might have several hundred thousand entries leading to vectors of very high dimen-
sionality Also, it does not capture any similarities or relationships that might exist
between words Both issues can be addressed by mapping the words into a lower-
dimensional space through a process called word embedding in which each word is
represented as a dense vector in a space of typically a few hundred dimensions 12.2.1 Word embedding
The embedding process can be deÔ¨Åned by a matrix Eof sizeD√óKwhere
Dis the dimensionality of the embedding space and Kis the dimensionality of the
dictionary For each one-hot encoded input vector xnwe can then calculate the
corresponding embedding vector using
vn=Exn: (12.26)
Because xnhas a one-hot encoding, the vector vnis simply given by the correspond-
ing column of the matrix E We can learn the matrix Efrom a corpus (i.e., a large data set) of text, and
there are many approaches to doing this Here we look at a popular technique called
word2vec (Mikolov et al., 2013), which can be viewed as a simple two-layer neural
network

============================================================

=== CHUNK 369 ===
Palavras: 370
Caracteres: 2232
--------------------------------------------------
A training set is constructed in which each sample is obtained by consid-
ering a ‚Äòwindow‚Äô of Madjacent words in the text, where a typical value might be
M= 5 The samples are considered to be independent, and the error function is de-
Ô¨Åned as the sum of the error functions for each sample There are two variants of this
approach In continuous bag of words, the target variable for network training is the
middle word, and the remaining context words form the inputs, so that the network
is being trained to ‚ÄòÔ¨Åll in the blank‚Äô A closely related approach, called skip-grams,
reverses the inputs and outputs, so that the centre word is presented as the input and
the target values are the context words These models are illustrated in Figure 12.11 This training procedure can be viewed as a form of self-supervised learning since
the data consists simply of a large corpus of unlabelled text from which many small
windows of word sequences are drawn at random Labels are obtained from the text
itself by ‚Äòmasking‚Äô out those words whose values the network is trying to predict Once the model is trained, the embedding matrix Eis given by the transpose
of the second-layer weight matrix for the continuous bag-of-words approach and
by the Ô¨Årst-layer weight matrix for skip-grams Words that are semantically related
are mapped to nearby positions in the embedding space This is to be expected
376 12 TRANSFORMERS
xn‚àí2xn‚àí1xn+1xn+2
v
xn
(a)xn‚àí2xn‚àí1xn+1xn+2
v
xn
(b)
Figure 12.11 Two-layer neural networks used to learn word embeddings, where (a) shows the continuous
bag-of-words approach, and (b) shows the skip-grams approach since related words are more likely to occur with similar context words compared
to unrelated words For example, the words ‚Äòcity‚Äô and ‚Äòcapital‚Äô might occur with
higher frequency as context for target words such as ‚ÄòParis‚Äô or ‚ÄòLondon‚Äô and less
frequently as context for ‚Äòorange‚Äô or ‚Äòpolynomial‚Äô The network can more easily
predict the probability of the missing words if ‚ÄòParis‚Äô and ‚ÄòLondon‚Äô are mapped to
nearby embedding vectors It turns out that the learned embedding space often has an even richer semantic
structure than just the proximity of related words, and that this allows for simple
vector arithmetic

============================================================

=== CHUNK 370 ===
Palavras: 374
Caracteres: 2193
--------------------------------------------------
For example, the concept that ‚ÄòParis is to France as Rome is to
Italy‚Äô can be expressed through operations on the embedding vectors If we use
v(word) to denote the embedding vector for ‚Äòword‚Äô, then we Ô¨Ånd
v(Paris)‚àív(France) + v(Italy)/similarequalv(Rome): (12.27)
Word embeddings were originally developed as natural language processing
tools in their own right Today, they are more likely to be used as pre-processing
steps for deep neural networks In this regard they can be viewed as the Ô¨Årst layer
in a deep neural network They can be Ô¨Åxed using some standard pre-trained em-
bedding matrix, or they can be treated as an adaptive layer that is learned as part of
the overall end-to-end training of the system In the latter case the embedding layer
can be initialized either using random weight values or using a standard embedding
matrix Natural Language 377
Figure 12.12 An illustration of the process of
tokenizing natural language by analogy with byte
pair encoding In this example, the most fre-
quently occurring pair of characters is ‚Äòpe‚Äô, which
occurs four times, and so these form a new to-
ken that replaces all the occurrences of ‚Äòpe‚Äô Note that ‚ÄòPe‚Äô is not included in this since upper-
case ‚ÄòP‚Äô and lower-case ‚Äòp‚Äô are distinct charac-
ters Next the pair ‚Äòck‚Äô is added since this occurs
three times This is followed by tokens such as
‚Äòpi‚Äô, ‚Äòed‚Äô, and ‚Äòper‚Äô, all of which occur twice, and
so on Peter Piper picked a peck of pickled peppers
Peter Pi per picked a peck of pickled pe ppers
Peter Pi per pi cked a peck of pi ckled peppers
Peter Pi per picked a peck of pickled peppers
Peter Pi per pickeda peck of pickledpeppers
Peter Pi perpickeda pe ckof pickledpeppers
12.2.2 Tokenization
One problem with using a Ô¨Åxed dictionary of words is that it cannot cope with
words not in the dictionary or which are misspelled It also does not take account
of punctuation symbols or other character sequences such as computer code An
alternative approach that addresses these problems would be to work at the level of
characters instead of using words, so that our dictionary comprises upper-case and
lower-case letters, numbers, punctuation, and white-space symbols such as spaces
and tabs

============================================================

=== CHUNK 371 ===
Palavras: 368
Caracteres: 2245
--------------------------------------------------
A disadvantage of this approach, however, is that it discards the semanti-
cally important word structure of language, and the subsequent neural network would
have to learn to reassemble words from elementary characters It would also require
a much larger number of sequential steps for a given body of text, thereby increasing
the computational cost of processing the sequence We can combine the beneÔ¨Åts of character-level and word-level representations
by using a pre-processing step that converts a string of words and punctuation sym-
bols into a string of tokens, which are generally small groups of characters and might
include common words in their entirety, along with fragments of longer words as
well as individual characters that can be assembled into less common words (Schus-
ter and Nakajima, 2012) This tokenization also allows the system to process other
kinds of sequences such as computer code or even other modalities such as images Section 12.4.1
It also means that variations of the same word can have related representations For
example, ‚Äòcook‚Äô, ‚Äòcooks‚Äô, ‚Äòcooked‚Äô, ‚Äòcooking‚Äô, and ‚Äòcooker‚Äô are all related and share
the common element ‚Äòcook‚Äô, which itself could be represented as one of the tokens There are many approaches to tokenization As an example, a technique called
byte pair encoding that is used for data compression, can be adapted to text tokeniza-
tion by merging characters instead of bytes (Sennrich, Haddow, and Birch, 2015) The process starts with the individual characters and iteratively merges them into
longer strings The list of tokens is Ô¨Årst initialized with the list of individual char-
acters Then a body of text is searched for the most frequently occurring adjacent
pairs of tokens and these are replaced with a new token To ensure that words are not
merged, a new token is not formed from two tokens if the second token starts with a
white space The process is repeated iteratively as illustrated in Figure 12.12 Initially the number of tokens is equal to the number of characters, which is
relatively small As tokens are formed, the total number of tokens increases, and
378 12 TRANSFORMERS
if this is continued long enough, the tokens will eventually correspond to the set of
words in the text

============================================================

=== CHUNK 372 ===
Palavras: 364
Caracteres: 2365
--------------------------------------------------
The total number of tokens is generally Ô¨Åxed in advance, as a
compromise between character-level and word-level representations The algorithm
is stopped when this number of tokens is reached In practical applications of deep learning to natural language, the input text is
typically Ô¨Årst mapped into a tokenized representation However, for the remainder of
this chapter, we will use word-level representations as this makes it easier to illustrate
and motivate key concepts 12.2.3 Bag of words
We now turn to the task of modelling the joint distribution p(x1;:::;xN)of an
ordered sequence of vectors, such as words (or tokens) in a natural language The
simplest approach is to assume that the words are drawn independently from the
same distribution and hence that the joint distribution is fully factorized in the form
p(x1;:::;xN) =N/productdisplay
n=1p(xn): (12.28)
This can be expressed as a probabilistic graphical model in which the nodes are
isolated with no interconnecting links Figure 11.28
The distribution p(x) is shared across the variables and can be represented, with-
out loss of generality, as a simple table listing the probabilities of each of the possi-
ble states of x(corresponding to the dictionary of words or tokens) The maximum
likelihood solution for this model is obtained simply by setting each of these proba-
bilities to the fraction of times that the word occurs in the training set This is known Exercise 12.11
as abag-of-words model because it completely ignores the ordering of the words We can use the bag-of-words approach to construct a simple text classiÔ¨Åer This
could be used for example in sentiment analysis in which a passage of text represent-
ing a restaurant review is to be classiÔ¨Åed as positive or negative The naive Bayes
classiÔ¨Åer assumes that the words are independent within each class Ck, but with a
different distribution for each class, so that
p(x1;:::;xN|Ck) =N/productdisplay
n=1p(xn|Ck): (12.29)
Given prior class probabilities p(Ck), the posterior class probabilities for a new se-
quence are given by:
p(Ck|x1;:::;xN)‚àùp(Ck)N/productdisplay
n=1p(xn|Ck): (12.30)
Both the class-conditional densities p(x|Ck)and the prior probabilities p(Ck)can
be estimated using frequencies from the training data set For a new sequence, the
table entries are multiplied together to get the desired posterior probabilities

============================================================

=== CHUNK 373 ===
Palavras: 362
Caracteres: 2287
--------------------------------------------------
Note
that if a word occurs in the test set that was not present in the training set then the
12.2 Natural Language 379
corresponding probability estimate will be zero, and so these estimates are typically
‚Äòsmoothed‚Äô after training by reassigning a small level of probability uniformly across
all entries to avoid zero values 12.2.4 Autoregressive models
One obvious major limitation of the bag-of-words model is that it completely
ignores word order To address this we can take an autoregressive approach Without
loss of generality we can decompose the distribution over the sequence of words into
a product of conditional distributions in the form
p(x1;:::;xN) =N/productdisplay
n=1p(xn|x1;:::;xn‚àí1): (12.31)
This can be represented as a probabilistic graphical model in which each node in the
sequence receives a link from every previous node We could represent each term Figure 11.27
on the right-hand side of (12.31) by a table whose entries are once again estimated
using simple frequency counts from the training set However, the size of these
tables grows exponentially with the length of the sequence, and so this approach Exercise 12.12
would become prohibitively expensive We can simplify the model dramatically by assuming that each of the condi-
tional distributions on the right-hand side of (12.31) is independent of all previous
observations except the Lmost recent words For example, if L= 2 then the joint
distribution for a sequence of Nobservations under this model is given by
p(x1;:::;xN) =p(x1)p(x 2|x1)N/productdisplay
n=3p(xn|xn‚àí1;xn‚àí2): (12.32)
In the corresponding graphical model each node has links from the two previous
nodes Here we assume that the conditional distributions p(xn|xn‚àí1)are shared Figure 11.30
across all variables Again each of the distributions on the right-hand side of (12.32)
can be represented as tables whose values are estimated from the statistics of triplets
of successive words drawn from a training corpus The case with L= 1 is known as a bi-gram model because it depends on pairs
of adjacent words Similarly L= 2, which involves triplets of adjacent words, is
called a tri-gram model, and in general these are called n-gram models All the models discussed so far in this section can be run generatively to synthe-
size novel text

============================================================

=== CHUNK 374 ===
Palavras: 378
Caracteres: 2312
--------------------------------------------------
For example, if we provide the Ô¨Årst and second words in a sequence,
then we can sample from the tri-gram statistics p(xn|xn‚àí1;xn‚àí2)to generate the
third word, and then we can use the second and third words to sample the fourth
word, and so on The resulting text, however, will be incoherent because each word
is predicted only on the basis of the two previous words High-quality text models
must take account of the long-range dependencies in language On the other hand,
we cannot simply increase the value of Lbecause the size of the probability tables
grows exponentially in Lso that it is prohibitively expensive to go much beyond
tri-gram models However, the autoregressive representation will play a central role
380 12 TRANSFORMERS
Figure 12.13 A general RNN with parame-
tersw It takes a sequence x1;:::;xNas input
and generates a sequence y1;:::;yNas out-
put Each of the boxes corresponds to a multi-
layer network with nonlinear hidden units z0 w w wy1 y2 y3
x1 x2 x3z1 z2
when
we consider modern language models based not on probability tables but on
deep neural networks conÔ¨Ågured as transformers One way to allow longer-range dependencies, while avoiding the exponential
growth in the number of parameters of an n-gram model, is to use a hidden Markov
model whose graphical structure is shown in Figure 11.31 The number of learn- Section 11.3.1
able parameters is governed by the dimensionality of the latent variables whereas
the distribution over a given observation xndepends, in principle, on all previous
observations However, the inÔ¨Çuence of more distant observations is still very lim-
ited since their effect must be carried through the chain of latent states which are
themselves being updated by more recent observations 12.2.5 Recurrent neural networks
Techniques such as n-grams have very poor scaling with sequence length be-
cause they store completely general tables of conditional distributions We can
achieve much better scaling by using parameterized models based on neural net-
works Suppose we simply apply a standard feed-forward neural network to se-
quences of words in natural language One problem that arises is that the network
has a Ô¨Åxed number of inputs and outputs, whereas we need to be able to handle se-
quences in the training and test sets that have variable length

============================================================

=== CHUNK 375 ===
Palavras: 351
Caracteres: 2138
--------------------------------------------------
Furthermore, if a word,
or group of words, at a particular location in a sequence represents some concept then
the same word, or group of words, at a different location is likely to represent the
same concept at that new location This is reminiscent of the equivariance property
we encountered in processing image data If we can construct a network architecture Chapter 10
that is able to share parameters across the sequence then not only can we capture this
equivariance property but we can greatly reduce the number of free parameters in
the model as well as handle sequences having different lengths To address this we can borrow inspiration from the hidden Markov model and
introduce an explicit hidden variable znassociated with each step nin the sequence The neural network takes as input both the current word xnand the current hidden
statezn‚àí1and produces an output word ynas well as the next state znof the hidden
variable We can then chain together copies of this network, in which the weight
values are shared across the copies The resulting architecture is called a recurrent
neural network (RNN) and is illustrated in Figure 12.13 Here the initial value of
the hidden state may be initialized for example to some default value such as z0=
12.2 Natural Language 381
z0 w w w w w w wIk ben gelukkig/angbracketleft
stop/angbracketright
I am happy/angbracketleftstart/angbracketright Ik ben gelukiggz‚àó
encoder decoder
Figure
12.14 An example of a recurrent neural network used for language translation See the text for details As an example of how an RNN might be used in practice, consider the spe-
ciÔ¨Åc task of translating sentences from English into Dutch The sentences can have
variable length, and each output sentence might have a different length from the cor-
responding input sentence Furthermore, the network may need to see the whole of
the input sentence before it can even start to generate the output sentence We can
address this using an RNN by feeding in the complete English sentence followed by
a special input token, which we denote by /angbracketleftstart/angbracketright, to trigger the start of translation

============================================================

=== CHUNK 376 ===
Palavras: 352
Caracteres: 2233
--------------------------------------------------
During training the network learns to associate /angbracketleftstart/angbracketright with the beginning of the
output sentence We also take each successively generated word and feed it into the
input at the next time step, as shown in Figure 12.14 The network can be trained
to generate a speciÔ¨Åc /angbracketleftstop/angbracketright token to signify the completion of the translation The
Ô¨Årst few stages of the network are used to absorb the input sequence, and the associ-
ated output vectors are simply ignored This part of the network can be viewed as an
‚Äòencoder‚Äô in which the entire input sentence has been compressed into the state z?of
the hidden variable The remaining network stages function as the ‚Äòdecoder‚Äô, which
generates the translated sentence as output one word at a time Notice that each out-
put word is fed as input to the next stage of the network, and so this approach has an
autoregressive structure analogous to (12.31) 12.2.6 Backpropagation through time
RNNs can be trained by stochastic gradient descent using gradients calculated
by backpropagation and evaluated through automatic differentiation, just as with reg-
ular neural networks The error function consists of a sum over all output units of
the error for each unit, in which each output unit has a softmax activation function
along with an associated cross-entropy error function During forward propagation, Section 5.4.4
the activation values are propagated all the way from the Ô¨Årst input in the sequence
through to all the output nodes in the sequence, and error signals are then backprop-
agated along the same paths This process is called backpropagation through time
382 12 TRANSFORMERS
and in principle is straightforward However, in practice, for very long sequences,
training can be difÔ¨Åcult due to the problems of vanishing gradients orexploding
gradients that arise with very deep network architectures Section 7.4.2
Another problem with standard RNNs is that they deal poorly with long-range
dependencies This is especially problematic for natural language where such depen-
dencies are widespread In a long passage of text, a concept might be introduced that
plays an important role in predicting words occurring much later in the text

============================================================

=== CHUNK 377 ===
Palavras: 359
Caracteres: 2249
--------------------------------------------------
In the
architecture shown in Figure 12.14, the entire concept of the English sentence must
be captured in the single hidden vector z?of Ô¨Åxed length, and this becomes increas-
ingly problematic with longer sequences This is known as the bottleneck problem
because a sequence of arbitrary length has to be summarized in a single hidden vec-
tor of activations and the network can start to generate the output translation only
once the full input sequence has been processed One approach for addressing both the vanishing and exploding gradients prob-
lems and the limited long-range dependencies is to modify the architecture of the
neural network to allow additional signal paths that bypass many of the processing
steps within each stage of the network and hence allow information to be remem-
bered over a larger number of time steps Long short-term memory (LSTM) models
(Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) models (Cho et
al., 2014) are the most widely known examples Although they improve performance
compared to standard RNNs, they still have a limited ability to model long-range de-
pendencies Also, the additional complexity of each cell means that LSTMs are even
slower to train than standard RNNs Furthermore, all recurrent networks have signal
paths that grow linearly with the number of steps in the sequence Moreover, they do
not support parallel computation within a single training example due to the sequen-
tial nature of the processing In particular, this means that RNNs struggle to make
efÔ¨Åcient use of modern highly parallel hardware based on GPUs These problems
are addressed by replacing RNNs with transformers T
ransformer Language Models
The
transformer processing layer is a highly Ô¨Çexible component for building pow-
erful neural network models with broad applicability In this section we explore the
application of transformers to natural language This has given rise to the develop-
ment of massive neural networks known as large language models (LLMs), which
have proven to be exceptionally capable (Zhao et al., 2023) Transformers can be applied to many different kinds of language processing
task, and can be grouped into three categories according to the form of the input
and output data

============================================================

=== CHUNK 378 ===
Palavras: 351
Caracteres: 2126
--------------------------------------------------
In a problem such as sentiment analysis, we take a sequence of
words as input and provide a single variable representing the sentiment of the text,
for example happy or sad, as output Here a transformer is acting as an ‚Äòencoder‚Äô
of the sequence Other problems might take a single vector as input and generate
a word sequence as output, for example if we wish to generate a text caption given
an input image In such cases the transformer functions as a ‚Äòdecoder‚Äô, generating
12.3 Transformer Language Models 383
a sequence as output Finally, in sequence-to-sequence processing tasks, both the
input and the output comprise a sequence of words, for example if our goal is to
translate from one language to another In this case, transformers are used in both
encoder and decoder roles We discuss each of these classes of language model in
turn, using illustrative examples of model architectures 12.3.1 Decoder transformers
We start by considering decoder-only transformer models These can be used as
generative models that create output sequences of tokens As an illustrative example,
we will focus on a class of models called GPT which stands for generative pre-
trained transformer (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) The
goal is to use the transformer architecture to construct an autoregressive model of the
form deÔ¨Åned by (12.31) in which the conditional distributions p(xn|x1;:::;xn‚àí1)
are expressed using a transformer neural network that is learned from data The model takes as input a sequence consisting of the Ô¨Årst n‚àí1tokens, and
its corresponding output represents the conditional distribution for token n If we
draw a sample from this distribution then we have extended the sequence to ntokens
and this new sequence can be fed back through the model to give a distribution over
tokenn+ 1, and so on The process can be repeated to generate sequences up to
a maximum length determined by the number of inputs to the transformer We will
shortly discuss strategies for sampling from the conditional distributions, but for the Section 12.3.2
moment we focus on how to construct and train the network

============================================================

=== CHUNK 379 ===
Palavras: 357
Caracteres: 2331
--------------------------------------------------
The architecture of a GPT model consists of a stack of transformer layers that
take a sequence x1;:::;xNof tokens, each of dimensionality D, as input and pro-
duce a sequence /tildewidex1;:::;/tildewidexNof tokens, again of dimensionality D, as output Each
output needs to represent a probability distribution over the dictionary of tokens at
that time step, and this dictionary has dimensionality Kwhereas the tokens have a
dimensionality of D We therefore make a linear transformation of each output to-
ken using a matrix W(p)of dimensionality D√óKfollowed by a softmax activation
function in the form
Y= Softmax/parenleftBig
/tildewideXW(p)/parenrightBig
(12.33)
where Yis a matrix whose nth row is yT
n, and/tildewideXis a matrix whose nth row is
/tildewidexT
n Each softmax output unit has an associated cross-entropy error function The Section 5.4.4
architecture of the model is shown in Figure 12.15 The model can be trained using a large corpus of unlabelled natural language
by taking a self-supervised approach Each training sample consists of a sequence
of tokens x1;:::;xn, which form the input to the network, along with an associated
target value xn+1consisting of the next token in the sequence The sequences are
considered to be independent and identically distributed so that the error function
used for training is the sum of the cross-entropy error values summed over the train-
ing set, grouped into appropriate mini-batches Naively we could process each such
training sample independently using a forward pass through the model However,
we can achieve much greater efÔ¨Åciency by processing an entire sequence at once so
that each token acts both as a target value for the sequence of previous tokens and as
384 12 TRANSFORMERS
/angbracketleftstart/angbracketright x1 xNembedding embedding embedding ...+ + +positional
encodingmasked transformer layer ...masked transformer layerLSM LSM LSMy1 y2 yN+1 Llay
ers
Figure 12.15 Architecture
of a GPT decoder transformer network Here ‚ÄòLSM‚Äô stands for linear-softmax and
denotes a linear transformation whose learnable parameters are shared across the token positions, followed by
a softmax activation function Masking is explained in the text an input value for subsequent tokens For example, consider the word sequence
I swam across the river to get to the other bank

============================================================

=== CHUNK 380 ===
Palavras: 357
Caracteres: 2182
--------------------------------------------------
We can use ‚ÄòI swam across‚Äô as an input sequence with an associated target of ‚Äòthe‚Äô,
and also use ‚ÄòI swam across the‚Äô as an input sequence with an associated target of
‚Äòriver‚Äô, and so on However, to process these in parallel we have to ensure that the
network is not able to ‚Äòcheat‚Äô by looking ahead in the sequence, otherwise it will
simply learn to copy the next input directly to the output If it did this, it would
then be unable to generate new sequences since the subsequent token by deÔ¨Ånition is
not available at test time To address this problem we do two things First, we shift
the input sequence to the right by one step, so that input xncorresponds to output
yn+1, with target xn+1, and an additional special token denoted /angbracketleftstart/angbracketright is pre-
pended in the Ô¨Årst position of the input sequence Second, note that the tokens in a
transformer are processed independently, except when they are used to compute the
attention weights, when they interact in pairs through the dot product We therefore
introduce masked attention, sometimes called causal attention, into each of the at-
12.3 Transformer Language Models 385
Figure 12.16 An illustration of the mask matrix
for masked self-attention Atten-
tion weights corresponding to the
red elements are set to zero Thus,
in predicting the token ‚Äòacross‚Äô,
the output can depend only on
the input tokens ‚Äò/angbracketleftstart/angbracketright‚Äô ‚ÄòI‚Äô and
‚Äòswam‚Äô.I
s
wam
across
the
river
/angbracketleftstart/angbracketright
I
swam
across
the
inputsoutputs
tention layers, in
which we set to zero all of the attention weights that correspond to
a token attending to any later token in the sequence This simply involves setting to
zero all the corresponding elements of the attention matrix Attention(Q; K;V)de-
Ô¨Åned by (12.14) and then normalizing the remaining elements so that each row once
again sums to one In practice, this can be achieved by setting the corresponding
pre-activation values to ‚àí‚àû so that the softmax evaluates to zero for the associated
outputs and also takes care of the normalization across the non-zero outputs The
structure of the masked attention matrix is illustrated in Figure 12.16

============================================================

=== CHUNK 381 ===
Palavras: 350
Caracteres: 2091
--------------------------------------------------
In practice, we wish to make efÔ¨Åcient use of the massive parallelism of GPUs,
and hence multiple sequences may be stacked together into an input tensor for par-
allel processing in a single batch However, this requires the sequences to be of the
same length, whereas text sequences naturally have variable length This can be ad-
dressed by introducing a speciÔ¨Åc token, which we denote by /angbracketleftpad/angbracketright, that is used to
Ô¨Åll unused positions to bring all sequences up to the same length so that they can
be combined into a single tensor An additional mask is then used in the attention
weights to ensure that the output vectors do not pay attention to any inputs occupied
by the/angbracketleftpad/angbracketright token Note that the form of this mask depends on the particular input
sequence The output of the trained model is a probability distribution over the space of
tokens, given by the softmax output activation function, which represents the prob-
ability of the next token given the current token sequence Once this next word is
chosen, the token sequence with the new token included can then be fed through the
model again to generate the subsequent token in the sequence, and this process can
be repeated indeÔ¨Ånitely or until an end-of-sequence token is generated This may ap-
pear to be quite inefÔ¨Åcient since data must be fed through the whole model for each
new generated token However, note that due to the masked attention, the embedding
learned for a particular token depends only on that token itself and on earlier tokens
386 12 TRANSFORMERS
and hence does not change when a new, later token is generated Consequently, much
of the computation can be recycled when processing a new token 12.3.2 Sampling strategies
We have seen that the output of a decoder transformer is a probability distribu-
tion over values for the next token in the sequence, from which a particular value
for that token must be chosen to extend the sequence There are several options for
selecting the value of the token based on the computed probabilities (Holtzman et
al., 2019)

============================================================

=== CHUNK 382 ===
Palavras: 355
Caracteres: 2225
--------------------------------------------------
One obvious approach, called greedy search, is simply to select the token
with the highest probability This has the effect of making the model deterministic,
in that a given input sequence always generates the same output sequence Note that
simply choosing the highest probability token at each stage is not the same as select-
ing the highest probability sequence of tokens To Ô¨Ånd the most probable sequence, Exercise 12.15
we would need to maximize the joint distribution over all tokens, which is given by
p(y1;:::;yN) =N/productdisplay
n=1p(yn|y1;:::;yn‚àí1): (12.34)
If there areNsteps in the sequence and the number of token values in the dictionary
isKthen the total number of sequences is O(KN), which grows exponentially with
the length of the sequence, and hence Ô¨Ånding the single most probable sequence is
infeasible By comparison, greedy search has cost O(KN ), which is linear in the
sequence length One technique that has the potential to generate higher probability sequences
than greedy search is called beam search Instead of choosing the single most proba-
ble token value at each step, we maintain a set of Bhypotheses, where Bis called the
beam width, each consisting of a sequence of token values up to step n We then feed
all these sequences through the network, and for each sequence we Ô¨Ånd the Bmost
probable token values, thereby creating B2possible hypotheses for the extended
sequence This list is then pruned by selecting the most probable Bhypotheses ac-
cording to the total probability of the extended sequence Thus, the beam search
algorithm maintains Balternative sequences and keeps track of their probabilities,
Ô¨Ånally selecting the most probable sequence amongst those considered Because the
probability of a sequence is obtained by multiplying the probabilities at each step of
the sequence and since these probability are always less than or equal to one, a long
sequence will generally have a lower probability than a short one, biasing the results
towards short sequences For this reason the sequence probabilities are generally
normalized by the corresponding lengths of the sequence before making compar-
isons Beam search has cost O(BKN ), which is again linear in the sequence length

============================================================

=== CHUNK 383 ===
Palavras: 362
Caracteres: 2219
--------------------------------------------------
However, the cost of generating a sequence is increased by a factor of B, and so for
very large language models, where the cost of inference can become signiÔ¨Åcant, this
makes beam search much less attractive One problem with approaches such as greedy search and beam search is that they
limit the diversity of potential outputs and can even cause the generation process to
become stuck in a loop, where the same sub-sequence of words is repeated over and
12.3 Transformer Language Models 387
Figure 12.17 A comparison of the token probabilities from beam search and human text for a given trained
transformer language model and a given initial input sequence, showing how the human sequence has much
lower token probabilities (2019) with permission.]
over As can be seen in Figure 12.17, human-generated text may have lower proba-
bility and hence be more surprising with respect to a given model than automatically
generated text Instead of trying to Ô¨Ånd a sequence with the highest probability, we can instead
generate successive tokens simply by sampling from the softmax distribution at each
step However, this can lead to sequences that are nonsensical This arises from
the typically very large size of the token dictionary, in which there is a long tail of
many token states each of which has a very small probability but which in aggregate
account for a signiÔ¨Åcant fraction of the total probability mass This leads to the
problem in which there is a signiÔ¨Åcant chance that the system will make a bad choice
for the next token As a balance between these extremes, we can consider only the states having the
topKprobabilities, for some choice of K, and then sample from these according to
their renormalized probabilities A variant of this approach, called top-p sampling
ornucleus sampling, calculates the cumulative probability of the top outputs until a
threshold is reached and then samples from this restricted set of token states A ‚Äòsofter‚Äô version of top-K sampling is to introduce a parameter Tcalled tem-
perature into the deÔ¨Ånition of the softmax function (Hinton, Vinyals, and Dean,
2015) so that
yi=exp(ai=T)/summationtext
jexp(aj=T)(12.35)
and then sample the next token from this modiÔ¨Åed distribution

============================================================

=== CHUNK 384 ===
Palavras: 355
Caracteres: 2304
--------------------------------------------------
When T= 0, the
probability mass is concentrated on the most probable state, with all other states
having zero probability, and hence this becomes greedy selection TRANSFORMERS
recover the unmodiÔ¨Åed softmax distribution, and as T‚Üí‚àû, the distribution be-
comes uniform across all states By choosing a value in the range 0< T < 1, the
probability is concentrated towards the higher values One challenge with sequence generation is that during the learning phase, the
model is trained on a human-generated input sequence, whereas when it is running
generatively, the input sequence is itself generated from the model This means that
the model can drift away from the distribution of sequences seen during training 12.3.3 Encoder transformers
We next consider transformer language models based on encoders, which are
models that take sequences as input and produce Ô¨Åxed-length vectors, such as class
labels, as output An example of such a model is BERT, which stands for bidirec-
tional encoder representations from transformers (Devlin et al., 2018) The goal is
to pre-train a language model using a large corpus of text and then to Ô¨Åne-tune the
model using transfer learning for a broad range of downstream tasks each of which
requires a smaller application-speciÔ¨Åc training data set The architecture of an en-
coder transformer is illustrated in Figure 12.18 This approach is a straightforward
application of the transformer layers discussed previously Section 12.1.7
The Ô¨Årst token of every input string is given by a special token /angbracketleftclass/angbracketright, and the
corresponding output of the model is ignored during pre-training Its role will be-
come apparent when we discuss Ô¨Åne-tuning The model is pre-trained by presenting
token sequences at the input A randomly chosen subset of the tokens, say 15%, are
replaced with a special token denoted /angbracketleftmask/angbracketright The model is trained to predict the
missing tokens at the corresponding output nodes This is analogous to the masking
used in word2vec to learn word embeddings For example, an input sequence might Section 12.2.1
be
I/angbracketleftmask/angbracketright across the river to get to the /angbracketleftmask/angbracketright bank and the network should predict ‚Äòswam‚Äô at output node 2 and ‚Äòother‚Äô at output node
10

============================================================

=== CHUNK 385 ===
Palavras: 355
Caracteres: 2209
--------------------------------------------------
In this case only two of the outputs contribute to the error function and the other
outputs are ignored The term ‚Äòbidirectional‚Äô refers to the fact that the network sees words both be-
fore and after the masked word and can use both sources of information to make a
prediction As a consequence, unlike decoder models, there is no need to shift the
inputs to the right by one place, and there is no need to mask the outputs of each layer
from seeing input tokens occurring later in the sequence Compared to the decoder
model, an encoder is less efÔ¨Åcient since only a fraction of the sequence tokens are
used as training labels Moreover, an encoder model is unable to generate sequences The procedure of replacing randomly selected tokens with /angbracketleftmask/angbracketright means the
training set has a mismatch compared to subsequent Ô¨Åne-tuning sets in that the lat-
ter will not contain any /angbracketleftmask/angbracketright tokens To mitigate any problems this might cause,
Devlin et al (2018) modiÔ¨Åed the procedure slightly, so that of the 15% of randomly
selected tokens, 80% are replaced with /angbracketleftmask/angbracketright, 10% are replaced with a word se-
lected at random from the vocabulary, and in 10% of the cases, the original words
are retained at the input, but they still have to be correctly predicted at the output Transformer Language Models 389
/angbracketleftclass/angbracketright x1 xNembedding embedding embedding ...+ + +positional
encodingtransformer layer ...transformer layerLSM LSM LSMc y1 yN Llay
ers
Figure 12.18 Architecture
of an encoder transformer model The boxes labelled ‚ÄòLSM‚Äô denote a linear trans-
formation whose learnable parameters are shared across the token positions, followed by a softmax activation
function The main differences compared to the decoder model are that the input sequence is not shifted to the
right, and the ‚Äòlook ahead‚Äô masking matrix is omitted and therefore, within each self-attention layer, every output
token can attend to any of the input tokens Once the encoder model is trained it can then be Ô¨Åne-tuned for a variety of
different tasks To do this a new output layer is constructed whose form is speciÔ¨Åc
to the task being solved

============================================================

=== CHUNK 386 ===
Palavras: 364
Caracteres: 2290
--------------------------------------------------
For a text classiÔ¨Åcation task, only the Ô¨Årst output position
is used, which corresponds to the /angbracketleftclass/angbracketright token that always appears in the Ô¨Årst
position of the input sequence If this output has dimension Dthen a matrix of
parameters of dimension D√óK, whereKis the number of classes, is appended to
the Ô¨Årst output node and this in turn feeds into a K-dimensional softmax function
or a vector of dimension D√ó1followed by a logistic sigmoid for K= 2 The
linear output transformation could alternatively be replaced with a more complex
differentiable model such as an MLP If the goal is to classify each token of the
input string, for example to assign each token to a category (such as person, place,
colour, etc) then the Ô¨Årst output is ignored and the subsequent outputs have a shared
linear-plus-softmax layer During Ô¨Åne-tuning all model parameters including the
new output matrix are learned by stochastic gradient descent using the log probability
390 12 TRANSFORMERS
of the correct label Alternatively the output of a pre-trained model might feed into a
sophisticated generative deep learning model for applications such as text-to-image Chapter 20
synthesis 12.3.4 Sequence-to-sequence transformers
For completeness, we discuss brieÔ¨Çy the third category of transformer model,
which combines an encoder with a decoder, as discussed in the original transformer
paper of Vaswani et al Consider the task of translating an English sentence
into a Dutch sentence We can use a decoder model to generate the token sequence
corresponding to the Dutch output, token by token, as discussed previously The Section 12.3.1
main difference is that this output needs to be conditioned on the entire input se-
quence corresponding to the English sentence An encoder transformer can be used
to map the input token sequence into a suitable internal representation, which we
denote by Z To incorporate Zinto the generative process for the output sequence,
we use a modiÔ¨Åed form of the attention mechanism called cross attention This is the
same as self-attention except that although the query vectors come from the sequence
being generated, in this case the Dutch output sequence, the key and value vectors
come from the sequence represented by Z, as illustrated in Figure 12.19

============================================================

=== CHUNK 387 ===
Palavras: 353
Caracteres: 2173
--------------------------------------------------
Returning
to our analogy with a video streaming service, this would be like the user sending
their query vector to a different streaming company who then compares it with their
own set of key vectors to Ô¨Ånd the best match and then returns the associated value
vector in the form of a movie When we combine the encoder and decoder modules, we obtain the architecture
of the model shown in Figure 12.20 The model can be trained using paired input
and output sentences 12.3.5 Large language models
The most important recent development in the Ô¨Åeld of machine learning has
been the creation of very large transformer-based neural networks for natural lan-
guage processing, known as large language models or LLMs Here ‚Äòlarge‚Äô refers to
the number of weight and bias parameters in the network, which can number up to
around one trillion (1012) at the time of writing Such models are expensive to train,
and the motivation for building them comes from their extraordinary capabilities In addition to the availability of large data sets, the training of ever larger mod-
els has been facilitated by the advent of massively parallel training hardware based
on GPUs (graphics processing units) and similar processors tightly coupled in large
clusters equipped fast interconnect and lots of onboard memory The transformer
architecture has played a key role in the development of these models because it is
able to make very efÔ¨Åcient use of such hardware Very often, increasing the size of
the training data set, along with a commensurate increase in the number of model pa-
rameters, leads to improvements in performance that outpace architectural improve-
ments or other ways to incorporate more domain knowledge (Sutton, 2019; Kaplan
et al., 2020) For example, the impressive increase in performance of the GPT se-
ries of models (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) through
successive generations has come primarily from an increase in scale Transformer Language Models 391
Figure 12.19 Schematic illustration of one cross-
attention layer as used in the decoder
section of a sequence-to-sequence trans-
former Here Zdenotes the output from
the encoder section

============================================================

=== CHUNK 388 ===
Palavras: 367
Caracteres: 2526
--------------------------------------------------
Zdetermines the
key and value vectors for the cross-
attention layer, whereas the query vec-
tors are determined within the decoder
section Xmasked
multi-head
self-attentionadd & normmulti-head
cross-attentionadd & normMLPadd & normeX
Z
K V Q
of performance improvements have driven a new kind of Moore‚Äôs law in which the
number of compute operations required to train a state-of-the-art machine learning
model has grown exponentially since about 2012 with a doubling time of around 3.4
months Figure 1.16
Early language models were trained using supervised learning For example, to
build a translation system, the training set would consist of matched pairs of sen-
tences in two languages A major limitation of supervised learning, however, is
that the data typically has to be human-curated to provide labelled examples, and
this severely limits the quantity of data available, thereby requiring heavy use of
inductive biases such as feature engineering and architecture constraints to achieve
reasonable performance Large language models are trained instead by self-supervised learning on very
large data sets of text, along with potentially other token sequences such as computer
code We have seen how a decoder transformer can be trained on token sequences Section 12.3.1
in which each token acts as a labelled target example, with the preceding sequence
as input, to learn a conditional probability distribution This ‚Äòself-labelling‚Äô hugely
expands the quantity of training data available and therefore allows exploitation of
deep neural networks having large numbers of parameters TRANSFORMERS
Xembedding+self-attention
transformer layer...self-attention
transformer layer
cross-attention
transformer layer...cross-attention
transformer layerLSMYN
+
embedding
{/angbracketleft
start/angbracketright; Y1:N‚àí1}Z
positional
encoding
encoder decoder
Figure
12.20 Schematic illustration of a sequence-to-sequence transformer To keep the diagram uncluttered
the input tokens are collectively shown as a single box, and likewise for the output tokens Positional-encoding
vectors are added to the input tokens for both the encoder and decoder sections Each layer in the encoder
corresponds to the structure shown in Figure 12.9, and each cross-attention layer is of the form shown in Fig-
ure 12.19 This use of self-supervised learning led to a paradigm shift in which a large
model is Ô¨Årst pre-trained using unlabelled data and then subsequently Ô¨Åne-tuned
using supervised learning based on a much smaller set of labelled data

============================================================

=== CHUNK 389 ===
Palavras: 357
Caracteres: 2231
--------------------------------------------------
This is
effectively a form of transfer learning, and the same pre-trained model can be used
for multiple ‚Äòdownstream‚Äô applications A model with broad capabilities that can be
subsequently Ô¨Åne-tuned for speciÔ¨Åc tasks is called a foundation model (Bommasani
et al., 2021) The Ô¨Åne-tuning can be done by adding extra layers to the outputs of the network
or by replacing the last few layers with fresh parameters and then using the labelled
data to train these Ô¨Ånal layers During the Ô¨Åne-tuning stage, the weights and biases
in the main model can either be left unchanged or be allowed to undergo small levels
of adaptation Typically the cost of the Ô¨Åne-tuning is small compared to that of pre-
training One very efÔ¨Åcient approach to Ô¨Åne-tuning is called low-rank adaptation or LoRA
(Huet al., 2021) This approach is inspired by results which show that a trained over-
parameterized model has a low intrinsic dimensionality with respect to Ô¨Åne-tuning,
meaning that changes in the model parameters during Ô¨Åne-tuning lie on a manifold
12.3 Transformer Language Models 393
√ó W0
D√óD
A
D√óR√ó B
R√óD√ó+XW 0
+
XAB
N√óDX
N√óD
Figure
12.21 Schematic illustration low-rank adaptation showing a weight matrix W0from one of the
attention layers in a pre-trained transformer Additional weights given by matrices Aand
Bare adapted during Ô¨Åne-tuning and their product ABis then added to the original matrix
for subsequent inference whose dimensionality is much smaller than the total number of learnable parameters
in the model (Aghajanyan, Zettlemoyer, and Gupta, 2020) LoRa exploits this by
freezing the weights of the original model and adding additional learnable weight
matrices into each layer of the transformer in the form of low-rank products Typi-
cally only attention-layer weights are modiÔ¨Åed, whereas MLP-layer weights are kept
Ô¨Åxed Consider a weight matrix W0having dimension D√óD, which might rep-
resent a query, key, or value matrix in which the matrices from multiple attention
heads are treated together as a single matrix We introduce a parallel set of weights
deÔ¨Åned by the product of two matrices AandBwith dimensions D√óRandR√óD,
respectively, as shown schematically in Figure 12.21 This layer then generates an
output given by XW 0+XAB

============================================================

=== CHUNK 390 ===
Palavras: 351
Caracteres: 2177
--------------------------------------------------
The number of parameters in the additional weight
matrix AB is2RD compared to the D2parameters in the original weight matrix
W0, and so ifR/lessmuchDthen the number of parameters that need to be adapted during
Ô¨Åne-tuning is much smaller than the number in the original transformer In prac-
tice, this can reduce the number of parameters that need to be trained by a factor of
10,000 Once the Ô¨Åne-tuning is complete, the additional weights can be added to the
original weight matrices to give a new weight matrix
/hatwiderW=W0+AB (12.36)
so that during inference there is no additional computational overhead compared to
running the original model since the updated model has the same size as the original As language models have become larger and more powerful, the need for Ô¨Åne-
tuning has diminished, with generative language models now able to solve a broad
range of tasks simply through text-based interaction For example, if a text string
English: the cat sat on the mat French:
is given as the input sequence, an autoregressive language model can continue to gen-
erate subsequent tokens until a /angbracketleftstop/angbracketright token is generated, in which the newly gen-
394 12 TRANSFORMERS
erated tokens represent the French translation Note that the model was not trained
speciÔ¨Åcally to do translation but has learned to do so as a result of being trained on a
vast corpus of data that includes multiple languages A user can interact with such models using a natural language dialogue, mak-
ing them very accessible to broad audiences To improve the user experience and
the quality of the generated outputs, techniques have been developed for Ô¨Åne-tuning
large language models through human evaluation of generated output, using methods
such as reinforcement learning through human feedback or RLHF (Christiano et al.,
2017) Such techniques have helped to create large language models with impres-
sively easy-to-use conversational interfaces, most notably the system from OpenAI
called ChatGPT The sequence of input tokens given by the user is called a prompt For example,
it might consist of the opening words of a story, which the model is required to com-
plete

============================================================

=== CHUNK 391 ===
Palavras: 352
Caracteres: 2212
--------------------------------------------------
Or it might comprise a question, and the model should provide the answer By
using different prompts, the same trained neural network may be capable of solving a
broad range of tasks such as generating computer code from a simple text request or
writing rhyming poetry on demand The performance of the model now depends on
the form of the prompt, leading to a new Ô¨Åeld called prompt engineering (Liu et al.,
2021), which aims to design a good form for a prompt that results in high-quality
output for the downstream task The behaviour of the model can also be modiÔ¨Åed by
adapting the user‚Äôs prompt before feeding it into the language model by pre-pending
an additional token sequence called a preÔ¨Åx prompt to the user prompt to modify
the form of the output For example, the pre-prompt might consist of instructions,
expressed in standard English, to tell the network not to include offensive language
in its output This allows the model to solve new tasks simply by providing some examples
within the prompt, without needing to adapt the parameters of the model This is an
example of few-shot learning Current state-of-the-art models such as GPT-4 have become so powerful that
they are exhibiting remarkable properties which have been described as the Ô¨Årst in-
dications of artiÔ¨Åcial general intelligence (Bubeck et al., 2023) and are driving a
new wave of technological innovation Moreover, the capabilities of these models
continue to improve at an impressive pace Multimodal Transformers
Although transformers were initially developed as an alternative to recurrent net-
works for processing sequential language data, they have become prevalent in nearly
all areas of deep learning They have proved to be general-purpose models, as they
make very few assumptions about the input data, in contrast, for example, to convo-
lutional networks, which make strong assumptions about equivariances and locality Chapter 10
Due to their generality, transformers have become the state-of-the-art for many dif-
ferent modalities, including text, image, video, point cloud, and audio data, and have
been used for both discriminative and generative applications within each of these
12.4 Multimodal Transformers 395
domains

============================================================

=== CHUNK 392 ===
Palavras: 353
Caracteres: 2164
--------------------------------------------------
The core architecture of the transformer layer has remained relatively con-
stant, both over time and across applications Therefore, the key innovations that
enabled the use of transformers in areas other than natural language have largely
focused on the representation and encoding of the inputs and outputs One big advantage of a single architecture that is capable of processing many
different kinds of data is that it makes multimodal computation relatively straight-
forward In this context, multimodal refers to applications that combine two or more
different types of of data, either in the inputs or outputs or both For example, we
may wish to generate an image from a text prompt or design a robot that can com-
bine information from multiple sensors such as cameras, radar, and microphones The important thing to note is that if we can tokenize the inputs and decode the
output tokens, then it is likely that we can use a transformer 12.4.1 Vision transformers
Transformers have been applied with great success to computer vision and have
achieved state-of-the-art performance on many tasks The most common choice for
discriminative tasks is a standard transformer encoder, and this approach in the vi-
sion domain is known as a vision transformer, or ViT (Dosovitskiy et al., 2020) When using a transformer, we need to decide how to convert an input image into
tokens, and the simplest choice is to use each pixel as a token, following a linear
projection However, the memory required by a standard transformer implementa-
tion grows quadratically with the number of input tokens, and so this approach is
generally infeasible Instead, the most common approach to tokenization is to split
the image into a set of patches of the same size Suppose the images have dimension
x‚ààRH√óW√óCwhereHandWare the height and width of the image in pixels
andCis the number of channels (where typically C= 3 for R, G, and B colours) Each image is split into non-overlapping patches of size P√óP(whereP= 16 is
a common choice) and then ‚ÄòÔ¨Çattened‚Äô into a one-dimensional vector, which gives a
representation xp‚ààRN√ó(P2C)whereN=HW=P2is the total number of patches
for one image

============================================================

=== CHUNK 393 ===
Palavras: 362
Caracteres: 2361
--------------------------------------------------
The ViT architecture is shown in Figure 12.22 Another approach to tokenization is to feed the image through a small convolu-
tional neural network (CNN) This can down-sample the image to give a manageable Chapter 10
number of tokens each represented by one of the network outputs For example a typ-
ical ResNet18 encoder architecture down-samples an image by a factor of 8 in both
the height and width dimensions, giving 64 times fewer tokens than pixels We also need a way to encode positional information in the tokens It is pos-
sible to construct explicit positional embeddings that encode the two-dimensional
positional information of the image patches, but in practice this does not generally
improve performance, and so it is most common to just use learned positional em-
beddings In contrast to the transformers used for natural language, vision trans-
formers generally take a Ô¨Åxed number of tokens as input, which avoids the problem
of learned positional encodings not generalizing to inputs of a different size A vision transformer has a very different architectural design compared to a
CNN Although strong inductive biases are baked into a CNN model, the only two-
dimensional inductive bias in a vision transformer is due to the patches used to tok-
396 12 TRANSFORMERS
/angbracketleftclass/angbracketright Ô¨Çatten Ô¨Çattenembedding embedding embedding ...+ + +learned
positional
encodingtransformer encoderLSMc
Figure 12.22 Illustration of the vision transformer architecture for a classiÔ¨Åcation task Here a learnable /angbracketleftclass/angbracketright
token is included as an additional input, and the associated output is transformed by a linear layer with a softmax
activation, denoted by LSM, to give the Ô¨Ånal class-vector output c A transformer therefore generally requires more training data than a
comparable CNN as it has to learn the geometrical properties of images from scratch However, because there are no strong assumptions about the structure of the inputs,
transformers are often able to converge to a higher accuracy This provides another
illustration of the trade-off between inductive bias and the scale of the training data
(Sutton, 2019) 12.4.2 Generative image transformers
In the language domain, the most impressive results have come when trans-
formers are used as an autoregressive generative model for synthesizing text

============================================================

=== CHUNK 394 ===
Palavras: 355
Caracteres: 2385
--------------------------------------------------
It is
therefore natural to ask whether we can also use transformers to synthesize realistic
images Since natural language is inherently sequential, it Ô¨Åts neatly into the au-
toregressive framework, whereas images have no natural ordering of their pixels so
that it is not as intuitive that decoding them autoregressively would be useful How-
ever, any distribution can be decomposed into a product of conditionals, provided we Section 11.1.2
Ô¨Årst deÔ¨Åne some ordering of the variables Thus, the joint distribution over ordered
12.4 Multimodal Transformers 397
Figure 12.23 Illustration of a raster scan that deÔ¨Ånes a speciÔ¨Åc linear
ordering of the pixels in a two-dimensional image x1x2x3x4
x5x6x7x8
x9x10x11x12
x13x14x15x16
variables x1;:::;xNcan be written
p(x1;:::;xN) =N/productdisplay
n=1p(xn|x1;:::;xn‚àí1): (12.37)
This factorization is completely general and makes no restrictions on the form of the
individual conditional distributions p(xn|x1;:::;xn‚àí1) For an image we can choose xnto represent the nth pixel as a three-dimensional
vector of the RGB values We now need to decide on an ordering for the pixels, and
one widely used choice is called a raster scan as illustrated in Figure 12.23 A
schematic illustration of an image being generated using an autoregressive model,
based on a raster-scan ordering, is shown in Figure 12.24 Note that the use of autoregressive generative models of images predates the
introduction of transformers For example, PixelCNN (Oord et al , 2016) and Pixel-
RNN (Oord, Kalchbrenner, and Kavukcuoglu, 2016) used bespoke masked convolu-
tion layers that preserve the conditional independence deÔ¨Åned for each pixel by the
corresponding term on the right-hand side of 12.37 Representations of an image using continuous values can work well in discrim-
inative tasks However, much better results are obtained for image generation by
using discrete representations Continuous conditional distributions learned by max-
imum likelihood, such as Gaussians for which the negative log likelihood function
is a sum-of-squares error function, tend to learn averages of the training data, lead- Section 4.2
ing to blurry images Conversely, discrete distributions can handle multimodality
with ease For example, one of the conditional distributions p(xn|x1;:::;xn‚àí1)in
Figure 12.24 An illustration of how an image can be sampled from an autoregressive model

============================================================

=== CHUNK 395 ===
Palavras: 379
Caracteres: 2387
--------------------------------------------------
The Ô¨Årst pixel is
sampled from the marginal distribution p(x11), the second pixel from the conditional distribution p(x12|x11), and
so on in raster scan order until we have a complete image TRANSFORMERS
(12.37) might learn that a pixel could be either black or white, whereas a regression
model might learn that the pixel should be grey However, working with discrete spaces also brings its challenges The R, G, and
B values of image pixels are typically represented with at least 8 bits of precision,
so that each pixel has 224/similarequal16M possible values Learning a conditional softmax
distribution over a such a high-dimensional space is infeasible One way to address the problem of the high dimensionality is to use the tech-
nique of vector quantization, which can be viewed as a form of data compression Section 15.1.1
Suppose we have a set of data vectors x1;:::;xNeach of dimensionality D, which
might, for example, represent image pixels, and we then introduce a set of Kcode-
book vectorsC=c1;:::;cKalso of dimensionality D, where typically K/lessmuchD We now approximate each data vector by its nearest codebook vector according to
some similarity metric, usually Euclidean distance, so that
xn‚Üíarg min
ck‚ààC||xn‚àíck||2: (12.38)
Since there are Kcodebook vectors, we can represent each xnby a one-hot encoded
K-dimensional vector, and since we can choose the value of K, we can control the
trade-off between more accurate representation of the data, by using a larger value
ofK, or greater compression, by using a smaller value of K We can therefore take the original image pixels and map them into the lower-
dimensional codebook space An autoregressive transformer can then be trained to
generate a sequence of codebook vectors, and this sequence can be mapped back into
the original image space by replacing each codebook index kwith the corresponding
D-dimensional codebook vector ck Autoregressive transformers were Ô¨Årst applied to images in ImageGPT (Chen,
Radford, et al., 2020) Here each pixel is treated as one of a discrete set of three-
dimensional colour codebook vectors, each corresponding to a cluster in a K-means
clustering of the colour space A one-hot encoding therefore gives discrete tokens, Section 15.1
analogous to language tokens, and allows the transformer to be trained in the same
way as language models, with a next-token classiÔ¨Åcation objective

============================================================

=== CHUNK 396 ===
Palavras: 352
Caracteres: 2257
--------------------------------------------------
This is a power-
ful objective for representation learning for subsequent Ô¨Åne-tuning, again in a similar Section 6.3.3
way to language modelling Using the individual pixels as tokens directly, however, can lead to high com-
putational cost since a forward pass is required per pixel, which means that both
training and inference scale poorly with image resolution Also, using individual
pixels as inputs means that low-resolution images have to be used to give a reason-
able context length when decoding the pixels later in the raster scan As we saw with
the ViT model, it is preferable to use patches of the image as tokens instead of pixels,
as this can result in dramatically fewer tokens and therefore facilitates working with
higher-resolution images As before, we need to work with a discrete space of token
values due to the potential multimodality of the conditional distributions Again, this
raises the challenge of dimensionality, which is now much more severe with patches
than with individual pixels since the dimensionality is exponential with respect to
the number of pixels in the patch For example, even with just two possible pixel
12.4 Multimodal Transformers 399
timefrequency
Figure 12.25 An example mel spectrogram of a humpback whale song [Source data copyright ¬©2013‚Äì
2023, librosa development team.]
tokens, representing black and white, and patches of size 16√ó16, we would have a
dictionary of patch tokens of size 2256/similarequal1077 Once again we turn to vector quantization to address the challenge of dimension-
ality The codebook vectors can be learned from a data set of image patches using
simple clustering algorithms such as K-means or with more sophisticated meth-
ods such as fully convolutional networks (Oord, Vinyals, and Kavukcuoglu, 2017;
Esser, Rombach, and Ommer, 2020) or even vision transformers (Yu et al., 2021) One problem with learning to map each patch to a discrete set of codes and back
again, is that vector quantization is a non-differentiable operation Fortunately we
can use a technique called straight-through gradient estimation (Bengio, L ¬¥eonard,
and Courville, 2013), which is a simple approximation that just copies the gradients
through the non-differentiable function during backpropagation

============================================================

=== CHUNK 397 ===
Palavras: 368
Caracteres: 2258
--------------------------------------------------
The use of autoregressive transformers to generate images can be extended to
videos by treating a video as one long sequence of these vector-quantized tokens
(Rakhimov et al., 2020; Yan et al., 2021; Hu et al., 2023) 12.4.3 Audio data
We next look at the application of transformers to audio data Sound is gener-
ally stored as a waveform obtained by measuring the amplitude of the air pressure
at regular time intervals Although this waveform could be used directly as input to
a deep learning model, in practice it is more effective to pre-process it into a mel
spectrogram This is a matrix whose columns represent time steps and whose rows
correspond to frequencies The frequency bands follow a standard convention that
was chosen through subjective assessment to give equal perceptual differences be-
tween successive frequencies (the word ‚Äòmel‚Äô comes from melody) An example of
a mel spectrogram is shown in Figure 12.25 One application for transformers in the audio domain is classiÔ¨Åcation in which
segments of audio are assigned to one of a number of predeÔ¨Åned categories For
example, the AudioSet data set (Gemmeke et al., 2017) is a widely used benchmark TRANSFORMERS
It contains classes such as ‚Äòcar‚Äô, ‚Äòanimal‚Äô, and ‚Äòlaughter‚Äô Until the development of
the transformer, the state-of-the-art approach for audio classiÔ¨Åcation was based on
mel spectrograms treated as images and used as the input to a convolutional neural
network (CNN) However, although a CNN is good at understanding local relation- Chapter 10
ships, one drawback is that it struggles with longer-range dependencies, which can
be important in processing audio Just as transformers replaced RNNs as the state-of-the-art in natural language
processing, they have also come to replace CNNs for tasks such as audio classiÔ¨Åca-
tion For example, a transformer encoder model of identical structure to that used
for both language and vision, as shown in Figure 12.18, can be used to predict the
class of audio inputs (Gong, Chung, and Glass, 2021) Here the mel spectrogram
is viewed as an image which is then tokenized This is done by splitting the image
into patches in a similar way to vision transformers, possibly with some overlap so
as not to lose any important neighbourhood relations

============================================================

=== CHUNK 398 ===
Palavras: 365
Caracteres: 2285
--------------------------------------------------
Each patch is then Ô¨Çattened,
meaning it is converted to a one-dimensional array, in this case of length 256 A
unique positional encoding is then added to each token, a speciÔ¨Åc /angbracketleftclass/angbracketright token is
appended, and the tokens are then fed through the transformer encoder The output
token corresponding to the /angbracketleftclass/angbracketright input token from the last transformer layer can
then be decoded using a linear layer followed by a softmax activation function, and
the whole model can be trained end-to-end using a cross-entropy loss 12.4.4 Text-to-speech
ClassiÔ¨Åcation is not the only task that deep learning, and more speciÔ¨Åcally the
transformer architecture, has revolutionized in the audio domain The success of
transformers at synthesizing speech that imitates the voice of a given speaker is an-
other demonstration of their versatility, and their application to this task is an infor-
mative case study in how to apply transformers in a new context Generating speech corresponding to a given passage of text is known as text-
to-speech synthesis A more traditional approach would be to collect recordings
of speech from a given speaker and train a supervised regression model to predict
the speech output, possibly in the form of a mel spectrogram, from corresponding
transcribed text During inference, the text for which we would like to synthesize
speech is presented as input and the resulting mel spectrogram output can then be
decoded back to an audio waveform since this is a Ô¨Åxed mapping This approach has a few major drawbacks, however First, if we predict speech
at a low level, for example using sub-word components known as phonemes, a
larger context is needed to make the resulting sentences sound Ô¨Çuid However, if we
predict longer segments, then the space of possible inputs grows signiÔ¨Åcantly, and an
infeasible amount of training data might be required to achieve good generalization Second, this approach does not transfer knowledge across speakers, and so a lot of
data will be required for each new speaker Finally, the problem is really a generative
modelling task, as there are multiple correct speech outputs for a given speaker and
text pair, so regression may not be suitable since it tends to average over target values

============================================================

=== CHUNK 399 ===
Palavras: 382
Caracteres: 2335
--------------------------------------------------
Section 4.2
If instead we treat audio data in the same way as natural language and frame
text-to-speech as a conditional language modelling task, then we should be able to
12.4 Multimodal Transformers 401
transformer
text prompt
tokens discrete
tokenizer
acoustic promptaudio decoder
synthesized speech
Figure 12.26 A diagram showing the high-level architecture of Vall-E The input to the transformer model con-
sists of standard text tokens, which prompt the model as to what words the synthesized speech should contain,
together with acoustic prompt tokens that determine the speaker style and tone information The sampled model
output tokens are decoded back to speech with the learned decoder For simplicity, the positional encodings and
linear projections are not shown train the model in much the same way as with text-based large language models There are two main implementation details that need to be addressed The Ô¨Årst is
how to tokenize the training data and decode the predictions, and the second is how
to condition the model on the speaker‚Äôs voice One approach to text-to-speech synthesis that makes use of transformers and lan-
guage modelling techniques is Vall-E (Wang et al New text can be mapped
into speech in the voice of a new speaker using only a few seconds of sample speech
from that person Speech data is converted into a sequence of discrete tokens from
a learned dictionary or codebook obtained using vector quantization, and we can Section 12.4.2
think of these tokens as analogous to the one-hot encoded tokens from the natural
language domain The input consists of text tokens from a passage of text whereas
the target outputs for training consist of the corresponding speech tokens Additional
speech tokens from a short segment of unrelated speech from the same speaker are
402 12 TRANSFORMERS
appended to the input text tokens, as illustrated in Figure 12.26 By including exam-
ples from many different speakers, the system can learn to read out a passage of text
while imitating the voice represented by the additional speech input tokens Once
trained the system can be presented with new text, along with audio tokens from a
brief segment of speech captured from a new speaker, and the resulting output tokens
can be decoded, using the same codebook used during training, to create a speech
waveform

============================================================

=== CHUNK 400 ===
Palavras: 360
Caracteres: 2173
--------------------------------------------------
This allows the system to synthesize speech corresponding to the input
text in the voice of the new speaker 12.4.5 Vision and language transformers
We have seen how to generate discrete tokens for text, audio, and images, and so
it is a natural next step to ask if we can train a model with input tokens of one modal-
ity and output tokens of another, or whether we can have a combination of different
modalities for either inputs or outputs or both We will focus on the combination
of text and vision data as this is the most widely studied example, but in principle
the approaches discussed here could be applied to other combinations of input and
output modalities The Ô¨Årst requirement is that we have a large data set for training The LAION-
400M data set (Schuhmann et al., 2021) has greatly accelerated research in text-to-
image generation and image-to-text captioning in much the same way that ImageNet
was critical in the development of deep image classiÔ¨Åcation models Text-to-image
generation is actually much like the unconditional image generation we have looked
at so far, except that we also allow the model to take as input the text information to
condition the generation process This is straightforward when using transformers as
we can simply provide the text tokens as additional input when decoding each image
token This approach can also be viewed as treating the text-to-image problem as a
sequence-to-sequence language modelling problem, such as machine translation, ex-
cept that the target tokens are discrete image tokens rather than language tokens It
therefore makes sense to choose a full encoder-decoder transformer model, as shown
inFigure 12.20, in which Xcorresponds to the input text tokens and Ycorresponds
to the output image tokens This is the approach taken in a model called Parti (Yuet
al., 2022) in which the transformer is scaled to 20 billion parameters while showing
consistent performance improvements with increasing model size A lot of research has also been done on using pre-trained language models,
and modifying or Ô¨Åne-tuning them so that they can also accept visual data as in-
put (Alayrac et al., 2022; Li et al., 2022)

============================================================

=== CHUNK 401 ===
Palavras: 356
Caracteres: 2216
--------------------------------------------------
These approaches largely use bespoke
architectures, along with continuous-valued image tokens, and therefore are not nat-
ural Ô¨Åts for also generating visual data Moreover, they cannot be used directly if we
wish to include new modalities such as audio tokens Although this is a step towards
multimodality, we would ideally like to use both text and image tokens as both input
and output The simplest approach is to treat everything as a sequence of tokens as
if this were natural language but with a dictionary that is the concatenation of a lan-
guage token dictionary and the image token codebook We can then treat any stream
of audio and visual data as simply a sequence of tokens Exer
cises 403
Figure
12.27 Examples of the CM3Leon model performing a variety of different tasks in the joint space of text
and images [From (Yu et al., 2023) with permission.]
In CM3 (Aghajanyan et al., 2022) and CM3Leon (Yu et al., 2023), a variation of
language modelling is used to train on HTML documents containing both image and
text data taken from online sources When this large quantity of training data was
combined with a scalable architecture, the models became very powerful Moreover,
the multimodal nature of the training means that the models are very Ô¨Çexible Such
models are capable of completing many tasks that otherwise might require task-
speciÔ¨Åc model architectures and training regimes, such as text-to-image generation,
image-to-text captioning, image editing, text completion, and many more, including
anything a regular language model is capable of Examples of the CM3Leon model
completing instances of a few different tasks are shown in Figure 12.27 Ex
ercises
12.1 (??) Consider a set of coefÔ¨Åcients anm, form= 1;:::;N , with the properties that
anm>0 (12.39)
/summationdisplay
manm= 1: (12.40)
404 12 TRANSFORMERS
By using a Lagrange multiplier show that the coefÔ¨Åcients must also satisfy Appendix C
anm61forn= 1;:::;N: (12.41)
12.2 (?)Verify that the softmax function (12.5) satisÔ¨Åes the constraints (12.3) and (12.4)
for any values of the vectors x1;:::;xN 12.3 (?)Consider the input vectors xnin the simple transformation deÔ¨Åned by (12.2), in
which the weighting coefÔ¨Åcients anmare deÔ¨Åned by (12.5)

============================================================

=== CHUNK 402 ===
Palavras: 360
Caracteres: 2269
--------------------------------------------------
Show that if all the input
vectors are orthogonal, so that xT
nxm= 0 forn/negationslash=m, then the output vectors will
simply be equal to the input vectors so that yn=xnforn= 1;:::;N 12.4 (?)Consider two independent random vectors aandbeach of dimension Dand
each being drawn from a Gaussian distribution with zero mean and unit variance
N(¬∑|0;I) Show that the expected value of (aTb)2is given byD 12.5 (???) Show that multi-head attention deÔ¨Åned by (12.19) can be rewritten in the form
Y=H/summationdisplay
h=1HhXW(h)(12.42)
where Hhis given by (12.15) and we have deÔ¨Åned
W(h)=W(v)
hW(o)
h: (12.43)
Here we have partitioned the matrix W(o)horizontally into sub-matrices denoted
W(o)
heach of dimension Dv√óD, corresponding to the vertical segments of the
concatenated attention matrix Since Dvis typically smaller than D, for example Figure 12.7
Dv=D=H is a common choice, this combined matrix is rank deÔ¨Åcient Therefore,
using a fully Ô¨Çexible matrix to replace W(v)
hW(o)
hwould not be equivalent to the
original formulation given in the text 12.6 (??) Express the self-attention function (12.14) as a fully connected network in the
form of a matrix that maps the full input sequence of concatenated word vectors
into an output vector of the same dimension Note that such a matrix would have
O(N2D2)parameters Show that the self-attention network corresponds to a sparse
version of this matrix with parameter sharing Draw a sketch showing the structure
of this matrix, indicating which blocks of parameters are shared and which blocks
have all elements equal to zero 12.7 (?)Show that if we omit the positional encoding of input vectors then the outputs
of a multi-head attention layer deÔ¨Åned by (12.19) are equivariant with respect to a
reordering of the input sequence 12.8 (???) Consider two D-dimensional unit vectors aandb, satisfying/bardbla/bardbl= 1 and
/bardblb/bardbl= 1, drawn from a random distribution Assume that the distribution is sym-
metrical around the origin, i.e., it depends only on the distance from the origin and
Exer
cises 405
not the direction Show that for large values of Dthe magnitude of the cosine of the
angle between these vectors is close to zero and hence that these random vectors are
nearly orthogonal in a high-dimensional space

============================================================

=== CHUNK 403 ===
Palavras: 362
Caracteres: 2298
--------------------------------------------------
To do this, consider an orthonormal
basis set{ui}where uT
iuj=ijand express aandbas expansions in this basis 12.9 (??) Consider a position encoding in which the input token vector xis concatenated
with a position-encoding vector e Show that when this concatenated vector under-
goes a general linear transformation by multiplication using a matrix, the result can
be expressed as the sum of a linearly transformed input and a linearly transformed
position vector 12.10 (??) Show that the positional encoding deÔ¨Åned by (12.25) has the property that,
for a Ô¨Åxed offset k, the encoding at position n+kcan be represented as a linear
combination of the encoding at position nwith coefÔ¨Åcients that depend only on k
and not onn To do this make use of the following trigonometric identities:
cos(A +B) = cosAcosB‚àísinAsinB (12.44)
sin(A +B) = cosAsinB+ sinAcosB: (12.45)
Show that if the encoding is based purely on sine functions, without cosine functions,
then this property no longer holds 12.11 (?)Consider the bag-of-words model (12.28) in which each of the component distri-
butionsp(xn)is given by a general probability table that is shared across all words Show that the maximum likelihood solution, given a training set of vectors, is given
by a table whose entries are the fractions of times each word occurs in the training
set 12.12 (?)Consider the autoregressive language model given by (12.31) and suppose that
the termsp(xn|x1;:::;xn‚àí1)on the right-hand side are represented by general
probability tables Show that the number of entries in these tables grows exponen-
tially with the value of n 12.13 (?)When using n-grams it is usual to train the n-gram and (n‚àí1)-gram models at
the same time and then compute the conditional probability using the product rule of
probability in the form
p(xn|xn‚àíL+1;:::;xn‚àí1) =pL(xn‚àíL+1;:::;xn)
pL
‚àí1(xn‚àíL+1;:::;xn‚àí1): (12.46)
Explain why this is more convenient than storing the left-hand-side directly, and
show that to obtain the correct probabilities the Ô¨Ånal token from each sequence must
be omitted when evaluating pL‚àí1(¬∑¬∑¬∑) 12.14 (??) Write down pseudo-code for the inference process in a trained RNN with an
architecture of the form depicted in Figure 12.13 12.15 (??) Consider a sequence of two tokens y1andy2each of which can take the states
AorB

============================================================

=== CHUNK 404 ===
Palavras: 362
Caracteres: 2201
--------------------------------------------------
The table below shows the joint probability distribution p(y1;y2):
406 12 TRANSFORMERS
y1=A y 1=B
y2=A 0.0 0.4
y2=B 0.1 0.25
We see that the most probable sequence is y1=B,y2=Band that this has prob-
ability 0:4 Using the sum and product rules of probability, write down the values
of the marginal distribution p(y1)and the conditional distribution p(y2|y1) Show
that if we Ô¨Årst maximize p(y1)to give a value y 1and then subsequently maximize
p(y2|y 1)then we obtain a sequence that is different from the overall most probable
sequence Find the probability of the sequence 12.16 (?)The BERT-Large model (Devlin et al., 2018) has a maximum input length of 512
tokens, each of dimensionality D= 1;024 and taken from a vocabulary of 30,000 It
has 24 transformer layers each with 16 self-attention heads with Dq=Dk=Dv=
64, and the MLP position-wise networks have two layers with 4,096 hidden nodes Show that the total number of parameters in the BERT encoder transformer language
model is approximately 340 million 13
Graph Neural
Networks
In previous chapters we have encountered structured data in the form of sequences
and images, corresponding to one-dimensional and two-dimensional arrays of vari-
ables respectively More generally, there are many types of structured data that are
best described by a graph as illustrated in Figure 13.1 In general a graph consists of
a set of objects, known as nodes, connected by edges Both the nodes and the edges
can have data associated with them For example, in a molecule the nodes and edges
are associated with discrete variables corresponding to the types of atom (carbon, ni-
trogen, hydrogen, etc.) and the types of bonds (single bond, double bond, etc.) For a
rail network, each railway line might be associated with a continuous variable given
by the average journey time between two cities Here we are assuming that the edges
are symmetrical, for example that the journey time from London to Cambridge is the
same as the journey time from Cambridge to London Such edges are depicted by
undirected links between the nodes For the worldwide web the edges are directed
407 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C

============================================================

=== CHUNK 405 ===
Palavras: 367
Caracteres: 2292
--------------------------------------------------
Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_13    
408 13 GRAPH NEURAL NETWORKS
(a)
(b)
(c)
Figure 13.1 Three examples of graph-structured data: (a) the caffeine molecule consisting of atoms connected
by chemical bonds, (b) a rail network consisting of cities connected by railway lines, and (c) the worldwide web
consisting of pages connected by hyperlinks since if there is a hyperlink on page A that points to page B there is not necessarily
a hyperlink on page B pointing back to page A Other examples of graph-structured data include a protein interaction network,
in which the nodes are proteins and the edges express how strongly pairs of pro-
teins interact, an electrical circuit where the nodes are components and the edges
are conductors, or a social network where the nodes are people and the edges are
‚Äòfriendships‚Äô More complex graphical structures are also possible, for example the
knowledge graph inside a company comprises multiple different kinds of nodes such
as people, documents, and meetings, along with multiple kinds of edges capturing
different properties such as a person being present at a meeting or a document refer-
encing another document In this chapter we explore how to apply deep learning to graph-structured data We have already encountered an example of structured data when we discussed im-
ages, in which the individual elements of an image data vector xcorrespond to pixels
on a regular grid An image is therefore a special instance of graph-structured data
in which the nodes are the pixels and the edges describe which pixels are adjacent Convolutional neural networks (CNNs) take this structure into account, incorporat- Chapter 10
ing prior knowledge of the relative positions of the pixels, together with the equiv-
ariance of properties such as segmentation and the invariance of properties such as
classiÔ¨Åcation We will use CNNs for images as a source of inspiration to construct
more general approaches to deep learning for graphical data known as graph neu-
ral networks (Zhou et al , 2019; Hamilton, 2020; Veli Àáckovi ¬¥c, 2023) We will see that a key consideration when applying deep learning to graph-structured
data is to ensure either equivariance or invariance with respect to a reordering of the
nodes in the graph

============================================================

=== CHUNK 406 ===
Palavras: 358
Caracteres: 2076
--------------------------------------------------
Machine Learning on Graphs 409
13.1 Machine Learning on Graphs
There are many kinds of applications that we might wish to address using graph-
structured data, and we can group these broadly according to whether the goal is
to predict properties of nodes, of edges, or of the whole graph An example of
node prediction would be to classify documents according to their topic based on the
hyperlinks and citations between the documents Regarding edges we might, for example, know some of the interactions in a pro-
tein network and would like to predict the presence of any additional ones Such
tasks are called edge prediction orgraph completion tasks There are also tasks
where the edges are known in advance and the goal is to discover clusters or ‚Äòcom-
munities‚Äô within the graph Finally, we may wish to predict properties that relate to the graph as a whole For
example, we might wish to predict whether a particular molecule is soluble in water Here instead of being given a single graph we will have a data set of different graphs,
which we can view as being drawn from some common distribution, in other words
we assume that the graphs themselves are independent and identically distributed Such tasks can be considered as graph regression or graph classiÔ¨Åcation tasks For the molecule solubility classiÔ¨Åcation example, we might be given a labelled
training set of molecules, along with a test set of new molecules whose solubility
needs to be predicted This is a standard example of an inductive task of the kind
we have seen many times in previous chapters However, some graph prediction
examples are transductive in which we are given the structure of the entire graph
along with labels for some of the nodes and the goal is to predict the labels of the
remaining nodes An example would be a large social network in which our goal is to
classify each node as either a real person or an automated bot Here a small number
of nodes might be manually labelled, but it would be prohibitive to investigate every
node individually in a large and ever-changing social network

============================================================

=== CHUNK 407 ===
Palavras: 353
Caracteres: 2001
--------------------------------------------------
During training, we
therefore have access to the whole graph along with labels for a subset of the nodes,
and we wish to predict the labels for the remaining nodes This can be viewed as a
form of semi-supervised learning As well as solving prediction tasks directly, we can also use deep learning on
graphs to discover useful internal representations that can subsequently facilitate a
range of downstream tasks This is known as graph representation learning For
example we could seek to build a foundation model for molecules by training a deep
learning system on a large corpus of molecular structures The goal is that once
trained, such a foundation model can be Ô¨Åne-tuned to speciÔ¨Åc tasks by using a small,
labelled data set Graph neural networks deÔ¨Åne an embedding vector for each of the nodes, usually
initialized with the observed node properties, which are then transformed through a
series of learnable layers to create a learned representation This is analogous to
the way word embeddings, or tokens, are processed through a series of layers in the
transformer to give a representation that better captures the meaning of the words Chapter 12
in the context of the rest of the text Graph neural networks can also use learned
embeddings associated with the edges and with the graph as a whole 410 13.GRAPH NEURAL NETWORKS
BA
CD
E
(a)ABC D E
A
B
C
D
E
(b)CE A D B
C
E
A
D
B
(c)
Figure 13.2 An e xample of an adjacency matrix showing (a) an example of a graph with Ô¨Åve nodes, (b) the
associated adjacency matrix for a particular choice of node order, and (c) the adjacency matrix corresponding to
a different choice for the node order 13.1.1 Graph properties
In this chapter we will focus on simple graphs where there is at most one edge
between any pair of nodes, where the edges are undirected, and where there are no
self-edges that connect a node to itself This sufÔ¨Åces to introduce the key concepts
of graph neural networks, and it also encompasses a wide range of practical applica-
tions

============================================================

=== CHUNK 408 ===
Palavras: 364
Caracteres: 2052
--------------------------------------------------
These concepts can then be applied to more complex graphical structures We begin by introducing some notation associated with graphs and by deÔ¨Åning
some important properties A graph G= (V;E)consists of a set of nodes orvertices ,
denoted byV, along with a set of edges orlinks , denoted byE We index the nodes
byn= 1;:::N , and we write the edge from node nto nodemas(n;m) If two
nodes are linked by an edge they are called neighbours , and the set of all neighbours
of nodenis denoted byN(n) In addition to the graph structure, we usually also have observed data associated
with the nodes For each node nwe can represent the corresponding node variables
as aD-dimensional column vector xnand we can group these into a data matrix X
of dimensionality N√óDin which row nis given by xT
n There may also be data
variables associated with the edges in the graph, although to start with we will focus Section 13.3.2
just on node variables 13.1.2 Adjacency matrix
A convenient way to specify the edges in a graph is to use an adjacency matrix
denoted by A To deÔ¨Åne the adjacency matrix we Ô¨Årst have to choose an ordering for
the nodes If there are Nnodes in the graph, we can index them using n= 1;:::;N The adjacency matrix has dimensions N√óNand contains a 1in every location n;m
for which there is an edge going from node nto nodem, with all other entries being
0 For graphs with undirected edges, the adjacency matrix will be symmetric since
the presence of an edge from node nto nodemimplies that there is also an edge
from nodemto noden, and therefore Amn=Anmfor allnandm An example of
an adjacency matrix is shown in Figure 13.2 Since the adjacency matrix deÔ¨Ånes the structure of a graph, we could consider
13.1 Machine Learning on Graphs 411
using it directly as the input to a neural network To do this we could ‚ÄòÔ¨Çatten‚Äô the ma-
trix, for example by concatenating the columns into one long column vector How-
ever, a major problem with this approach is that the adjacency matrix depends on the
arbitrary choice of node ordering, as seen in Figure 13.2

============================================================

=== CHUNK 409 ===
Palavras: 415
Caracteres: 2419
--------------------------------------------------
Suppose for instance that
we want to predict the solubility of a molecule This clearly should not depend on
the ordering assigned to the nodes when writing down an adjacency matrix Because
the number of permutations increases factorially with the number of nodes, it is im-
practical to try to learn permutation invariance by using large data sets or by data
augmentation Instead, we should treat this invariance property as an inductive bias
when constructing a network architecture 13.1.3 Permutation equivariance
We can express node label permutation mathematically by introducing the con-
cept of a permutation matrix P, which has the same size as the adjacency matrix
and which speciÔ¨Åes a particular permutation of a node ordering It contains a single
1in each row and a single 1in each column, with 0in all the other elements, such
that a 1in positionn;m indicates that node nwill be relabelled as node mafter
the permutation Consider, for example, the permutation from (A;B;C;D;E )‚Üí
(C;E;A;D;B )corresponding to the two choices of node ordering in Figure 13.2 The corresponding permutation matrix takes the form Exercise 13.1
P=Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠0 0 1 0 0
0 0 0 0 1
1 0 0 0 0
0 0 0 1 0
0 1 0 0 0Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏: (13.1)
We can deÔ¨Åne the permutation matrix more formally as follows First we in-
troduce the standard unit vector un, forn= 1;:::;N This is a column vector
in which all elements are 0except element n, which equals 1 In this notation the
identity matrix is given by
I=Ô£´
Ô£¨Ô£≠uT
1
uT
2
¬∑¬∑¬∑
uT
NÔ£∂
Ô£∑Ô£∏: (13.2)
We can now introduce a permutation function (¬∑)that mapsntom=(n) The
associated permutation matrix is given by
P=Ô£´
Ô£¨Ô£¨Ô£≠uT
(1)
uT
(2)
:::
uT
(N)Ô£∂
Ô£∑Ô£∑Ô£∏: (13.3)
When we reorder the labelling on the nodes of a graph, the effect on the corre-
sponding node data matrix Xis to permute the rows according to (¬∑), which can be
achieved by pre-multiplication by Pto give Exercise 13.4
412 13 GRAPH NEURAL NETWORKS
/tildewideX=PX: (13.4)
For the adjacency matrix, both the rows and the columns become permuted Again
the rows can be permuted using pre-multiplication by Pwhereas the columns are
permuted using post-multiplication by PT, giving a new adjacency matrix: Exercise 13.5
/tildewideA=PAPT: (13.5)
When applying deep learning to graph-structured data, we will need to repre-
sent the graph structure in numerical form so that it can be fed into a neural network,
which requires that we assign an ordering to the nodes

============================================================

=== CHUNK 410 ===
Palavras: 363
Caracteres: 2305
--------------------------------------------------
However, the speciÔ¨Åc or-
dering we choose is arbitrary and so it will be important to ensure that any global
property of the graph does not depend on this ordering In other words, the network
predictions must be invariant to node label reordering, so that
y(/tildewideX;/tildewideA) =y(X;A) Invariance (13.6)
wherey(¬∑;¬∑)is the output of the network We may also want to make predictions that relate to individual nodes In this
case, if we reorder the node labelling then the corresponding predictions should show
the same reordering so that a given prediction is always associated with the same
node irrespective of the choice of order In other words, node predictions should be
equivariant with respect to node label reordering This can be expressed as
y(/tildewideX;/tildewideA) = Py(X;A) Equivariance (13.7)
where y(¬∑;¬∑)is a vector of network outputs, with one element per node Neural Message-Passing
Ensuring invariance or equivariance under node label permutations is a key design
consideration when we apply deep neural networks to graph-structured data An-
other consideration is that we want to exploit the representational capabilities of deep
neural networks and so we retain the concept of a ‚Äòlayer‚Äô as a computational trans-
formation that can be applied repeatedly If each layer of the network is equivariant
under node reordering then multiple layers applied in succession will also exhibit
equivariance, while allowing each layer of the network to be informed by the graph
structure For networks whose outputs represent node-level predictions, the whole network
will be equivariant as required If the network is being used to predict a graph-
level property then a Ô¨Ånal layer can be included that is invariant to permutations
of its inputs We also want to ensure that each layer is a highly Ô¨Çexible nonlinear
function and is differentiable with respect to its parameters so that it can be trained
by stochastic gradient descent using gradients obtained by automatic differentiation Graphs come in various sizes For example different molecules can have differ-
ent numbers of atoms, so a Ô¨Åxed-length representation as used for standard neural
13.2 Neural Message-Passing 413
i
i
l l+ 1
(a)i
(b)
Figure 13.3 A convolutional Ô¨Ålter for images can be represented as a graph-structured computation

============================================================

=== CHUNK 411 ===
Palavras: 378
Caracteres: 2292
--------------------------------------------------
(a) A Ô¨Ålter
computed by node iin layerl+ 1of a deep convolutional network is a function of the activation values in layer
lover a local patch of pixels (b) The same computation structure expressed as a graph showing ‚Äòmessages‚Äô
Ô¨Çowing into node ifrom its neighbours networks is unsuitable A further requirement is therefore that the network should be
able to handle variable-length inputs, as we saw with transformer networks Some Chapter 12
graphs can be very large, for example a social network with many millions of par-
ticipants, and so we also want to construct models that scale well Not surpris-
ingly, parameter sharing will play an important role, both to allow the invariance and
equivariance properties to be built into the network architecture but also to facilitate
scaling to large graphs 13.2.1 Convolutional Ô¨Ålters
To develop a framework that meets all of these requirements, we can seek inspi-
ration from image processing using convolutional neural networks First note that Chapter 10
an image can be viewed as a speciÔ¨Åc instance of graph-structured data, in which the
nodes are the pixels and the edges represent pairs of pixels that are adjacent in the
image, where adjacency includes nodes that are diagonally adjacent as well as those
that are horizontally or vertically adjacent In a convolutional network, we make successive transformations of the image
domain such that a pixel at a particular layer computes a function of states of pixels
in the previous layer through a local function called a Ô¨Ålter Consider a convolutional Section 10.2
layer using 3√ó3Ô¨Ålters, as illustrated in Figure 13.3(a) The computation performed
414 13 GRAPH NEURAL NETWORKS
by a single Ô¨Ålter at a single pixel in layer l+ 1can be expressed as
z(l+1)
i =fÔ£´
Ô£≠/summationdisplay
jwjz(l)
j+bÔ£∂
Ô£∏ (13.8)
wheref(¬∑)is a differentiable nonlinear activation function such as ReLU, and the
sum overjis taken over all nine pixels in a small patch in layer l The same function
is applied across multiple patches in the image, so that the weights wjand biasbare
shared across the patches (and therefore do not carry the index i) As it stands, (13.8) is not equivariant under reordering of the nodes in layer l
because the weight vector, with elements wj, is not invariant under permutation of
its elements

============================================================

=== CHUNK 412 ===
Palavras: 397
Caracteres: 2612
--------------------------------------------------
However, we can achieve equivariance with some simple modiÔ¨Åcations
as follows We Ô¨Årst view the Ô¨Ålter as a graph, as shown in Figure 13.3(b), and
separate out the contribution from node i The other eight 8 nodes are its neighbours
N(i) We then assume that a single weight parameter wneigh is shared across the
neighbours so that
z(l+1)
i =fÔ£´
Ô£≠wneigh/summationdisplay
j‚ààN(i)z(l)
j+wselfz(l)
i+bÔ£∂
Ô£∏ (13.9)
where node ihas its own weight parameter wself We can interpret (13.9) as updating a local representation ziat nodeiby gather-
ing information from the neighbouring nodes by passing messages from the neigh-
bouring nodes into node i In this case the messages are simply the activations of
the other nodes These messages are then combined with information from node i,
and the result is transformed using a nonlinear function The information from the
neighbouring nodes is aggregated through a simple summation in (13.9), and this is
clearly invariant to any permutation of the labels associated with those nodes Fur-
thermore, the operation (13.9) is applied synchronously to every node in a graph, and
so if the nodes are permuted then the resulting computations will be unchanged but
their ordering will be likewise permuted, and hence, this calculation is equivariant
under node reordering Note that this depends on the parameters wneigh,wself, andb
being shared across all nodes 13.2.2 Graph convolutional networks
We now use the convolution example as a template to construct deep neural net-
works for graph-structured data Our goal is to deÔ¨Åne a Ô¨Çexible, nonlinear transfor-
mation of the node embeddings that is differentiable with respect to a set of weight
and bias parameters and which maps the variables in layer linto corresponding vari-
ables in layer l+ 1 For each node nin the graph and for each layer lin the net-
work, we introduce a D-dimensional column vector h(l)
nof node-embedding vari-
ables, where n= 1;:::;N andl= 1;:::;L We see that the transformation given by (13.9) Ô¨Årst gathers and combines in-
formation from neighbouring nodes and then updates the node as a function of the
13.2 Neural Message-Passing 415
Algorithm 13.1: Simple message-passing neural network
Input: Undirected graphG= (V;E)
Initial node embeddings {h(0)
n=xn}
Aggregate(¬∑) function
Update(¬∑;¬∑)function
Output: Final node embeddings {h(L)
n}
// Iterative message-passing
forl‚àà{0;:::;L‚àí1}do
z(l)
n‚ÜêAggregate/parenleftBig/braceleftBig
h(l)
m:m‚ààN(n)/bracerightBig/parenrightBig
h(l+1)
n‚ÜêUpdate/parenleftBig
h(l)
n;z(l)
n/parenrightBig
end for
return{h(L)
n}
current embedding of the node and the incoming messages

============================================================

=== CHUNK 413 ===
Palavras: 368
Caracteres: 2338
--------------------------------------------------
We can therefore view
each layer of processing as having two successive stages The Ô¨Årst is the aggregation
stage in which, for each node n, messages are passed to that node from its neigh-
bours and combined to form a new vector z(l)
nin a way that is permutation invariant This is followed by an update step in which the aggregated information from neigh-
bouring nodes is combined with local information from the node itself and used to
calculate a revised embedding vector for that node Consider a speciÔ¨Åc node nin the graph We Ô¨Årst aggregate the node vectors
from all the neighbours of node n:
z(l)
n= Aggregate/parenleftbig/braceleftbig
h(l)
m:m‚ààN(n)/bracerightbig/parenrightbig
: (13.10)
The form of this aggregation function is very Ô¨Çexible if it is well deÔ¨Åned for a vari-
able number of neighbouring nodes and does not depend on the ordering of those
nodes It can potentially contain learnable parameters as long as it is a differentiable
function with respect to those parameters to facilitate gradient descent training We then use another operation to update the embedding vector at node n:
h(l+1)
n = Update/parenleftbig
h(l)
n;z(l)
n/parenrightbig
: (13.11)
Again, this can be a differentiable function of a set of learnable parameters Appli-
cation of the Aggregate operation followed by the Update operation in parallel for
every node in the graph represents one layer of the network The node embeddings
are typically initialized using observed node data so that h(0)
n=xn Note that each
layer generally has its own independent parameters, although the parameters can also
be shared across layers This framework is called a message-passing neural network
(Gilmer et al., 2017) and is summarized in Algorithm 13.1 GRAPH NEURAL NETWORKS
13.2.3 Aggregation operators
There are many possible forms for the Aggregate function, but it must depend
only on the set of inputs and not on their ordering It must also be a differentiable
function of any learnable parameters The simplest such aggregation function, fol-
lowing from (13.9), is summation:
Aggregate/parenleftbig/braceleftbig
h(l)
m:m‚ààN(n)/bracerightbig/parenrightbig
=/summationdisplay
m‚ààN (n)h(l)
m: (13.12)
A simple summation is clearly independent of the ordering of the neighbouring nodes
and is also well deÔ¨Åned no matter how many nodes are in the neighbourhood set

============================================================

=== CHUNK 414 ===
Palavras: 353
Caracteres: 2341
--------------------------------------------------
Note that this has no learnable parameters A summation gives a stronger inÔ¨Çuence over nodes that have many neighbours
compared to those with few neighbours, and this can lead to numerical issues, par-
ticularly in applications such as social networks where the size of the neighbourhood
set can vary by several orders of magnitude A variation of this approach is to deÔ¨Åne
the Aggregation operation to be the average of the neighbouring embedding vectors
so that
Aggregate/parenleftbig/braceleftbig
h(l)
m:m‚ààN(n)/bracerightbig/parenrightbig
=1
|N(
n)|/summationdisplay
m‚ààN (n)h(l)
m (13.13)
where|N(n)| denotes the number of nodes in the neighbourhood set N(n) How-
ever, this normalization also discards information about the network structure and is
provably less powerful than a simple summation (Hamilton, 2020), and so the choice
of whether to use it depends on the relative importance of node features compared to
graph structure Another variation of this approach (Kipf and Welling, 2016) takes account of
the number of neighbours for each of the neighbouring nodes:
Aggregate/parenleftbig/braceleftbig
h(l)
m:m‚ààN(n)/bracerightbig/parenrightbig
=/summationdisplay
m‚ààN (n)h(l)
m/radicalbig
|N(
n)||N (m)|: (13.14)
Yet another possibility is to take the element-wise maximum (or minimum) of the
neighbouring embedding vectors, which also satisÔ¨Åes the desired properties of being
well deÔ¨Åned for a variable number of neighbours and of being independent of their
order Since each node in a given layer of the network is updated by aggregating infor-
mation from its neighbours in the previous layer, this deÔ¨Ånes a receptive Ô¨Åeld anal-
ogous to the receptive Ô¨Åelds of Ô¨Ålters used in CNNs As information is processed Chapter 10
through successive layers, the updates to a given node depend on a steadily increas-
ing fraction of other nodes in earlier layers until the effective receptive Ô¨Åeld poten-
tially spans the whole graph as illustrated in Figure 13.4 However, large, sparse
graphs may require an excessive number of layers before each output is inÔ¨Çuenced
by every input Some architectures therefore introduce an additional ‚Äòsuper-node‚Äô
13.2 Neural Message-Passing 417
Figure 13.4 Schematic illustration of infor-
mation Ô¨Çow through successive layers of a
graph neural network In the third layer a sin-
gle node is highlighted in red

============================================================

=== CHUNK 415 ===
Palavras: 392
Caracteres: 2551
--------------------------------------------------
It receives in-
formation from its two neighbours in the previ-
ous layer and those in turn receive informa-
tion from their neighbours in the Ô¨Årst layer As with convolutional neural networks for im-
ages, we see that the effective receptive Ô¨Åeld,
corresponding to the number of nodes shown
in red, grows with the number of processing
layers that connects directly to every node in the original graph to ensure fast propagation
of information The aggregation operators discussed so far have no learnable parameters We
can introduce such parameters if we Ô¨Årst transform each of the embedding vectors
from neighbouring nodes using a multilayer neural network, denoted by MLP,
before combining their outputs, where MLP denotes ‚Äòmultilayer perceptron‚Äô and 
represents the parameters of the network So long as the network has a structure and
parameter values that are shared across nodes then this aggregation operator again
be permutation invariant We can also transform the combined vector with another
neural network MLP, with parameters , to give an overall aggregation operator:
Aggregate/parenleftbig/braceleftbig
h(l)
m:m‚ààN(n)/bracerightbig/parenrightbig
= MLPÔ£´
Ô£≠/summationdisplay
m‚ààN(n)MLP(h(l)
m)Ô£∂
Ô£∏ (13.15)
in which MLPandMLPare shared across layer l Due to the Ô¨Çexibility of MLPs,
the transformation deÔ¨Åned by (13.15) represents a universal approximator for any
permutation-invariant function that maps a set of embeddings to a single embedding
(Zaheer et al Note that the summation can be replaced by other invariant
functions such as averages or an element-wise maximum or minimum A special case of graph neural networks arises if we consider a graph having no
edges, which corresponds simply to an unstructured set of nodes In this case if we
use (13.15) for each vector h(l)
nin the set, in which the summation is taken over all
other vectors except h(l)
n, then we have a general framework for learning functions
over unstructured sets of variables known as deep sets GRAPH NEURAL NETWORKS
13.2.4 Update operators
Having chosen a suitable Aggregate operator, we similarly need to decide on the
form of the Update operator By analogy with (13.9) for the CNN, a simple form for
this operator would be
Update/parenleftbig
h(l)
n;z(l)
n/parenrightbig
=f/parenleftbig
Wselfh(l)
n+Wneighz(l)
n+b/parenrightbig
(13.16)
wheref(¬∑)is a nonlinear activation function such as ReLU applied element-wise to
its vector argument, and where Wself,Wneigh, andbare the learnable weights and
biases and z(l)
nis deÔ¨Åned by the Aggregate operator (13.10)

============================================================

=== CHUNK 416 ===
Palavras: 353
Caracteres: 2331
--------------------------------------------------
If we choose a simple summation (13.12) as the aggregation function and if
we also share the same weight matrix between nodes and their neighbours so that
Wself=Wneigh, we obtain a particularly simple form of Update operator given by
h(l+1)
n = Update/parenleftbig
h(l)
n;z(l)
n/parenrightbig
=fÔ£´
Ô£≠Wneigh/summationdisplay
m‚ààN (n);nh(l)
m+bÔ£∂
Ô£∏: (13.17)
The message-passing algorithm is typically initialized by setting h(0)
n=xn Sometimes, however, we may want to have an internal representation vector for each
node that has a higher, or lower, dimensionality than that of xn Such a represen-
tation can be initialized by padding the node vectors xnwith additional zeros (to
achieve a higher dimensionality) or simply by transforming the node vectors using a
learnable linear transformation to a space of the desired number of dimensions An
alternative form of initialization, particularly when there are no data variables asso-
ciated with the nodes, is to use a one-hot vector that labels the degree of each node
(i.e., the number of neighbours) Overall, we can represent a graph neural network as a sequence of layers that
successively transform the node embeddings If we group these embeddings into a
matrix Hwhosenth row is the vector hT
n, which is initialized to the data matrix X,
then we can write the successive transformations in the form
H(1)=F/parenleftbig
X;A;W(1)/parenrightbig
H(2)=F/parenleftbig
H(1);A;W(2)/parenrightbig
...= H(L)=F/parenleftbig
H(L‚àí1);A;W(L)/parenrightbig
(13.18)
where Ais the adjacency matrix, and W(l)represents the complete set of weight
and biases in layer lof the network Under a node reordering deÔ¨Åned by a permu-
tation matrix P, the transformation of the node embeddings computed by layer lis
equivariant:
PH(l)=F/parenleftbig
PH(l‚àí1);PAPT;W(l)/parenrightbig
: (13.19)
As a consequence, the complete network computes an equivariant transformation Neural Message-Passing 419
13.2.5 Node classiÔ¨Åcation
A graph neural network can be viewed as a series of layers each of which trans-
forms a set of node-embedding vectors {h(l)
n}into a new set{h(l+1)
n}of the same
size and dimensionality After the Ô¨Ånal convolutional layer of the network, we need
to obtain predictions so that we can deÔ¨Åne a cost function for training and also for
making predictions on new data using the trained network

============================================================

=== CHUNK 417 ===
Palavras: 383
Caracteres: 2353
--------------------------------------------------
Consider Ô¨Årst the task of classifying the nodes in a graph, which is one of the
most common uses for graph neural networks We can deÔ¨Åne an output layer, some-
times called a readout layer, which calculates a softmax function for each node cor-
responding to a classiÔ¨Åcation over Cclasses, of the form
yni=exp(wT
ih(L)
n)
/summationtext
jexp(wT
jh(L)
n)(13.20)
where{wi}is a set of learnable weight vectors and i= 1;:::;C We can then
deÔ¨Åne a loss function as the sum of the cross-entropy loss across all nodes and all
classes:
L=‚àí/summationdisplay
n‚ààV trainC/summationdisplay
i=1ytni
ni (13.21)
where{tni}are target values with a one-hot encoding for each value of n Because
the weight vectors {wi}are shared across the output nodes, the outputs yniare
equivariant to permutation of the node ordering, and hence the loss function (13.21)
is invariant If the goal is to predict continuous values at the outputs then a sim-
ple linear transformation can be combined with a sum-of-squares error to deÔ¨Åne a
suitable loss function The sum over nin (13.21) is taken over the subset of the nodes denoted by Vtrain
and used for training We can distinguish between three types of nodes as follows:
1 The nodesVtrain are labelled and included in the message-passing operations
of the graph neural network and are also used to compute the loss function
used for training There is potentially also a transductive subset of nodes denoted by Vtrans,
which are unlabelled and which do not contribute to the evaluation of the
loss function used for training However, they still participate in the message-
passing operations during both training and inference, and their labels may be
predicted as part of the inference process The remaining nodes, denoted Vinduct , are a set of inductive nodes that are not
used to compute the loss function, and neither these nodes nor their associated
edges participate in message-passing during the training phase However, they
do participate in message-passing during the inference phase and their labels
are predicted as the outcome of inference GRAPH NEURAL NETWORKS
If there are no transductive nodes, and hence the test nodes (and their associated
edges) are not available during the training phase, then the training is generally
referred to as inductive learning, which can be considered to be a form of super-
vised learning

============================================================

=== CHUNK 418 ===
Palavras: 359
Caracteres: 2268
--------------------------------------------------
However, if there are transductive nodes then it is called transductive
learning, which may be viewed as a form of semi-supervised learning 13.2.6 Edge classiÔ¨Åcation
In some applications we wish to make predictions about the edges of the graph
rather than the nodes A common form of edge classiÔ¨Åcation task is edge completion
in which the goal is to determine whether an edge should be present between two
nodes Given a set of node embeddings, the dot product between pairs of embeddings
can be used to deÔ¨Åne a probability p(n;m) for the presence of an edge between nodes
nandmby using the logistic sigmoid function:
p(n;m) =/parenleftbig
hT
nhm/parenrightbig
: (13.22)
An example application would be predicting whether two people in a social network
have shared interests and therefore might wish to connect 13.2.7 Graph classiÔ¨Åcation
In some applications of graph neural networks, the goal is to predict the proper-
ties of new graphs given a training set of labelled graphs G1;:::;GN This requires
that we combine the Ô¨Ånal-layer embedding vectors in a way that does not depend
on the arbitrary node ordering, thereby ensuring that the output predictions will be
invariant to that ordering The goal is somewhat like that of the Aggregate function
except that all nodes in the graph are included, not just the neighbourhood sets of the
individual nodes The simplest approach is to take the sum of the node-embedding
vectors:
y=f/parenleftBigg/summationdisplay
n‚ààVh(L)
n/parenrightBigg
(13.23)
where the function fmay contain learnable parameters such as a linear transforma-
tion or a neural network Other invariant aggregation functions can be used such as
averages or element-wise minimum or maximum A cross-entropy loss is typically used for classiÔ¨Åcation problems, such as la-
belling a candidate drug molecule as toxic or safe, and a squared-error loss for re-
gression problems, such as predicting the solubility of a candidate drug molecule Graph-level predictions correspond to an inductive task since there must be separate
sets of graphs for training and for inference General Graph Networks
There are many variations and extensions of the graph networks considered so far Here we outline a few of the key concepts along with some practical considerations

============================================================

=== CHUNK 419 ===
Palavras: 357
Caracteres: 2395
--------------------------------------------------
General Graph Networks 421
13.3.1 Graph attention networks
The attention mechanism is very powerful when used as the basis of a trans- Section 12.1
former architecture It can be used in the context of graph neural networks to con-
struct an aggregation function that combines messages from neighbouring nodes The incoming messages are weighted by attention coefÔ¨Åcients Anmto give
z(l)
n= Aggregate/parenleftbig/braceleftbig
h(l)
m:m‚ààN(n)/bracerightbig/parenrightbig
=/summationdisplay
m‚ààN (n)Anmh(l)
m (13.24)
where the attention coefÔ¨Åcients satisfy
Anm>0 (13.25)
/summationdisplay
m‚ààN (n)Anm= 1: (13.26)
This is known as a graph attention network (Veli Àáckovi ¬¥cet al., 2017) and can capture
an inductive bias that says some neighbouring nodes will be more important than
others in determining the best update in a way that depends on the data itself There are multiple ways to construct the attention coefÔ¨Åcients, and these gener-
ally employ a softmax function For example, we can use a bilinear form:
Anm=exp/parenleftbig
hT
nWhm/parenrightbig
/summationtext
m/prime‚ààN(n)exp (hTnWhm/prime)(13.27)
where Wis aD√óDmatrix of learnable parameters A more general option is to
use a neural network to combine the embedding vectors from the nodes at each end
of the edge:
Anm=exp{MLP (hn;hm)}/summationtext
m/prime‚ààN(n)exp{MLP (hn;hm/prime)}(13.28)
where the MLP has a single continuous output variable whose value is invariant if
the input vectors are exchanged Provided the MLP is shared across all the nodes in
the network, this aggregation function will be equivariant under node reordering Exercise 13.8
A graph attention network can be extended by introducing multiple attention
heads in which Hdistinct sets of attention weights A(h)
nmare deÔ¨Åned, for h= Section 12.1.6
1;:::;H , in which each head is evaluated using one of the mechanisms described
above and with its own independent parameters These are then combined in the
aggregation step using concatenation and linear projection Note that, for a fully-
connected network, a multi-head graph attention network becomes a standard trans-
former encoder Exercise 13.9
13.3.2 Edge embeddings
The graph neural networks discussed above use embedding vectors that are as-
sociated with the nodes We have seen that some networks also have data associated
with the edges Even when there are no observable values associated with the edges,
422 13

============================================================

=== CHUNK 420 ===
Palavras: 380
Caracteres: 2671
--------------------------------------------------
GRAPH NEURAL NETWORKS
we can still maintain and update edge-based hidden variables and these can con-
tribute to the internal representations learned by the graph neural network In addition to the node embeddings given by h(l)
n, we therefore introduce edge
embeddings e(l)
nm We can then deÔ¨Åne general message-passing equations in the form
e(l+1)
nm = Updateedge/parenleftbig
e(l)
nm;h(l)
n;h(l)
m/parenrightbig
(13.29)
z(l+1)
n = Aggregatenode/parenleftbig/braceleftbig
e(l+1)
nm :m‚ààN(n)/bracerightbig/parenrightbig
(13.30)
h(l+1)
n = Updatenode/parenleftbig
h(l)
n;z(l+1)
n/parenrightbig
: (13.31)
The learned edge embeddings e(L)
nmfrom the Ô¨Ånal layer can be used directly to make
predictions associated with the edges 13.3.3 Graph embeddings
In addition to node and edge embeddings we can also maintain and update an
embedding vector g(l)that relates to the graph as a whole Bringing all these aspects
together allows us to deÔ¨Åne a more general set of message-passing functions, and a
richer set of learned representations, for graph-structured applications SpeciÔ¨Åcally,
we can deÔ¨Åne general message-passing equations (Battaglia et al., 2018):
e(l+1)
nm = Updateedge/parenleftbig
e(l)
nm;h(l)
n;h(l)
m;g(l)/parenrightbig
(13.32)
z(l+1)
n = Aggregatenode/parenleftbig/braceleftbig
e(l+1)
nm :m‚ààN(n)/bracerightbig/parenrightbig
(13.33)
h(l+1)
n = Updatenode/parenleftbig
h(l)
n;z(l+1)
n;g(l)/parenrightbig
(13.34)
g(l+1)= Updategraph/parenleftbig
g(l);{h(l+1)
n :n‚ààV};{e(l+1)
nm : (n;m)‚ààE}/parenrightbig
:(13.35)
These update equations start in (13.32) by updating the edge embedding vectors
e(l+1)
nm based on the previous states of those vectors, on the node embeddings for the
nodes connected by each edge, and on a graph-level embedding vector g(l) These
updated edge embeddings are then aggregated across every edge connected to each
node using (13.33) to give a set of aggregated vectors These in turn then contribute
to the update of the node-embedding vector {h(l+1)
n}based on the current node-
embedding vectors and on the graph-level embedding vector using (13.34) Finally,
the graph-level embedding vector is updated using (13.35) based on information
from all the nodes and all the edges in the graph along with the graph-level em-
bedding from the previous layer These message-passing updates are illustrated in
Figure 13.5 and are summarized in Algorithm 13.2 13.3.4 Over-smoothing
One signiÔ¨Åcant problem that can arise with some graph neural networks is called
over-smoothing in which the node-embedding vectors tend to become very similar to
each other after a number of iterations of message-passing, which effectively limits
the depth of the network

============================================================

=== CHUNK 421 ===
Palavras: 352
Caracteres: 2217
--------------------------------------------------
One way to help alleviate this issue is to introduce residual
connections For example, we can modify the update operator (13.34): Section 9.5
h(l+1)
n = Updatenode/parenleftbig
h(l)
n;z(l+1)
n;g(l)/parenrightbig
+h(l)
n: (13.36)
13.3 General Graph Networks 423
nhn
m
hmGg
enm
(a)nhn
mGg
e
ee
enm
(b)h
h
h
hh
hGg
ee
eee
ee
(c)
Figure 13.5 Illustration of the general graph message-passing updates deÔ¨Åned by (13.32) to (13.35), showing
(a) edge updates, (b) node updates, and (c) global graph updates In each case the variable being updated is
shown in red and the variables that contribute to that update are those shown in red and blue Another approach for mitigating the effects of over-smoothing is to allow the
output layer to take information from all previous layers of the network and not just
the Ô¨Ånal convolutional layer This can be done for example by concatenating the
representations from previous layers:
yn=f/parenleftbig
h(1)
n‚äïh(2)
n‚äï¬∑¬∑¬∑‚äï h(L)
n/parenrightbig
(13.37)
where a‚äïbdenotes the concatenation of vectors aandb A variant of this would be
to combine the vectors using max pooling instead of concatenation In this case each
element of the output vector is given by the max of all the corresponding elements
of the embedding vectors from the previous layers 13.3.5 Regularization
Standard techniques for regularization can be used with graph neural networks, Chapter 9
including the addition of penalty terms, such as the sum-of-squares of the parameter
values, to the loss function In addition, some regularization methods have been
developed speciÔ¨Åcally for graph neural networks Graph neural networks already employ weight sharing to achieve permutation
equivariance and invariance, but typically they have independent parameters in each
layer However, weights and biases can also be shared across layers to reduce the
number of independent parameters Dropout in the context of graph neural networks involves omitting random sub-
sets of the graph nodes during training, with a fresh random subset chosen for each
forward pass This can likewise be applied to the edges in the graph in which ran-
domly selected subsets of entries in the adjacency matrix are removed, or masked,
during training

============================================================

=== CHUNK 422 ===
Palavras: 424
Caracteres: 3128
--------------------------------------------------
GRAPH NEURAL NETWORKS
Algorithm 13.2: Graph neural network with node, edge, and graph embeddings
Input: Undirected graphG= (V;E)
Initial node embeddings {h(0)
n}
Initial edge embeddings {e(0)
nm}
Initial graph embedding g(0)
Output: Final node embeddings {h(L)
n}
Final edge embeddings {e(L)
nm}
Final graph embedding g(L)
// Iterative message-passing
forl‚àà{0;:::;L‚àí1}do
e(l+1)
nm‚ÜêUpdateedge/parenleftBig
e(l)
nm;h(l)
n;h(l)
m;g(l)/parenrightBig
z(l+1)
n‚ÜêAggregatenode/parenleftBig/braceleftBig
e(l+1)
nm :m‚ààN(n)/bracerightBig/parenrightBig
h(l+1)
n‚ÜêUpdatenode/parenleftBig
h(l)
n;z(l+1)
n;g(l)/parenrightBig
g(l+1)‚ÜêUpdategraph/parenleftBig
g(l);{h(l+1)
n};{e(l+1)
nm}/parenrightBig
end for
return{h(L)
n},{e(L)
nm},g(L)
13.3.6 Geometric deep learning
We have seen how permutation symmetry is a key consideration when design-
ing deep learning models for graph-structured data It acts as a form of inductive
bias, dramatically reducing the data requirements while improving predictive perfor-
mance In applications of graph neural networks associated with spatial properties,
such as graphics meshes, Ô¨Çuid Ô¨Çow simulations, or molecular structures, there are
additional equivariance and invariance properties that can be built into the network
architecture Consider the task of predicting the properties of a molecule, for example when
exploring the space of candidate drugs The molecule can be represented as a list
of atoms of given types (carbon, hydrogen, nitrogen, etc.) along with the spatial
coordinates of each atom expressed as a three-dimensional column vector We can
introduce an associated embedding vector for each atom nat each layer l, denoted
byr(l)
n, and these vectors can be initialized with the known atom coordinates How-
ever, the values for the elements of these vectors depends on the arbitrary choice of
coordinate system, whereas the properties of the molecule do not For example, the
solubility of the molecule is unchanged if it is rotated in space or translated to a new
position relative to the origin of the coordinate system, or if the coordinate system
itself is reÔ¨Çected to give the mirror image version of the molecule The molecular
Exer
cises 425
properties should therefore be invariant under such transformations By making careful choices of the functional forms for the update and aggre-
gation operations (Satorras, Hoogeboom, and Welling, 2021), the new embeddings
r(l)
ncan be incorporated into the graph neural network update equations (13.29) to
(13.31) to achieve the required symmetry properties:
e(l+1)
nm = Updateedge/parenleftbig
e(l)
nm;h(l)
n;h(l)
m;/bardblr(l)
n‚àír(l)
m/bardbl2/parenrightbig
(13.38)
r(l+1)
n =r(l)
n+C/summationdisplay
(n;m)‚ààE/parenleftbig
r(l)
n‚àír(l)
m/parenrightbig
/parenleftbig
e(l+1)
nm/parenrightbig
(13.39)
z(l+1)
n = Aggregatenode/parenleftbig
{e(l+1)
nm :m‚ààN(n)}/parenrightbig
(13.40)
h(l+1)
n = Updatenode/parenleftbig
h(l)
n;z(l+1)
n/parenrightbig
(13.41)
Note that the quantity /bardblr(l)
n‚àír(l)
m/bardbl2represents the squared distance between the
coordinates r(l)
nandr(l)
m, and this does not depend on translations, rotations, or re-
Ô¨Çections

============================================================

=== CHUNK 423 ===
Palavras: 356
Caracteres: 2142
--------------------------------------------------
Also, the coordinates r(l)
nare updated through a linear combination of the
relative differences/parenleftBig
r(l)
n‚àír(l)
m/parenrightBig Here/parenleftBig
e(l+1)
nm/parenrightBig
is a general scalar function of the
edge embeddings and is represented by a neural network, and the coefÔ¨Åcient Cis
typically set equal to the reciprocal of the number of terms in the sum It follows
that under such transformations, the messages in (13.38), (13.40), and (13.41) are
invariant and the coordinate embeddings given by (13.39) are equivariant Exercise 13.10
We have seen many examples of symmetries in structured data, from transla-
tions of objects within images and the permutation of node orderings on graphs, to
rotations and translations of molecules in three-dimensional space Capturing these
symmetries in the structure of a deep neural network is a powerful form of inductive
bias and forms the basis of a rich Ô¨Åeld of research known as geometric deep learning
(Bronstein et al., 2017; Bronstein et al., 2021) Ex
ercises
13.1 (?)Show that the permutation (A;B;C;D;E )‚Üí(C;E;A;D;B )corresponding to
the two choices of node ordering in Figure 13.2 can be expressed in the form (13.5)
with a permutation matrix given by (13.1) 13.2 (??) Show that the number of edges connected to each node of a graph is given
by the corresponding diagonal element of the matrix A2where Ais the adjacency
matrix 13.3 (?)Draw the graph whose adjacency matrix is given by
A=Ô£´
Ô£¨Ô£¨Ô£¨Ô£≠0 1 1 0 1
1 0 1 1 1
1 1 0 1 0
0 1 1 0 0
1 1 0 0 0Ô£∂
Ô£∑Ô£∑Ô£∑Ô£∏: (13.42)
426 13 GRAPH NEURAL NETWORKS
13.4 (??) Show that the effect of pre-multiplying a data matrix Xusing a permutation
matrix PdeÔ¨Åned by (13.3) is to create a new data matrix /tildewideXgiven by (13.4) whose
rows are permuted according to the permutation function (¬∑) 13.5 (??) Show that the transformed adjacency matrix /tildewideAdeÔ¨Åned by (13.5), where Pis
deÔ¨Åned by (13.3), is such that both the rows and the columns are permuted according
to the permutation function (¬∑)relative to the original adjacency matrix A 13.6 (??) In this exercise we write the update equations (13.16) as graph-level equations
using matrices

============================================================

=== CHUNK 424 ===
Palavras: 355
Caracteres: 2242
--------------------------------------------------
To keep the notation uncluttered, we omit the layer index l First,
gather the node-embedding vectors {hn}into anN√óDmatrix Hin which row n
is given by hT
n Then show that the neighbourhood-aggregated vectors zngiven by
zn=/summationdisplay
m‚ààN (n)hm (13.43)
can be written in matrix form as Z=AH where Zis theN√óDmatrix in which
rownis given by zT
n, andAis the adjacency matrix Finally, show that the argument
to the nonlinear activation function in (13.16) can be written in matrix form as
AHW neigh+HW self+1DbT(13.44)
where 1Dis theD-dimensional column vector in which all elements are 1 13.7 (??) By making use of the equivariance property (13.19) for layer lof a deep graph
convolutional network along with the permutation property (13.4) for the node vari-
ables, show that a complete deep graph convolutional network deÔ¨Åned by (13.18) is
also equivariant 13.8 (??) Explain why the aggregation function deÔ¨Åned by (13.24), in which the attention
weights are given by (13.28), is equivariant under a reordering of the nodes in the
graph 13.9 (?)Show that a graph attention network in which the graph is fully connected, so that
there is an edge between every pair of nodes, is equivalent to a standard transformer
architecture 13.10 (??) When a coordinate system is translated, the location of an object deÔ¨Åned by
that coordinate system is transformed using
/tildewider=r+c (13.45)
where cis a Ô¨Åxed vector describing the translation Similarly, if the coordinate sys-
tem is rotated and/or mirror reÔ¨Çected, the location vector of an object is transformed
using
/tildewider=Rr (13.46)
where Ris an orthogonal matrix whose inverse is given by its transpose so that
RRT=RTR=I: (13.47)
Exercises 427
Using these properties, show that under translations, rotations, and reÔ¨Çections, the
messages in (13.38), (13.40), and (13.41) are invariant, and that the coordinate em-
beddings given by (13.39) are equivariant 14
Sampling
There are many situations in deep learning where we need to create synthetic exam-
ples of a variable zfrom a probability distribution p(z) Here zmight be a scalar
and the distribution might be a univariate Gaussian, or zmight be a high-resolution
image andp(z)might be a generative model deÔ¨Åned by a deep neural network

============================================================

=== CHUNK 425 ===
Palavras: 361
Caracteres: 2379
--------------------------------------------------
The
process of creating such examples is known as sampling, also known as Monte Carlo
sampling For many simple distributions there are numerical techniques that gener-
ate suitable samples directly, whereas for more complex distributions, including ones
that are deÔ¨Åned implicitly, we may need more sophisticated approaches We adopt
the convention of referring to each instantiated value as a sample, in contrast to the
convention used in classical statistics whereby ‚Äòsample‚Äô refers to a set of values In this chapter we focus on aspects of sampling that are most relevant to deep
learning Further information on Monte Carlo methods more generally can be found
in Gilks, Richardson, and Spiegelhalter (1996) and Robert and Casella (1999) 429 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_14    
430 14 Basic
Sampling Algorithms
In
this section, we explore a variety of relatively simple strategies for generating
random samples from a given distribution Because the samples will be generated
by a computer algorithm, they will in fact be pseudo-random, that is, they will be
calculated using a deterministic algorithm but must nevertheless pass appropriate
tests for randomness Here we will assume that an algorithm has been provided
that generates pseudo-random numbers distributed uniformly over (0;1), and indeed
most software environments have such a facility built in 14.1.1 Expectations
Although for some applications the samples themselves may be of direct inter-
est, in other situations the goal is to evaluate expectations with respect to the distri-
bution Suppose we wish to Ô¨Ånd the expectation of a function f(z)with respect to a
probability distribution p(z) Here, the components of zmight comprise discrete or
continuous variables or some combination of the two For continuous variables the
expectation is deÔ¨Åned by
E[f] =/integraldisplay
f(z)p(z) d z (14.1)
where the integral is replaced by summation for discrete variables This is illus-
trated schematically for a single continuous variable in Figure 14.1 We will suppose
that such expectations are too complex to be evaluated exactly using analytical tech-
niques The general idea behind sampling methods is to obtain a set of samples z(l)
(wherel= 1;:::;L ) drawn independently from the distribution p(z)

============================================================

=== CHUNK 426 ===
Palavras: 374
Caracteres: 2526
--------------------------------------------------
This allows
the expectation (14.1) to be approximated by a Ô¨Ånite sum:
f=1
LL/summationdisplay
l=1f(
z(l)): (14.2)
If the samples z(l)are drawn from the distribution p(z), then E[f]
=E[f(z)]and so
the estimator fhas
the correct mean We can also write this in the form Exercise 14.1
Figure 14.1 Schematic illustration of a function f(z)
whose expectation is to be evaluated
with respect to a distribution p(z) Basic Sampling Algorithms 431
E[f(z)]/similarequal1
LL/summationdisplay
l=1f(
z(l)) (14.3)
where the symbol/similarequaldenotes that the right-hand side is an unbiased estimator of the
left-hand side, that is the two sides are equal when averaged over the noise distribu-
tion The variance of the estimator (14.2) is given by Exercise 14.2
var[f]
=1
LE/bracketleftbig
(
f‚àíE[f])2/bracketrightbig
; (14.4)
which is the variance of the function f(z)under the distribution p(z) Note that the
linear decrease of this variance with increasing Ldoes not depend on the dimension-
ality of z, and that, in principle, high accuracy may be achievable with a relatively
small number of samples {z(l)} The problem, however, is that the samples {z(l)}
might not be independent, and so the effective sample size might be much smaller
than the apparent sample size Also, referring back to Figure 14.1, note that if f(z)
is small in regions where p(z) is large and vice versa, then the expectation may be
dominated by regions of small probability, implying that relatively large sample sizes
will be required to achieve sufÔ¨Åcient accuracy 14.1.2 Standard distributions
We now consider how to generate random numbers from simple nonuniform dis-
tributions, assuming that we already have available a source of uniformly distributed
random numbers Suppose that zis uniformly distributed over the interval (0;1),
and that we transform the values of zusing some function g(¬∑)so thaty=g(z) The
distribution of ywill be governed by Section 2.4
p(y) =p(z)/vextendsingle/vextendsingle/vextendsingle/vextendsingledz
d
y/vextendsingle/vextendsingle/vextendsingle/vextendsingle(14.5)
where, in this case, p(z) = 1 Our goal is to choose the function g(z)such that the
resulting values of yhave some speciÔ¨Åc desired distribution p(y) Integrating (14.5)
we obtain
z=/integraldisplayy
‚àí‚àûp(/hatwidey)‚â°h(y) d/hatwidey (14.6)
which is the indeÔ¨Ånite integral of p(y) Thus,y=h‚àí1(z), and so we have to Exercise 14.3
transform the uniformly distributed random numbers using a function that is the
inverse of the indeÔ¨Ånite integral of the desired distribution

============================================================

=== CHUNK 427 ===
Palavras: 382
Caracteres: 2869
--------------------------------------------------
This is illustrated in
Figure 14.2 Consider for example the exponential distribution
p(y) =exp(‚àíy ) (14.7)
where 06y <‚àû In this case the lower limit of the integral in (14.6) is 0, and so
h(y) = 1‚àíexp(‚àíy ) Thus, if we transform our uniformly distributed variable z
usingy=‚àí‚àí1ln(1‚àíz), thenywill have an exponential distribution SAMPLING
Figure 14.2 Geometrical interpretation of the
transformation method for generating
non-uniformly distributed random
numbers.h(y)is the indeÔ¨Ånite integral
of the desired distribution p(y) If a
uniformly distributed random variable
zis transformed using y=h‚àí1(z),
thenywill be distributed according to
p(y) p(y)h(y)
y01
Another
example of a distribution to which the transformation method can be
applied is given by the Cauchy distribution
p(y) =1
1
1
+y2: (14.8)
In this case, the inverse of the indeÔ¨Ånite integral can be expressed in terms of the tan
function Exercise 14.4
The generalization to multiple variables involves the Jacobian of the change of
variables, so that Section 2.4
p(y1;:::;yM) =p(z1;:::;zM)/vextendsingle/vextendsingle/vextendsingle/vextendsingle@(z1;:::;zM)
@(
y1;:::;yM)/vextendsingle/vextendsingle/vextendsingle/vextendsingle: (14.9)
As a Ô¨Ånal example of the transformation technique, we consider the Box‚ÄìMuller
method for generating samples from a Gaussian distribution First, suppose we gen-
erate pairs of uniformly distributed random numbers z1;z2‚àà(‚àí1; 1), which we can
do by transforming a variable distributed uniformly over (0;1)usingz‚Üí2z‚àí1 Next we discard each pair unless it satisÔ¨Åes z2
1+z2
261 This leads to a uniform
distribution of points inside the unit circle with p(z1;z2) = 1=, as illustrated in
Figure 14.3 Then, for each pair z1;z2we evaluate the quantities
y1=z1/parenleftbigg‚àí2 lnr2
r2/parenrightbigg1
=2
(14.10)
y2=z2/parenleftbigg‚àí2 lnr2
r2/parenrightbigg1
=2
(14.11)
wherer2=z2
1+z2
2 Then the joint distribution of y1andy2is given by Exercise 14.5
p(y1;y2) =p(z1;z2)/vextendsingle/vextendsingle/vextendsingle/vextendsingle@(z1;z2)
@(
y1;y2)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/bracketleftbigg1‚àö
2
exp(‚àíy2
1=2)/bracketrightbigg/bracketleftbigg1‚àö
2
exp(‚àíy2
2=2)/bracketrightbigg
(14.12)
14.1 Basic Sampling Algorithms 433
Figure 14.3 The Box‚ÄìMuller method for generating Gaussian-
distributed random numbers starts by generating samples
from a uniform distribution inside the unit circle ‚àí1‚àí11
1 z1z2
and soy1andy2are independent and each has a Gaussian distribution with zero
mean and unit variance Ifyhas a Gaussian distribution with zero mean and unit variance, then y+
will have a Gaussian distribution with mean and variance 2 To generate vector-
valued variables having a multivariate Gaussian distribution with mean and co-
variance , we can make use of the Cholesky decomposition, which takes the form
=LLT(Deisenroth, Faisal, and Ong, 2020)

============================================================

=== CHUNK 428 ===
Palavras: 351
Caracteres: 2321
--------------------------------------------------
Then, if zis a random vector whose
components are independent and Gaussian distributed with zero mean and unit vari-
ance, then y=+Lzwill be Gaussian with mean and covariance  Exercise 14.6
Clearly, the transformation technique depends for its success on the ability to
calculate and then invert the indeÔ¨Ånite integral of the required distribution Such
operations are feasible only for a limited number of simple distributions, and so we
must turn to alternative approaches in search of a more general strategy Here we
consider two techniques called rejection sampling andimportance sampling Al-
though mainly limited to univariate distributions and thus not directly applicable to
complex problems in many dimensions, they do form important components in more
general strategies 14.1.3 Rejection sampling
The rejection sampling framework allows us to sample from relatively complex
distributions, subject to certain constraints We begin by considering univariate dis-
tributions and subsequently discuss the extension to multiple dimensions Suppose we wish to sample from a distribution p(z) that is not one of the simple,
standard distributions considered so far and that sampling directly from p(z) is dif-
Ô¨Åcult Furthermore suppose, as is often the case, that we are easily able to evaluate
p(z) for any given value of z, up to some normalizing constant Z, so that
p(z) =1
Zp/tildewidep(z) (14.13)
where/tildewidep(z)can readily be evaluated, but Zpis unknown To apply rejection sampling, we need some simpler distribution q(z), sometimes
called a proposal distribution, from which we can readily draw samples We next
introduce a constant kwhose value is chosen such that kq(z)>/tildewidep(z)for all val-
ues ofz The function kq(z)is called the comparison function and is illustrated
434 14 SAMPLING
Figure 14.4 In the rejection sampling
method, samples are drawn
from a simple distribution
q(z)and rejected if they fall
in the grey area between the
unnormalized distribution ep(z)
and the scaled distribution
kq(z) The resulting samples
are distributed according to
p(z), which is the normalized
version ofep(z) z0 zu0kq(
z0)kq(z)
ep(z)
for
a univariate distribution in Figure 14.4 Each step of the rejection sampler in-
volves generating two random numbers First, we generate a number z0from the
distributionq(z)

============================================================

=== CHUNK 429 ===
Palavras: 372
Caracteres: 2381
--------------------------------------------------
Next, we generate a number u0from the uniform distribution over
[0;kq (z0)] This pair of random numbers has uniform distribution under the curve
of the function kq(z) Finally, if u0>/tildewidep(z0)then the sample is rejected, otherwise
u0is retained Thus, the pair is rejected if it lies in the grey shaded region in Fig-
ure 14.4 The remaining pairs then have uniform distribution under the curve of /tildewidep(z),
and hence the corresponding zvalues are distributed according to p(z), as desired Exercise 14.7
The original values of zare generated from the distribution q(z), and these sam-
ples are then accepted with probability /tildewidep(z)=kq(z), and so the probability that a
sample will be accepted is given by
p(accept) =/integraldisplay
{/tildewidep(z)=kq(z)}q(z) dz
=1
k/integraldisplay
/tildewidep
(z) dz: (14.14)
Thus, the fraction of points that are rejected by this method depends on the ratio of
the area under the unnormalized distribution /tildewidep(z)to the area under the curve kq(z) We therefore see that the constant kshould be as small as possible subject to the
limitation that kq(z)must be nowhere less than /tildewidep(z) As an illustration of the use of rejection sampling, consider the task of sampling
from the gamma distribution
Gam(z|a;b) =baza‚àí1exp(‚àíbz )
Œì(
a); (14.15)
which, fora > 1, has a bell-shaped form, as shown in Figure 14.5 A suitable
proposal distribution is therefore the Cauchy (14.8) because this too is bell-shaped
and because we can use the transformation method, discussed earlier, to sample from
it We need to generalize the Cauchy slightly to ensure that it nowhere has a smaller
value than the gamma distribution This can be achieved by transforming a uniform
random variable yusingz=btany+c, which gives random numbers distributed
according to Exercise 14.8
q(z) =k
1
+ (z‚àíc)2=b2: (14.16)
14.1 Basic Sampling Algorithms 435
Figure 14.5 Plot showing the gamma dis-
tribution given by (14.15) as
the green curve, with a scaled
Cauchy proposal distribution
shown by the red curve Sam-
ples from the gamma distribu-
tion can be obtained by sam-
pling from the Cauchy and
then applying the rejection
sampling criterion zp(z)
0 10 20 3000.050.10.15
The
minimum reject rate is obtained by setting c=a‚àí1, andb2= 2a‚àí1and choos-
ing the constant kto be as small as possible while still satisfying the requirement
kq(z)>/tildewidep(z)

============================================================

=== CHUNK 430 ===
Palavras: 352
Caracteres: 2230
--------------------------------------------------
The resulting comparison function is also illustrated in Figure 14.5 14.1.4 Adaptive rejection sampling
In many instances where we might wish to apply rejection sampling, it can be
difÔ¨Åcult to determine a suitable analytic form for the envelope distribution q(z) An
alternative approach is to construct the envelope function on the Ô¨Çy based on mea-
sured values of the distribution p(z)(Gilks and Wild, 1992) Constructing an enve-
lope function is particularly straightforward when p(z)is log concave, in other words
when lnp(z)has derivatives that are non-increasing functions of z The construction
of a suitable envelope function is illustrated graphically in Figure 14.6 The function lnp(z)and its gradient are evaluated at some initial set of grid
points, and the intersections of the resulting tangent lines are used to construct the
envelope function Next a sample value is drawn from the envelope distribution This is straightforward because the log of the envelope distribution is a succession Exercise 14.10
of linear functions, and hence the envelope distribution itself comprises a piecewise
exponential distribution of the form
q(z) =kiiexp{‚àíi(z‚àízi‚àí1)}; zi‚àí1<z6zi: (14.17)
Once a sample has been drawn, the usual rejection criterion can be applied If the
sample is accepted, then it will be a draw from the desired distribution If, however,
the sample is rejected, then it is incorporated into the set of grid points, a new tangent
line is computed, and the envelope function is thereby reÔ¨Åned As the number of
grid points increases, so the envelope function becomes a better approximation of
the desired distribution p(z)and the probability of rejection decreases There is a variant of the algorithm exists that avoids the evaluation of derivatives
(Gilks, 1992) The adaptive rejection sampling framework can also be extended to
distributions that are not log concave, simply by following each rejection sampling
436 14 SAMPLING
Figure 14.6 In rejection sampling, if a dis-
tribution is log concave then an
envelope function can be con-
structed using the tangent lines
computed at a set of grid points If a sample point is rejected, it
is added to the set of grid points
and used to reÔ¨Åne the envelope
distribution

============================================================

=== CHUNK 431 ===
Palavras: 370
Caracteres: 2406
--------------------------------------------------
z1 z2 z3 zlnp(z)
step
with a Metropolis‚ÄìHastings step (to be discussed in Section 14.2.3), giving rise
toadaptive rejection Metropolis sampling (Gilks, Best, and Tan, 1995) For rejection sampling to be of practical value, we require that the comparison
function is close to the required distribution so that the rate of rejection is kept to a
minimum Now let us examine what happens when we try to use rejection sampling
in spaces of high dimensionality Consider, for illustration, a somewhat artiÔ¨Åcial
problem in which we wish to sample from a zero-mean multivariate Gaussian distri-
bution with covariance 2
pI, where Iis the unit matrix, by rejection sampling from a
proposal distribution that is itself a zero-mean Gaussian distribution having covari-
ance2
qI Clearly, we must have 2
q>2
pto ensure that there exists a ksuch that
kq(z)>p(z) InD-dimensions, the optimum value of kis given byk= (q=p)D,
as illustrated for D= 1 inFigure 14.7 The acceptance rate will be the ratio of
volumes under p(z)andkq(z), which, because both distributions are normalized, is
just1=k Thus, the acceptance rate diminishes exponentially with dimensionality Even ifqexceedspby just 1%, for D= 1;000 the acceptance ratio will be ap-
proximately 1=20;000 In this illustrative example, the comparison function is close
to the required distribution For more practical examples, where the desired distri-
bution may be multimodal and sharply peaked, it will be extremely difÔ¨Åcult to Ô¨Ånd
Figure 14.7 Illustrative example used to
highlight a limitation of rejec-
tion sampling Samples are
drawn from a Gaussian dis-
tributionp(z)shown by the
green curve, by using rejec-
tion sampling from a proposal
distribution q(z)that is also
Gaussian and whose scaled
versionkq(z)is shown by the
red curve zp(z)
‚àí5 0 500.250.5
14.1 Basic Sampling Algorithms 437
a good proposal distribution and comparison function Furthermore, the exponential
decrease of the acceptance rate with dimensionality is a generic feature of rejection
sampling Although rejection can be a useful technique in one or two dimensions, it
is unsuited to problems of high dimensionality It can, however, play a role as a sub-
routine in more sophisticated algorithms for sampling in high-dimensional spaces 14.1.5 Importance sampling
One reason for wishing to sample from complicated probability distributions is
to evaluate expectations of the form (14.1)

============================================================

=== CHUNK 432 ===
Palavras: 360
Caracteres: 2419
--------------------------------------------------
The technique of importance sampling
provides a framework for approximating expectations directly but does not itself
provide a mechanism for drawing samples from a distribution p(z) The Ô¨Ånite sum approximation to the expectation, given by (14.2), depends on
being able to draw samples from the distribution p(z) Suppose, however, that it is
impractical to sample directly from p(z) but that we can evaluate p(z) easily for any
given value of z One simplistic strategy for evaluating expectations would be to
discretize z-space into a uniform grid and to evaluate the integrand as a sum of the
form
E[f]/similarequalL/summationdisplay
l=1p(z(l))f(z(l)): (14.18)
An obvious problem with this approach is that the number of terms in the summation
grows exponentially with the dimensionality of z Furthermore, as we have already
noted, the kinds of probability distributions of interest will often have much of their
mass conÔ¨Åned to relatively small regions of z-space and so uniform sampling will be
very inefÔ¨Åcient because in high-dimensional problems, only a very small proportion
of the samples will make a signiÔ¨Åcant contribution to the sum We would really
like to choose sample points from regions where p(z) is large or ideally where the
productp(z)f (z)is large As with rejection sampling, importance sampling is based a proposal distribution
q(z)from which it is easy to draw samples, as illustrated in Figure 14.8 We can then
express the expectation in the form of a Ô¨Ånite sum over samples {z(l)}drawn from
q(z):
E[f] =/integraldisplay
f(z)p(z) d z
=/integraldisplay
f(z)p(z)
q(
z)q(z) dz
/similarequal1
LL/summationdisplay
l=1p
(z(l))
q(
z(l))f(z(l)): (14.19)
The quantities rl=p(z(l))=q(z(l))are known as importance weights, and they cor-
rect the bias introduced by sampling from the wrong distribution Note that, unlike
rejection sampling, all the samples generated are retained Often the distribution p(z) can be evaluated only up to a normalization constant,
so thatp(z) =/tildewidep(z)=Zpwhere/tildewidep(z) can be evaluated easily, whereas Zpis unknown SAMPLING
Figure 14.8 Importance sampling addresses the
problem of evaluating the expectation of
a functionf(z)with respect to a distri-
butionp(z)from which it is difÔ¨Åcult to
draw samples directly Instead, samples
{z(l)}are drawn from a simpler distribu-
tionq(z), and the corresponding terms
in the summation are weighted by the
ratiosp(z(l))=q(z(l))

============================================================

=== CHUNK 433 ===
Palavras: 365
Caracteres: 2642
--------------------------------------------------
p(z)f(z)
zq(z)
Similarly, we may wish to use an importance sampling distribution q(z) =/tildewideq(z)=Zq,
which has the same property We then have
E[f] =/integraldisplay
f(z)p(z) d z
=Zq
Zp/integraldisplay
f(z)/tildewidep(z)
/tildewideq(z)q(z) dz
/similarequalZq
Zp1
LL/summationdisplay
l=1/tildewiderlf(z(l)) (14.20)
where/tildewiderl=/tildewidep(z(l))=/tildewideq(z(l)) We can use the same sample set to evaluate the ratio
Zp=Zqwith the result
Zp
Zq=1
Zq/integraldisplay
/tildewidep(z) d z=/integraldisplay/tildewidep(z)
/tildewideq(z)q(z) dz
/similarequal1
LL/summationdisplay
l=1/tildewiderl (14.21)
and hence the expectation in (14.20) is given by a weighted sum:
E[f]/similarequalL/summationdisplay
l=1wlf(z(l)) (14.22)
where we have deÔ¨Åned
wl=/tildewiderl/summationtext
m/tildewiderm=/tildewidep(z(l))=q(z(l))/summationtext
m/tildewidep(z(m))=q(z(m)): (14.23)
Note that{wl}are non-negative numbers that sum to one As with rejection sampling, the success of importance sampling depends cru-
cially on how well the sampling distribution q(z)matches the desired distribution
p(z) If, as is often the case, p(z)f (z)is strongly varying and has a signiÔ¨Åcant pro-
portion of its mass concentrated over relatively small regions of z-space, then the
14.1 Basic Sampling Algorithms 439
set of importance weights {rl}may be dominated by a few weights having large
values, with the remaining weights being relatively insigniÔ¨Åcant Thus, the effective
sample size can be much smaller than the apparent sample size L The problem is
even more severe if none of the samples falls in the regions where p(z)f (z)is large In that case, the apparent variances of rlandrlf(z(l))may be small even though
the estimate of the expectation may be severely wrong Hence, a major drawback of
importance sampling is its potential to produce results that are arbitrarily in error and
with no diagnostic indication This also highlights a key requirement for the sam-
pling distribution q(z), namely that it should not be small or zero in regions where
p(z) may be signiÔ¨Åcant 14.1.6 Sampling-importance-resampling
The rejection sampling method discussed in Section 14.1.3 depends in part for
its success on the determination of a suitable value for the constant k For many
pairs of distributions p(z) andq(z), it will be impractical to determine a suitable
value forkas any value that is sufÔ¨Åciently large to guarantee a bound on the desired
distribution will lead to impractically small acceptance rates As with rejection sampling, the sampling-importance-resampling approach also
makes use of a sampling distribution q(z)but avoids having to determine the constant
k

============================================================

=== CHUNK 434 ===
Palavras: 356
Caracteres: 2617
--------------------------------------------------
There are two stages to the scheme In the Ô¨Årst stage, Lsamples z(1);:::;z(L)are
drawn from q(z) Then in the second stage, weights w1;:::;wLare constructed us-
ing (14.23) Finally, a second set of Lsamples is drawn from the discrete distribution
(z(1);:::;z(L))with probabilities given by the weights (w1;:::;wL) The resulting Lsamples are only approximately distributed according to p(z),
but the distribution becomes correct in the limit L‚Üí‚àû To see this, consider the
univariate case, and note that the cumulative distribution of the resampled values is
given by
p(z6a) =/summationdisplay
l:z(l)6awl
=/summationtext
lI(z(l)6a)/tildewidep(z(l))=q(z(l))/summationtext
l/tildewidep(z(l))=q(z(l))(14.24)
whereI(:)is the indicator function (which equals 1if its argument is true and 0
otherwise) Taking the limit L‚Üí‚àû and assuming suitable regularity of the dis-
tributions, we can replace the sums by integrals weighted according to the original
440 14 SAMPLING
sampling distribution q(z):
p(z6a) =/integraldisplay
I(z6a){/tildewidep(z)=q(z)}q(z) dz
/integraldisplay
{/tildewidep(z)=q(z)}q(z) dz
=/integraldisplay
I(z6a)/tildewidep(z) dz
/integraldisplay
/tildewidep(z) dz
=/integraldisplay
I(z6a)p(z ) dz; (14.25)
which is the cumulative distribution function of p(z) Again, we see that normaliza-
tion ofp(z)is not required For a Ô¨Ånite value of Land a given initial sample set, the resampled values will
only approximately be drawn from the desired distribution As with rejection sam-
pling, the approximation improves as the sampling distribution q(z)gets closer to
the desired distribution p(z) When q(z) =p(z), the initial samples (z(1);:::;z(L))
have the desired distribution and the weights wn= 1=L, so that the resampled values
also have the desired distribution If moments with respect to the distribution p(z) are required, then they can be
evaluated directly using the original samples together with the weights, because
E[f(z)] =/integraldisplay
f(z)p(z) d z
=/integraldisplay
f(z)[/tildewidep(z)=q (z)]q(z) dz
/integraldisplay
[/tildewidep(z)=q (z)]q(z) dz
/similarequalL/summationdisplay
l=1wlf(zl): (14.26)
14.2 Markov Chain Monte Carlo
In the previous section, we discussed the rejection sampling and importance sam-
pling strategies for evaluating expectations of functions, and we saw that they suffer
from severe limitations particularly in spaces of high dimensionality We therefore
turn in this section to a very general and powerful framework called Markov chain
Monte Carlo, which allows sampling from a large class of distributions and which
scales well with the dimensionality of the sample space

============================================================

=== CHUNK 435 ===
Palavras: 363
Caracteres: 2257
--------------------------------------------------
Markov chain Monte Carlo
methods have their origins in physics (Metropolis and Ulam, 1949), and it was only
14.2 Markov Chain Monte Carlo 441
towards the end of the 1980s that they started to have a signiÔ¨Åcant impact in the Ô¨Åeld
of statistics As with rejection and importance sampling, we again sample from a proposal
distribution This time, however, we maintain a record of the current state z(),
and the proposal distribution q(z|z())is conditioned on this current state, and so
the sequence of samples z(1);z(2);::: forms a Markov chain Again, if we write Section 14.2.2
p(z) =/tildewidep(z)=Zp, we will assume that /tildewidep(z) can readily be evaluated for any given
value of z, although the value of Zpmay be unknown The proposal distribution
is chosen to be sufÔ¨Åciently simple that it is straightforward to draw samples from it
directly At each cycle of the algorithm, we generate a candidate sample z?from
the proposal distribution and then accept the sample according to an appropriate
criterion 14.2.1 The Metropolis algorithm
In the basic Metropolis algorithm (Metropolis et al., 1953), we assume that the
proposal distribution is symmetric, that is q(zA|zB) =q(zB|zA)for all values of
zAandzB The candidate sample is then accepted with probability
A(z?;z()) = min/parenleftbigg
1;/tildewidep(z?)
/tildewidep
(z())/parenrightbigg
: (14.27)
This can be achieved by choosing a random number uwith uniform distribution over
the unit interval (0;1)and then accepting the sample if A(z?;z())> u Note that
if the step from z()toz?causes an increase in the value of p(z), then the candidate
point is certain to be kept If the candidate sample is accepted, then z(+1)=z?, otherwise the candidate
pointz?is discarded, z(+1)is set to z(), and another candidate sample is drawn
from the distribution q(z|z(+1)) This is in contrast to rejection sampling, where re-
jected samples are simply discarded In the Metropolis algorithm, when a candidate
point is rejected, the previous sample is included in the Ô¨Ånal list of samples, leading
to multiple copies of samples Of course, in a practical implementation, only a single
copy of each retained sample would be kept, along with an integer weighting factor
recording how many times that state appears

============================================================

=== CHUNK 436 ===
Palavras: 363
Caracteres: 2316
--------------------------------------------------
As we will see, if q(zA|zB)is positive
for any values of zAandzB(this is a sufÔ¨Åcient but not necessary condition), the
distribution of z()tends top(z) as‚Üí‚àû It should be emphasized, however, that
the sequence z(1);z(2);::: is not a set of independent samples from p(z) because
successive samples are highly correlated If we wish to obtain independent samples,
then we can discard most of the sequence and just retain every Mth sample For
MsufÔ¨Åciently large, the retained samples will for all practical purposes be inde-
pendent The Metropolis algorithm in summarized in Algorithm 14.1 Figure 14.9
shows a simple illustrative example of sampling from a two-dimensional Gaussian
distribution using the Metropolis algorithm in which the proposal distribution is an
isotropic Gaussian Further insight into the nature of Markov chain Monte Carlo algorithms can be
gleaned by looking at the properties of a speciÔ¨Åc example, namely a simple random
442 14 SAMPLING
Algorithm 14.1: Metropolis sampling
Input: Unnormalized distribution /tildewidep(z)
Proposal distribution q(z|/hatwidez)
Initial state z(0)
Number of iterations T
Output: z‚àº/tildewidep(z)
zprev‚Üêz(0)
// Iterative message-passing
for‚àà{1;:::;T}do
z?‚àºq(z|z prev)// Sample from proposal distribution
u‚àºU(0;1)// Sample from uniform
if/tildewidep(z?)=/tildewidep(zprev)>u then
zprev‚Üêz?//z()=z else
zprev‚Üêzprev//z()=z(‚àí1)
end if
end for
return zprev //z(T)
walk Consider a state space zconsisting of the integers, with probabilities
p(z(+1)=z()) = 0:5 (14.28)
p(z(+1)=z()+ 1) = 0 :25 (14.29)
p(z(+1)=z()‚àí1) = 0:25 (14.30)
wherez()denotes the state at step If the initial state is z(0)= 0, then by symmetry
the expected state at time will also be zero E[z()] = 0, and similarly it is easily
seen that E[(z())2] ==2 Thus, after steps, the random walk has travelled Exercise 14.11
only a distance that on average is proportional to the square root of This square
root dependence is typical of random walk behaviour and shows that random walks
are very inefÔ¨Åcient in exploring the state space As we will see, a central goal in
designing Markov chain Monte Carlo methods is to avoid random walk behaviour 14.2.2 Markov chains
Before discussing Markov chain Monte Carlo methods in more detail, it is useful
to study some general properties of Markov chains

============================================================

=== CHUNK 437 ===
Palavras: 388
Caracteres: 2757
--------------------------------------------------
In particular, we ask under what
circumstances will a Markov chain converge to the desired distribution Markov Chain Monte Carlo 443
Figure 14.9 A simple illustration using the
Metropolis algorithm to sam-
ple from a Gaussian distri-
bution whose one standard-
deviation contour is shown by
the ellipse The proposal dis-
tribution is an isotropic Gaus-
sian distribution whose stan-
dard deviation is 0.2 Steps
that are accepted are shown
as green lines, and rejected
steps are shown in red A
total of 150 candidate sam-
ples are generated, of which
43 are rejected 0 0.5 1 1.5 2 2.5 300.511.522.53
Markov chain is deÔ¨Åned to be a series of random variables z(1);:::;z(M)such that
the following conditional independence property holds for m‚àà{1;:::;M‚àí1}:
p(z(m+1)|z(1);:::;z(m)) =p(z(m+1)|z(m)); (14.31)
which can be represented as a directed graphical model in the form of a chain We Figure 11.29
can then specify the Markov chain by giving the probability distribution for the ini-
tial variable p(z(0))together with the conditional distributions for subsequent vari-
ables in the form of transition probabilities Tm(z(m);z(m+1))‚â°p(z(m+1)|z(m)) A
Markov chain is called homogeneous if the transition probabilities are the same for
allm The marginal probability for a particular variable can be expressed in terms of
the marginal probability for the previous variable in the chain:
p(z(m+1)) =/integraldisplay
p(z(m+1)|z(m))p(z(m)) dz(m)(14.32)
where the integral is replaced by a summation for discrete variables A distribution
is said to be invariant, or stationary, with respect to a Markov chain if each step in
the chain leaves that distribution invariant Thus, for a homogeneous Markov chain
with transition probabilities T(z/prime;z), the distribution p?(z)is invariant if
p?(z) =/integraldisplay
T(z/prime;z)p?(z/prime) dz/prime: (14.33)
Note that a given Markov chain may have more than one invariant distribution For
instance, if the transition probabilities are given by the identity transformation, then
any distribution will be invariant SAMPLING
A sufÔ¨Åcient (but not necessary) condition for ensuring that the required distribu-
tionp(z) is invariant is to choose the transition probabilities to satisfy the property
ofdetailed balance, deÔ¨Åned by
p?(z)T(z;z/prime) =p?(z/prime)T(z/prime;z) (14.34)
for the particular distribution p?(z) It is easily seen that a transition probability
that satisÔ¨Åes detailed balance with respect to a particular distribution will leave that
distribution invariant, because
/integraldisplay
p?(z/prime)T(z/prime;z) dz/prime=/integraldisplay
p?(z)T(z;z/prime) dz/prime(14.35)
=p?(z)/integraldisplay
p(z/prime|z) dz/prime(14.36)
=p?(z): (14.37)
A Markov chain that respects detailed balance is said to be reversible

============================================================

=== CHUNK 438 ===
Palavras: 387
Caracteres: 2705
--------------------------------------------------
Our goal is to use Markov chains to sample from a given distribution We can
achieve this if we set up a Markov chain such that the desired distribution is invariant However, we must also require that for m‚Üí‚àû, the distribution p(z(m))converges
to the required invariant distribution p?(z), irrespective of the choice of initial dis-
tributionp(z(0)) This property is called ergodicity, and the invariant distribution
is then called the equilibrium distribution Clearly, an ergodic Markov chain can
have only one equilibrium distribution It can be shown that a homogeneous Markov
chain will be ergodic, subject only to weak restrictions on the invariant distribution
and the transition probabilities (Neal, 1993) In practice we often construct the transition probabilities from a set of ‚Äòbase‚Äô
transitionsB1;:::;BK This can be achieved through a mixture distribution of the
form
T(z/prime;z) =K/summationdisplay
k=1kBk(z/prime;z) (14.38)
for some set of mixing coefÔ¨Åcients 1;:::;Ksatisfyingk>0and/summationtext
kk= 1 Alternatively, the base transitions may be combined through successive application,
so that
T(z/prime;z) =/summationdisplay
z1:::/summationdisplay
zn‚àí1B1(z/prime;z1):::BK‚àí1(zK‚àí2;zK‚àí1)BK(zK‚àí1;z): (14.39)
If a distribution is invariant with respect to each of the base transitions, then clearly it
will also be invariant with respect to either of the T(z/prime;z)given by (14.38) or (14.39) For the mixture (14.38), if each of the base transitions satisÔ¨Åes detailed balance, then
the mixture transition Twill also satisfy detailed balance This does not hold for the
transition probability constructed using (14.39), although by symmetrizing the order
of application of the base transitions, namely B1;B2;:::;BK;BK;:::;B 2;B1, de-
tailed balance can be restored A common example of the use of composite transition
probabilities is where each base transition changes only a subset of the variables Markov Chain Monte Carlo 445
14.2.3 The Metropolis‚ÄìHastings algorithm
Earlier we introduced the basic Metropolis algorithm without actually demon-
strating that it samples from the required distribution Before giving a proof, we Ô¨Årst
discuss a generalization, known as the Metropolis‚ÄìHastings algorithm (Hastings,
1970), which applies when the proposal distribution is no longer a symmetric func-
tion of its arguments In particular at step of the algorithm, in which the current
state is z(), we draw a sample z?from the distribution qk(z|z())and then accept it
with probability Ak(z?;z())where
Ak(z?;z()) = min/parenleftbigg
1;/tildewidep(z?)qk(z()|z?)
/tildewidep
(z())qk(z?|z())/parenrightbigg
: (14.40)
Hereklabels the members of the set of possible transitions being considered

============================================================

=== CHUNK 439 ===
Palavras: 404
Caracteres: 2840
--------------------------------------------------
Again,
evaluating the acceptance criterion does not require knowledge of the normalizing
constantZpin the probability distribution p(z) =/tildewidep(z)=Zp For a symmetric pro-
posal distribution, the Metropolis‚ÄìHastings criterion (14.40) reduces to the standard
Metropolis criterion given by (14.27) Metropolis‚ÄìHastings sampling is summarized
in Algorithm 14.2 We can show that p(z) is an invariant distribution of the Markov chain deÔ¨Åned
by the Metropolis‚ÄìHastings algorithm by showing that detailed balance, deÔ¨Åned by
(14.34), is satisÔ¨Åed Using (14.40) we have
p(z)qk(z/prime|z)Ak(z/prime;z) = min (p(z)q k(z/prime|z);p(z/prime)qk(z|z/prime))
= min (p(z/prime)qk(z|z/prime);p(z)qk(z/prime|z))
=p(z/prime)qk(z|z/prime)Ak(z;z/prime) (14.41)
as required The speciÔ¨Åc choice of proposal distribution can have a marked effect on the per-
formance of the algorithm For continuous state spaces, a common choice is a Gaus-
sian centred on the current state, leading to an important trade-off in determining the
variance parameter of this distribution If the variance is small, then the proportion of
accepted transitions will be high, but progress through the state space takes the form
of a slow random walk leading to long correlation times However, if the variance
parameter is large, then the rejection rate will be high because, in the kind of com-
plex problems we are considering, many of the proposed steps will be to states for
which the probability p(z) is low Consider a multivariate distribution p(z) having
strong correlations between the components of z, as illustrated in Figure 14.10 The
scaleof the proposal distribution should be as large as possible without incurring
high rejection rates This suggests that should be of the same order as the smallest
length scale min The system then explores the distribution along the more extended
direction by means of a random walk, and so the number of steps to arrive at a state
that is more or less independent of the original state is of order (max=min)2 In fact
in two dimensions, the increase in rejection rate as increases is offset by the larger
step sizes of those transitions that are accepted, and more generally for a multivari-
ate Gaussian, the number of steps required to obtain independent samples scales like
446 14 SAMPLING
Algorithm 14.2: Metropolis-Hastings sampling
Input: Unnormalized distribution /tildewidep(z)
Proposal distributions {qk(z|/hatwidez) :k‚àà1;:::;K}
Mapping from iteration index to distribution index M(¬∑)
Initial state z(0)
Number of iterations T
Output: z‚àº/tildewidep(z)
zprev‚Üêz(0)
// Iterative message-passing
for‚àà{1;:::;T}do
k‚ÜêM()// get distribution index for this iteration
z?‚àºqk(z|z prev)// sample from proposal distribution
u‚àºU(0;1)// sample from uniform
if/tildewidep(z?)q(zprev|z?)=/tildewidep(zprev)q(z?|zprev)>u then
zprev‚Üêz?//z()=z

============================================================

=== CHUNK 440 ===
Palavras: 373
Caracteres: 2365
--------------------------------------------------
else
zprev‚Üêzprev//z()=z(‚àí1)
end if
end for
return zprev //z(T)
(max=2)2where2is the second-smallest standard deviation (Neal, 1993) These
details aside, if the length scales over which the distributions vary are very different
in different directions, then the Metropolis Hastings algorithm can have very slow
convergence 14.2.4 Gibbs sampling
Gibbs sampling (Geman and Geman, 1984) is a simple and widely applica-
ble Markov chain Monte Carlo algorithm and can be seen as a special case of the
Metropolis‚ÄìHastings algorithm Consider the distribution p(z) =p(z1;:::;zM)
from which we wish to sample, and suppose that we have chosen some initial state
for the Markov chain Each step of the Gibbs sampling procedure involves replacing
the value of one of the variables by a value drawn from the distribution of that vari-
able conditioned on the values of the remaining variables Thus, we replace ziby
a value drawn from the distribution p(zi|z\i), wherezidenotes the ith component
ofz, and z\idenotes{z1;:::;zM}but withziomitted This procedure is repeated
either by cycling through the variables in some particular order or by choosing the
14.2 Markov Chain Monte Carlo 447
Figure 14.10 Schematic illustration of using an isotropic
Gaussian proposal distribution (blue circle) to
sample from a correlated multivariate Gaus-
sian distribution (red ellipse) having very differ-
ent standard deviations in different directions,
using the Metropolis‚ÄìHastings algorithm To
keep the rejection rate low, the scale of
the proposal distribution should be of the or-
der of the smallest standard deviation min,
which leads to random walk behaviour in which
the number of steps separating states that
are approximately independent is of order
(max=min)2wheremaxis the largest stan-
dard deviation max
min
variable to be updated at each step at random from some distribution For example, suppose we have a distribution p(z1;z2;z3)over three variables,
and at stepof the algorithm we have selected values z()
1;z()
2, andz()
3 We Ô¨Årst
replacez()
1by a new value z(+1)
1 obtained by sampling from the conditional distri-
bution
p(z1|z()
2;z()
3): (14.42)
Next we replace z()
2by a valuez(+1)
2 obtained by sampling from the conditional
distribution
p(z2|z(+1)
1;z()
3) (14.43)
so that the new value for z1is used straight away in subsequent sampling steps

============================================================

=== CHUNK 441 ===
Palavras: 356
Caracteres: 2386
--------------------------------------------------
Then
we updatez3with a sample z(+1)
3 drawn from
p(z3|z(+1)
1;z(+1)
2 ) (14.44)
and so on, cycling through the three variables in turn Gibbs sampling is summarized
in Algorithm 14.3 To show that this procedure samples from the required distribution, we Ô¨Årst note
that the distribution p(z) is an invariant of each of the Gibbs sampling steps individu-
ally and hence of the whole Markov chain This follows since when we sample from
p(zi|z\i), the marginal distribution p(z\i)is clearly invariant because the value of
z\iis unchanged Also, each step by deÔ¨Ånition samples from the correct conditional
distributionp(zi|z\i) Because these conditional and marginal distributions together
specify the joint distribution, we see that the joint distribution is itself invariant The second requirement to be satisÔ¨Åed to ensure that the Gibbs sampling proce-
dure samples from the correct distribution is that it is ergodic A sufÔ¨Åcient condition
for ergodicity is that none of the conditional distributions are anywhere zero If this
is the case, then any point in z-space can be reached from any other point in a Ô¨Ånite
number of steps involving one update of each of the component variables If this
requirement is not satisÔ¨Åed, so that some of the conditional distributions have zeros,
then ergodicity, if it applies, must be proven explicitly SAMPLING
Algorithm
14.3: Gibbs sampling
Input: Initial values{zi:i‚àà1;:::;M}
Conditional distributions {p(zi|{zj/negationslash=i}) :i‚àà1;:::;M}
Number of iterations T
Output: Final values{zi:i‚àà1;:::;M}
f
or‚àà{1;:::;T}do
f
ori‚àà{1;:::;M}do
zi‚àºp
(zi|{zj/negationslash=i})
end for
end for
return{zi:i‚àà1;:::;M}
The distribution of initial states must also be speciÔ¨Åed to complete the algorithm,
although samples drawn after many iterations will effectively become independent
of this distribution Of course, successive samples from the Markov chain will be
highly correlated, and so to obtain samples that are nearly independent it will be
necessary to sub-sample the sequence We can obtain the Gibbs sampling procedure as a particular instance of the
Metropolis‚ÄìHastings algorithm as follows Consider a Metropolis‚ÄìHastings sam-
pling step involving the variable zkin which the remaining variables z\kremain
Ô¨Åxed, and for which the transition probability from ztoz?is given byqk(z?|z) =
p(z \k=z\kbecause these components are unchanged by the
sampling step

============================================================

=== CHUNK 442 ===
Palavras: 371
Caracteres: 2435
--------------------------------------------------
Also, p(z) =p(zk|z\k)p(z\k) Thus, the factor that determines the
acceptance probability in the Metropolis‚ÄìHastings (14.40) is given by
A(z?;z) =p(z?)qk(z|z?)
p
(z)qk(z?|z)=p(z \k)
p
(zk|z\k)p(z\k)p(z k|z\k)= 1 (14.45)
where we have used z Thus, the Metropolis‚ÄìHastings steps are always
accepted As with the Metropolis algorithm, we can gain some insight into the behaviour of
Gibbs sampling by investigating its application to a Gaussian distribution Consider
a correlated Gaussian in two variables, as illustrated in Figure 14.11, having con-
ditional distributions of width land marginal distributions of width L The typical
step size is governed by the conditional distributions and will be of order l Because
the state evolves according to a random walk, the number of steps needed to obtain
independent samples from the distribution will be of order (L=l)2 Of course if the
Gaussian distribution were uncorrelated, then the Gibbs sampling procedure would
be optimally efÔ¨Åcient For this simple problem, we could rotate the coordinate sys-
tem such that the new variables are uncorrelated However, in practical applications
14.2 Markov Chain Monte Carlo 449
Figure 14.11 Illustration of Gibbs sampling by al-
ternate updates of two variables
whose distribution is a correlated
Gaussian The step size is governed
by the standard deviation of the con-
ditional distribution (green curve),
and isO(l), leading to slow progress
in the direction of elongation of the
joint distribution (red ellipse) The
number of steps needed to obtain an
independent sample from the distri-
bution isO((L=l )2) z1z2
L
l
it
will generally be infeasible to Ô¨Ånd such transformations One approach to reducing the random walk behaviour in Gibbs sampling is
called over-relaxation (Adler, 1981) In its original form, it applies to problems for
which the conditional distributions are Gaussian, which represents a more general
class of distributions than the multivariate Gaussian because, for example, the non-
Gaussian distribution p(z;y )‚àùexp(‚àíz2y2)has Gaussian conditional distributions At each step of the Gibbs sampling algorithm, the conditional distribution for a par-
ticular component zihas some mean iand some variance 2
i In the over-relaxation
framework, the value of ziis replaced with
z/prime
i=i+i(zi‚àíi) +i(1‚àí2
i)1=2 (14.46)
whereis a Gaussian random variable with zero mean and unit variance, and 
is a parameter such that ‚àí1<  < 1

============================================================

=== CHUNK 443 ===
Palavras: 356
Caracteres: 2364
--------------------------------------------------
For= 0, the method is equivalent to
standard Gibbs sampling, and for <0the step is biased to the opposite side of the
mean This step leaves the desired distribution invariant because if zihas meani
and variance 2
i, then so too does z/prime
i The effect of over-relaxation is to encourage Exercise 14.14
directed motion through state space when the variables are highly correlated The
framework of ordered over-relaxation (Neal, 1999) generalizes this approach to non-
Gaussian distributions The practical applicability of Gibbs sampling depends on the ease with which
samples can be drawn from the conditional distributions p(zk|z\k) For probability
distributions speciÔ¨Åed using directed graphical models, the conditional distributions
for individual nodes depend only on the variables in the corresponding Markov blan-
ket, as illustrated in Figure 14.12 For directed graphs, a wide choice of conditional
distributions for the individual nodes conditioned on their parents will lead to condi-
tional distributions for Gibbs sampling that are log concave The adaptive rejection
sampling methods discussed in Section 14.1.4 therefore provide a framework for
Monte Carlo sampling from directed graphs with broad applicability SAMPLING
Figure 14.12 The Gibbs sampling method requires samples
to be drawn from the conditional distribution
of a variable zconditioned on the remaining
variables For directed graphical models, this
conditional distribution is a function of only the
states of the nodes in the Markov blanket,
shaded in blue, which comprises the parents,
the children, and the co-parents.z
Because the basic Gibbs sampling technique considers one variable at a time,
there are strong dependencies between successive samples At the opposite extreme,
if we could draw samples directly from the joint distribution (an operation that we
are supposing is intractable), then successive samples would be independent We
can hope to improve on the simple Gibbs sampler by adopting an intermediate strat-
egy in which we sample successively from groups of variables rather than individual
variables This is achieved in the blocking Gibbs sampling algorithm by choosing
blocks of variables, not necessarily disjoint, and then sampling jointly from the vari-
ables in each block in turn, conditioned on the remaining variables (Jensen, Kong,
and Kjaerulff, 1995)

============================================================

=== CHUNK 444 ===
Palavras: 388
Caracteres: 2422
--------------------------------------------------
14.2.5 Ancestral sampling
For many models, the joint distribution p(z) is conveniently speciÔ¨Åed in terms
of a graphical model For a directed graph with no observed variables, it is straight-
forward to sample from the joint distribution using the following ancestral sampling
approach The joint distribution is speciÔ¨Åed by
p(z) =M/productdisplay
i=1p(zi|pa(i)) (14.47)
where ziare the set of variables associated with node i, and pa(i)denotes the set
of variables associated with the parents of node i To obtain a sample from the joint
distribution, we make one pass through the set of variables in the order z1;:::;zM
sampling from the conditional distributions p(zi|pa(i)) This is always possible
because at each step, all the parent values will have been instantiated After one pass
through the graph, we will have obtained a sample from the joint distribution This
assumes that it is possible to sample from the individual conditional distributions at
each node Now consider a directed graph in which some of the nodes, which comprise the
evidence setE, are instantiated with observed values We can in principle extend
the above procedure, at least for nodes representing discrete variables, to give the
following logic sampling approach (Henrion, 1988), which can be seen as a special
case of importance sampling At each step, when a sampled value is obtained for a Section 14.1.5
variable ziwhose value is observed, the sampled value is compared to the observed
value, and if they agree then the sample value is retained and the algorithm proceeds
to the next variable in turn However, if the sampled value and the observed value
disagree, then the whole sample so far is discarded and the algorithm starts again
14.3 Langevin Sampling 451
with the Ô¨Årst node in the graph This algorithm samples correctly from the posterior
distribution because it corresponds simply to drawing samples from the joint distri-
bution of hidden variables and data variables and then discarding those samples that
disagree with the observed data (with the slight saving of not continuing with the
sampling from the joint distribution as soon as one contradictory value is observed) However, the overall probability of accepting a sample from the posterior decreases
rapidly as the number of observed variables increases and as the number of states that
those variables can take increases, and so this approach is rarely used in practice

============================================================

=== CHUNK 445 ===
Palavras: 363
Caracteres: 2367
--------------------------------------------------
An improvement on this approach is called likelihood weighted sampling (Fung
and Chang, 1990; Shachter and Peot, 1990) It is based on ancestral sampling com-
bined with importance sampling For each variable in turn, if that variable is in the
evidence set, then it is just set to its instantiated value If it is not in the evidence set,
then it is sampled from the conditional distribution p(zi|pa(i))in which the condi-
tioning variables are set to their currently sampled values The weighting associated
with the resulting sample zis then given by Exercise 14.15
r(z) =/productdisplay
zi/negationslash‚ààep(zi|pa(i))
p(zi|pa(i))/productdisplay
zi‚ààep(zi|pa(i))
1=/productdisplay
zi‚ààep(zi|pa(i)): (14.48)
This method can be further extended using self-importance sampling (Shachter and
Peot, 1990) in which the importance sampling distribution is continually updated to
reÔ¨Çect the current estimated posterior distribution Langevin Sampling
The Metropolis‚ÄìHastings algorithm draws samples from a probability distribution
by creating a Markov chain of candidate samples using a proposal distribution and
then accepting or rejecting them using the criterion (14.40) This can be relatively
inefÔ¨Åcient since the proposal distribution is often a simple, Ô¨Åxed distribution that can
generate updates in any direction in the data space, leading to a random walk We have seen that when training neural networks, it is hugely advantageous to
make use of the gradient of the log likelihood with respect to the learnable param-
eters of the model in order to maximize the likelihood function By analogy, we
can introduce Markov chain sampling algorithms that make use of the gradient of
the probability density with respect to the data vector so as to take steps that pref-
erentially move towards regions of higher probability One such technique is called
Hamiltonian Monte Carlo, also known as hybrid Monte Carlo This again makes
use of a Metropolis acceptance test (Duane et al., 1987; Bishop, 2006) Here we will
focus on a different approach that is widely used in deep learning, called Langevin
sampling Although it avoids the use of an acceptance test, the algorithm has to be
designed carefully to ensure that the resulting samples are unbiased An important
application of Langevin sampling arises in the context of machine learning models
deÔ¨Åned in terms of energy functions

============================================================

=== CHUNK 446 ===
Palavras: 374
Caracteres: 2391
--------------------------------------------------
SAMPLING
14.3.1 Energy-based models
Many generative models can be expressed as conditional probability distribu-
tionsp(x|w )where xis the data vector and wrepresents a vector of learnable pa-
rameters Such models can be trained by maximizing the corresponding likelihood
function deÔ¨Åned with respect to a training data set However, to represent a valid
probability distribution, the model must satisfy
/integraldisplay
p(x|w )p(x) d x= 1: (14.49)
Ensuring that this requirement is met can signiÔ¨Åcantly limit the allowable forms
for the model If we put aside the normalization constraint then we can consider
a much broader class of models called energy-based models (LeCun et al., 2006) Suppose we have a function E(x;w), called the energy function, which is a real-
valued function of its arguments but which has no other constraints The exponential
exp{‚àíE (x;w)}is a non-negative quantity and can therefore be viewed as an un-
normalized probability distribution over x Here the introduction of the minus sign
in the exponent is simply a convention, and it means that higher values of energy cor-
respond to lower values of probability We can then deÔ¨Åne a normalized distribution
using
p(x|w ) =1
Z(w)exp{‚àíE(x;w)} (14.50)
where the normalizing constant Z(w), known as the partition function, is deÔ¨Åned by Exercise 14.16
Z(w) =/integraldisplay
exp{‚àíE(x;w)}dx: (14.51)
The energy function is often modelled using a deep neural network with input vector
xand a scalar output E(x;w), where wrepresents the weights and biases in the
network Note that the partition function depends on w, which creates problems for train-
ing For example, the log likelihood function for a data set D= (x 1;:::;xN)of
i.i.d data has the form Exercise 14.17
lnp(D|w ) =‚àíN/summationdisplay
n=1E(xn;w)‚àíNlnZ(w): (14.52)
To compute the gradient of lnp(D|w )with respect to w, we need to know the form
ofZ(w) However, for many choices of the energy function E(x;w), it will be
impractical to evaluate the partition function in (14.51) because this involves inte-
grating (or summing for discrete variables) over all the whole of x-space The term
‚Äòenergy-based model‚Äô is generally used for models where this integral is intractable Note, however, that probabilistic models can be seen as special cases of energy-based
models, and therefore many of the models discussed in this book can be viewed as
energy-based models

============================================================

=== CHUNK 447 ===
Palavras: 369
Caracteres: 2388
--------------------------------------------------
The big advantage of energy-based models, therefore, is their
Ô¨Çexibility in that they bypass the requirement for normalization A corresponding
disadvantage, however, is that since the normalizing constant is unknown, they can
be more difÔ¨Åcult to train Langevin Sampling 453
14.3.2 Maximizing the likelihood
Various approximation methods have been developed to train energy-based mod-
els without having to evaluate the partition function (Song and Kingma, 2021) Here
we look at techniques based on Markov chain Monte Carlo An alternative approach,
called score matching, will be discussed in the context of diffusion models Chapter 20
We have seen that for energy-based models, the likelihood function cannot be
evaluated explicitly due to the unknown partition function Z(w) However, we can
make use of Monte Carlo sampling methods to approximate the gradient of the log
likelihood with respect to the model parameters Once an energy-based model has
been trained, by whatever means, we also need a way to draw samples from the
model, and again we can make use of Monte Carlo methods Using (14.50), the gradient, with respect to the model parameters, of the log
likelihood function for an energy-based model can be written in the form
‚àáwlnp(x|w ) =‚àí‚àáwE(x;w)‚àí‚àá wlnZ(w): (14.53)
This is the likelihood function for a single data point x, but in practice we want to
maximize the likelihood deÔ¨Åned over a training set of data points drawn from some
unknown distribution pD(x) If we assume the data points are i.i.d., then we can
consider the gradient of the expectation of the log likelihood with respect to pD(x),
which is then given by
Ex‚àºpD[‚àáwlnp(x|w )] =‚àíEx‚àºpD[‚àáwE(x;w)]‚àí‚àá wlnZ(w) (14.54)
where we have made use of the fact that the Ô¨Ånal term ‚àí‚àáwlnZ(w)does not depend
onxand can therefore be taken outside the expectation The partition function Z(w)
is assumed to be unknown, but we can make use of (14.51) and rearrange to obtain Exercise 14.18
‚àí‚àáwlnZ(w) =/integraldisplay
{‚àáwE(x;w)}p(x|w ) dx: (14.55)
The right-hand side of (14.55) corresponds to an expectation over the model distri-
butionp(x|w )given by
/integraldisplay
{‚àáwE(x;w)}p(x|w ) dx=Ex‚àºM [‚àáwE(x;w)]: (14.56)
Combining (14.54), (14.55), and (14.56) we obtain Exercise 14.18
‚àáwEx‚àºpD[lnp(x|w )] =‚àíEx‚àºpD[‚àáwE(x;w)]
+Ex‚àºpM(x)[‚àáwE(x;w)]:(14.57)
This result is illustrated in Figure 14.13, and has a nice interpretation, as follows

============================================================

=== CHUNK 448 ===
Palavras: 358
Caracteres: 2231
--------------------------------------------------
Our
goal is to Ô¨Ånd values for the parameters wthat maximize the likelihood function, and
therefore consider a small change to win the direction of the gradient ‚àáwlnp(x|w ) From (14.57) we see that expected value of this gradient can be expressed as two
terms, having opposite signs The Ô¨Årst term on the right-hand side of (14.57) acts
454 14 SAMPLING
xpD(x)pM(x)E(x,w)
Figure 14.13 Illustration of the training of an energy-based model by maximizing the likelihood, show-
ing the energy function E(x;w)in green along with the associated model distribution
pM(x)and the true data distribution pD(x) Increasing the expected log likelihood by us-
ing (14.57) pushes the energy function up at points corresponding to samples from the
model (shown as blue dots) and pushes it down at points corresponding to samples from
the data set (shown as red dots) to decrease E(x;w), and therefore to increase the probability density deÔ¨Åned by
the model, for points xdrawn from pD(x) The second term on the right-hand
side of (14.57) acts to increase the value of E(x;w), and therefore to decrease the
probability density deÔ¨Åned by the model, for data points drawn from the model itself In regions where the model density exceeds the training data density, the net effect
will be to increase the energy and therefore reduce the probability Conversely, in
regions where training data density exceeds the model density, the net effect will be
to reduce the energy and therefore increase the probability density Together these
two terms move probability mass away from regions where there is a low density of
training data and towards regions of high data density, as desired The two terms will
be equal in magnitude when the model distribution matches the data distribution, at
which point the gradient on the left-hand-side of (14.57) will equal zero 14.3.3 Langevin dynamics
When applying (14.57) as a practical training method, we need to approximate
the two terms on the right-hand side For any given value of x, we can evaluate
‚àáwE(x;w)using automatic differentiation For the Ô¨Årst term in (14.57), we can
use the training data set to estimate the expectation over x:
Ex‚àºpD[‚àáwE(x;w)]/similarequal1
NN/summationdisplay
n=1‚àáwE(xn;w): (14.58)
14.3

============================================================

=== CHUNK 449 ===
Palavras: 357
Caracteres: 2249
--------------------------------------------------
Langevin Sampling 455
The second term is more challenging because we need to draw samples from the
model distribution deÔ¨Åned by an energy function whose corresponding partition
function is intractable This can be done using Markov chain Monte Carlo methods One popular approach is called stochastic gradient Langevin dynamics or simply
Langevin sampling (Parisi, 1981; Welling and Teh, 2011) This term depends on
the distribution p(x|w )only through the score function, which is deÔ¨Åned to be the
gradient of the log likelihood with respect to the data vector x, and is given by
s(x;w) =‚àáxlnp(x|w ): (14.59)
It is worth emphasising that this gradient is taken with respect to the data point xand
is therefore not the usual gradient with respect to the learnable parameters w If we
substitute (14.50) into (14.59) we obtain
s(x;w) =‚àí‚àáxE(x;w) (14.60)
where we see that the partition function no longer appears at it is independent of x We start by drawing an initial value x(0)from a prior distribution, and then we
iterate the following Markov chain steps:
x(+1)=x()+‚àáxlnp(x();w) +/radicalbig
2(); ‚àà1;:::;T (14.61)
where()‚àºN (0;I)are independent samples from a zero-mean, unit-covariance
Gaussian distribution, and the parameter controls the step size Each iteration of the
Langevin equation takes a step in the direction of the gradient of the log likelihood,
and then adds Gaussian noise It can be show that, in the limits of ‚Üí0and
T ‚Üí‚àû, the value of z(T)is an independent sample from the distribution p(x) Langevin sampling is summarized in Algorithm 14.4 We can repeat the process to generate a set of samples {x1;:::;xM}and then
approximate the second term in (14.57) using
Ex‚àºpM(x)[‚àáwE(x;w)]/similarequal1
MM/summationdisplay
m=1‚àáwE(xm;w): (14.62)
Running long Markov chains to generate independent samples can be compu-
tationally expensive, and so we need to consider practical approximations One ap-
proach is called contrastive divergence (Hinton, 2002) Here the samples used to
evaluate (14.62) are obtained by running a Monte Carlo chain starting with one of
the training data points xn If the chain is run for a large number of steps, then the
resulting value will be essentially an unbiased sample from the model distribution

============================================================

=== CHUNK 450 ===
Palavras: 356
Caracteres: 2268
--------------------------------------------------
Instead Hinton (2002) proposes running for only a few steps of Monte Carlo, perhaps
even as few as one step, which is computationally much less costly The resulting
sample will be far from unbiased and will lie close to the data manifold As a result,
the effect of using gradient descent will be to shape the energy surface, and hence
the probability density, only in the neighbourhood of the data manifold This can
prove effective for tasks such as discrimination but is expected to be less effective in
learning a generative model SAMPLING
Algorithm
14.4: Langevin sampling
Input: Initial value x(0)
Probability density p(x;w)
Learning rate parameter 
Number of iterations T
Output: Final value x(T)
x‚Üêx0
f
or‚àà{1;:::;T}do
‚àº
N(|0;I)
x‚Üêx+‚àáxlnp(x;w) +‚àö2

end for
return x// Final value x(T)
Ex
ercises
14.1 (?)Show thatfdeÔ¨Åned
by (14.2) is an unbiased estimator, in other words that the
expectation of the right-hand side is equal to E[f(z)] 14.2 (?)Show thatfdeÔ¨Åned
by (14.2) has variance given by (14.4) 14.3 (?)Suppose that zis a random variable with uniform distribution over (0;1)and that
we transform zusingy=h‚àí1(z)whereh(y)is given by (14.6) Show that yhas
the distribution p(y) 14.4 (??) Given a random variable zthat is uniformly distributed over (0;1), Ô¨Ånd a trans-
formationy=f(z)such thatyhas a Cauchy distribution given by (14.8) 14.5 (??) Suppose that z1andz2are uniformly distributed over the unit circle, as shown in
Figure 14.3, and that we make the change of variables given by (14.10) and (14.11) Show that (y1;y2)will be distributed according to (14.12) 14.6 (??) Letzbe aD-dimensional random variable having a Gaussian distribution with
zero mean and unit covariance matrix, and suppose that the positive deÔ¨Ånite sym-
metric matrix has the Cholesky decomposition =LLT, where Lis a lower-
triangular matrix (i.e., one with zeros above the leading diagonal) Show that the
variable y=+Lzhas a Gaussian distribution with mean and covariance  This provides a technique for generating samples from a general multivariate Gaus-
sian using samples from a univariate Gaussian having zero mean and unit variance 14.7 (??) In this exercise, we show more carefully that rejection sampling does indeed
draw samples from the desired distribution p(z)

============================================================

=== CHUNK 451 ===
Palavras: 354
Caracteres: 2322
--------------------------------------------------
Suppose the proposal distribution
isq(z) Show that the probability of a sample value zbeing accepted is given by
Exer
cises 457
/tildewidep(z)=kq (z)where/tildewidepis any unnormalized distribution that is proportional to p(z),
and the constant kis set to the smallest value that ensures kq(z)>/tildewidep(z) for all
values of z Note that the probability of drawing a value zis given by the probability
of drawing that value from q(z)times the probability of accepting that value given
that it has been drawn Make use of this, along with the sum and product rules of
probability, to write down the normalized form for the distribution over z, and show
that it equals p(z) 14.8 (?)Suppose that zhas a uniform distribution over the interval [0;1] Show that the
variabley=btanz+chas a Cauchy distribution given by (14.16) 14.9 (??) Determine expressions for the coefÔ¨Åcients kiin the envelope distribution (14.17)
for adaptive rejection sampling using the requirements of continuity and normaliza-
tion 14.10 (??) By making use of the technique discussed in Section 14.1.2 for sampling from a
single exponential distribution, devise an algorithm for sampling from the piecewise
exponential distribution deÔ¨Åned by (14.17) 14.11 (?)Show that the simple random walk over the integers deÔ¨Åned by (14.28), (14.29),
and (14.30) has the property that E[(z())2] =E[(z(‚àí1))2] + 1=2 and hence by
induction that E[(z())2] ==2 14.12 (??) Show that the Gibbs sampling algorithm, discussed in Section 14.2.4, satisÔ¨Åes
detailed balance as deÔ¨Åned by (14.34) 14.13 (?)Consider the distribution shown in Figure 14.14 Discuss whether the standard
Gibbs sampling procedure for this distribution is ergodic and therefore whether it
would sample correctly from this distribution
Figure 14.14 A probability distribution over two vari-
ablesz1andz2that is uniform over
the shaded regions and zero everywhere
else z1z2
14.14 (
?)Verify that the over-relaxation update (14.46), in which zihas meaniand vari-
anceiand wherehas zero mean and unit variance gives a value z/prime
iwith meani
and variance 2
i SAMPLING
14.15 (?)Show that in likelihood weighted sampling from a directed graph the importance
sampling weights are given by (14.48) 14.16 (?)Show that the distribution (14.50) is normalized with respect to xprovidedZ(w)
satisÔ¨Åes (14.51)

============================================================

=== CHUNK 452 ===
Palavras: 362
Caracteres: 2373
--------------------------------------------------
14.17 (??) By making use of (14.50) show that the gradient of the log likelihood function
for an energy-based model can be written in the form (14.52) 14.18 (??) By making use of (14.54), (14.55), and (14.56), show that the gradient of the
log likelihood function for an energy-based model can be written in the form (14.57) 15
Discrete
Latent Variables
We have seen how complex distributions can be constructed by combining multi-
ple simple distributions and how the resulting models can be described by directed
graphs In addition to the observed variables, which form part of the data set, such Chapter 11
models often introduce additional hidden, or latent, variables These might corre-
spond to speciÔ¨Åc quantities involved in the data generation process, such as the un-
known orientation of an object in three-dimensional space in the case of images, or
they may be introduced simply as modelling constructs to allow much richer models
to be created If we deÔ¨Åne a joint distribution over observed and latent variables, the
corresponding distribution of the observed variables alone is obtained by marginal-
ization This allows relatively complex marginal distributions over observed vari-
ables to be expressed in terms of more tractable joint distributions over the expanded
space of observed and latent variables In this chapter, we will see that marginalizing over discrete latent variables gives
459 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_15    
460 15 DISCRETE LATENT V ARIABLES
rise to mixture distributions Our focus will be on mixtures of Gaussians that pro-
vide a good illustration of mixture distributions and that are also widely used in
machine learning One simple application for mixture models is to discover clusters
in data, and we begin our discussion by considering a technique for clustering called
theK-means algorithm, which corresponds to a particular non-probabilistic limit of
Gaussian mixtures Then we introduce the latent-variable view of mixture distribu-
tions in which the discrete latent variables can be interpreted as deÔ¨Åning assignments
of data points to speciÔ¨Åc components of the mixture A general technique for Ô¨Ånding maximum likelihood estimators in latent-variable
models is the expectation‚Äìmaximization (EM) algorithm

============================================================

=== CHUNK 453 ===
Palavras: 374
Caracteres: 2333
--------------------------------------------------
We Ô¨Årst use the Gaussian
mixture distribution to motivate the EM algorithm in an informal way, and then we
give a more careful treatment based on the latent-variable viewpoint Finally we pro-
vide a general perspective by introducing the evidence lower bound (ELBO), which
will play an important role in generative models such as variational autoencoders
and diffusion models 15.1.K-means Clustering
We begin by considering the problem of identifying groups, or clusters, of data points
in a multi-dimensional space Suppose we have a data set {x1;:::;xN}consisting of
Nobservations of a D-dimensional Euclidean variable x Our goal is to partition the
data set into some number Kof clusters, where we will suppose for the moment that
the value ofKis given Intuitively, we might think of a cluster as comprising a group
of data points whose inter-point distances are small compared with the distances to
points outside the cluster We can formalize this notion by Ô¨Årst introducing a set
ofD-dimensional vectors k, wherek= 1;:::;K , in whichkis a ‚Äòprototype‚Äô
associated with the kth cluster As we will see shortly, we can think of the kas
representing the centres of the clusters Our goal is then to Ô¨Ånd a set of cluster
vectors{k}, along with an assignment of data points to clusters, such that the sum
of the squares of the distances of each data point to its closest cluster vector kis a
minimum It is convenient at this point to deÔ¨Åne some notation to describe the assignment
of data points to clusters For each data point xn, we introduce a corresponding set
of binary indicator variables rnk‚àà{0; 1}, wherek= 1;:::;K These indicators
describe which of the Kclusters the data point xnis assigned to, so that if data point
xnis assigned to cluster kthenrnk= 1, andrnj= 0forj/negationslash=k This is an example
of the 1-of-K coding scheme We can then deÔ¨Åne an error function:
J=N/summationdisplay
n=1K/summationdisplay
k=1rnk/bardblxn‚àík/bardbl2; (15.1)
which represents the sum of the squares of the distances of each data point to its
assigned vector k Our goal is to Ô¨Ånd values for the {rnk}and the{k}so as to
minimizeJ We can do this through an iterative procedure in which each iteration
15.1.K-means
Clustering 461
involves two successive steps corresponding to successive optimizations with respect
to the{rnk}and the{k}

============================================================

=== CHUNK 454 ===
Palavras: 359
Caracteres: 2246
--------------------------------------------------
First we choose some initial values for the {k} Then
in the Ô¨Årst step, we minimize Jwith respect to the {rnk}, keeping the{k}Ô¨Åxed In the second step, we minimize Jwith respect to the {k}, keeping{rnk}Ô¨Åxed This two-step optimization is then repeated until convergence We will see that these
two stages of updating {rnk}and updating{k}correspond, respectively, to the
E (expectation) and M (maximization) steps of the EM algorithm, and to emphasize Section 15.3
this, we will use the terms E step and M step in the context of the K-means algorithm Consider Ô¨Årst the determination of the {rnk}with the{k}held Ô¨Åxed (the E
step) Because Jin (15.1) is a linear function of the {rnk}, this optimization can be
performed easily to give a closed-form solution The terms involving different nare
independent, and so we can optimize for each nseparately by choosing rnkto be 1
for whichever value of kgives the minimum value of /bardblxn‚àík/bardbl2 In other words,
we simply assign the nth data point to the closest cluster centre More formally, this
can be expressed as
rnk=/braceleftBigg
1;ifk= arg minj/bardblxn‚àíj/bardbl2;
0;otherwise:(15.2)
Now consider the optimization of the {k}with the{rnk}held Ô¨Åxed (the M
step) The objective function Jis a quadratic function of k, and it can be minimized
by setting its derivative with respect to kto zero giving
2N/summationdisplay
n=1rnk(xn‚àík) = 0; (15.3)
which we can easily solve for kto give
k=/summationtext
nrnkxn/summationtext
nrnk: (15.4)
The
denominator in this expression is equal to the number of points assigned to
clusterk, and so this result has a simple interpretation, namely that kis equal
to the mean of all the data points xnassigned to cluster k For this reason, the
procedure is known as the K-means algorithm (Lloyd, 1982) It is summarized in
Algorithm 15.1 Because the assignments {rnk}are discrete and each iteration will
not lead to an increase in the error function, the K-means algorithm is guaranteed to
converge in a Ô¨Ånite number of steps Exercise 15.1
The two phases of reassigning data points to clusters and recomputing the cluster
means are repeated in turn until there is no further change in the assignments (or until
some maximum number of iterations is exceeded)

============================================================

=== CHUNK 455 ===
Palavras: 403
Caracteres: 2474
--------------------------------------------------
However, this approach may
converge to a local rather than a global minimum of J The convergence properties
of theK-means algorithm were studied by MacQueen (1967) TheK-means algorithm is illustrated in Figure 15.1 using data derived from
eruptions of the Old Faithful geyser in Yellowstone National Park The data set Section 3.2.9
consists of 272 data points, each of which gives the duration of an eruption on the
462 15 DISCRETE LATENT V ARIABLES
Algorithm
15.1:K-means algorithm
Input: Initial prototype vectors 1;:::;K
Data set x1;:::;xN
Output: Final prototype vectors 1;:::;K
{
rnk‚Üê0}// Initially set all assignments to zero
repeat
{
r(old)
nk}‚Üê{rnk}
// Update assignments
forN‚àà{1;:::;N}do
k‚Üêarg
minj/bardblxn‚àíj/bardbl2
rnk‚Üê1
rnj‚Üê0; j‚àà{1;:::;K}; j/negationslash=k
end for
// Update prototype vectors
fork‚àà{1;:::;K}do
k‚Üê/summationtext
nrnkxn=/summationtext
nrnk
end
for
until{rnk}={r(old)
nk}// Assignments unchanged
return1;:::;K;{rnk}
horizontal axis and the time to the next eruption on the vertical axis Here we have
made a linear re-scaling of the data, known as standardizing, such that each of the
variables has zero mean and unit standard deviation For this example, we have chosen K= 2 and so the assignment of each data
point to the nearest cluster centre is equivalent to a classiÔ¨Åcation of the data points
according to which side they lie of the perpendicular bisector of the two cluster
centres A plot of the cost function Jgiven by (15.1) for the Old Faithful example is
shown in Figure 15.2 Note that we have deliberately chosen poor initial values for
the cluster centres so that the algorithm takes several steps before convergence In
practice, a better initialization procedure would be to choose the cluster centres kto
be equal to a random subset of Kdata points Also note that the K-means algorithm
is often used to initialize the parameters in a Gaussian mixture model before applying
the EM algorithm Section 15.2.2
So far, we have considered a batch version of K-means in which the whole data
set is used together to update the prototype vectors We can also derive a sequential
update in which, for each data point xnin turn, we update the nearest prototype k
using Exercise 15.2
15.1.K-means Clustering 463
(a)
‚àí2 0 2‚àí202
(b)
‚àí2 0 2‚àí202
(c)
‚àí2 0 2‚àí202
(d)
‚àí2 0 2‚àí202
(e)
‚àí2 0 2‚àí202
(f)
‚àí2 0 2‚àí202
(g)
‚àí2 0 2‚àí202
(h)
‚àí2 0 2‚àí202
(i)
‚àí2 0 2‚àí202
Figure 15.1 Illustration of the K-means algorithm using the re-scaled Old Faithful data set

============================================================

=== CHUNK 456 ===
Palavras: 367
Caracteres: 2120
--------------------------------------------------
(a) Green points
denote the data set in a two-dimensional Euclidean space The initial choices for centres 1and2are shown
by the red and blue crosses, respectively (b) In the initial E step, each data point is assigned either to the red
cluster or to the blue cluster, according to which cluster centre is nearer This is equivalent to classifying the
points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta
line, they lie (c) In the subsequent M step, each cluster centre is recomputed to be the mean of the points
assigned to the corresponding cluster (d)‚Äì(i) show successive E and M steps through to Ô¨Ånal convergence of
the algorithm DISCRETE LATENT V ARIABLES
Figure 15.2 Plot of the cost function J
given by (15.1) after each
E step (blue points) and M
step (red points) of the K-
means algorithm for the ex-
ample shown in Figure 15.1 The algorithm has converged
after the third M step, and
the Ô¨Ånal EM cycle produces
no changes in either the as-
signments or the prototype
vectors J
1 2 3 405001000
new
k=old
k+1
Nk(
xn‚àíold
k) (15.5)
whereNkis the number of data points that have so far been used to update k This
allows each data point to be used once and then discarded before seeing the next data
point One notable feature of the K-means algorithm is that at each iteration, every
data point is assigned to one, and only one, of the clusters Although some data
points will be much closer to a particular centre kthan to any other centre, there
may be other data points that lie roughly midway between cluster centres In the
latter case, it is not clear that the hard assignment to the nearest cluster is the most
appropriate We will see that by adopting a probabilistic approach, we obtain ‚Äòsoft‚Äô Section 15.2
assignments of data points to clusters in a way that reÔ¨Çects the level of uncertainty
over the most appropriate assignment This probabilistic formulation has numerous
beneÔ¨Åts 15.1.1 Image segmentation
As an illustration of the application of the K-means algorithm, we consider
the related problems of image segmentation and image compression

============================================================

=== CHUNK 457 ===
Palavras: 370
Caracteres: 2242
--------------------------------------------------
The goal of
segmentation is to partition an image into regions such that each region has a rea-
sonably homogeneous visual appearance or which corresponds to objects or parts
of objects (Forsyth and Ponce, 2003) Each pixel in an image is a point in a three-
dimensional space comprising the intensities of the red, blue, and green channels,
and our segmentation algorithm simply treats each pixel in the image as a sepa-
rate data point Note that strictly this space is not Euclidean because the channel
intensities are bounded by the interval [0;1] Nevertheless, we can apply the K-
means algorithm without difÔ¨Åculty We illustrate the result of running K-means to
convergence, for any particular value of K, by redrawing the image in which we
replace each pixel vector with the {R;G;B}intensity triplet given by the centre k
to which that pixel has been assigned Results for various values of Kare shown in
Figure 15.3 We see that for a given value of K, the algorithm represents the image
15.1.K-means Clustering 465
K= 2
K= 3
K= 10
Original image
Figure 15.3 An example of the application of the K-means clustering algorithm to image segmentation showing
an initial image together with their K-means segmentations obtained using various values of K This also illus-
trates the use of vector quantization for data compression, in which smaller values of Kgive higher compression
at the expense of poorer image quality using a palette of only Kcolours It should be emphasized that this use of K-means
is not a particularly sophisticated approach to image segmentation, not least because
it takes no account of the spatial proximity of different pixels Image segmentation
is in general extremely difÔ¨Åcult and remains the subject of active research and is
introduced here simply to illustrate the behaviour of the K-means algorithm We can also use a clustering algorithm to perform data compression It is im-
portant to distinguish between lossless data compression , in which the goal is to
be able to reconstruct the original data exactly from the compressed representation,
andlossy data compression , in which we accept some errors in the reconstruction
in return for higher levels of compression than can be achieved in the lossless case

============================================================

=== CHUNK 458 ===
Palavras: 384
Caracteres: 2359
--------------------------------------------------
We can apply the K-means algorithm to the problem of lossy data compression as
follows For each of the Ndata points, we store only the identity kof the cluster to
which it is assigned We also store the values of the Kcluster centres{k}, which
typically requires signiÔ¨Åcantly less data, provided we choose K/lessmuchN Each data
point is then approximated by its nearest centre k New data points can similarly
be compressed by Ô¨Årst Ô¨Ånding the nearest kand then storing the label kinstead of
the original data vector This framework is often called vector quantization , and the
vectors{k}are called codebook vectors The image segmentation problem discussed above also provides an illustration
of the use of clustering for data compression Suppose the original image has N
pixels comprising{R;G;B}values, each of which is stored with 8 bits of precision Directly transmitting the whole image would cost 24Nbits Now suppose we Ô¨Årst
runK-means on the image data, and then instead of transmitting the original pixel
intensity vectors, we transmit the identity of the nearest vector k Because there
areKsuch vectors, this requires log2Kbits per pixel We must also transmit the
Kcode book vectors {k}, which requires 24 Kbits, and so the total number of
bits required to transmit the image is 24K+Nlog2K(rounding up to the nearest
466 15 DISCRETE LATENT V ARIABLES
integer) The original image shown in Figure 15.3 has240√ó180 = 43;200 pixels
and so requires 24√ó43;200 = 1;036;800 bits to transmit directly By comparison,
the compressed images require 43;248 bits (K = 2), 86;472 bits (K = 3), and
173;040 bits (K = 10), respectively, to transmit These represent compression ratios
compared to the original image of 4.2%, 8.3%, and 16.7%, respectively We see that
there is a trade-off between the degree of compression and image quality Note that
our aim in this example is to illustrate the K-means algorithm If we had been aiming
to produce a good image compressor, then it would be more fruitful to consider small
blocks of adjacent pixels, for instance 5√ó5, and thereby exploit the correlations that
exist in natural images between nearby pixels Mixtures
of Gaussians
W
e have previously motivated the Gaussian mixture model as a simple linear super-
position of Gaussian components, aimed at providing a richer class of density mod-
els than a single Gaussian

============================================================

=== CHUNK 459 ===
Palavras: 362
Caracteres: 2405
--------------------------------------------------
We now turn to a formulation of Gaussian mixtures in Section 3.2.9
terms of discrete latent variables This will provide us with a deeper insight into this
important distribution and will also serve to motivate the expectation‚Äìmaximization
algorithm Recall from (3.111) that the Gaussian mixture distribution can be written as a
linear superposition of Gaussians in the form
p(x) =K/summationdisplay
k=1kN(x|k;k): (15.6)
Let us introduce a K-dimensional binary random variable zhaving a 1-of-K repre-
sentation in which one of the elements is equal to 1and all other elements are equal
to0 The values of zktherefore satisfy zk‚àà{0; 1}and/summationtext
kzk= 1, and we see that
there areKpossible states for the vector zaccording to which element is non-zero We will deÔ¨Åne the joint distribution p(x;z)in terms of a marginal distribution p(z)
and a conditional distribution p(x|z) The marginal distribution over zis speciÔ¨Åed
in terms of the mixing coefÔ¨Åcients k, such that
p(zk= 1) =k
where the parameters {k}must satisfy
06k61 (15.7)
together with
K/summationdisplay
k=1k= 1 (15.8)
15.2 Mixtures of Gaussians 467
Figure 15.4 Graphical representation of a mixture model, in which the joint distribution is
expressed in the form p(x;z) =p(z)p(x|z).z
x
if
they are to be valid probabilities Because zuses a 1-of-K representation, we can
also write this distribution in the form
p(z) =K/productdisplay
k=1zk
k: (15.9)
Similarly, the conditional distribution of xgiven a particular value for zis a Gaus-
sian:
p(x|zk= 1) =N(x|k;k);
which can also be written in the form
p(x|z) =K/productdisplay
k=1N(x|k;k)zk: (15.10)
The joint distribution is given by p(z)p(x|z) and is described by the graphical model
inFigure 15.4 The marginal distribution of xis then obtained by summing the joint
distribution over all possible states of zto give Exercise 15.3
p(x) =/summationdisplay
zp(z)p(x|z) =K/summationdisplay
k=1kN(x|k;k) (15.11)
where we have made use of (15.9) and (15.10) Thus, the marginal distribution of xis
a Gaussian mixture of the form (15.6) If we have several observations x1;:::;xN,
then, because we have represented the marginal distribution in the form p(x) =/summationtext
zp(x;z), it follows that for every observed data point xnthere is a corresponding
latent variable zn We have therefore found an equivalent formulation of the Gaussian mixture in-
volving explicit latent variables

============================================================

=== CHUNK 460 ===
Palavras: 379
Caracteres: 2394
--------------------------------------------------
It might seem that we have not gained much by do-
ing so However, we are now able to work with the joint distribution p(x;z)instead
of the marginal distribution p(x), and this will lead to signiÔ¨Åcant simpliÔ¨Åcations,
most notably through the introduction of the EM algorithm Another quantity that will play an important role is the conditional probability
ofzgiven x We will use 
(zk)to denotep(zk= 1|x), whose value can be found
468 15 DISCRETE LATENT V ARIABLES
using Bayes‚Äô theorem:

(zk)‚â°p(zk= 1|x) =p(zk= 1)p(x|zk= 1)
K/summationdisplay
j=1p
(zj= 1)p(x|zj= 1)
=kN(x|k;k)
K/summationdisplay
j=1jN(
x|j;j): (15.12)
We will view kas the prior probability of zk= 1, and the quantity 
(zk)as the
corresponding posterior probability once we have observed x As we will see later,

(zk)can also be viewed as the responsibility that component ktakes for ‚Äòexplain-
ing‚Äô the observation x We can use ancestral sampling to generate random samples distributed according Section 14.2.5
to the Gaussian mixture model To do this, we Ô¨Årst generate a value for z, which we
denote/hatwidez, from the marginal distribution p(z) and then generate a value for xfrom
the conditional distribution p(x|/hatwidez) We can depict samples from the joint distribution
p(x;z)by plotting points at the corresponding values of xand then colouring them
according to the value of z, in other words according to which Gaussian component
was responsible for generating them, as shown in Figure 15.5(a) Similarly samples
from the marginal distribution p(x) are obtained by taking the samples from the joint
distribution and ignoring the values of z These are illustrated in Figure 15.5(b) by
plotting the xvalues without any coloured labels We can also use this synthetic data set to illustrate the ‚Äòresponsibilities‚Äô by eval-
uating, for every data point, the posterior probability for each component in the
mixture distribution from which this data set was generated In particular, we can
represent the value of the responsibilities 
(znk)associated with data point xnby
plotting the corresponding point using proportions of red, blue, and green ink given
by
(znk)fork= 1;2;3, respectively, as shown in Figure 15.5(c) So, for instance,
a data point for which 
(zn1) = 1 will be coloured red, whereas one for which

(zn2) =
(zn3) = 0:5will be coloured with equal proportions of blue and green
ink and so will appear cyan

============================================================

=== CHUNK 461 ===
Palavras: 370
Caracteres: 2281
--------------------------------------------------
This should be compared with Figure 15.5(a) in which
the data points were labelled using the true identity of the component from which
they were generated 15.2.1 Likelihood function
Suppose we have a data set of observations {x1;:::;xN}, and we wish to model
this data using a mixture of Gaussians We can represent this data set as an N√óD
matrix Xin which the nth row is given by xT
n From (15.6) the log of the likelihood
function is given by
lnp(X|;;) =N/summationdisplay
n=1ln/braceleftBiggK/summationdisplay
k=1kN(xn|k;k)/bracerightBigg
: (15.13)
15.2 Mixtures of Gaussians 469
(a)
0 0.5 100.51
(b)
0 0.5 100.51
(c)
0 0.5 100.51
Figure
15.5 Example of 500 points drawn from the mixture of three Gaussians shown in Figure 3.8 (a) Sam-
ples from the joint distribution p(z)p(x| z)in which the three states of z, corresponding to the three components
of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal dis-
tributionp(x), which is obtained by simply ignoring the values of zand just plotting the xvalues The data set
in (a) is said to be complete, whereas that in (b) is incomplete, as discussed further in Section 15.3 (c) The
same samples in which the colours represent the value of the responsibilities 
(znk)associated with data point
xn, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by 
(znk)for
k= 1;2;3, respectively Maximizing this log likelihood function (15.13) is a more complex problem than for
a single Gaussian The difÔ¨Åculty arises from the presence of the summation over k
that appears inside the logarithm in (15.13), so that the logarithm function no longer
acts directly on the Gaussian If we set the derivatives of the log likelihood to zero,
we will no longer obtain a closed-form solution, as we will see shortly Before discussing how to maximize this function, it is worth emphasizing that
there is a signiÔ¨Åcant problem associated with the maximum likelihood framework
when applied to Gaussian mixture models, due to the presence of singularities For
simplicity, consider a Gaussian mixture whose components have covariance matrices
given by k=2
kI, where Iis the unit matrix, although the conclusions will hold
for general covariance matrices

============================================================

=== CHUNK 462 ===
Palavras: 372
Caracteres: 2240
--------------------------------------------------
Suppose that one of the components of the mixture
model, let us say the jth component, has its mean jexactly equal to one of the data
points so that j=xnfor some value of n This data point will then contribute a
term in the likelihood function of the form
N(xn|xn;2
jI) =1
(2
)1=21
j: (15.14)
If
we consider the limit j‚Üí0, then we see that this term goes to inÔ¨Ånity and
so the log likelihood function will also go to inÔ¨Ånity Thus, the maximization of
the log likelihood function is not a well posed-problem because such singularities
will always be present and will occur whenever one of the Gaussian components
‚Äòcollapses‚Äô onto a speciÔ¨Åc data point Recall that this problem did not arise with
a single Gaussian distribution To understand the difference, note that if a single
Gaussian collapses onto a data point, it will contribute multiplicative factors to the
470 15 DISCRETE LATENT V ARIABLES
Figure 15.6 Illustration of how singularities
in the likelihood function arise
with mixtures of Gaussians This should be compared with
a single Gaussian shown in
Figure 2.9 for which no singu-
larities arise xp(x)
lik
elihood function arising from the other data points, and these factors will go to
zero exponentially fast, giving an overall likelihood that goes to zero rather than
inÔ¨Ånity However, once we have (at least) two components in the mixture, one of
the components can have a Ô¨Ånite variance and therefore assign Ô¨Ånite probability to
all the data points while the other component can shrink onto one speciÔ¨Åc data point
and thereby contribute an ever increasing additive value to the log likelihood This
is illustrated in Figure 15.6 These singularities provide an example of the over-
Ô¨Åtting that can occur in a maximum likelihood approach When applying maximum
likelihood to Gaussian mixture models, we must take steps to avoid Ô¨Ånding such
pathological solutions and instead seek local maxima of the likelihood function that
are well behaved We can try to avoid the singularities by using suitable heuristics,
for instance by detecting when a Gaussian component is collapsing and resetting
its mean to a randomly chosen value while also resetting its covariance to some
large value and then continuing with the optimization

============================================================

=== CHUNK 463 ===
Palavras: 369
Caracteres: 2503
--------------------------------------------------
The singularities can also be
avoided by adding a regularization term to the log likelihood corresponding to a prior
distribution over the parameters Section 15.4.3
A further issue in Ô¨Ånding maximum likelihood solutions arises because for any
given maximum likelihood solution, a K-component mixture will have a total of K equivalent solutions corresponding to the K!ways of assigning Ksets of parameters
toKcomponents In other words, for any given (non-degenerate) point in the space
of parameter values, there will be a further K!‚àí1additional points all of which give
rise to exactly the same distribution This problem is known as identiÔ¨Åability (Casella
and Berger, 2002) and is an important issue when we wish to interpret the parameter
values discovered by a model IdentiÔ¨Åability will also arise when we discuss models
having continuous latent variables However, when Ô¨Ånding a good density model, it Chapter 16
is irrelevant because any of the equivalent solutions is as good as any other 15.2.2 Maximum likelihood
An elegant and powerful method for Ô¨Ånding maximum likelihood solutions for
models with latent variables is called the expectation‚Äìmaximization algorithm or EM
algorithm (Dempster, Laird, and Rubin, 1977; McLachlan and Krishnan, 1997) In
this chapter we will give three different derivations of the EM algorithm, each more
15.2 Mixtures of Gaussians 471
general than the previous We begin here with a relatively informal treatment in
the context of a Gaussian mixture model We emphasize, however, that EM has
broad applicability, and the underlying concepts will be encountered in the context
of several different models in this book We begin by writing down the conditions that must be satisÔ¨Åed at a maximum
of the likelihood function Setting the derivatives of lnp(X|;;)in (15.13) with
respect to the means kof the Gaussian components to zero, we obtain
0 =N/summationdisplay
n=1kN(xn|k;k)/summationtext
jjN(xn|j;j)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright

(znk)‚àí1
k(xn‚àík) (15.15)
where we have made use of the form (3.26) for the Gaussian distribution Note
that the posterior probabilities, or responsibilities, 
(znk)given by (15.12) appear
naturally on the right-hand side Multiplying by k(which we assume to be non-
singular) and rearranging we obtain
k=1
NkN/summationdisplay
n=1
(znk)xn (15.16)
where we have deÔ¨Åned
Nk=N/summationdisplay
n=1
(znk): (15.17)
We can interpret Nkas the effective number of points assigned to cluster k

============================================================

=== CHUNK 464 ===
Palavras: 359
Caracteres: 2299
--------------------------------------------------
Note
carefully the form of this solution We see that the mean kfor thekth Gaussian
component is obtained by taking a weighted mean of all the points in the data set,
in which the weighting factor for data point xnis given by the posterior probability

(znk)that component kwas responsible for generating xn If we set the derivative of lnp(X|;;)with respect to kto zero and follow
a similar line of reasoning by making use of the result for the maximum likelihood
solution for the covariance matrix of a single Gaussian, we obtain Section 3.2.7
k=1
NkN/summationdisplay
n=1
(znk)(xn‚àík)(xn‚àík)T; (15.18)
which has the same form as the corresponding result for a single Gaussian Ô¨Åtted to
the data set, but again with each data point weighted by the corresponding poste-
rior probability and with the denominator given by the effective number of points
associated with the corresponding component Finally, we maximize lnp(X|;;)with respect to the mixing coefÔ¨Åcients
k Here we must take account of the constraint (15.8), which requires the mixing
coefÔ¨Åcients to sum to one This can be achieved using a Lagrange multiplier and Appendix C
472 15.DISCRETE
LATENT V ARIABLES
maximizing the following quantity:
lnp(X|;;) +K/summationdisplay
k=1k‚àí1/parenrightBigg
; (15.19)
which gives
0 =N/summationdisplay
n=1N(xn|k;k)/summationtext
jjN(xn|
j;j)+ (15.20)
where again we see the appearance of the responsibilities If we now multiply both
sides bykand sum over kmaking use of the constraint (15.8), we Ô¨Ånd =‚àíN Using this to eliminate and rearranging, we obtain
k=Nk
N(15.21)
sothat
the mixing coefÔ¨Åcient for the kth component is given by the average respon-
sibility which that component takes for explaining the data points Note that the results (15.16), (15.18), and (15.21) do not constitute a closed-form
solution for the parameters of the mixture model because the responsibilities 
(znk)
depend on those parameters in a complex way through (15.12) However, these re-
sults do suggest a simple iterative scheme for Ô¨Ånding a solution to the maximum
likelihood problem, which as we will see turns out to be an instance of the EM algo-
rithm for the particular case of the Gaussian mixture model We Ô¨Årst choose some
initial values for the means, covariances, and mixing coefÔ¨Åcients

============================================================

=== CHUNK 465 ===
Palavras: 356
Caracteres: 2055
--------------------------------------------------
Then we alternate
between the following two updates, which we will call the E step and the M step for
reasons that will become apparent shortly In the expectation step, or E step, we use
the current values for the parameters to evaluate the posterior probabilities, or re-
sponsibilities, given by (15.12) We then use these probabilities in the maximization
step, or M step, to re-estimate the means, covariances, and mixing coefÔ¨Åcients using
the results (15.16), (15.18), and (15.21) Note that in so doing, we Ô¨Årst evaluate the
new means using (15.16) and then use these new values to Ô¨Ånd the covariances using
(15.18), in keeping with the corresponding result for a single Gaussian distribution We will show that each update to the parameters resulting from an E step followed
by an M step is guaranteed to increase the log likelihood function In practice, the al- Section 15.3
gorithm is deemed to have converged when the change in the log likelihood function,
or alternatively in the parameters, falls below some threshold We illustrate the EM algorithm for a mixture of two Gaussians applied to the
re-scaled Old Faithful data in Figure 15.7 Here a mixture of two Gaussians is used,
with centres initialized using the same values as for the K-means algorithm in Fig-
ure 15.1 and with covariance matrices initialized to be proportional to the unit matrix Plot (a) shows the data points in green, together with the initial conÔ¨Åguration of the
mixture model in which the one standard-deviation contours for the two Gaussian
components are shown as blue and red circles Plot (b) shows the result of the initial
E step, in which each data point is depicted using a proportion of blue ink equal to
the posterior probability of having been generated from the blue component and a
15.2 Mixtures of Gaussians 473
(a) ‚àí2 0 2‚àí202
(b) ‚àí2 0 2‚àí202
(c)L= 1
‚àí2 0 2‚àí202
(d)L=2
‚àí2 0 2‚àí202
(e)L=5
‚àí2 0 2‚àí202
(f)L=20
‚àí2 0 2‚àí202
Figure
15.7 Application of the EM algorithm to the Old Faithful data set as used for the illustration of the
K-means algorithm in Figure 15.1

============================================================

=== CHUNK 466 ===
Palavras: 372
Caracteres: 2220
--------------------------------------------------
See the text for details corresponding proportion of red ink given by the posterior probability of having been
generated by the red component Thus, points that have a roughly equal probability
for belonging to either cluster appear purple The situation after the Ô¨Årst M step is
shown in plot (c), in which the mean of the blue Gaussian has moved to the mean of
the data set, weighted by the probabilities of each data point belonging to the blue
cluster In other words it has moved to the centre of mass of the blue ink Similarly,
the covariance of the blue Gaussian is set equal to the covariance of the blue ink Analogous results hold for the red component Plots (d), (e), and (f) show the results
after 2, 5, and 20 complete cycles of EM, respectively In plot (f) the algorithm is
close to convergence Note that the EM algorithm takes many more iterations to reach (approximate)
convergence compared with the K-means algorithm and that each cycle requires sig-
niÔ¨Åcantly more computation It is therefore common to run the K-means algorithm
to Ô¨Ånd a suitable initialization for a Gaussian mixture model that is subsequently
adapted using EM The covariance matrices can conveniently be initialized to the
sample covariances of the clusters found by the K-means algorithm, and the mix-
ing coefÔ¨Åcients can be set to the fractions of data points assigned to the respective
474 15 DISCRETE LATENT V ARIABLES
Figure 15.8 A Gaussian mixture model Ô¨Åtted
to the ‚Äòtwo-moons‚Äô data set, show-
ing that a large number of mixture
components may be required to
give an accurate representation of
a complex data distribution Here
the ellipses represent the contours
of constant density for the corre-
sponding mixture components As
we move to spaces of larger di-
mensionality, the number of com-
ponents required to model a distri-
bution accurately can become un-
acceptably large Techniques such as parameter regularization must be employed to avoid
singularities of the likelihood function in which a Gaussian component collapses
onto a particular data point It should be emphasized that there will generally be
multiple local maxima of the log likelihood function and that EM is not guaranteed
to Ô¨Ånd the largest of these maxima

============================================================

=== CHUNK 467 ===
Palavras: 362
Caracteres: 2493
--------------------------------------------------
Because the EM algorithm for Gaussian mixtures
plays such an important role, we summarize it in Algorithm 15.2 Mixture models are very Ô¨Çexible and can approximate complicated distributions
to high accuracy given a sufÔ¨Åcient number of components if the model parameters
are chosen appropriately In practice, however, the number of components can be ex-
tremely large, especially in spaces of high dimensionality This problem is illustrated
for the two-moons data set in Figure 15.8 Nevertheless, mixture models are useful
in many applications Also, an understanding of mixture models lays the foundations
for models with continuous latent variables and for generative models based on deep Chapter 16
neural networks, which have much better scaling to spaces of high dimensionality Expectation‚ÄìMaximization
Algorithm
W
e turn now to a more general view of the EM algorithm in which we focus on the
role of latent variables As before we denote the set of all observed data points by X,
in which the nth row represents xT
n Similarly, the corresponding latent variables will
be denoted by an N√óKmatrix Zwith rows zT
n If we assume that the data points are
drawn independently from the distribution, then we can express the Gaussian mixture
model for this i.i.d data set using the graphical representation shown in Figure 15.9 The set of all model parameters is denoted by , and so the log likelihood function
15.3 Expectation‚ÄìMaximization Algorithm 475
Algorithm 15.2: EM algorithm for a Gaussian mixture model
Input: Initial model parameters {k};{k};{k}
Data set{x1;:::;xN}
Output: Final model parameters {k};{k};{k}
repeat
// E step
forn‚àà{1;:::;N}do
fork‚àà{1;:::;K}do

(znk)‚ÜêkN(xn|k;k)/summationtextK
j=1jN(xn|j;j)
end for
end for
// M step
fork‚àà{1;:::;K}do
Nk‚ÜêN/summationdisplay
n=1
(znk)
k‚Üê1
NkN/summationdisplay
n=1
(znk)xn
k‚Üê1
NkN/summationdisplay
n=1
(znk) (xn‚àík) (xn‚àík)T
k‚ÜêNk
N
end for
// Log likelihood
L‚ÜêN/summationdisplay
n=1ln/braceleftBiggK/summationdisplay
k=1kN(xn|k;k)/bracerightBigg
until convergence
return{xk};{k};{k}
476 15 DISCRETE LATENT V ARIABLES
Figure 15.9 Graphical representation of a Gaussian mixture model
for a set of Ni.i.d data points{xn}, with corresponding
latent points{zn}, wheren= 1;:::;N . zn
xn 
N
is
given by
lnp(X| ) = ln/braceleftBigg/summationdisplay
Zp(X;Z|)/bracerightBigg
: (15.22)
Note that our discussion will apply equally well to continuous latent variables simply Chapter 16
by replacing the sum over Zwith an integral

============================================================

=== CHUNK 468 ===
Palavras: 430
Caracteres: 2825
--------------------------------------------------
A key observation is that the summation over the latent variables appears inside
the logarithm Even if the joint distribution p(X;Z|)belongs to the exponential
family, the marginal distribution p(X| )typically does not as a result of this sum-
mation The presence of the sum prevents the logarithm from acting directly on the
joint distribution, resulting in complicated expressions for the maximum likelihood
solution Now suppose that, for each observation in X, we were told the corresponding
value of the latent variable Z We will call{X;Z}thecomplete data set, and we will
refer to the actual observed data Xasincomplete, as illustrated in Figure 15.5 The
likelihood function for the complete data set simply takes the form lnp(X;Z|), and
we will suppose that maximization of this complete-data log likelihood function is
straightforward In practice, however, we are not given the complete data set {X;Z}but only
the incomplete data X Our state of knowledge of the values of the latent variables
inZis given only by the posterior distribution p(Z|X;) Because we cannot use
the complete-data log likelihood, we consider instead its expected value under the
posterior distribution of the latent variables, which corresponds (as we will see) to the
E step of the EM algorithm In the subsequent M step, we maximize this expectation If the current estimate for the parameters is denoted by old, then a pair of successive
E and M steps gives rise to a revised estimate new The algorithm is initialized
by choosing some starting value for the parameters 0 Although this use of the
expectation may seem somewhat arbitrary, we will see the motivation for this choice
when we give a deeper treatment of EM in Section 15.4 In the E step, we use the current parameter values oldto Ô¨Ånd the posterior
distribution of the latent variables given by p(Z|X;old) We then use this posterior
distribution to Ô¨Ånd the expectation of the complete-data log likelihood evaluated for
some general parameter value  This expectation, denoted by Q(;old), is given
by
Q(;old) =/summationdisplay
Zp(Z|X;old) lnp(X;Z|): (15.23)
15.3 Expectation‚ÄìMaximization Algorithm 477
Algorithm
15.3: General EM algorithm
Input: Joint distribution p(X;Z|)
Initial parameters old
Data set x1;:::;xN
Output: Final parameters 
r
epeat
Q
(;old)‚Üê/summationtext
Zp(Z|X;old) lnp(X;Z|)// E step
new‚Üêarg maxQ(;old)// M step
L‚Üêp(X|new)// Evaluate log likelihood
old‚Üênew// Update the parameters
until convergence
returnnew
In the M step, we determine the revised parameter estimate newby maximizing this
function:
new= arg max
Q(;old): (15.24)
Note that in the deÔ¨Ånition of Q(;old), the logarithm acts directly on the joint dis-
tributionp(X;Z|), and so the corresponding M-step maximization will, according
to our assumption, be tractable

============================================================

=== CHUNK 469 ===
Palavras: 352
Caracteres: 2027
--------------------------------------------------
The general EM algorithm is summarized in Al-
gorithm 15.3 It has the property, as we will show later, that each cycle of EM will
increase the incomplete-data log likelihood (unless it is already at a local maximum) Section 15.4.1
The EM algorithm can also be used to Ô¨Ånd MAP (maximum posterior) solutions
for models in which a prior p()is deÔ¨Åned over the parameters In this case the E Exercise 15.5
step remains the same as in the maximum likelihood case, whereas in the M step the
quantity to be maximized is given by Q(;old) + lnp() Suitable choices for the
prior will remove the singularities of the kind illustrated in Figure 15.6 Here we have considered the use of the EM algorithm to maximize a likelihood
function when there are discrete latent variables However, it can also be applied
when the unobserved variables correspond to missing values in the data set The
distribution of the observed values is obtained by taking the joint distribution of all
the variables and then marginalizing over the missing ones EM can then be used to
maximize the corresponding likelihood function This will be a valid procedure if
the data values are missing at random, meaning that the mechanism causing values
to be missing does not depend on the unobserved values In many situations this will
not be the case, for instance if a sensor fails to return a value whenever the quantity
it is measuring exceeds some threshold DISCRETE LATENT V ARIABLES
Figure 15.10 This shows the same graph as in Figure 15.9 except that
we now suppose that the discrete variables znare ob-
served, as well as the data variables xn. zn
xn 
N
15.3.1
Gaussian mixtures
We now consider the application of this latent-variable view of EM to the spe-
ciÔ¨Åc case of a Gaussian mixture model Recall that our goal is to maximize the log
likelihood function (15.13), which is computed using the observed data set X, and
we saw that this was more difÔ¨Åcult than with a single Gaussian distribution due to
the summation over kthat occurs inside the logarithm

============================================================

=== CHUNK 470 ===
Palavras: 369
Caracteres: 2451
--------------------------------------------------
Suppose then that in addi-
tion to the observed data set X, we were also given the values of the corresponding
discrete variables Z Recall that Figure 15.5(a) shows a complete data set (i.e., one
that includes labels showing which component generated each data point) whereas
Figure 15.5(b) shows the corresponding incomplete data set A graphical model for
the complete data is shown in Figure 15.10 Now consider the problem of maximizing the likelihood for the complete data
set{X;Z} From (15.9) and (15.10), this likelihood function takes the form
p(X;Z|;;) =N/productdisplay
n=1K/productdisplay
k=1znk
kN(xn|k;k)znk(15.25)
whereznkdenotes thekth component of zn Taking the logarithm, we obtain
lnp(X;Z|;;) =N/summationdisplay
n=1K/summationdisplay
k=1znk{lnk+ lnN(xn|k;k)}: (15.26)
Comparison with the log likelihood function (15.13) for the incomplete data shows
that the summation over kand the logarithm have been interchanged The loga-
rithm now acts directly on the Gaussian distribution, which itself is a member of
the exponential family Not surprisingly, this leads to a much simpler solution to the
maximum likelihood problem, as we now show Consider Ô¨Årst the maximization with
respect to the means and covariances Because znis aK-dimensional vector with all
elements equal to 0except for a single element having the value 1, the complete-data
log likelihood function is simply a sum of Kindependent contributions, one for each
mixture component Thus, the maximization with respect to a mean or a covariance
is exactly as for a single Gaussian, except that it involves only the subset of data
points that are ‚Äòassigned‚Äô to that component For the maximization with respect to
the mixing coefÔ¨Åcients, note that these are coupled for different values of kby virtue
of the summation constraint (15.8) Again, this can be enforced using a Lagrange
15.3 Expectation‚ÄìMaximization Algorithm 479
multiplier as before, which leads to the result
k=1
NN/summationdisplay
n
=1znk (15.27)
so that the mixing coefÔ¨Åcients are equal to the fractions of data points assigned to
the corresponding components Thus, we see that the complete-data log likelihood function can be maximized
trivially in closed form In practice, however, we do not have values for the latent
variables Therefore, as discussed earlier, we consider the expectation, with respect
to the posterior distribution of the latent variables, of the complete-data log like-
lihood

============================================================

=== CHUNK 471 ===
Palavras: 361
Caracteres: 2525
--------------------------------------------------
Using (15.9) and (15.10) together with Bayes‚Äô theorem, we see that this
posterior distribution takes the form
p(Z|X;;;)‚àùN/productdisplay
n=1K/productdisplay
k=1[kN(xn|k;k)]znk: (15.28)
We see that this factorizes over nso that under the posterior distribution, the {zn}are
independent This is easily veriÔ¨Åed by inspecting the directed graph in Figure 15.9 Exercise 15.6
and making use of the d-separation criterion The expected value of the indicator Section 11.2
variableznkunder this posterior distribution is then given by
E[znk] =/summationdisplay
znznk/productdisplay
k/prime[k/primeN(xn|k/prime;k/prime)]znk/prime
/summationdisplay
zn/productdisplay
j/bracketleftbig
jN(
xn|j;j)/bracketrightbigznj
=kN(xn|k;k)
K/summationdisplay
j=1jN(
xn|j;j)=
(znk); (15.29)
which is just the responsibility of component kfor data point xn The expected value
of the complete-data log likelihood function is therefore given by
EZ[lnp(X;Z|;;)] =N/summationdisplay
n=1K/summationdisplay
k=1
(znk){lnk+ lnN(xn|k;k)}:(15.30)
We can now proceed as follows First we choose some initial values for the param-
etersold,old, andold, and we use these to evaluate the responsibilities (the E
step) We then keep the responsibilities Ô¨Åxed and maximize (15.30) with respect to
k,k, andk(the M step) This leads to closed-form solutions for new,new,
andnewgiven by (15.16), (15.18), and (15.21) as before This is precisely the Exercise 15.9
EM algorithm for Gaussian mixtures as derived earlier We will gain more insight
into the role of the expected complete-data log likelihood function when discuss the
convergence of the EM algorithm in Section 15.4 DISCRETE LATENT V ARIABLES
Figure 15.11 The probabilistic graphical model for se-
quential data corresponding to a hid-
den Markov model The discrete latent
variables are no longer independent but
form a Markov chain.z1 z2 zN
x1 x2 xN
Throughout
this chapter we assume that the data observations are i.i.d For or-
dered observations that form a sequence, the mixture model can be extended by con-
necting the latent variables in a Markov chain to give a hidden Markov model whose
graphical structure is shown in Figure 15.11 The EM algorithm can be extended
to this more complex model in which the E step involves a sequential calculation in
which messages are passed along the chain of latent variables (Bishop, 2006) 15.3.2 Relation to K-means
Comparison of the K-means algorithm with the EM algorithm for Gaussian
mixtures shows that there is a close similarity

============================================================

=== CHUNK 472 ===
Palavras: 380
Caracteres: 2506
--------------------------------------------------
Whereas the K-means algorithm
performs a hard assignment of data points to clusters in which each data point is
associated uniquely with one cluster, the EM algorithm makes a softassignment
based on the posterior probabilities In fact, we can derive the K-means algorithm
as a particular limit of EM for Gaussian mixtures as follows Consider a Gaussian mixture model in which the covariance matrices of the
mixture components are given by I, whereis a variance parameter that is shared
by all the components, and Iis the identity matrix, so that
p(x|k;k) =1
(2
)D=2exp/braceleftbigg
‚àí1
2
/bardblx‚àík/bardbl2/bracerightbigg
: (15.31)
We now consider the EM algorithm for a mixture of KGaussians of this form in
which we treat as a Ô¨Åxed constant, instead of a parameter to be re-estimated From
(15.12) the posterior probabilities, or responsibilities, for a particular data point xn
are given by

(znk) =kexp{‚àí/bardblxn‚àík/bardbl2=2}/summationtext
jjexp/braceleftbig
‚àí/bardbl
xn‚àíj/bardbl2=2/bracerightbig: (15.32)
Consider the limit ‚Üí0 The denominator consists of a sum of terms indexed by j
each of which goes to zero The particular term for which /bardblxn‚àíj/bardbl2is smallest,
sayj=l, will go to zero most slowly and will then dominate this sum Therefore,
the responsibilities 
(znk)for the data point xnall go to zero except for term l, for
which the responsibility 
(znl)will go to unity Note that this holds independently
of the values of the kso long as none of the kis zero Thus, in this limit, we obtain
a hard assignment of data points to clusters, just as in the K-means algorithm, so that

(znk)‚Üírnkwherernkis deÔ¨Åned by (15.2) Each data point is thereby assigned to
the cluster having the closest mean The EM re-estimation equation for the k, given
by (15.16), then reduces to the K-means result (15.4) Note that the re-estimation
formula for the mixing coefÔ¨Åcients (15.21) simply resets the value of kto be equal
15.3 Expectation‚ÄìMaximization Algorithm 481
to the fraction of data points assigned to cluster k, although these parameters no
longer play an active role in the algorithm Finally, in the limit ‚Üí0, the expected complete-data log likelihood, given by
(15.30), becomes Exercise 15.12
EZ[lnp(X;Z|;;)]‚Üí‚àí1
2N/summationdisplay
n=1K/summationdisplay
k=1rnk/bardblxn‚àík/bardbl2+ const: (15.33)
Thus, we see that in this limit, maximizing the expected complete-data log likelihood
is equivalent to minimizing the error measure Jfor theK-means algorithm given by
(15.1)

============================================================

=== CHUNK 473 ===
Palavras: 351
Caracteres: 2455
--------------------------------------------------
Note that the K-means algorithm does not estimate the covariances of the
clusters but only the cluster means 15.3.3 Mixtures of Bernoulli distributions
So far in this chapter, we have focused on distributions over continuous variables
described by mixtures of Gaussians As a further example of mixture modelling and
to illustrate the EM algorithm in a different context, we now discuss mixtures of
discrete binary variables described by Bernoulli distributions This model is also
known as latent class analysis (Lazarsfeld and Henry, 1968; McLachlan and Peel,
2000) Consider a set of Dbinary variables xi, wherei= 1;:::;D , each of which is
governed by a Bernoulli distribution with parameter i, so that Section 3.1.1
p(x|) =D/productdisplay
i=1xi
i(1‚àíi)(1‚àíx i)(15.34)
where x= (x 1;:::;xD)Tand= ( 1;:::;D)T We see that the individual
variablesxiare independent, given  The mean and covariance of this distribution
are easily seen to be Exercise 15.13
E[x] = (15.35)
cov[x] = diag{i(1‚àíi)}: (15.36)
Now let us consider a Ô¨Ånite mixture of these distributions given by
p(x|;) =K/summationdisplay
k=1kp(x|k) (15.37)
where={1;:::;K},={1;:::;K}, and
p(x|k) =D/productdisplay
i=1xi
ki(1‚àíki)(1‚àíx i): (15.38)
The mixing coefÔ¨Åcients satisfy (15.7) and (15.8) The mean and covariance of this
mixture distribution are given by Exercise 15.14
482 15 DISCRETE LATENT V ARIABLES
E[x] =K/summationdisplay
k=1kk (15.39)
cov[x] =K/summationdisplay
k=1k/braceleftbig
k+kT
k/bracerightbig
‚àíE[x]E[x]T(15.40)
where k= diag{ki(1‚àíki)} Because the covariance matrix cov[x] is no
longer diagonal, the mixture distribution can capture correlations between the vari-
ables, unlike a single Bernoulli distribution If we are given a data set X={x1;:::;xN}then the log likelihood function
for this model is given by
lnp(X|;) =N/summationdisplay
n=1ln/braceleftBiggK/summationdisplay
k=1kp(xn|k)/bracerightBigg
: (15.41)
Again we see the appearance of the summation inside the logarithm, so that the
maximum likelihood solution no longer has closed form We now derive the EM algorithm for maximizing the likelihood function for the
mixture of Bernoulli distributions To do this, we Ô¨Årst introduce an explicit discrete
latent variable zassociated with each instance of x As with the Gaussian mixture,
zhas a 1-of-K coding so that z= (z1;:::;zK)Tis a binaryK-dimensional vector
having a single component equal to 1, with all other components equal to 0

============================================================

=== CHUNK 474 ===
Palavras: 364
Caracteres: 2668
--------------------------------------------------
We can
then write the conditional distribution of x, given the latent variable, as
p(x|z;) =K/productdisplay
k=1p(x|k)zk(15.42)
whereas the prior distribution for the latent variables is the same as for the mixture-
of-Gaussians model, so that
p(z| ) =K/productdisplay
k=1zk
k: (15.43)
If we form the product of p(x|z;)andp(z| )and then marginalize over z, then we
recover (15.37) Exercise 15.16
To derive the EM algorithm, we Ô¨Årst write down the complete-data log likelihood
function, which is given by
lnp(X;Z|;) =N/summationdisplay
n=1K/summationdisplay
k=1znk/braceleftBigg
lnk
+D/summationdisplay
i=1[xnilnki+ (1‚àíxni) ln(1‚àíki)]/bracerightBigg
(15.44)
15.3 Expectation‚ÄìMaximization Algorithm 483
where X={xn}andZ={zn} Next we take the expectation of the complete-data
log likelihood with respect to the posterior distribution of the latent variables to give
EZ[lnp(X;Z|;)] =N/summationdisplay
n=1K/summationdisplay
k=1
(znk)/braceleftBigg
lnk
+D/summationdisplay
i=1[xnilnki+ (1‚àíxni) ln(1‚àíki)]/bracerightBigg
(15.45)
where
(znk) =E[znk]is the posterior probability, or responsibility, of component
kgiven data point xn In the E step, these responsibilities are evaluated using Bayes‚Äô
theorem, which takes the form

(znk) =E[znk] =/summationdisplay
znznk/productdisplay
k/prime[k/primep(xn|k/prime)]znk/prime
/summationdisplay
zn/productdisplay
j/bracketleftbig
jp(xn|j)/bracketrightbigznj
=kp(xn|k)
K/summationdisplay
j=1jp(xn|j): (15.46)
If we consider the sum over nin (15.45), we see that the responsibilities enter
only through two terms, which can be written as
Nk=N/summationdisplay
n=1
(znk) (15.47)
xk=1
NkN/summationdisplay
n=1
(znk)xn (15.48)
whereNkis the effective number of data points associated with component k In the
M step, we maximize the expected complete-data log likelihood with respect to the
parameterskand If we set the derivative of (15.45) with respect to kequal to
zero and rearrange the terms, we obtain Exercise 15.17
k=xk: (15.49)
We see that this sets the mean of component kequal to a weighted mean of the
data, with weighting coefÔ¨Åcients given by the responsibilities that component ktakes
for each of the data points For the maximization with respect to k, we need to
introduce a Lagrange multiplier to enforce the constraint/summationtext
kk= 1 Following
analogous steps to those used for the mixture of Gaussians, we then obtain Exercise 15.18
k=Nk
N; (15.50)
484 15 DISCRETE LATENT V ARIABLES
Figure 15.12 Illustration of the Bernoulli mixture model in which the top row shows examples from the digits
data set after converting the pixel values from grey scale to binary using a threshold of 0:5

============================================================

=== CHUNK 475 ===
Palavras: 376
Caracteres: 2271
--------------------------------------------------
On the bottom row
the Ô¨Årst three images show the parameters kifor each of the three components in the mixture model As a
comparison, we also Ô¨Åt the same data set using a single multivariate Bernoulli distribution, again using maximum
likelihood This amounts to simply averaging the counts in each pixel and is shown by the right-most image on
the bottom row which represents the intuitively reasonable result that the mixing coefÔ¨Åcient for com-
ponentkis given by the effective fraction of points in the data set explained by that
component Note that in contrast to the mixture of Gaussians, there are no singularities in
which the likelihood function goes to inÔ¨Ånity This can be seen by noting that the
likelihood function is bounded above because 06p(xn|k)61 There exist so- Exercise 15.19
lutions for which the likelihood function is zero, but these will not be found by EM
provided it is not initialized to a pathological starting point, because the EM algo-
rithm always increases the value of the likelihood function, until a local maximum
is found Section 15.3
We illustrate the Bernoulli mixture model in Figure 15.12 by using it to model
handwritten digits Here the digit images have been turned into binary vectors by
setting all elements whose values exceed 0:5to1and setting the remaining elements
to0 We now Ô¨Åt a data set of N= 600 such digits, comprising the digits ‚Äò2‚Äô, ‚Äò3‚Äô,
and ‚Äò4‚Äô, with a mixture of K= 3Bernoulli distributions by running 10iterations of
the EM algorithm The mixing coefÔ¨Åcients were initialized to k= 1=K, and the
parameterskjwere set to random values chosen uniformly in the range (0:25;0:75)
and then normalized to satisfy the constraint that/summationtext
jkj= 1 We see that a mix-
ture of three Bernoulli distributions is able to Ô¨Ånd the three clusters in the data set
corresponding to the different digits It is straightforward to extend the analysis of
Bernoulli mixtures to the case of multinomial binary variables having M > 2states Exercise 15.20
by making use of the discrete distribution (3.14) Evidence Lower Bound 485
15.4 Evidence Lower Bound
We now present an even more general perspective on the EM algorithm by deriving
a lower bound on the log likelihood function, which is known as the evidence lower
bound orELBO

============================================================

=== CHUNK 476 ===
Palavras: 370
Caracteres: 2469
--------------------------------------------------
It is sometimes called a variational lower bound Here the term
evidence refers to the (log) likelihood function, which is sometimes called the ‚Äòmodel
evidence‚Äô in a Bayesian setting as it allows different models to be compared without
the use of hold-out data (Bishop, 2006) As an illustration of this bound, we use it
to re-derive the EM algorithm for Gaussian mixtures from a third perspective The
ELBO will play an important role in several of the deep generative models discussed
in later chapters It also provides an example of a variational framework in which we
introduce a distribution q(Z)over the latent variables and then optimize with respect
to this distribution using the calculus of variations Appendix B
Consider a probabilistic model in which we collectively denote all the observed
variables by Xand all the hidden variables by Z The joint distribution p(X;Z|)is
governed by a set of parameters denoted by  Our goal is to maximize the likelihood
function:
p(X| ) =/summationdisplay
Zp(X;Z|): (15.51)
Here we are assuming that Zis discrete, although the discussion is identical if Z
comprises continuous variables or a combination of discrete and continuous vari-
ables, with summation replaced by integration as appropriate We will suppose that direct optimization of p(X| )is difÔ¨Åcult, but that optimiza-
tion of the complete-data likelihood function p(X;Z|)is signiÔ¨Åcantly easier Next
we introduce a distribution q(Z)deÔ¨Åned over the latent variables, and we observe
that, for any choice of q(Z), the following decomposition holds:
lnp(X| ) =L(q;) + KL(q/bardblp) (15.52)
where we have deÔ¨Åned
L(q;) =/summationdisplay
Zq(Z) ln/braceleftbiggp(X;Z|)
q(Z)/bracerightbigg
(15.53)
KL(q/bardblp) =‚àí/summationdisplay
Zq(Z) ln/braceleftbiggp(Z|X;)
q(Z)/bracerightbigg
: (15.54)
Note thatL(q;)is a functional of the distribution q(Z)and a function of the pa- Appendix B
rameters It is worth studying carefully the forms of the expressions (15.53) and
(15.54), and in particular noting that they differ in sign and also that L(q;)contains
the joint distribution of XandZwhereas KL(q/bardblp)contains the conditional distri-
bution of Zgiven X To verify the decomposition (15.52), we Ô¨Årst make use of the Exercise 15.21
product rule of probability to give
lnp(X;Z|) = lnp(Z|X;) + lnp(X| ); (15.55)
486 15 DISCRETE LATENT V ARIABLES
Figure 15.13 Illustration of the decomposition
given by (15.52), which holds for
any choice of distribution q(Z)

============================================================

=== CHUNK 477 ===
Palavras: 370
Caracteres: 2340
--------------------------------------------------
Because the Kullback‚ÄìLeibler di-
vergence satisÔ¨Åes KL(q/bardblp)>0,
we see that the quantity L(q;)is
a lower bound on the log likelihood
function lnp(X| ) lnp(Xj ) L(q;)KL(qjjp)
which
we then substitute into the expression for L(q;) This gives rise to two terms,
one of which cancels KL(q/bardblp)whereas the other gives the required log likelihood
lnp(X| )after noting that q(Z)is a normalized distribution that sums to 1 From (15.54), we see that KL(q/bardblp)is the Kullback‚ÄìLeibler divergence between
q(Z)and the posterior distribution p(Z|X;) Recall that the Kullback‚ÄìLeibler di-
vergence satisÔ¨Åes KL(q/bardblp)>0, with equality if, and only if, q(Z) =p(Z|X;) It Section 2.5.7
therefore follows from (15.52) that L(q;)6lnp(X| ), in other words that L(q;)
is a lower bound on lnp(X| ) The decomposition (15.52) is illustrated in Fig-
ure 15.13 15.4.1 EM revisited
We can use the decomposition (15.52) to derive the EM algorithm and to demon-
strate that it does indeed maximize the log likelihood Suppose that the current value
of the parameter vector is old In the E step, the lower bound L(q;old)is maxi-
mized with respect to q(Z)while holding oldÔ¨Åxed The solution to this maximiza-
tion problem is easily seen by noting that the value of lnp(X|old)does not depend
onq(Z)and so the largest value of L(q;old)will occur when the Kullback‚ÄìLeibler
divergence vanishes, in other words when q(Z)is equal to the posterior distribu-
tionp(Z|X;old) In this case, the lower bound will equal the log likelihood, as
illustrated in Figure 15.14 In the subsequent M step, the distribution q(Z)is held Ô¨Åxed and the lower bound
Figure 15.14 Illustration of the E step of the
EM algorithm The qdistribution is set equal to
the posterior distribution for the current parame-
ter valuesold, causing the lower bound to move
up to the same value as the log likelihood func-
tion, with the KL divergence vanishing lnp(Xjold) L(q;old)KL(
qjjp) = 0
15.4 Evidence Lower Bound 487
Figure 15.15 Illustration of the M step of the
EM algorithm The distribu-
tionq(Z)is held Ô¨Åxed and the
lower boundL(q;)is maxi-
mized with respect to the pa-
rameter vector to give a re-
vised valuenew Because the
Kullback‚ÄìLeibler divergence is
non-negative, this causes the log
likelihood lnp(X| )to increase
by at least as much as the lower
bound does

============================================================

=== CHUNK 478 ===
Palavras: 350
Caracteres: 2198
--------------------------------------------------
lnp(Xjnew) L(q;new)KL(qjjp)
L
(q;)is maximized with respect to to give some new value new This will
cause the lower bound Lto increase (unless it is already at a maximum), which will
necessarily cause the corresponding log likelihood function to increase Because the
distributionqis determined using the old parameter values rather than the new values
and is held Ô¨Åxed during the M step, it will not equal the new posterior distribution
p(Z|X;new), and hence there will be a non-zero Kullback‚ÄìLeibler divergence The
increase in the log likelihood function is therefore greater than the increase in the
lower bound, as shown in Figure 15.15 If we substitute q(Z) =p(Z|X;old)into
(15.53), we see that, after the E step, the lower bound takes the form
L(q;) =/summationdisplay
Zp(Z|X;old) lnp(X;Z|)‚àí/summationdisplay
Zp(Z|X;old) lnp(Z|X;old)
=Q(;old) + const (15.56)
where the constant is simply the negative entropy of the qdistribution and is therefore
independent of  Here we recognize Q(;old)as the expected complete-data log-
likelihood deÔ¨Åned by (15.23), and it is therefore the quantity that is being maximized
in the M step, as we saw earlier distribution for mixtures of Gaussians Note that Section 15.3
the variableover which we are optimizing appears only inside the logarithm If
the joint distribution p(Z;X|)is a member of the exponential family or a product
of such members, then we see that the logarithm will cancel the exponential and
lead to an M step that will be typically much simpler than the maximization of the
corresponding incomplete-data log likelihood function p(X| ) The operation of the EM algorithm can also be viewed in the space of param-
eters, as illustrated schematically in Figure 15.16 Here the red curve depicts the
(incomplete-data) log likelihood function whose value we wish to maximize We
start with some initial parameter value old, and in the Ô¨Årst E step we evaluate the
posterior distribution over latent variables, which gives rise to a lower bound L(q;)
whose value equals the log likelihood at (old), as shown by the blue curve Note that
the bound makes a tangential contact with the log likelihood at (old), so that both
488 15

============================================================

=== CHUNK 479 ===
Palavras: 386
Caracteres: 2464
--------------------------------------------------
DISCRETE LATENT V ARIABLES
Figure 15.16 The EM algorithm involves al-
ternately computing a lower
bound on the log likelihood
for the current parameter val-
ues and then maximizing this
bound to obtain the new pa-
rameter values See the text
for a full discussion Œ∏(old)Œ∏(new)lnp(X|Œ∏)
L(Œ∏,Œ∏(old)) EM
curves have the same gradient This bound is a convex function having a unique Exercise 15.22
maximum (for mixture components from the exponential family) In the M step,
the bound is maximized giving the value (new), which gives a larger value of the
log likelihood than (old) The subsequent E step then constructs a bound that is
tangential at(new)as shown by the green curve We have seen that both the E and the M steps of the EM algorithm increase the
value of a well-deÔ¨Åned bound on the log likelihood function and that the complete
EM cycle will change the model parameters in such a way as to cause the log like-
lihood to increase (unless it is already at a maximum, in which case the parameters
remain unchanged) 15.4.2 Independent and identically distributed data
For the particular case of an i.i.d data set, Xwill comprise Ndata points
{xn}whereas Zwill comprise Ncorresponding latent variables {zn}, wheren=
1;:::;N From the independence assumption, we have p(X;Z) =/producttext
np(xn;zn),
and by marginalizing over the {zn}we havep(X) =/producttext
np(xn) Using the sum
and product rules, we see that the posterior probability that is evaluated in the E step
takes the form
p(Z|X;) =p(X;Z|)/summationdisplay
Zp(X;Z|)=N/productdisplay
n=1p(xn;zn|)
/summationdisplay
ZN/productdisplay
n=1p(xn;zn|)=N/productdisplay
n=1p(zn|xn;)(15.57)
and so the posterior distribution also factorizes with respect to n For a Gaussian
mixture model, this simply says that the responsibility that each of the mixture com-
ponents takes for a particular data point xndepends only on the value of xnand
15.4 Evidence Lower Bound 489
on the parameters of the mixture components, not on the values of the other data
points 15.4.3 Parameter priors
We can also use the EM algorithm to maximize the posterior distribution p(|X)
for models in which we have introduced a prior p()over the parameters To see this,
note that as a function of , we havep(|X) =p(;X)=p(X) and so
lnp(|X) = lnp(;X)‚àílnp(X): (15.58)
Making use of the decomposition (15.52), we have
lnp(|X) =L(q;) + KL(q/bardblp) + lnp()‚àílnp(X)
>L(q;) + lnp()‚àílnp(X) (15.59)
where lnp(X) is a constant

============================================================

=== CHUNK 480 ===
Palavras: 368
Caracteres: 2265
--------------------------------------------------
We can again optimize the right-hand side alternately
with respect to qand The optimization with respect to qgives rise to the same E-
step equations as for the standard EM algorithm, because qappears only inL(q;) The M-step equations are modiÔ¨Åed through the introduction of the prior term lnp(),
which typically requires only a small modiÔ¨Åcation to the standard maximum likeli-
hood M-step equations The additional term represents a form of regularization and Chapter 9
has the effect of removing the singularities of the likelihood function for Gaussian
mixture models 15.4.4 Generalized EM
The EM algorithm breaks down the potentially difÔ¨Åcult problem of maximizing
the likelihood function into two stages, the E step and the M step, each of which will
often prove simpler to implement Nevertheless, for complex models it may be the
case that either the E step or the M step, or indeed both, remain intractable This
leads to two possible extensions of the EM algorithm, as follows Thegeneralized EM, or GEM, algorithm addresses the problem of an intractable
M step Instead of aiming to maximize L(q;)with respect to , it seeks instead to
change the parameters in such a way as to increase its value Again, because L(q;)
is a lower bound on the log likelihood function, each complete EM cycle of the
GEM algorithm is guaranteed to increase the value of the log likelihood (unless the
parameters already correspond to a local maximum) One way to exploit the GEM
approach would be to use gradient-based iterative optimization algorithms during
the M step Another form of GEM algorithm, known as the expectation conditional
maximization algorithm, involves making several constrained optimizations within
each M step (Meng and Rubin, 1993) For instance, the parameters might be par-
titioned into groups and the M step broken down into multiple steps each of which
involves optimizing one of the groups with the remainder held Ô¨Åxed We can similarly generalize the E step of the EM algorithm by performing a
partial, rather than complete, optimization of L(q;)with respect to q(Z)(Neal and
Hinton, 1999) As we have seen, for any given value of there is a unique maximum
ofL(q;)with respect to q(Z)that corresponds to the posterior distribution q(Z) =
490 15

============================================================

=== CHUNK 481 ===
Palavras: 385
Caracteres: 2428
--------------------------------------------------
DISCRETE LATENT V ARIABLES
p(Z|X;)and that for this choice of q(Z), the boundL(q;)is equal to the log
likelihood function lnp(X| ) It follows that any algorithm that converges to the
global maximum of L(q;)will Ô¨Ånd a value of that is also a global maximum
of the log likelihood lnp(X| ) Provided p(X;Z|)is a continuous function of 
then, by continuity, any local maximum of L(q;)will also be a local maximum of
lnp(X| ) 15.4.5 Sequential EM
ConsiderNindependent data points x1;:::;xNwith corresponding latent vari-
ablesz1;:::;zN The joint distribution p(X;Z|)factorizes over the data points,
and this structure can be exploited in an incremental form of EM in which at each
EM cycle, only one data point is processed at a time In the E step, instead of recom-
puting the responsibilities for all the data points, we just re-evaluate the responsibil-
ities for one data point It might appear that the subsequent M step would require
a computation involving the responsibilities for all the data points However, if the
mixture components are members of the exponential family, then the responsibilities
enter only through simple sufÔ¨Åcient statistics, and these can be updated efÔ¨Åciently Consider, for instance, a Gaussian mixture, and suppose we perform an update for
data pointmin which the corresponding old and new values of the responsibilities
are denoted by 
old(zmk)and
new(zmk) In the M step, the required sufÔ¨Åcient
statistics can be updated incrementally For instance, for the means, the sufÔ¨Åcient
statistics are deÔ¨Åned by (15.16) and (15.17) from which we obtain Exercise 15.23
new
k=old
k+/parenleftbigg
new(zmk)‚àí
old(zmk)
Nnew
k/parenrightbigg/parenleftbig
xm‚àíold
k/parenrightbig
(15.60)
together with
Nnew
k=Nold
k+
new(zmk)‚àí
old(zmk): (15.61)
The corresponding results for the covariances and the mixing coefÔ¨Åcients are analo-
gous Thus, both the E step and the M step take a Ô¨Åxed time that is independent of
the total number of data points Because the parameters are revised after each data
point, rather than waiting until after the whole data set is processed, this incremental
version can converge faster than the batch version Each E or M step in this incre-
mental algorithm increases the value of L(q;), and as we have shown above, if the
algorithm converges to a local (or global) maximum of L(q;), this will correspond
to a local (or global) maximum of the log likelihood function lnp(X| )

============================================================

=== CHUNK 482 ===
Palavras: 379
Caracteres: 2317
--------------------------------------------------
Exercises
15.1 (?)Consider the K-means algorithm discussed in Section 15.1 Show that as a con-
sequence of there being a Ô¨Ånite number of possible assignments for the set of discrete
indicator variables rnkand that for each such assignment there is a unique optimum
for the{k}, theK-means algorithm must converge after a Ô¨Ånite number of itera-
tions Exer
cises 491
15.2 (??) In this exercise we derive the sequential form for the K-means algorithm At
each step we consider a new data point xn, and only the prototype vector that is
closest to xnis updated Starting from the expression (15.4) for the prototype vectors
in the batch setting, separate out the contribution from the Ô¨Ånal data point xn By
rearranging the formula, show that this update takes the form (15.5) Note that, since
no approximation is made in this derivation, the resulting prototype vectors will have
the property that they each equal the mean of all the data vectors that were assigned
to them 15.3 (?)Consider a Gaussian mixture model in which the marginal distribution p(z) for
the latent variable is given by (15.9) and the conditional distribution p(x|z) for the
observed variable is given by (15.10) Show that the marginal distribution p(x),
obtained by summing p(z)p(x|z) over all possible values of z, is a Gaussian mixture
of the form (15.6) 15.4 (?)Show that the number of equivalent parameter settings due to interchange sym-
metries in a mixture model with Kcomponents is K 15.5 (??) Suppose we wish to use the EM algorithm to maximize the posterior distri-
bution over parameters p(|X)for a model containing latent variables, where Xis
the observed data set Show that the E step remains the same as in the maximum
likelihood case, whereas in the M step the quantity to be maximized is given by
Q(;old) + lnp()whereQ(;old)is deÔ¨Åned by (15.23) 15.6 (?)Consider the directed graph for a Gaussian mixture model shown in Figure 15.9 By making use of the d-separation criterion, show that the posterior distribution of Section 11.2
the latent variables factorizes with respect to the different data points so that
p(Z|X;;;) =N/productdisplay
n=1p(zn|xn;;;): (15.62)
15.7 (??) Consider a special case of a Gaussian mixture model in which the covariance
matrices kof the components are all constrained to have a common value 

============================================================

=== CHUNK 483 ===
Palavras: 398
Caracteres: 2558
--------------------------------------------------
De-
rive the EM equations for maximizing the likelihood function under such a model 15.8 (??) Verify that maximization of the complete-data log likelihood (15.26) for a
Gaussian mixture model leads to the result that the means and covariances of each
component are Ô¨Åtted independently to the corresponding group of data points and
that the mixing coefÔ¨Åcients are given by the fractions of points in each group 15.9 (??) Show that if we maximize (15.30) with respect to kwhile keeping the respon-
sibilities
(znk)Ô¨Åxed, we obtain the closed-form solution given by (15.16) 15.10 (??) Show that if we maximize (15.30) with respect to kandkwhile keeping the
responsibilities 
(znk)Ô¨Åxed, we obtain the closed-form solutions given by (15.18)
and (15.21) DISCRETE LATENT V ARIABLES
15.11 (??) Consider a density model given by a mixture distribution:
p(x) =K/summationdisplay
k=1kp(x|k ) (15.63)
and suppose that we partition the vector xinto two parts so that x= (xa;xb) Show that the conditional density p(xb|xa)is itself a mixture distribution, and Ô¨Ånd
expressions for the mixing coefÔ¨Åcients and for the component densities 15.12 (?)In Section 15.3.2, we obtained a relationship between Kmeans and EM for
Gaussian mixtures by considering a mixture model in which all components have
covarianceI Show that in the limit ‚Üí0, maximizing the expected complete-data
log likelihood for this model, given by (15.30), is equivalent to minimizing the error
measureJfor theK-means algorithm given by (15.1) 15.13 (??) Verify the results (15.35) and (15.36) for the mean and covariance of the Bernoulli
distribution 15.14 (??) Consider a mixture distribution of the form
p(x) =K/summationdisplay
k=1kp(x|k ) (15.64)
where the elements of xcould be discrete or continuous or a combination of these Denote the mean and covariance of p(x|k )bykandk, respectively By making
use of the results of Exercise 15.13, show that the mean and covariance of the mixture
distribution are given by (15.39) and (15.40) 15.15 (??) Using the re-estimation equations for the EM algorithm, show that a mixture
of Bernoulli distributions, with its parameters set to values corresponding to a maxi-
mum of the likelihood function, has the property that
E[x] =1
NN/summationdisplay
n=1xn‚â°x: (15.65)
Hence, show that if the parameters of this model are initialized such that all compo-
nents have the same mean k=/hatwidefork= 1;:::;K , then the EM algorithm will
converge after one iteration, for any choice of the initial mixing coefÔ¨Åcients, and that
this solution has the property k=x

============================================================

=== CHUNK 484 ===
Palavras: 357
Caracteres: 2451
--------------------------------------------------
Note that this represents a degenerate case of
the mixture model in which all the components are identical, and in practice we try
to avoid such solutions by using an appropriate initialization 15.16 (?)Consider the joint distribution of latent and observed variables for the Bernoulli
distribution obtained by forming the product of p(x|z;)given by (15.42) and
p(z| )given by (15.43) Show that if we marginalize this joint distribution with
respect to z, then we obtain (15.37) Exercises 493
15.17 (?)Show that if we maximize the expected complete-data log likelihood function
(15.45) for a mixture of Bernoulli distributions with respect to k, we obtain the
M-step equation (15.49) 15.18 (?)Show that if we maximize the expected complete-data log likelihood function
(15.45) for a mixture of Bernoulli distributions with respect to the mixing coefÔ¨Åcients
k, and use a Lagrange multiplier to enforce the summation constraint, we obtain the
M-step equation (15.50) 15.19 (?)Show that as a consequence of the constraint 06p(xn|k)61for the discrete
variable xn, the incomplete-data log likelihood function for a mixture of Bernoulli
distributions is bounded above and hence that there are no singularities for which the
likelihood goes to inÔ¨Ånity 15.20 (???) Consider aD-dimensional variable xeach of whose components iis itself a
multinomial variable of degree Mso that xis a binary vector with components xij
wherei= 1;:::;D andj= 1;:::;M , subject to the constraint that/summationtext
jxij= 1for
alli Suppose that the distribution of these variables is described by a mixture of the
discrete multinomial distributions so that Section 3.1.3
p(x) =K/summationdisplay
k=1kp(x|k) (15.66)
where
p(x|k) =D/productdisplay
i=1M/productdisplay
j=1xij
kij: (15.67)
The parameters kijrepresent the probabilities p(xij= 1|k)and must satisfy
06kij61together with the constraint/summationtext
jkij= 1 for all values of kandi Given an observed data set {xn}, wheren= 1;:::;N , derive the E-step and M-step
equations of the EM algorithm for optimizing the mixing coefÔ¨Åcients kand the
component parameters kijof this distribution by maximum likelihood 15.21 (?)Verify the relation (15.52) in which L(q;)andKL(q/bardblp)are deÔ¨Åned by (15.53)
and (15.54), respectively 15.22 (?)Show that the lower bound L(q;)given by (15.53), with q(Z) =p(Z|X;(old)),
has the same gradient with respect to as the log likelihood function lnp(X| )at
the point=(old)

============================================================

=== CHUNK 485 ===
Palavras: 359
Caracteres: 2237
--------------------------------------------------
15.23 (??) Consider the incremental form of the EM algorithm for a mixture of Gaussians,
in which the responsibilities are recomputed only for a speciÔ¨Åc data point xm Start-
ing from the M-step formulae (15.16) and (15.17), derive the results (15.60) and
(15.61) for updating the component means 15.24 (??) Derive M-step formulae for updating the covariance matrices and mixing co-
efÔ¨Åcients in a Gaussian mixture model when the responsibilities are updated incre-
mentally, analogous to the result (15.60) for updating the means 16
Continuous
Latent Variables
In the previous chapter we discussed probabilistic models having discrete latent vari-
ables, such as a mixture of Gaussians We now explore models in which some, or
all, of the latent variables are continuous An important motivation for such models
is that many data sets have the property that the data points lie close to a manifold Section 6.1.3
of much lower dimensionality than that of the original data space To see why this
might arise, consider an artiÔ¨Åcial data set constructed by taking a handwritten digit
from the MNIST data set (LeCun et al., 1998), represented by a 64√ó64pixel grey-
level image, and embedding it in a larger image of size 100√ó100by padding with
pixels having the value zero (corresponding to white pixels) in which the location and
orientation of the digit are varied at random, as illustrated in Figure 16.1 Each of the
resulting images is represented by a point in the 100√ó100 = 10; 000-dimensional
data space However, across a data set of such images, there are only three degrees
of freedom of variability, corresponding to vertical and horizontal translations and
495 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_16    
496 16 CONTINUOUS LATENT V ARIABLES
Figure 16.1 A synthetic data set obtained by taking an image of a handwritten digit and creating multiple copies
in each of which the digit has undergone a random displacement and rotation within some larger image Ô¨Åeld The resulting images each have 100√ó100 = 10;000pixels The data points will therefore live on a subspace of the data space whose
intrinsic dimensionality is three

============================================================

=== CHUNK 486 ===
Palavras: 359
Caracteres: 2252
--------------------------------------------------
Note that the manifold will be nonlinear because,
for instance, if we translate the digit past a particular pixel, that pixel value will go
from zero (white) to one (black) and back to zero again, which is clearly a nonlinear
function of the digit position In this example, the translation and rotation parame-
ters are latent variables because we observe only the image vectors and are not told
which values of the translation or rotation variables were used to create them For real data sets of handwritten digits, there will be further degrees of freedom
arising from scaling and other variations due, for example, to the variability in an
individual‚Äôs writing as well as the differences in writing styles between individuals Nevertheless, the number of such degrees of freedom will be small compared to the
dimensionality of the data set In practice, the data points will not be conÔ¨Åned precisely to a smooth low-
dimensional manifold, and we can interpret the departures of data points from the
manifold as ‚Äònoise‚Äô This leads naturally to a generative view of such models in
which we Ô¨Årst select a point within the manifold according to some latent-variable
distribution and then generate an observed data point by adding noise drawn from
some conditional distribution of the data variables given the latent variables The simplest continuous latent-variable model assumes Gaussian distributions
for both the latent and observed variables and makes use of a linear-Gaussian de-
pendence of the observed variables on the state of the latent variables This leads Section 11.1.4
to a probabilistic formulation of the well-known technique of principal component
analysis (PCA) as well as to a related model called factor analysis In this chap-
ter we will begin with a standard, non-probabilistic treatment of PCA, and then we Section 16.1
show how PCA arises naturally as the maximum likelihood solution for a linear-
Gaussian latent-variable model This probabilistic reformulation brings many ad- Section 16.2
vantages, such as the use of EM for parameter estimation, principled extensions to
mixtures of PCA models, and Bayesian formulations that allow the number of prin-
cipal components to be determined automatically from the data (Bishop, 2006)

============================================================

=== CHUNK 487 ===
Palavras: 351
Caracteres: 2203
--------------------------------------------------
This
chapter also lays the foundations for nonlinear models having continuous latent vari-
ables including normalizing Ô¨Çows, variational autoencoders, and diffusion models Principal Component Analysis 497
Figure 16.2 Principal component analysis seeks a
space of lower dimensionality, known as the
principal subspace and denoted by the ma-
genta line, such that the orthogonal projec-
tion of the data points (red dots) onto this
subspace maximizes the variance of the
projected points (green dots) An alterna-
tive deÔ¨Ånition of PCA is based on minimiz-
ing the sum-of-squares of the projection er-
rors, indicated by the blue lines Principal
Component Analysis
Principal
component analysis, or PCA, is widely used for applications such as di-
mensionality reduction, lossy data compression, feature extraction, and data visual-
ization (Jolliffe, 2002) It is also known as the Kosambi‚ÄìKarhunen‚ÄìLo `evetransform Consider the orthogonal projection of a data set onto a lower-dimensional lin-
ear space, known as the principal subspace, as shown in Figure 16.2 PCA can be
deÔ¨Åned as the linear projection that maximizes the variance of the projected data
(Hotelling, 1933) Equivalently, it can be deÔ¨Åned as the linear projection that min-
imizes the average projection cost, deÔ¨Åned as the mean squared distance between
the data points and their projections (Pearson, 1901) We consider each of these
deÔ¨Ånitions in turn 16.1.1 Maximum variance formulation
Consider a data set of observations {xn}wheren= 1;:::;N , and xnis a
Euclidean variable with dimensionality D Our goal is to project the data onto a
space having dimensionality M <D while maximizing the variance of the projected
data For the moment, we will assume that the value of Mis given Later in this
chapter, we will consider techniques to determine an appropriate value of Mfrom
the data To begin with, consider the projection onto a one-dimensional space (M = 1) We can deÔ¨Åne the direction of this space using a D-dimensional vector u1, which
for convenience (and without loss of generality) we will choose to be a unit vector
so that uT
1u1= 1 (note that we are interested only in the direction deÔ¨Åned by u1,
not in the magnitude of u1itself)

============================================================

=== CHUNK 488 ===
Palavras: 351
Caracteres: 2231
--------------------------------------------------
Each data point xnis then projected onto a scalar
valueuT
1xn The mean of the projected data is uT
1xwhere xis
the sample set mean
given by
x=1
NN/summationdisplay
n
=1xn (16.1)
498 16 CONTINUOUS LATENT V ARIABLES
and the variance of the projected data is given by
1
NN/summationdisplay
n=1/braceleftbig
uT
1xn‚àíuT
1x/bracerightbig2=uT
1Su1 (16.2)
where Sis the data covariance matrix deÔ¨Åned by
S=1
NN/summationdisplay
n=1(xn‚àíx)(xn‚àíx)T: (16.3)
We now maximize the projected variance uT
1Su1with respect to u1 Clearly, this has
to be a constrained maximization to prevent /bardblu1/bardbl‚Üí‚àû The appropriate constraint
comes from the normalization condition uT
1u1= 1 To enforce this constraint,
we introduce a Lagrange multiplier that we will denote by 1, and then make an Appendix C
unconstrained maximization of
uT
1Su1+1/parenleftbig
1‚àíuT
1u1/parenrightbig
: (16.4)
By setting the derivative with respect to u1equal to zero, we see that this quantity
will have a stationary point when
Su1=1u1; (16.5)
which says that u1must be an eigenvector of S If we left-multiply by uT
1and make
use of uT
1u1= 1, we see that the variance is given by
uT
1Su1=1 (16.6)
and so the variance will be a maximum when we set u1equal to the eigenvector
having the largest eigenvalue 1 This eigenvector is known as the Ô¨Årst principal
component We can deÔ¨Åne additional principal components in an incremental fashion by
choosing each new direction to be that which maximizes the projected variance
amongst all possible directions orthogonal to those already considered If we con-
sider the general case of an M-dimensional projection space, the optimal linear pro-
jection for which the variance of the projected data is maximized is now deÔ¨Åned by
theMeigenvectors u1;:::;uMof the data covariance matrix Scorresponding to the
Mlargest eigenvalues 1;:::;M This is easily shown using proof by induction Exercise 16.1
To summarize, PCA involves evaluating the mean xand the covariance matrix
Sof a data set and then Ô¨Ånding the Meigenvectors of Scorresponding to the M
largest eigenvalues Algorithms for Ô¨Ånding eigenvectors and eigenvalues, as well as
additional theorems related to eigenvector decomposition, can be found in Golub and
Van Loan (1996)

============================================================

=== CHUNK 489 ===
Palavras: 355
Caracteres: 2355
--------------------------------------------------
Note that the computational cost of computing the full eigenvector
decomposition for a matrix of size D√óDisO(D3) If we plan to project our
data onto the Ô¨Årst Mprincipal components, then we only need to Ô¨Ånd the Ô¨Årst M
eigenvalues and eigenvectors This can be done with more efÔ¨Åcient techniques, such
as the power method (Golub and Van Loan, 1996), that scale like O(MD2), or
alternatively we can make use of the EM algorithm Principal Component Analysis 499
16.1.2 Minimum-error formulation
We now discuss an alternative formulation of PCA based on projection error
minimization To do this, we introduce a complete orthonormal set of D-dimensional Appendix A
basis vectors{ui}wherei= 1;:::;D that satisfy
uT
iuj=ij: (16.7)
Because this basis is complete, each data point can be represented exactly by a linear
combination of the basis vectors
xn=D/summationdisplay
i=1niui (16.8)
where the coefÔ¨Åcients niwill be different for different data points This simply
corresponds to a rotation of the coordinate system to a new system deÔ¨Åned by the
{ui}, and the original Dcomponents{xn1;:::;xnD}are replaced by an equivalent
set{n1;:::;nD} Taking the inner product with uj, and making use of the or-
thonormality property, we obtain nj=xT
nuj, and so without loss of generality we
can write
xn=D/summationdisplay
i=1/parenleftbig
xT
nui/parenrightbig
ui: (16.9)
Our goal, however, is to approximate this data point using a representation in-
volving a restricted number M <D of variables corresponding to a projection onto
a lower-dimensional subspace The M-dimensional linear subspace can be repre-
sented, without loss of generality, by the Ô¨Årst Mof the basis vectors, and so we
approximate each data point xnby
/tildewidexn=M/summationdisplay
i=1zniui+D/summationdisplay
i=M +1biui (16.10)
where the{zni}depend on the particular data point, whereas the {bi}are constants
that are the same for all data points We are free to choose the {ui}, the{zni}, and
the{bi}so as to minimize the error introduced by the reduction in dimensionality As our error measure, we will use the squared distance between the original data
pointxnand its approximation /tildewidexn, averaged over the data set, so that our goal is to
minimize
J=1
NN/summationdisplay
n=1/bardblxn‚àí/tildewidexn/bardbl2: (16.11)
Consider Ô¨Årst the minimization with respect to the quantities {zni}

============================================================

=== CHUNK 490 ===
Palavras: 366
Caracteres: 2428
--------------------------------------------------
Substituting
for/tildewidexn, setting the derivative with respect to znjto zero, and making use of the
orthonormality conditions, we obtain
znj=xT
nuj (16.12)
500 16 CONTINUOUS LATENT V ARIABLES
wherej= 1;:::;M Similarly, setting the derivative of Jwith respect to bito zero
and again making use of the orthonormality relations, gives
bj=xTuj (16.13)
wherej=M+
1;:::;D If we substitute for zniandbiand make use of the general
expansion (16.9), we obtain
xn‚àí/tildewidexn=D/summationdisplay
i=M +1/braceleftbig
(xn‚àíx
)Tui/bracerightbig
ui (16.14)
from which we see that the displacement vector from xnto/tildewidexnlies in the space
orthogonal to the principal subspace, because it is a linear combination of {ui}for
i=M+ 1;:::;D , as illustrated in Figure 16.2 This is to be expected because the
projected points /tildewidexnmust lie within the principal subspace, but we can move them
freely within that subspace, and so the minimum error is given by the orthogonal
projection We therefore obtain an expression for the error measure Jas a function purely
of the{ui}in the form
J=1
NN/summationdisplay
n
=1D/summationdisplay
i=M +1/parenleftbig
xT
nui‚àíxTui/parenrightbig2=D/summationdisplay
i
=M+1uT
iSui: (16.15)
There remains the task of minimizing Jwith respect to the {ui}, which must be
a constrained minimization otherwise we will obtain the vacuous result ui= 0 The
constraints arise from the orthonormality conditions, and as we will see, the solution
will be expressed in terms of the eigenvector expansion of the covariance matrix Before considering a formal solution, let us try to obtain some intuition about the
result by considering a two-dimensional data space D= 2 and a one-dimensional
principal subspace M= 1 We have to choose a direction u2so as to minimize
J=uT
2Su2, subject to the normalization constraint uT
2u2= 1 Using a Lagrange
multiplier2to enforce the constraint, we consider the minimization of
/tildewideJ=uT
2Su2+2/parenleftbig
1‚àíuT
2u2/parenrightbig
: (16.16)
Setting the derivative with respect to u2to zero, we obtain Su2=2u2so that
u2is an eigenvector of Swith eigenvalue 2 Thus, any eigenvector will deÔ¨Åne
a stationary point of the error measure To Ô¨Ånd the value of Jat the minimum,
we back-substitute the solution for u2into the error measure to give J=2 We
therefore obtain the minimum value of Jby choosing u2to be the eigenvector corre-
sponding to the smaller of the two eigenvalues

============================================================

=== CHUNK 491 ===
Palavras: 368
Caracteres: 2320
--------------------------------------------------
Thus, we should choose the principal
subspace to be aligned with the eigenvector having the larger eigenvalue This result
accords with our intuition that, to minimize the average squared projection distance,
we should choose the principal component subspace so that it passes through the
mean of the data points and is aligned with the directions of maximum variance Principal Component Analysis 501
the eigenvalues are equal, any choice of principal direction will give rise to the same
value ofJ The general solution to the minimization of Jfor arbitrary Dand arbitrary M < Exercise 16.2
Dis obtained by choosing the {ui}to be eigenvectors of the covariance matrix given
by
Sui=iui (16.17)
wherei= 1;:::;D , and as usual the eigenvectors {ui}are chosen to be orthonor-
mal The corresponding value of the error measure is then given by
J=D/summationdisplay
i=M +1i; (16.18)
which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal
to the principal subspace We therefore obtain the minimum value of Jby selecting
these eigenvectors to be those having the D‚àíMsmallest eigenvalues, and hence
the eigenvectors deÔ¨Åning the principal subspace are those corresponding to the M
largest eigenvalues Although we have considered M < D , the PCA analysis still holds if M=
D, in which case there is no dimensionality reduction but simply a rotation of the
coordinate axes to align with the principal components Finally, note that there is a related linear dimensionality reduction technique
called canonical correlation analysis (Hotelling, 1936; Bach and Jordan, 2002) Whereas PCA works with a single random variable, canonical correlation analy-
sis considers two (or more) variables and tries to Ô¨Ånd a corresponding pair of linear
subspaces that have high cross-correlation, so that each component within one of the
subspaces is correlated with a single component from the other subspace Its solution
can be expressed in terms of a generalized eigenvector problem 16.1.3 Data compression
One application for PCA is data compression, and we can illustrate this by con-
sidering a data set of images of handwritten digits Because each eigenvector of the
covariance matrix is a vector in the original D-dimensional space, we can represent
the eigenvectors as images of the same size as the data points

============================================================

=== CHUNK 492 ===
Palavras: 357
Caracteres: 2349
--------------------------------------------------
The mean image and
the Ô¨Årst four eigenvectors, along with their corresponding eigenvalues, are shown in
Figure 16.3 A plot of the complete spectrum of eigenvalues, sorted into decreasing order, is
shown in Figure 16.4(a) The error measure Jassociated with choosing a particular
value ofMis given by the sum of the eigenvalues from M+1up toDand is plotted
for different values of MinFigure 16.4(b) If we substitute (16.12) and (16.13) into (16.10), we can write the PCA approx-
502 16 CONTINUOUS LATENT V ARIABLES
Mean Œª1= 3.4¬∑105Œª2= 2.8¬∑105Œª3= 2.4¬∑105Œª4= 1.6¬∑105
Figure 16.3 Illustration of PCA applied to a data set of 6,000 images of size 28 √ó28, each comprising a hand-
written image of the numeral ‚Äò3‚Äô , showing the mean vector xalong with the Ô¨Årst four PCA eigenvectors u1;:::;u4,
together with their corresponding eigenvalues imation to a data vector xnin the form
/tildewidexn=M/summationdisplay
i=1(xT
nui)ui+D/summationdisplay
i=M+1(xTui)ui (16.19)
=x+M/summationdisplay
i=1/parenleftbig
xT
nui‚àíxTui/parenrightbig
ui (16.20)
where we have made use of the relation
x=D/summationdisplay
i=1/parenleftbig
xTui/parenrightbig
ui; (16.21)
which follows from the completeness of the {ui} This represents a compression
of the data set, because for each data point we have replaced the D-dimensional
vector xnwith anM-dimensional vector having components/parenleftbig
xT
nui‚àíxTui/parenrightbig The
smaller the value of M, the greater the degree of compression Examples of PCA
reconstructions of data points for the digits data set are shown in Figure 16.5 16.1.4 Data whitening
Another application of PCA is to data pre-processing In this case, the goal is
not dimensionality reduction but rather the transformation of a data set to standard-
ize certain of its properties This can be important in allowing subsequent machine
learning algorithms to be applied successfully to the data set Typically, it is done
when the original variables are measured in various different units or have signif-
icantly different variabilities For instance in the Old Faithful data set, the time
between eruptions is typically an order of magnitude greater than the duration of an
eruption When we applied the K-means algorithm to this data set, we Ô¨Årst made a Section 15.1
separate linear re-scaling of the individual variables such that each variable had zero
16.1

============================================================

=== CHUNK 493 ===
Palavras: 357
Caracteres: 2129
--------------------------------------------------
Principal Component Analysis 503
 
iŒªi
(a)0 200 400 6000123x 105
 
MJ
(b)0 200 400 6000123x 106
Figure 16.4 (a) Plot of the eigenvalue spectrum for the data set of handwritten digits used in Figure 16.3 (b) Plot of the sum of the discarded eigenvalues, which represents the sum-of-squares error Jintroduced by
projecting the data onto a principal component subspace of dimensionality M mean and unit variance This is known as standardizing the data, and the covariance
matrix for the standardized data has components
ij=1
NN/summationdisplay
n=1(xni‚àíxi)
i(xnj‚àíxj)
j(16.22)
whereiis the standard deviation of xi This is known as the correlation matrix of
the original data and has the property that if two components xiandxjof the data
are perfectly correlated, then ij= 1, and if they are uncorrelated, then ij= 0 However, using PCA we can make a more substantial normalization of the data
to give it zero mean and unit covariance, so that different variables become decorre-
Original M= 1 M= 10 M= 50 M= 250
Figure 16.5 An example from the data set of handwritten digits together with its PCA reconstructions obtained
by retaining Mprincipal components for various values of M AsMincreases, the reconstruction becomes
more accurate and would become perfect when M=D= 28√ó28 = 784 CONTINUOUS LATENT V ARIABLES
1 2 3 4 5 6507090
‚àí2 0 2‚àí202
‚àí2 0 2‚àí202
Figure
16.6 Illustration of the effects of linear pre-processing applied to the Old Faithful data set The plot on
the left shows the original data The centre plot shows the result of standardizing the individual variables to zero
mean and unit variance Also shown are the principal axes of this normalized data set, plotted over the range
¬±1=2
i The plot on the right shows the result of whitening the data to give it zero mean and unit covariance To do this, we Ô¨Årst write the eigenvector equation (16.17) in the form
SU=UL (16.23)
where Lis aD√óDdiagonal matrix with elements i, andUis aD√óDorthog-
onal matrix with columns given by ui Then we deÔ¨Åne, for each data point xn, a
transformed value given by
yn=L‚àí1=2UT(xn‚àíx
) (16.24)
where xis
the sample mean deÔ¨Åned by (16.1)

============================================================

=== CHUNK 494 ===
Palavras: 373
Caracteres: 2343
--------------------------------------------------
Clearly, the set {yn}has zero mean,
and its covariance is given by the identity matrix because
1
NN/summationdisplay
n
=1ynyT
n=1
NN/summationdisplay
n
=1L‚àí1=2UT(xn‚àíx
)(xn‚àíx
)TUL‚àí1=2
=L‚àí1=2UTSUL‚àí1=2=L‚àí1=2LL‚àí1=2=I: (16.25)
This operation is known as whitening orsphering the data and is illustrated for the
Old Faithful data set in Figure 16.6 Section 15.1
16.1.5 High-dimensional data
In some applications of PCA, the number of data points is smaller than the di-
mensionality of the data space For example, we might want to apply PCA to a data
set of a few hundred images, each of which corresponds to a vector in a space of po-
tentially several million dimensions (corresponding to three colour values for each
of the pixels in the image) Note that in a D-dimensional space, a set of Npoints,
whereN <D , deÔ¨Ånes a linear subspace whose dimensionality is at most N‚àí1, and
so there is little point in applying PCA for values of Mthat are greater than N‚àí1 Indeed, if we perform PCA we will Ô¨Ånd that at least D‚àíN+1of the eigenvalues are
16.1 Principal Component Analysis 505
zero, corresponding to eigenvectors along whose directions the data set has zero vari-
ance Furthermore, typical algorithms for Ô¨Ånding the eigenvectors of a D√óDmatrix
have a computational cost that scales like O(D3), and so for applications such as the
image example, a direct application of PCA will be computationally infeasible We can resolve this problem as follows First, let us deÔ¨Åne Xto be the (N√óD)-
dimensional centred data matrix, whose nth row is given by (xn‚àíx)T The covari-
ance matrix (16.3) can then be written as S=N‚àí1XTX, and the corresponding
eigenvector equation becomes
1
NXTXui=iui: (16.26)
Now pre-multiply both sides by Xto give
1
NXXT(Xui) =i(Xui): (16.27)
If we now deÔ¨Åne vi=Xui, we obtain
1
NXXTvi=ivi; (16.28)
which is an eigenvector equation for the N√óNmatrixN‚àí1XXT We see that this
has the same N‚àí1eigenvalues as the original covariance matrix (which itself has an
additionalD‚àíN+1eigenvalues of value zero) Thus, we can solve the eigenvector
problem in spaces of lower dimensionality with computational cost O(N3)instead
ofO(D3) To determine the eigenvectors, we multiply both sides of (16.28) by XT
to give/parenleftbigg1
NXTX/parenrightbigg
(XTvi) =i(XTvi) (16.29)
from which we see that (XTvi)is an eigenvector of Swith eigenvalue i

============================================================

=== CHUNK 495 ===
Palavras: 353
Caracteres: 2230
--------------------------------------------------
Note,
however, that these eigenvectors are not necessarily normalized To determine the
appropriate normalization, we re-scale ui‚àùXTviby a constant such that /bardblui/bardbl= 1,
which, assuming vihas been normalized to unit length, gives Exercise 16.3
ui=1
(Ni)1=2XTvi: (16.30)
In summary, to apply this approach we Ô¨Årst evaluate XXTand then Ô¨Ånd its eigen-
vectors and eigenvalues and then compute the eigenvectors in the original data space
using (16.30) CONTINUOUS LATENT V ARIABLES
16.2 Probabilistic Latent Variables
We have seen in the previous section that PCA can be deÔ¨Åned in terms of a linear
projection of the data onto a subspace of lower dimensionality than the original data
space Each data point projects to a unique value of the quantities znjdeÔ¨Åned by
(16.12), and we can view these quantities as deterministic latent variables To intro-
duce and motivate probabilistic continuous latent variables, we now show that PCA
can also be expressed as the maximum likelihood solution of a probabilistic latent-
variable model This reformulation of PCA, known as probabilistic PCA, has several
advantages compared with conventional PCA:
‚Ä¢ A probabilistic PCA model represents a constrained form of a Gaussian dis-
tribution in which the number of free parameters can be restricted while still
allowing the model to capture the dominant correlations in a data set ‚Ä¢ We can derive an EM algorithm for PCA that is computationally efÔ¨Åcient in
situations where only a few leading eigenvectors are required and that avoids
having to evaluate the data covariance matrix as an intermediate step Section 16.3.2
‚Ä¢ The combination of a probabilistic model and EM allows us to deal with miss-
ing values in the data set ‚Ä¢ Mixtures of probabilistic PCA models can be formulated in a principled way
and trained using the EM algorithm ‚Ä¢ The existence of a likelihood function allows direct comparison with other
probabilistic density models By contrast, conventional PCA will assign a low
reconstruction cost to data points that are close to the principal subspace even
if they lie arbitrarily far from the training data ‚Ä¢ Probabilistic PCA can be used to model class-conditional densities and hence
be applied to classiÔ¨Åcation problems

============================================================

=== CHUNK 496 ===
Palavras: 357
Caracteres: 2319
--------------------------------------------------
‚Ä¢ A probabilistic PCA model can be run generatively to provide samples from
the distribution ‚Ä¢ Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which
the dimensionality of the principal subspace can be found automatically from
the data (Bishop, 2006) This formulation of PCA as a probabilistic model was proposed independently by
Tipping and Bishop 1997; 1999 and by Roweis (1998) As we will see later, it is
closely related to factor analysis (Basilevsky, 1994) 16.2.1 Generative model
Probabilistic PCA is a simple example of the linear-Gaussian framework in Section 11.1.4
which all the marginal and conditional distributions are Gaussian We can formu-
late probabilistic PCA by Ô¨Årst introducing an explicit M-dimensional latent variable
16.2 Probabilistic Latent Variables 507
zcorresponding to the principal-component subspace Next we deÔ¨Åne a Gaussian
prior distribution p(z) over the latent variable, together with a Gaussian conditional
distribution p(x|z) for theD-dimensional observed variable xconditioned on the
value of the latent variable SpeciÔ¨Åcally, the prior distribution over zis given by a
zero-mean unit-covariance Gaussian:
p(z) =N(z|0;I): (16.31)
Similarly, the conditional distribution of the observed variable x, conditioned on the
value of the latent variable z, is again Gaussian:
p(x|z) =N(x|Wz +;2I) (16.32)
in which the mean of xis a general linear function of zgoverned by the D√óM
matrix Wand theD-dimensional vector  Note that this factorizes with respect to
the elements of x In other words this is an example of a naive Bayes model As Section 11.2.3
we will see shortly, the columns of Wspan a linear subspace within the data space
that corresponds to the principal subspace The other parameter in this model is the
scalar2governing the variance of the conditional distribution Note that there is no
loss of generality in assuming a zero-mean unit-covariance Gaussian for the latent
distributionp(z) because a more general Gaussian distribution would give rise to an
equivalent probabilistic model Exercise 16.4
We can view the probabilistic PCA model from a generative viewpoint in which
a sampled value of the observed variable is obtained by Ô¨Årst choosing a value for
the latent variable and then sampling the observed variable conditioned on this latent
value

============================================================

=== CHUNK 497 ===
Palavras: 365
Caracteres: 2448
--------------------------------------------------
SpeciÔ¨Åcally, the D-dimensional observed variable xis deÔ¨Åned by a linear
transformation of the M-dimensional latent variable zplus additive Gaussian noise,
so that
x=Wz++ (16.33)
where zis anM-dimensional Gaussian latent variable, and is aD-dimensional
zero-mean Gaussian-distributed noise variable with covariance 2I This generative
process is illustrated in Figure 16.7 Note that this framework is based on a mapping
from latent space to data space, in contrast to the more conventional view of PCA
discussed above The reverse mapping, from data space to the latent space, will be
obtained shortly using Bayes‚Äô theorem 16.2.2 Likelihood function
Suppose we wish to determine the values of the parameters W,, and2using
maximum likelihood To write down the likelihood function, we need an expression
for the marginal distribution p(x) of the observed variable This is expressed, from
the sum and product rules of probability, in the form
p(x) =/integraldisplay
p(x|z)p(z) d z: (16.34)
Because this corresponds to a linear-Gaussian model, this marginal distribution is
again Gaussian, and is given by Exercise 16.6
508 16 CONTINUOUS LATENT V ARIABLES
zp(z)
bzx2
x1p(xjbz)
gbzjwjwx2
x1
p(x)
Figure 16.7 An illustration of the generative view of a probabilistic PCA model for a two-dimensional data
space and a one-dimensional latent space An observed data point xis generated by Ô¨Årst drawing a value bz
for the latent variable from its prior distribution p(z)and then drawing a value for xfrom an isotropic Gaussian
distribution (illustrated by the red circles) having mean wbz+and covariance 2I The green ellipses show the
density contours for the marginal distribution p(x) p(x) =N(x|; C) (16.35)
where theD√óDcovariance matrix Cis deÔ¨Åned by
C=WWT+2I: (16.36)
This result can also be derived more directly by noting that the predictive distribution
will be Gaussian and then evaluating its mean and covariance using (16.33) This
gives
E[x] = E[Wz ++] = (16.37)
cov[x] = E/bracketleftbig
(Wz +)(Wz +)T/bracketrightbig
=E/bracketleftbig
WzzTWT/bracketrightbig
+E[T] (16.38)
=WWT+2I (16.39)
where we have used the fact that zandare independent random variables and hence
are uncorrelated Intuitively, we can think of the distribution p(x) as being deÔ¨Åned by taking an
isotropic Gaussian ‚Äòspray can‚Äô and moving it across the principal subspace spraying
Gaussian ink with density determined by 2and weighted by the prior distribution

============================================================

=== CHUNK 498 ===
Palavras: 375
Caracteres: 2538
--------------------------------------------------
The accumulated ink density gives rise to a ‚Äòpancake‚Äô shaped distribution represent-
ing the marginal density p(x) The predictive distribution p(x) is governed by the parameters ,W, and2 However, there is redundancy in this parameterization corresponding to rotations of
the latent space coordinates To see this, consider a matrix /tildewiderW=WR where Ris
an orthogonal matrix Using the orthogonality property RRT=I, we see that the
quantity/tildewiderW/tildewiderWTthat appears in the covariance matrix Ctakes the form
/tildewiderW/tildewiderWT=WRRTWT=WWT(16.40)
16.2 Probabilistic Latent Variables 509
Figure 16.8 The probabilistic PCA model for a data set of Nobserva-
tions of xcan be expressed as a directed graph in which
each observation xnis associated with a value znof the
latent variable.2 zn
xn W
N
and
hence is independent of R Thus, there is a whole family of matrices /tildewiderWall of
which give rise to the same predictive distribution This invariance can be understood
in terms of rotations within the latent space We will return to a discussion of the
number of independent parameters in this model later When we evaluate the predictive distribution, we require C‚àí1, which involves
the inversion of a D√óDmatrix The computation required to do this can be reduced
by making use of the matrix inversion identity (A.7) to give
C‚àí1=‚àí2I‚àí‚àí2WM‚àí1WT(16.41)
where theM√óMmatrix Mis deÔ¨Åned by
M=WTW+2I: (16.42)
Because we invert Mrather than inverting Cdirectly, the cost of evaluating C‚àí1is
reduced fromO(D3)toO(M3) As well as the predictive distribution p(x), we will also require the posterior
distributionp(z|x), which can again be written down directly using the result (3.100)
for linear-Gaussian models to give Exercise 16.8
p(z|x) =N/parenleftbig
z|M‚àí1WT(x‚àí);2M‚àí1/parenrightbig
: (16.43)
Note that the posterior mean depends on x, whereas the posterior covariance is in-
dependent of x 16.2.3 Maximum likelihood
We next consider the determination of the model parameters using maximum
likelihood Given a data set X={xn}of observed data points, the probabilistic
PCA model can be expressed as a directed graph, as shown in Figure 16.8 The
corresponding log likelihood function is given, from (16.35), by
lnp(X|; W;2) =N/summationdisplay
n=1lnp(xn|W;;2)
=‚àíND
2ln(2
)‚àíN
2ln|
C|‚àí1
2N/summationdisplay
n
=1(xn‚àí)TC‚àí1(xn‚àí): (16.44)
510 16 CONTINUOUS LATENT V ARIABLES
Setting the derivative of the log likelihood with respect to equal to zero gives
the expected result =xwhere xis the data mean deÔ¨Åned by (16.1)

============================================================

=== CHUNK 499 ===
Palavras: 364
Caracteres: 2389
--------------------------------------------------
Because Exercise 16.9
the log likelihood is a quadratic function of , this solution represents the unique
maximum, as can be conÔ¨Årmed by computing second derivatives Back-substituting,
we can then write the log likelihood function in the form
lnp(X|W;;2) =‚àíN
2/braceleftbig
Dln(2 ) + ln|C|+Tr/parenleftbig
C‚àí1S/parenrightbig/bracerightbig
(16.45)
where Sis the data covariance matrix deÔ¨Åned by (16.3) Maximization with respect to Wand2is more complex but nonetheless has
an exact closed-form solution It was shown by Tipping and Bishop (1999) that all
the stationary points of the log likelihood function can be written as
WML=UM(LM‚àí2I)1=2R (16.46)
where UMis aD√óMmatrix whose columns are given by any subset (of size M)
of the eigenvectors of the data covariance matrix S TheM√óMdiagonal matrix
LMhas elements given by the corresponding eigenvalues i, andRis an arbitrary
M√óMorthogonal matrix Furthermore, Tipping and Bishop (1999) showed that the maximum of the like-
lihood function is obtained when the Meigenvectors are chosen to be those whose
eigenvalues are the Mlargest (all other solutions being saddle points) A similar re-
sult was conjectured independently by Roweis (1998), although no proof was given Again, we will assume that the eigenvectors have been arranged in order of decreas-
ing values of the corresponding eigenvalues, so that the Mprincipal eigenvectors
areu1;:::;uM In this case, the columns of WdeÔ¨Åne the principal subspace of
standard PCA The corresponding maximum likelihood solution for 2is then given
by
2
ML=1
D‚àíMD/summationdisplay
i=M +1i (16.47)
so that2
MLis the average variance associated with the discarded dimensions Because Ris orthogonal, it can be interpreted as a rotation matrix in the M-
dimensional latent space If we substitute the solution for Winto the expression for
Cand make use of the orthogonality property RRT=I, we see that Cis indepen-
dent of R This simply says that the predictive density is unchanged by rotations
in the latent space as discussed earlier For the particular case R=I, we see that
the columns of Ware the principal component eigenvectors scaled by the variance
parametersi‚àí2 The interpretation of these scaling factors is clear once we rec-
ognize that for a convolution of independent Gaussian distributions (in this case the
latent space distribution and the noise model) the variances are additive

============================================================

=== CHUNK 500 ===
Palavras: 360
Caracteres: 2315
--------------------------------------------------
Thus, the
varianceiin the direction of an eigenvector uiis composed of the sum of a contri-
butioni‚àí2from the projection of the unit-variance latent space distribution into
data space through the corresponding column of Wplus an isotropic contribution of
variance2, which is added in all directions by the noise model Probabilistic Latent Variables 511
It is worth taking a moment to study the form of the covariance matrix given
by (16.36) Consider the variance of the predictive distribution along some direction
speciÔ¨Åed by the unit vector v, where vTv= 1, which is given by vTCv First
suppose that vis orthogonal to the principal subspace, in other words it is given by
some linear combination of the discarded eigenvectors Then vTU=0and hence
vTCv=2 Thus, the model predicts a noise variance orthogonal to the principal
subspace, which from (16.47) is just the average of the discarded eigenvalues Now
suppose that v=uiwhere uiis one of the retained eigenvectors deÔ¨Åning the prin-
cipal subspace Then vTCv= (i‚àí2) +2=i In other words, this model
correctly captures the variance of the data along the principal axes and approximates
the variance in all remaining directions with a single average value 2 One way to construct the maximum likelihood density model would simply be
to Ô¨Ånd the eigenvectors and eigenvalues of the data covariance matrix and then to
evaluate Wand2using the results given above In this case, we would choose
R=Ifor convenience However, if the maximum likelihood solution is found by
numerical optimization of the likelihood function, for instance using an algorithm
such as conjugate gradients (Fletcher, 1987; Nocedal and Wright, 1999) or through
the EM algorithm, then the resulting value of Ris essentially arbitrary This implies Section 16.3.2
that the columns of Wneed not be orthogonal If an orthogonal basis is required,
the matrix Wcan be post-processed appropriately (Golub and Van Loan, 1996) Al-
ternatively, the EM algorithm can be modiÔ¨Åed in such a way as to yield orthonormal
principal directions, sorted in descending order of the corresponding eigenvalues,
directly (Ahn and Oh, 2003) The rotational invariance in latent space represents a form of statistical non-
identiÔ¨Åability, analogous to that encountered for mixture models for discrete latent
variables

============================================================

=== CHUNK 501 ===
Palavras: 367
Caracteres: 2338
--------------------------------------------------
Here there is a continuum of parameters, any value of which leads to the
same predictive density, in contrast to the discrete non-identiÔ¨Åability associated with
component relabelling in the mixture setting If we consider M=D, so that there is no reduction of dimensionality, then
UM=UandLM=L Making use of the orthogonality properties UUT=Iand
RRT=I, we see that the covariance Cof the marginal distribution for xbecomes
C=U(L‚àí2I)1=2RRT(L‚àí2I)1=2UT+2I=ULUT=S (16.48)
and so we obtain the standard maximum likelihood solution for an unconstrained
Gaussian distribution in which the covariance matrix is given by the sample covari-
ance Conventional PCA is generally formulated as a projection of points from the D-
dimensional data space onto an M-dimensional linear subspace Probabilistic PCA,
however, is most naturally expressed as a mapping from the latent space into the data
space via (16.33) For applications such as visualization and data compression, we
can reverse this mapping using Bayes‚Äô theorem Any point xin data space can then
be summarized by its posterior mean and covariance in latent space From (16.43)
the mean is given by
E[z|x] = M‚àí1WT
ML(x‚àíx) (16.49)
512 16 CONTINUOUS LATENT V ARIABLES
where Mis given by (16.42) This projects to a point in data space given by
WE[z|x] +: (16.50)
Note that this takes the same form as the equations for regularized linear regression Section 4.1.6
and is a consequence of maximizing the likelihood function for a linear-Gaussian
model Similarly, from (16.43) the posterior covariance is given by 2M‚àí1and is
independent of x If we take the limit 2‚Üí0, then the posterior mean reduces to
(WT
MLWML)‚àí1WT
ML(x‚àíx); (16.51)
which represents an orthogonal projection of the data point onto the latent space,
and so we recover the standard PCA model The posterior covariance in this limit is Exercise 16.11
zero, however, and the density becomes singular For 2>0, the latent projection
is shifted towards the origin, relative to the orthogonal projection Exercise 16.12
Finally, note that an important role for the probabilistic PCA model is in deÔ¨Ån-
ing a multivariate Gaussian distribution in which the number of degrees of freedom,
in other words the number of independent parameters, can be controlled while still
allowing the model to capture the dominant correlations in the data

============================================================

=== CHUNK 502 ===
Palavras: 350
Caracteres: 2161
--------------------------------------------------
Recall that a
general Gaussian distribution has D(D+ 1)=2 independent parameters in its co-
variance matrix (plus another Dparameters in its mean) Thus, the number of pa- Section 3.2
rameters scales quadratically with Dand can become excessive in spaces of high
dimensionality If we restrict the covariance matrix to be diagonal, then it has only
Dindependent parameters, and so the number of parameters now grows linearly
with dimensionality However, it now treats the variables as if they were indepen-
dent and hence can no longer express any correlations between them Probabilistic
PCA provides an elegant compromise in which the Mmost signiÔ¨Åcant correlations
can be captured while still ensuring that the total number of parameters grows only
linearly with D We can see this by evaluating the number of degrees of freedom in
the probabilistic PCA model as follows The covariance matrix Cdepends on the
parameters W, which has size D√óM, and2, giving a total parameter count of
DM + 1 However, we have seen that there is some redundancy in this parameter-
ization associated with rotations of the coordinate system in the latent space The
orthogonal matrix Rthat expresses these rotations has size M√óM In the Ô¨Årst
column of this matrix, there are M‚àí1independent parameters, because the column
vector must be normalized to unit length In the second column, there are M‚àí2
independent parameters, because the column must be normalized and also must be
orthogonal to the previous column, and so on Summing this arithmetic series, we
see that Rhas a total of M(M‚àí1)=2 independent parameters Thus, the number of
degrees of freedom in the covariance matrix Cis given by
DM + 1‚àíM(M‚àí1)=2: (16.52)
The number of independent parameters in this model therefore only grows linearly
withD, for Ô¨ÅxedM If we take M=D‚àí1, then we recover the standard result
for a full covariance Gaussian In this case, the variance along D‚àí1linearly in- Exercise 16.14
16.2 Probabilistic Latent Variables 513
dependent directions is controlled by the columns of W, and the variance along the
remaining direction is given by 2 IfM= 0, the model is equivalent to the isotropic
covariance case

============================================================

=== CHUNK 503 ===
Palavras: 385
Caracteres: 2502
--------------------------------------------------
16.2.4 Factor analysis
Factor analysis is a linear-Gaussian latent-variable model that is closely related
to probabilistic PCA Its deÔ¨Ånition differs from that of probabilistic PCA only in that
the conditional distribution of the observed variable xgiven the latent variable zhas
a diagonal rather than an isotropic covariance so that
p(x|z) =N(x|Wz +;	) (16.53)
where 	is aD√óDdiagonal matrix Note that the factor analysis model, in com-
mon with probabilistic PCA, assumes that the observed variables x1;:::;xDare
independent, given the latent variable z In essence, a factor analysis model explains
the observed covariance structure of the data by representing the independent vari-
ance associated with each coordinate in the matrix 	and capturing the covariance
between variables in the matrix W In the factor analysis literature, the columns
ofW, which capture the correlations between observed variables, are called factor
loadings, and the diagonal elements of 	, which represent the independent noise
variances for each of the variables, are called uniquenesses The origins of factor analysis are as old as those of PCA, and discussions of
factor analysis can be found in the books by Everitt (1984), Bartholomew (1987),
and Basilevsky (1994) Links between factor analysis and PCA were investigated
by Lawley (1953) and Anderson (1963), who showed that at stationary points of
the likelihood function, for a factor analysis model with 	=2I, the columns of
Ware scaled eigenvectors of the sample covariance matrix and 2is the average
of the discarded eigenvalues Later, Tipping and Bishop (1999) showed that the
maximum of the log likelihood function occurs when the eigenvectors comprising
Ware chosen to be the principal eigenvectors Making use of (16.34), we see that the marginal distribution for the observed
variable is given by p(x) =N(x|; C)where now
C=WWT+	: (16.54)
As with probabilistic PCA, this model is invariant to rotations in the latent space Exercise 16.16
Historically, factor analysis has been the subject of controversy when attempts
have been made to place an interpretation on the individual factors (the coordinates
inz-space), which has proven problematic due to the non-identiÔ¨Åability of factor
analysis associated with rotations in this space From our perspective, however, we
shall view factor analysis as a form of latent-variable density model, in which the
form of the latent space is of interest but not the particular choice of coordinates
used to describe it

============================================================

=== CHUNK 504 ===
Palavras: 363
Caracteres: 2275
--------------------------------------------------
If we wish to remove the degeneracy associated with latent-
space rotations, we must consider non-Gaussian latent-variable distributions, giving
rise to independent component analysis models Section 16.2.5
Another difference between probabilistic PCA and factor analysis is their be-
haviour under transformations of the data set For PCA and probabilistic PCA, if we Exercise 16.17
514 16 CONTINUOUS LATENT V ARIABLES
rotate the coordinate system in data space, then we obtain exactly the same Ô¨Åt to the
data but with the Wmatrix transformed by the corresponding rotation matrix How-
ever, for factor analysis, the analogous property is that if we make a component-wise
re-scaling of the data vectors, then this is absorbed into a corresponding re-scaling
of the elements of 16.2.5 Independent component analysis
One generalization of the linear-Gaussian latent-variable model is to consider
models in which the observed variables are related linearly to the latent variables,
but for which the latent distribution is non-Gaussian An important class of such
models, known as independent component analysis, or ICA, arises when we consider
a distribution over the latent variables that factorizes, so that
p(z) =M/productdisplay
j=1p(zj): (16.55)
To understand the role of such models, consider a situation in which two people
are talking at the same time, and we record their voices using two microphones If we ignore effects such as time delay and echoes, then the signals received by
the microphones at any point in time will be given by linear combinations of the
amplitudes of the two voices The coefÔ¨Åcients of this linear combination will be
constant, and if we can infer their values from sample data, then we can invert the
mixing process (assuming it is non-singular) and thereby obtain two clean signals
each of which contains the voice of just one person This is an example of a problem
called blind source separation in which ‚Äòblind‚Äô refers to the fact that we are given
only the mixed data, and neither the original sources nor the mixing coefÔ¨Åcients are
observed (Cardoso, 1998) This type of problem is sometimes addressed using the following approach
(MacKay, 2003) in which we ignore the temporal nature of the signals and treat the
successive samples as i.i.d

============================================================

=== CHUNK 505 ===
Palavras: 353
Caracteres: 2284
--------------------------------------------------
We consider a generative model in which there are two
latent variables corresponding to the unobserved speech signal amplitudes, and there
are two observed variables given by the signal values at the microphones The latent
variables have a joint distribution that factorizes as above, and the observed variables
are given by a linear combination of the latent variables There is no need to include
a noise distribution because the number of latent variables equals the number of ob-
served variables, and therefore the marginal distribution of the observed variables
will not in general be singular, so the observed variables are simply deterministic
linear combinations of the latent variables Given a data set of observations, the
likelihood function for this model is a function of the coefÔ¨Åcients in the linear com-
bination The log likelihood can be maximized using gradient-based optimization
giving rise to a particular version of ICA The success of this approach requires that the latent variables have non-Gaussian
distributions To see this, recall that in probabilistic PCA (and in factor analysis) the
latent-space distribution is given by a zero-mean isotropic Gaussian The model
therefore cannot distinguish between two different choices for the latent variables
16.2 Probabilistic Latent Variables 515
if these differ simply by a rotation in latent space This can be veriÔ¨Åed directly
by noting that the marginal density (16.35), and hence the likelihood function, is
unchanged if we make the transformation W‚ÜíWR where Ris an orthogonal
matrix satisfying RRT=I, because the matrix Cgiven by (16.36) is itself invariant Extending the model to allow more general Gaussian latent distributions does not
change this conclusion because, as we have seen, such a model is equivalent to the
zero-mean isotropic Gaussian latent-variable model Another way to see why a Gaussian latent-variable distribution in a linear model
is insufÔ¨Åcient to Ô¨Ånd independent components is to note that the principal compo-
nents represent a rotation of the coordinate system in data space so as to diagonalize
the covariance matrix The data distribution in the new coordinates is then uncorre-
lated Although zero correlation is a necessary condition for independence it is not,
however, sufÔ¨Åcient

============================================================

=== CHUNK 506 ===
Palavras: 356
Caracteres: 2233
--------------------------------------------------
In practice, a common choice for the latent-variable distribution Exercise 2.39
is given by
p(zj) =1
cosh(
zj)=2
(
ezj+e‚àízj); (16.56)
which has heavy tails compared to a Gaussian, reÔ¨Çecting the observation that many
real-world distributions also exhibit this property The original ICA model (Bell and Sejnowski, 1995) was based on the optimiza-
tion of an objective function deÔ¨Åned by information maximization One advantage
of a probabilistic latent-variable formulation is that it helps to motivate and formu-
late generalizations of basic ICA For instance, independent factor analysis (Attias,
1999) considers a model in which the number of latent and observed variables can
differ, the observed variables are noisy, and the individual latent variables have Ô¨Çex-
ible distributions modelled by mixtures of Gaussians The log likelihood for this
model is maximized using EM, and the reconstruction of the latent variables is ap-
proximated using a variational approach Many other types of model have been
considered, and there is now a huge literature on ICA and its applications (Jutten
and Herault, 1991; Comon, Jutten, and Herault, 1991; Amari, Cichocki, and Yang,
1996; Pearlmutter and Parra, 1997; Hyv ¬®arinen and Oja, 1997; Hinton et al., 2001;
Miskin and MacKay, 2001; Hojen-Sorensen, Winther, and Hansen, 2002; Choudrey
and Roberts, 2003; Chan, Lee, and Sejnowski, 2003; Stone, 2004) 16.2.6 Kalman Ô¨Ålters
So far we have assumed that the data values are i.i.d A common situation in
which this assumption does not hold is when the data points form an ordered se-
quence We have seen that a hidden Markov model can be viewed as an extension
of the mixture models to allow for sequential correlations in the data In a similar Section 15.3.1
way, a continuous latent-variable model can be extended to handle sequential data
by connecting the latent variables to form a Markov chain, as shown in the graph-
ical model of Figure 16.9 This is known as a linear dynamical system orKalman
Ô¨Ålter (Zarchan and Musoff, 2005) Note that this is the same graphical structure as
a hidden Markov model It is interesting to note that, historically, hidden Markov Section 15.3.1
models and linear dynamical systems were developed independently

============================================================

=== CHUNK 507 ===
Palavras: 402
Caracteres: 2639
--------------------------------------------------
Once they are
both expressed as graphical models, however, the deep relationship between them
516 16 CONTINUOUS LATENT V ARIABLES
Figure 16.9 A probabilistic graphical model for se-
quential data, known as a linear dynami-
cal system, or Kalman Ô¨Ålter, in which the
latent variables form a Markov chain.z1 z2 zN
x1 x2 xN
immediately
becomes apparent Kalman Ô¨Ålters are widely used in many real-time
tracking applications, for example to track aircraft using radar reÔ¨Çections In the simplest such model, the distributions p(xn|zn)inFigure 16.9 represent a
linear-Gaussian latent-variable model for that particular observation, of the kind we
have discussed previously for i.i.d However, the latent variables {zn}are no
longer treated as independent but now form a Markov chain in which the distribution
p(zn|zn‚àí1)of each latent variable is conditioned on the state of the previous latent
variable in the chain Again these can be chosen to be linear-Gaussian in which
the distribution of znis Gaussian with a mean given by a linear function of zn‚àí1 Typically the parameters of all the distributions p(xn|zn)are shared, and likewise
the parameters of the distributions p(zn|zn‚àí1)are shared, so that the total number
of parameters in the model is Ô¨Åxed, independently of the length of the sequence These parameters can be learned from data by maximum likelihood with efÔ¨Åcient
algorithms that involve propagating messages around the graph (Bishop, 2006) For
the rest of this chapter, however, we will focus on i.i.d Evidence
Lower Bound
In
our discussion of models with discrete latent variables, we derived the evidence
lower bound (ELBO) on the marginal log likelihood and showed how this forms the Section 15.4
basis for deriving the expectation‚Äìmaximization (EM) algorithm including its gener-
alizations such as variational inference The same framework applies to continuous
latent variables as well as to models that combine both discrete and continuous vari-
ables Here we present a slightly different derivation of the ELBO, and we assume
that the latent variables zare continuous Consider a model p(x;z|w)with an observed variable x, a latent variable z, and
a learnable parameter vector w If we introduce an arbitrary distribution q(z)over
the latent variable then we can write the log likelihood function lnp(x|w )as a sum
of two terms in the form Exercise 16.18
lnp(x|w ) =L(w) + KL (q(z)/bardblp(z|x; w)) (16.57)
where we have deÔ¨Åned
L(q;w) =/integraldisplay
q(z) ln/braceleftbiggp(x;z|w)
q(
z)/bracerightbigg
dz (16.58)
KL (q(z)/bardblp(z|x; w)) =‚àí/integraldisplay
q(z) ln/braceleftbiggp(z|x; w)
q(
z)/bracerightbigg
dz: (16.59)
16.3

============================================================

=== CHUNK 508 ===
Palavras: 403
Caracteres: 2567
--------------------------------------------------
Evidence Lower Bound 517
Since KL (q(z)/bardblp(z|x; w))is a Kullback‚ÄìLeibler divergence, it satisÔ¨Åes the prop-
ertyKL (¬∑/bardbl¬∑)>0from which it follows that Section 2.5.5
lnp(x|w )>L(w) (16.60)
and we therefore see that L(q;w)given by (16.58) forms a lower bound on the log
likelihood, known as the evidence lower bound or ELBO We see that L(q;w)takes
the same form (15.53 ) as derived for the discrete case but with summations replaced
by integrals We can maximize the log likelihood function using a two-stage iterative proce-
dure called the expectation maximization algorithm, or EM algorithm, in which we
alternately maximize L(q;w)with respect to q(z)(the E step) and w(the M step) We Ô¨Årst initialize the parameters w(old) Then in the E step we keep wÔ¨Åxed and
we maximize the lower bound with respect to q(z) This is easily done by noting
that the highest value for the bound is obtained by minimizing the Kullback‚ÄìLeibler
divergence in (16.59) and hence is achieved when q(z) =p(z|x; w(old))for which
the Kullback‚ÄìLeibler divergence is zero In the M step, we keep this choice of q(z)
Ô¨Åxed and maximize L(q;w)with respect to w Substituting for q(z)in (16.58) we
obtain
L(q;w) =/integraldisplay
p(z|x; w(old)) lnp(x;z|w) dz
‚àí/integraldisplay
p(z|x; w(old)) lnp(z|x; w(old)) dz: (16.61)
We now maximize this with respect to win the M step while keeping w(old)Ô¨Åxed Note that the second term on the right-hand side of (16.61) is independent of wand
so can be ignored during the M step The Ô¨Årst term on the right-hand side is the
expectation of the complete data log likelihood where the expectation is taken with Section 15.3
respect to the posterior distribution of zcomputed using w(old) If we have a data set x1;:::;xNof i.i.d observations then the likelihood func-
tion takes the form
lnp(X|w ) =N/summationdisplay
n=1lnp(xn|w) (16.62)
where the data matrix Xcomprises x1;:::;xN, and the parameters ware shared
across all data points For each data point we introduce a corresponding latent vari-
ableznwith its associated distribution q(zn), and by following similar steps to those
used to derive (16.58), we obtain the ELBO in the form Exercise 16.19
L(q;w) =N/summationdisplay
n=1/integraldisplay
q(zn) ln/braceleftbiggp(xn;zn|w)
q(zn)/bracerightbigg
dzn: (16.63)
When we discuss variational autoencoders, we will encounter a model for which Section 19.2
an exact solution to the E step is not feasible so instead a partial maximization is
performed by modelling q(z)using a deep neural network and then using the ELBO
to learn the parameters of the network

============================================================

=== CHUNK 509 ===
Palavras: 363
Caracteres: 2506
--------------------------------------------------
CONTINUOUS LATENT V ARIABLES
16.3.1 Expectation maximization
We can now use the EM algorithm, derived by iteratively maximizing the ev-
idence lower bound, to learn the parameters of the probabilistic PCA model This
may seem rather pointless because we have already obtained an exact closed-form
solution for the maximum likelihood parameter values However, in spaces of high
dimensionality, there may be computational advantages in using an iterative EM
procedure rather than working directly with the sample covariance matrix This EM
procedure can also be extended to the factor analysis model, for which there is no Section 16.2.4
closed-form solution Finally, it allows missing data to be handled in a principled
way We can derive the EM algorithm for probabilistic PCA by following the general
framework for EM Thus, we write down the complete-data log likelihood and take Section 15.3
its expectation with respect to the posterior distribution of the latent distribution
evaluated using ‚Äòold‚Äô parameter values Maximization of this expected complete-
data log likelihood then yields the ‚Äònew‚Äô parameter values Because the data points
are assumed independent, the complete-data log likelihood function takes the form
lnp/parenleftbig
X;Z|;W;2/parenrightbig
=N/summationdisplay
n=1{lnp(xn|zn) + lnp(zn)} (16.64)
where thenth row of the matrix Zis given by zn We already know that the exact
maximum likelihood solution for is given by the sample mean xdeÔ¨Åned by (16.1),
and it is convenient to substitute for at this stage Making use of the expressions
(16.31) and (16.32) for the latent and conditional distributions, respectively, and tak-
ing the expectation with respect to the posterior distribution over the latent variables,
we obtain
E[lnp/parenleftbig
X;Z|;W;2/parenrightbig
] =‚àíN/summationdisplay
n=1/braceleftbiggD
2ln(22) +1
2Tr/parenleftbig
E[znzT
n]/parenrightbig
+1
22/bardblxn‚àí/bardbl2‚àí1
2E[zn]TWT(xn‚àí)
+1
22Tr/parenleftbig
E[znzT
n]WTW/parenrightbig
+M
2ln(2 )/bracerightbigg
: (16.65)
Note that this depends on the posterior distribution only through the sufÔ¨Åcient statis-
tics of the Gaussian Thus, in the E step, we use the old parameter values to evaluate
E[zn] = M‚àí1WT(xn‚àíx) (16.66)
E[znzT
n] =2M‚àí1+E[zn]E[zn]T; (16.67)
which follow directly from the posterior distribution (16.43) together with the stan-
dard result E[znzT
n] = cov[ zn] +E[zn]E[zn]T Here Mis deÔ¨Åned by (16.42) In the M step, we maximize with respect to Wand2, keeping the posterior
statistics Ô¨Åxed

============================================================

=== CHUNK 510 ===
Palavras: 373
Caracteres: 2569
--------------------------------------------------
Maximization with respect to 2is straightforward For the maxi-
mization with respect to W, we make use of (A.24) to obtain the M-step equations: Exercise 16.21
16.3 Evidence Lower Bound 519
Wnew =/bracketleftBiggN/summationdisplay
n=1(xn‚àíx)E[zn]T/bracketrightBigg/bracketleftBiggN/summationdisplay
n=1E[znzT
n]/bracketrightBigg‚àí1
(16.68)
2
new =1
NDN/summationdisplay
n=1/braceleftbig
/bardblxn‚àíx/bardbl2‚àí2E[zn]TWT
new(xn‚àíx)
+Tr/parenleftbig
E[znzT
n]WT
newWnew/parenrightbig/bracerightbig
: (16.69)
The EM algorithm for probabilistic PCA proceeds by initializing the parameters
and then alternately computing the sufÔ¨Åcient statistics of the latent space posterior
distribution using (16.66) and (16.67) in the E step and revising the parameter values
using (16.68) and (16.69) in the M step One of the beneÔ¨Åts of the EM algorithm for PCA is its computational efÔ¨Åciency
for large-scale applications (Roweis, 1998) Unlike conventional PCA based on an
eigenvector decomposition of the sample covariance matrix, the EM approach is
iterative and so might appear to be less attractive However, each cycle of the EM
algorithm can be computationally much more efÔ¨Åcient than conventional PCA in
spaces of high dimensionality To see this, note that the eigendecomposition of the
covariance matrix requires O(D3)computation Often we are interested only in the
Ô¨ÅrstMeigenvectors and their corresponding eigenvalues, in which case we can use
algorithms that are O(MD2) However, evaluating the covariance matrix requires
O(ND2)computations, where Nis the number of data points Algorithms such
as the snapshot method (Sirovich, 1987), which assume that the eigenvectors are
linear combinations of the data vectors, avoid a direct evaluation of the covariance
matrix but areO(N3)and hence unsuited to large data sets The EM algorithm
described here also does not construct the covariance matrix explicitly Instead, the
most computationally demanding steps are those involving sums over the data set
that areO(NDM ) For large D, andM/lessmuchD, this can be a signiÔ¨Åcant saving
compared toO(ND2)and can offset the iterative nature of the EM algorithm Note that this EM algorithm can be implemented in an online form in which
eachD-dimensional data point is read in and processed and then discarded before
the next data point is considered To see this, note that the quantities evaluated in
the E step (an M-dimensional vector and an M√óMmatrix) can be computed for
each data point separately, and in the M step we need to accumulate sums over data
points, which we can do incrementally

============================================================

=== CHUNK 511 ===
Palavras: 362
Caracteres: 2155
--------------------------------------------------
This approach can be advantageous if both
NandDare large Because we now have a fully probabilistic model for PCA, we can deal with
missing data, provided that it is missing at random, in other words that the process
that determines which values are missing does not depend on the values of any ob-
served or unobserved variables Such data sets can be handled by marginalizing over
the distribution of the unobserved variables, and the resulting likelihood function can
be maximized using the EM algorithm Exercise 16.22
16.3.2 EM for PCA
Another elegant feature of the EM approach is that we can take the limit 2‚Üí0,
corresponding to standard PCA, and still obtain a valid EM-like algorithm (Roweis,
520 16 CONTINUOUS LATENT V ARIABLES
1998) From (16.67), we see that the only quantity we need to compute in the E step
isE[zn] Furthermore, the M step is simpliÔ¨Åed because M=WTW To emphasize
the simplicity of the algorithm, let us deÔ¨Åne /tildewideXto be a matrix of size N√óDwhose
nth row is given by the vector xn‚àíxand
similarly deÔ¨Åne 
to be a matrix of size
M√óNwhosenth column is given by the vector E[zn] The E step (16.66) of the
EM algorithm for PCA then becomes

= (WT
oldWold)‚àí1WT
old/tildewideXT(16.70)
and the M step (16.68) takes the form
Wnew=/tildewideXT
T(

T)‚àí1: (16.71)
Again these can be implemented in an online form These equations have a simple
interpretation as follows From our earlier discussion, we see that the E step involves
an orthogonal projection of the data points onto the current estimate for the principal
subspace Correspondingly, the M step represents a re-estimation of the principal
subspace to minimize the reconstruction error in which the projections are Ô¨Åxed Exercise 16.23
We can give a simple physical analogy for this EM algorithm, which is easily
visualized for D= 2 andM= 1 Consider a collection of data points in two
dimensions, and let the one-dimensional principal subspace be represented by a solid
rod Now attach each data point to the rod via a spring obeying Hooke‚Äôs law (force
is proportional to the length of the spring and therefore stored energy is proportional
to the square of the spring‚Äôs length)

============================================================

=== CHUNK 512 ===
Palavras: 380
Caracteres: 2205
--------------------------------------------------
In the E step, we keep the rod Ô¨Åxed and allow
the attachment points to slide up and down the rod so as to minimize the energy This causes each attachment point (independently) to position itself at the orthogonal
projection of the corresponding data point onto the rod In the M step, we keep the
attachment points Ô¨Åxed and then release the rod and allow it to move to the minimum
energy position The E step and M step are then repeated until a suitable convergence
criterion is satisÔ¨Åed, as is illustrated in Figure 16.10 16.3.3 EM for factor analysis
We can determine the parameters ,W, and	in a factor analysis model by Section 16.2.4
maximum likelihood The solution for is again given by the sample mean How-
ever, unlike probabilistic PCA, there is no longer a closed-form maximum likelihood
solution for W, which must therefore be found iteratively Because factor analysis is
a latent-variable model, this can be done using an EM algorithm (Rubin and Thayer, Exercise 16.24
1982) that is analogous to the one used for probabilistic PCA SpeciÔ¨Åcally, the E-step
equations are given by
E[zn] = GWT	‚àí1(xn‚àíx
) (16.72)
E[znzT
n] = G+E[zn]E[zn]T(16.73)
where we have deÔ¨Åned
G= (I+WT	‚àí1W)‚àí1: (16.74)
16.3 Evidence Lower Bound 521
(a)
‚àí2 0 2‚àí202
(b)
‚àí2 0 2‚àí202
(c)
‚àí2 0 2‚àí202
(d)
‚àí2 0 2‚àí202
(e)
‚àí2 0 2‚àí202
(f)
‚àí2 0 2‚àí202
Figure 16.10 Synthetic data illustrating the EM algorithm for PCA deÔ¨Åned by (16.70) and (16.71) (a) A set
of data points shown in green, together with the true principal components (shown as eigenvectors scaled by
the square roots of the eigenvalues) (b) Initial conÔ¨Åguration of the principal subspace deÔ¨Åned by W, shown in
red, together with the projections of the latent points Zinto the data space, given by ZWT, shown in cyan (c)
After one M step, Whas been updated with Zheld Ô¨Åxed (d) After the successive E step, the values of Zhave
been updated, giving orthogonal projections, with Wheld Ô¨Åxed (e) After the second M step (f) The converged
solution Note that this is expressed in a form that involves inversion of matrices of size M√óM
rather thanD√óD(except for the D√óDdiagonal matrix 	whose inverse is trivial
to compute inO(D)steps), which is convenient because often M/lessmuchD

============================================================

=== CHUNK 513 ===
Palavras: 365
Caracteres: 2392
--------------------------------------------------
Similarly,
the M-step equations take the form Exercise 16.25
Wnew =/bracketleftBiggN/summationdisplay
n=1(xn‚àíx)E[zn]T/bracketrightBigg/bracketleftBiggN/summationdisplay
n=1E[znzT
n]/bracketrightBigg‚àí1
(16.75)
	new = diag/braceleftBigg
S‚àíWnew1
NN/summationdisplay
n=1E[zn](xn‚àíx)T/bracerightBigg
(16.76)
where the diag operator sets all the non-diagonal elements of a matrix to zero CONTINUOUS LATENT V ARIABLES
16.4 Nonlinear
Latent Variable Models
So
far in this chapter we have focused on latent variable models based on linear trans-
formations from the latent space to the data space It is natural to ask whether we
can use the Ô¨Çexibility of deep neural networks to represent more complex transfor-
mations, while exploiting the learning ability of deep networks to allow the resulting
distribution to be Ô¨Åtted to a data set Consider a simple distribution over a vector
variable z, for example a Gaussian of the form
pz(z) =N(z|0;I): (16.77)
Now suppose we transform zusing a function x=g(z;w)given by a deep neu-
ral network, where wrepresents the weights and biases The combination of the
distribution over ztogether with the neural network deÔ¨Ånes a distribution over x Sampling from such a model is straightforward because we can generate samples
frompz(z)and then transform each of them using the neural network function to
give corresponding samples of x This is an efÔ¨Åcient process since it does not in-
volve iteration To learn g(z;w)from data, consider how to evaluate the likelihood function
p(x|w ) The distribution of xis given by the change of variables formula for densi-
ties: Section 2.4
px(x) =pz(z(x))|detJ(x)| (16.78)
where Jis the Jacobian matrix of partial derivatives whose elements are given by
Jij(x) =@zi
@
xj: (16.79)
To evaluate the distribution pz(z(x)) on the right-hand side of (16.78) for a given
data vector xand to evaluate the Jacobian matrix in (16.79) for that same value of
x, we need the inverse z=g‚àí1(x;w)of the neural network function For most
neural networks this inverse will not be well deÔ¨Åned For example, the network may
represent a many-to-one function in which multiple different input values map to
the same output value, in which case the change of variable formula does not give a
well-deÔ¨Åned density Moreover, if the dimensionality of the latent space is different
from that of the data space then the transformation will not be invertible

============================================================

=== CHUNK 514 ===
Palavras: 370
Caracteres: 2332
--------------------------------------------------
One approach is to restrict our attention to functions g(z;w)that are invertible,
which requires that zandxhave the same dimensionality We will explore this
approach in more detail when we introduce the technique of normalizing Ô¨Çows Chapter 18
16.4.1 Nonlinear manifolds
Requiring that the latent and data spaces have the same number of dimensions
is a signiÔ¨Åcant limitation Consider the situation in which zhas dimensionality M
andxhas dimensionality D, whereM < D In this case the distribution over x
is conÔ¨Åned to a manifold, or subspace, of dimensionality M, as illustrated in Fig-
ure 16.11 Low-dimensional manifolds arise in many machine learning applications,
16.4 Nonlinear Latent Variable Models 523
Figure 16.11 Illustration of a mapping
from a two-dimensional la-
tent space z= (z1;z2)
to a three-dimensional data
space x= (x1;x2;x3)us-
ing a nonlinear function x=
g(z;w)represented by a
neural network with param-
eter vector w z1z2
x1x3
x2x
zg(
z;w)
for example when modelling the distribution of natural images Nonlinear latent- Section 6.1.4
variable models can be very useful in modelling such data because they express the
strong inductive bias that the data does not ‚ÄòÔ¨Åll‚Äô the data space but is conÔ¨Åned to a
manifold, although the shape and dimensionality of this manifold are typically not
known in advance However, one problem with this framework is that it assigns zero probability
density to any data vector that does not lie exactly on the manifold, which is a prob-
lem for gradient-based learning since the likelihood function will be zero at each of
the data points and constant for small changes in w, for any realistic data set To ad-
dress this, we follow the approach used previously with regression and classiÔ¨Åcation
problems and deÔ¨Åne a conditional distribution across the entire data space, whose pa-
rameters are given by the output of the neural network If, for example, xcomprises
a vector of continuous variables then we can choose the conditional distribution to
be a Gaussian:
p(x|z; w) =N(x|g(z;w);2I) (16.80)
in which the neural network g(z;w)has linear output-unit activation functions, and
g‚ààRD The generative model is speciÔ¨Åed by the latent distribution over zto-
gether with the conditional distribution over x, and can be represented by the simple
graphical model shown in Figure 16.12

============================================================

=== CHUNK 515 ===
Palavras: 360
Caracteres: 2353
--------------------------------------------------
Note that it is straightforward, and computationally efÔ¨Åcient, to draw indepen-
dent samples from this distribution We Ô¨Årst draw a sample from the Gaussian distri-
bution (16.77) using standard methods Next, we use this value as input to the neural Section 14.1.2
network, giving an output value g(z;w) Finally, we draw a sample from a Gaus-
sian distribution with mean g(z;w)and covariance 2I, as deÔ¨Åned by (16.80) This
three-step process can then be repeated to generate multiple independent samples The combination of a latent-variable distribution p(z) and a conditional distri-
Figure 16.12 Graphical model representing the distribution given by (16.77) and
(16.80), which together deÔ¨Åne a joint distribution p(x;z) =p(x|z)p(z).z
x
524 16 CONTINUOUS LATENT V ARIABLES
‚àí4‚àí2 0 2 4
zp(z)
(a)
zx1x2 (b)
Figure
16.13 Illustration of a nonlinear latent-variable model for a one-dimensional latent space and a two-
dimensional data space (a) The prior distribution in latent space is given by a zero-mean unit-variance Gaussian
distribution (b) The three left-most plots show examples of the Gaussian conditional distribution p(x|z )for
different values of z, whereas the right-most plot shows the marginal distribution p(x) The nonlinear function
g(z), which deÔ¨Ånes the mean of the conditional distribution, is given by g1(z) = sin(z ),g2(z) = cos(z ), and,
therefore, traces out a circle in data space The standard deviation of the conditional distribution is given by
= 0:3 [Based on Prince (2020) with permission.]
butionp(x|z) deÔ¨Ånes a marginal distribution over the data space given by
p(x) =/integraldisplay
p(z)p(x|z) d z: (16.81)
We illustrate this using a simple example involving a one-dimensional latent space
and a two-dimensional data space in Figure 16.13 16.4.2 Likelihood function
We have seen that it is easy to draw samples from this nonlinear latent-variable
model Now suppose we wish to Ô¨Åt the model to an observed data set by maximizing
the likelihood function The likelihood is obtained from the product and sum rules
of probability by integrating over z:
p(x|w ) =/integraldisplay
p(x|z; w)p(z) d z
=/integraldisplay
N(x|g(z;w);2I)N(z|0;I) dz: (16.82)
Although both distributions inside the integral are Gaussian, the integral is analyti-
cally intractable due to the highly nonlinear function g(z;w)deÔ¨Åned by the neural
network

============================================================

=== CHUNK 516 ===
Palavras: 355
Caracteres: 2099
--------------------------------------------------
Nonlinear Latent Variable Models 525
Figure 16.14 Three example images
of handwritten digits, illustrating why
sampling from the latent space to evalu-
ate the likelihood function requires large
numbers of samples (a) shows the
original image, (b) shows a corrupted
image with part of the stroke removed,
and (c) shows the original image shifted
by half a pixel down and half a pixel to
the right Image (b) is closer to (a) in
terms of likelihood, even though image
(c) is much closer to (a) in appearance [From Doersch (2016) with permission.]
One
approach for evaluating the likelihood function would be to draw samples
from the latent space distribution and use these to approximate (16.82) by
p(x|w )/similarequal1
KK/summationdisplay
i
=1p(x|zi;w) (16.83)
where zi‚àºp(z) This expresses the distribution over zas a mixture of Gaussians
with Ô¨Åxed mixing coefÔ¨Åcients given by 1=K , and in the limit of an inÔ¨Ånite number of
samples, this gives the true likelihood function However, the value of Kneeded for
effective training will typically be far too high to be practical To see why, consider
the three images of handwritten digits shown in Figure 16.14, and suppose that image
(a) represents the vector xfor which we wish to evaluate the likelihood function If
a trained model generated image (b), we would consider this a poor model as this
image is not a good representation of a digit ‚Äò2‚Äô, and so this should be assigned
a much lower likelihood Conversely, image (c), which was obtained by shifting
the digit in (a) down and to the right by half a pixel, is a good example of a digit
‚Äò2‚Äô and should therefore have a high likelihood Since the distribution (16.80) is
Gaussian, the likelihood function is proportional to the exponential of the negative
squared distance between the output of the network and the data vector x However,
the squared distance between (a) and (b) is 0:0387 whereas the squared distance
between (a) and (c) is 0:2693 So if the variance parameter 2is set to a sufÔ¨Åciently
small value that image (b) has low likelihood, then image (c) will have an even lower
likelihood

============================================================

=== CHUNK 517 ===
Palavras: 370
Caracteres: 2521
--------------------------------------------------
Even if the model is good at generating digits, we would have to consider
extremely large numbers of samples for zbefore seeing a digit that is sufÔ¨Åciently
close to (a) We therefore seek more sophisticated techniques for training nonlinear
latent variable models that can be used in practical applications Before outlining
such methods, we Ô¨Årst discuss brieÔ¨Çy some considerations regarding discrete data
spaces CONTINUOUS LATENT V ARIABLES
Figure 16.15 Schematic illustration of de-
quantization, showing (a)
a discrete distribution over
a single variable and (b)
an associated dequantized
continuous distribution 0 1 2
(a)
0 1 2 (b)
16.4.3
Discrete data
If the observed data set comprises independent binary variables then we can use
a conditional distribution of the form
p(x|z; w) =D/productdisplay
i=1gi(z;w)xi(1‚àígi(z;w))1‚àíxi(16.84)
wheregi(z;w) =(ai(z;w))represents the activation of output unit i, the activa-
tion function (¬∑)is given by the logistic sigmoid, and ai(z;w)is the pre-activation
for output unit i Similarly, for one-hot encoded categorical variables, we can use a
multinomial distribution:
p(x|z; w) =D/productdisplay
i=1gi(z;w)xi(16.85)
where
gi(z;w) =exp(ai(z;w))/summationtext
jexp(
aj(z;w))(16.86)
is the softmax activation function We can also consider combinations of discrete
and continuous variables by forming the product of the associated conditional distri-
butions In practice, continuous variables are represented with discrete values, for exam-
ple in images, the red, green, and blue channel intensities might be expressed using
8-bit numbers representing the values {0; :::; 255} This can cause problems when
we employ highly Ô¨Çexible models based on deep neural networks, as the likelihood
function can go to zero if the density collapses onto one or more of the discrete val-
ues The problem can be resolved using a technique called dequantization, which
involves adding noise to the variables, typically drawn from a uniform distribution
over the region between successive discrete values, as shown in Figure 16.15 A
training set is dequantized by replacing each observed value with a sample drawn
randomly from the associated continuous distribution associated with that discrete
value, and this makes it less likely that the model will discover a pathological solu-
tion Exercises 527
16.4.4 Four approaches to generative modelling
We have seen that nonlinear latent-variable models based on deep neural net-
works offer a highly Ô¨Çexible framework for building generative models

============================================================

=== CHUNK 518 ===
Palavras: 356
Caracteres: 2243
--------------------------------------------------
Due to the
universality of the neural network transformation, such models are capable, in prin-
ciple, of approximating essentially any desired distribution to high accuracy More-
over, such models offer the potential, once trained, to generate samples from the
distribution in using an efÔ¨Åcient, non-iterative process However, we have also iden-
tiÔ¨Åed some challenges associated with training such models that force us to develop
more sophisticated techniques than those needed for linear models Many such meth-
ods have been proposed, each having their own strengths and limitations These can
be broadly grouped into four approaches, as follows With generative adversarial networks, or GANs, we relax the requirement for Chapter 17
the network mapping to be invertible, thereby allowing the latent space to have a
lower dimensionality than the data space We also abandon the concept of a likeli-
hood function and instead introduce a second neural network whose function is to
provide a training signal for the generative network Due to the absence of a well-
deÔ¨Åned likelihood function, the training procedure may be brittle, but once trained it
is straightforward to generate samples from the model, and the results can be of high
quality The framework of variational autoencoders, or V AEs, also uses a second neural Chapter 19
network whose role is to approximate the posterior distribution over the latent vari-
ables, thereby allowing an approximation to the likelihood function to be evaluated Training is more robust than with GANs, and sampling from the trained model is
straightforward, although it can be harder to obtain the highest quality results Innormalizing Ô¨Çows, we set the dimensionality of the latent space to be equal Chapter 18
to that of the data space and then modify the generative neural network so that it
becomes invertible The requirement that the network is invertible restricts its func-
tional form but it allows the likelihood function to be evaluated without approxima-
tion and it also allows for efÔ¨Åcient sampling Finally, diffusion models use a network that learns to transform a sample from Chapter 20
the prior distribution into a sample from the data distribution through a sequence of
denoising steps

============================================================

=== CHUNK 519 ===
Palavras: 393
Caracteres: 2564
--------------------------------------------------
This leads to state-of-the-art performance in many applications, al-
though the cost of sampling can be high due to the multiple denoising passes through
the network We explore these approaches in detail in the Ô¨Ånal four chapters of this book Exercises
16.1 (??) In this exercise, we use proof by induction to show that the linear projection
onto anM-dimensional subspace that maximizes the variance of the projected data
is deÔ¨Åned by the Meigenvectors of the data covariance matrix S, given by (16.3),
corresponding to the Mlargest eigenvalues In Section 16.1, this result was proven
forM= 1 Now suppose the result holds for some general value of Mand show that
it consequently holds for dimensionality M+ 1 To do this, Ô¨Årst set the derivative
of the variance of the projected data with respect to a vector uM+1deÔ¨Åning the new
528 16 CONTINUOUS LATENT V ARIABLES
direction in data space equal to zero This should be done subject to the constraints
thatuM+1are orthogonal to the existing vectors u1;:::;uM, and also that it is
normalized to unit length Use Lagrange multipliers to enforce these constraints Appendix C
Then make use of the orthonormality properties of the vectors u1;:::;uMto show
that the new vector uM+1is an eigenvector of S Finally, show that the variance is
maximized if the eigenvector is chosen to be the one corresponding to eigenvalue
M+1where the eigenvalues have been ordered in decreasing value 16.2 (??) Show that the minimum value of the PCA error measure Jgiven by (16.15)
with respect to the ui, subject to the orthonormality constraints (16.7), is obtained
when the uiare eigenvectors of the data covariance matrix S To do this, introduce a
matrix Hof Lagrange multipliers, one for each constraint, so that the modiÔ¨Åed error
measure, in matrix notation reads
/tildewideJ=Tr/braceleftBig
/hatwideUTS/hatwideU/bracerightBig
+Tr/braceleftBig
H(I‚àí/hatwideUT/hatwideU)/bracerightBig
(16.87)
where/hatwideUis a matrix of dimension D√ó(D‚àíM)whose columns are given by ui Now minimize /tildewideJwith respect to /hatwideUand show that the solution satisÔ¨Åes S/hatwideU=/hatwideUH Clearly, one possible solution is that the columns of /hatwideUare eigenvectors of S, in
which case His a diagonal matrix containing the corresponding eigenvalues To
obtain the general solution, show that Hcan be assumed to be a symmetric matrix,
and by using its eigenvector expansion, show that the general solution to S/hatwideU=/hatwideUH
gives the same value for /tildewideJas the speciÔ¨Åc solution in which the columns of /hatwideUare
the eigenvectors of S

============================================================

=== CHUNK 520 ===
Palavras: 367
Caracteres: 2356
--------------------------------------------------
Because these solutions are all equivalent, it is convenient to
choose the eigenvector solution 16.3 (?)Verify that the eigenvectors deÔ¨Åned by (16.30) are normalized to unit length,
assuming that the eigenvectors vihave unit length 16.4 (?)Suppose we replace the zero-mean, unit-covariance latent space distribution (16.31)
in the probabilistic PCA model by a general Gaussian distribution of the form N(z|m; ) By redeÔ¨Åning the parameters of the model, show that this leads to an identical model
for the marginal distribution p(x) over the observed variables for any valid choice of
mand 16.5 (??) Letxbe aD-dimensional random variable having a Gaussian distribution given
byN(x|; ), and consider the M-dimensional random variable given by y=
Ax+bwhere Ais anM√óDmatrix Show that yalso has a Gaussian distribution,
and Ô¨Ånd expressions for its mean and covariance Discuss the form of this Gaussian
distribution for M <D , forM=D, and forM >D 16.6 (??) By making use of the results (2.122) and (2.123) for the mean and covariance
of a general distribution, derive the result (16.35) for the marginal distribution p(x)
in the probabilistic PCA model 16.7 (?)Draw a directed probabilistic graph for the probabilistic PCA model described in
Section 16.2 in which the components of the observed variable xare shown explicitly
Exercises 529
as separate nodes Hence, verify that the probabilistic PCA model has the same
independence structure as the naive Bayes model discussed in Section 11.2.3 16.8 (??) By making use of the result (3.100), show that the posterior distribution p(z|x)
for the probabilistic PCA model is given by (16.43) 16.9 (?)Verify that maximizing the log likelihood (16.44) for the probabilistic PCA model
with respect to the parameter gives the result ML=xwhere xis the mean of the
data vectors 16.10 (??) By evaluating the second derivatives of the log likelihood function (16.44) for
the probabilistic PCA model with respect to the parameter , show that the stationary
pointML=xrepresents the unique maximum 16.11 (??) Show that in the limit 2‚Üí0, the posterior mean for the probabilistic PCA
model becomes an orthogonal projection onto the principal subspace, as in conven-
tional PCA 16.12 (??) For2>0show that the posterior mean in the probabilistic PCA model is
shifted towards the origin relative to the orthogonal projection

============================================================

=== CHUNK 521 ===
Palavras: 353
Caracteres: 2291
--------------------------------------------------
16.13 (??) Show that the optimal reconstruction of a data point under probabilistic PCA,
according to the least-squares projection cost of conventional PCA, is given by
/tildewidex=WML(WT
MLWML)‚àí1ME[z|x]: (16.88)
16.14 (?)The number of independent parameters in the covariance matrix for a probabilis-
tic PCA model with an M-dimensional latent space and a D-dimensional data space
is given by (16.52) Verify that for M=D‚àí1, the number of independent param-
eters is the same as in a general covariance Gaussian, whereas for M= 0 it is the
same as for a Gaussian with an isotropic covariance 16.15 (?)Derive an expression for the number of independent parameters in the factor
analysis model described in Section 16.2.4 16.16 (??) Show that the factor analysis model described in Section 16.2.4 is invariant
under rotations of the latent space coordinates 16.17 (??) Consider a linear-Gaussian latent-variable model having a latent space distri-
butionp(z) =N(x|0;I)and a conditional distribution for the observed variable
p(x|z) =N(x|Wz +;)where is an arbitrary symmetric positive-deÔ¨Ånite
noise covariance matrix Now suppose that we make a non-singular linear transfor-
mation of the data variables x‚ÜíAx, where Ais aD√óDmatrix IfML,WML,
andMLrepresent the maximum likelihood solution corresponding to the original
un-transformed data, show that AML,AW ML, andA MLATrepresent the corre-
sponding maximum likelihood solution for the transformed data set Finally, show
that the form of the model is preserved in two cases: (i) Ais a diagonal matrix
andis a diagonal matrix This corresponds to factor analysis The transformed
remains diagonal, and hence factor analysis is covariant under component-wise
530 16 CONTINUOUS LATENT V ARIABLES
re-scaling of the data variables; (ii) Ais orthogonal and is proportional to the unit
matrix so that =2I This corresponds to probabilistic PCA The transformed 
matrix remains proportional to the unit matrix, and hence probabilistic PCA is co-
variant under a rotation of the axes of the data space, as is the case for conventional
PCA 16.18 (?)Verify that the log likelihood function for a model with continuous latent vari-
ables can be written as the sum of two terms in the form (16.57) in which the terms
are deÔ¨Åned by (16.58) and (16.59)

============================================================

=== CHUNK 522 ===
Palavras: 368
Caracteres: 2268
--------------------------------------------------
This can be done by using the product rule of
probability in the form
p(x;z|w) =p(z|x; w)p(x|w ) (16.89)
and then substituting for p(x;z|w)in (16.58) 16.19 (?)Show that, for a set of i.i.d data, the evidence lower bound (ELBO) takes the
form (16.63) 16.20 (??) Draw a directed probabilistic graphical model representing a discrete mixture
of probabilistic PCA models in which each PCA model has its own values of W,
, and2 Now draw a modiÔ¨Åed graph in which these parameter values are shared
between the components of the mixture 16.21 (??) Derive the M-step equations (16.68) and (16.69) for the probabilistic PCA
model by maximizing the expected complete-data log likelihood function given by
(16.65) 16.22 (???) One beneÔ¨Åt of a probabilistic formulation of principal component analysis is
that it can be applied to a data set in which some of the values are missing, provided
they are missing at random Derive the EM algorithm for maximizing the likelihood
function for the probabilistic PCA model in this situation Note that the {zn}, as well
as the missing data values that are components of the vectors {xn}, are now latent
variables Show that in the special case in which all the data values are observed,
this reduces to the EM algorithm for probabilistic PCA derived in Section 16.3.2 16.23 (??) LetWbe aD√óMmatrix whose columns deÔ¨Åne a linear subspace of di-
mensionality Membedded within a data space of dimensionality D, and letbe a
D-dimensional vector Given a data set {xn}wheren= 1;:::;N , we can approx-
imate the data points using a linear mapping from a set of M-dimensional vectors
{zn}, so that xnis approximated by Wzn+ The associated sum-of-squares
reconstruction cost is given by
J=N/summationdisplay
n=1/bardblxn‚àí‚àíWzn/bardbl2: (16.90)
First show that minimizing Jwith respect to leads to an analogous expression with
xnandznreplaced by zero-mean variables xn‚àíxandzn‚àíz, respectively, where x
andzdenote sample means Then show that minimizing Jwith respect to zn, where
Exercises 531
Wis kept Ô¨Åxed, gives rise to the PCA E step (16.70), and that minimizing Jwith
respect to W, where{zn}is kept Ô¨Åxed, gives rise to the PCA M step (16.71) 16.24 (??) Derive the formulae (16.72) and (16.73) for the E step of the EM algorithm for
factor analysis

============================================================

=== CHUNK 523 ===
Palavras: 375
Caracteres: 2399
--------------------------------------------------
Note that from the result of Exercise 16.26, the parameter can be
replaced by the sample mean x 16.25 (??) Write down an expression for the expected complete-data log likelihood func-
tion for the factor analysis model, and hence derive the corresponding M-step equa-
tions (16.75) and (16.76) 16.26 (??) By considering second derivatives, show that the only stationary point of the
log likelihood function for the factor analysis model discussed in Section 16.2.4
with respect to the parameter is given by the sample mean deÔ¨Åned by (16.1) Furthermore, show that this stationary point is a maximum 17
Generative
Adversarial
Networks
Generative models use machine learning algorithms to learn a distribution from a set
of training data and then generate new examples from that distribution For example,
a generative model might be trained on images of animals and then used to generate
new images of animals We can think of such a generative model in terms of a
distribution p(x|w)in which xis a vector in the data space, and wrepresent the
learnable parameters of the model In many cases we are interested in conditional
generative models of the form p(x|c;w)where crepresents a vector of conditioning
variables In the case of our generative model for animal images, we may wish to
specify that a generated image should be of a particular animal, such as a cat or a
dog, speciÔ¨Åed by the value of c For real-world applications such as image generation, the distributions are ex-
tremely complex, and consequently the introduction of deep learning has dramati-
cally improved the performance of generative models We have already encountered
533 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_17    
534 17 GENERATIVE ADVERSARIAL NETWORKS
z Generator
g(z;w)
synthetic imagesreal images
Discriminator
d(x;)t
Figure 17.1 Schematic illustration of a GAN in which a discriminator neural network d(x;)is trained
to distinguish between real samples from the training set, in this case images of kittens,
and synthetic samples produced by the generator network g(z;w) The generator aims
to maximize the error of the discriminator network by producing realistic images, whereas
the discriminator network tries to minimize the same error by becoming better at distin-
guishing real from synthetic examples

============================================================

=== CHUNK 524 ===
Palavras: 352
Caracteres: 2229
--------------------------------------------------
an important class of deep generative models when we discussed autoregressive
large language models based on transformers We have also outlined four impor- Chapter 12
tant classes of generative model based on nonlinear latent variable models, and in Section 16.4.4
this chapter we discuss the Ô¨Årst of these, called generative adversarial networks The
other three approaches will be discussed in subsequent chapters Adversarial Training
Consider a generative model based on a nonlinear transformation from a latent space
zto a data space x We introduce a latent distribution p(z), which might take the
form of a simple Gaussian
p(z) =N(z|0;I); (17.1)
along with with a nonlinear transformation x=g(z;w)deÔ¨Åned by a deep neural
network with learnable parameters wknown as the generator Together these im-
plicitly deÔ¨Åne a distribution over x, and our goal is to Ô¨Åt this distribution to a data
set of training examples {xn}wheren= 1;:::;N However, we cannot determine
wby optimizing the likelihood function because this cannot, in general, be evalu-
ated in closed form The key idea of generative adversarial networks, or GANs,
(Goodfellow et al., 2014; Ruthotto and Haber, 2021) is to introduce a second dis-
criminator network, which is trained jointly with the generator network and which
provides a training signal to update the weights of the generator This is illustrated
inFigure 17.1 Adversarial Training 535
The goal of the discriminator network is to distinguish between real examples
from the data set and synthetic, or ‚Äòfake‚Äô, examples produced by the generator net-
work, and it is trained by minimizing a conventional classiÔ¨Åcation error function Conversely, the goal of the generator network is to maximize this error by synthe-
sizing examples from the same distribution as the training set The generator and
discriminator networks are therefore working against each other, hence the term ‚Äòad-
versarial‚Äô This is an example of a zero-sum game in which any gain by one network
represents a loss to the other It allows the discriminator network to provide a training
signal, which can be used to train the generator network, and this turns the unsuper-
vised density modelling problem into a form of supervised learning

============================================================

=== CHUNK 525 ===
Palavras: 351
Caracteres: 2269
--------------------------------------------------
17.1.1 Loss function
To make this precise, we deÔ¨Åne a binary target variable given by
t= 1; real data; (17.2)
t= 0; synthetic data: (17.3)
The discriminator network has a single output unit with a logistic-sigmoid activation
function, whose output represents the probability that a data vector xis real:
P(t= 1) =d(x;): (17.4)
We train the discriminator network using the standard cross-entropy error function,
which takes the form
E(w;) =‚àí1
NN/summationdisplay
n=1{tnlndn+ (1‚àítn) ln(1‚àídn)} (17.5)
wheredn=d(xn;)is the output of the discriminator network for input vector
n, and we have normalized by the total number of data points The training set Section 1.2.4
comprises both real data examples denoted xnand synthetic examples given by the
output of the generator network g(zn;w)where znis a random sample from the
latent space distribution p(z) Since tn= 1 for real examples and tn= 0 for
synthetic examples, we can write the error function (17.5) in the form
EGAN(w;) =‚àí1
Nreal/summationdisplay
n‚ààreallnd(xn;)
‚àí1
Nsynth/summationdisplay
n‚ààsynthln(1‚àíd(g(zn;w);)) (17.6)
where typically the number Nrealof real data points is equal to the number Nsynth
of synthetic data points This combination of generator and discriminator networks
can be trained end-to-end using stochastic gradient descent with gradients evalu- Chapter 7
ated using backpropagation However, the unusual aspect is the adversarial training
whereby the error is minimized with respect to butmaximized with respect to w GENERATIVE ADVERSARIAL NETWORKS
This maximization can be done using standard gradient-based methods with the sign
of the gradient reversed, so that the parameter updates become
‚àÜ=‚àí‚àáEn(w;) (17.7)
‚àÜw=‚àáwEn(w;) (17.8)
whereEn(w;)denotes the error deÔ¨Åned for data point nor more generally for
a mini-batch of data points Note that the two terms in (17.7) and (17.8) have dif-
ferent signs since the discriminator is trained to decrease the error rate whereas the
generator is trained to increase it In practice, training alternates between updating
the parameters of the generative network and updating those of the discriminative
network, in each case taking just one gradient descent step using a mini-batch, af-
ter which a new set of synthetic samples is generated

============================================================

=== CHUNK 526 ===
Palavras: 365
Caracteres: 2255
--------------------------------------------------
If the generator succeeds in
Ô¨Ånding a perfect solution, then the discriminator network will be unable to tell the
difference between the real and synthetic data and hence will always produce an out-
put of 0:5 Once the GAN is trained, the discriminator network is discarded and the
generator network can be used to synthesize new examples in the data space by sam-
pling from the latent space and propagating those samples through the trained gen-
erator network We can show that for generative and discriminative networks having
unlimited Ô¨Çexibility, a fully optimized GAN will have a generative distribution that
matches the data distribution exactly Some impressive examples of synthetic face Exercise 17.1
images generated by a GAN are shown in Figure 1.3 The GAN model discussed so far generates samples from the unconditional dis-
tributionp(x) For example, it could generate synthetic images of dogs if it is trained
on dog images We can also create conditional GANs (Mirza and Osindero, 2014),
which sample from a conditional distribution p(x|c) in which the conditioning vec-
torcmight, for example, represent different species of dog To do this, both the
generator and the discriminator network take cas an additional input, and labelled
examples of images, comprising pairs {xn;cn}, are used for training Once the GAN
has been trained, images from a desired class can be generated by setting cto the
corresponding class vector Compared to training separate GANs for each class, this
has the advantage that shared internal representations can be learned jointly across
all classes, thereby making more efÔ¨Åcient use of the data 17.1.2 GAN training in practice
Although GANs can produce high quality results, they are not easy to train suc-
cessfully due to the adversarial learning Also, unlike standard error function min- Exercise 17.2
imization, there is no metric of progress because the objective can go up as well as
down during training One challenge that can arise is called mode collapse, in which the generator net-
work weights adapt during training such that all latent-variable samples zare mapped
to a subset of possible valid outputs In extreme cases the output can correspond to
just one, or a small number, of the output values x

============================================================

=== CHUNK 527 ===
Palavras: 377
Caracteres: 2450
--------------------------------------------------
The discriminator then assigns
the value 0:5to these instances, and training ceases For example, a GAN trained on
handwritten digits might learn to generate only examples of the digit ‚Äò3‚Äô, and while
17.1 Adversarial Training 537
xpData(x)
pG(x)
d(x)
/tildewided(x)
Figure
17.2 Conceptual illustration of why it can be difÔ¨Åcult to train GANs, showing a simple one-
dimensional data space xwith the Ô¨Åxed, but unknown, data distribution pData(x)and the
initial generative distribution pG(x) The optimal discriminator function d(x)has virtually
zero gradient in the vicinity of either the training or synthetic data points, making learn-
ing very slow A smoothed version ed(x) of the discriminator function can lead to faster
learning the discriminator is unable to distinguish these from genuine examples of the digit
‚Äò3‚Äô, it fails to recognize that the generator is not generating the full range of digits Insight into the difÔ¨Åculty of training GANs can be obtained by considering Fig-
ure 17.2, which shows a simple one¬ß-dimensional data space xwith samples{xn}
drawn from the Ô¨Åxed, but unknown, data distribution pData(x) Also shown is the
initial generative distribution pG(x)together with samples drawn from this distri-
bution Because the data and generative distributions are so different, the optimal
discriminator function d(x) is easy to learn and has a very steep fall-off with virtu-
ally zero gradient in the vicinity of either the real or synthetic samples Consider
the second term in the GAN error function (17.6) Because d(g(z;w);)is equal
to zero across the region spanned by the generated samples, small changes in the
parameters wof the generative network produce very little change in the output of
the discriminator and so the gradients are small and learning proceeds slowly This can be addressed by using a smoothed version /tildewided(x) of the discriminator
function, illustrated in Figure 17.2, thereby providing a stronger gradient to drive
the training of the generator network The least-squares GAN (Mao et al., 2016)
achieves smoothing by modifying the discriminator to produce a real-valued output
rather than a probability in the range (0;1)and by replacing the cross-entropy error
function with a sum-of-squares error function Alternatively, the technique of in-
stance noise (S√∏nderby et al., 2016) adds Gaussian noise to both the real data and
the synthetic samples, again leading to a smoother discriminator function

============================================================

=== CHUNK 528 ===
Palavras: 375
Caracteres: 2497
--------------------------------------------------
Numerous other modiÔ¨Åcations to the GAN error function and training procedure
have been proposed to improve training (Mescheder, Geiger, and Nowozin, 2018) One change that is often used is to replace the generative network term in the original
error function
538 17 GENERATIVE ADVERSARIAL NETWORKS
Figure 17.3 Plots of‚àíln(d) andln(1‚àíd)showing the very dif-
ferent behaviour of the gradients close to d= 0and
d= 1 0 1
x‚àí202‚àíln(d)
ln(1‚àíd)
‚àí1
Nsyn
th/summationdisplay
n‚ààsynthln(1‚àíd(g(zn;w);)) (17.9)
with the modiÔ¨Åed form
1
Nsyn
th/summationdisplay
n‚ààsynthlnd(g(zn;w);): (17.10)
Although the Ô¨Årst form minimizes the probability that the image is fake, the second
version maximizes the probability that the image is real The different properties
of these two forms can be understood from Figure 17.3 When the generative dis-
tributionpG(x)is very different from the true data distribution pData(x), the quan-
tityd(g(z;w))is close to zero, and hence the Ô¨Årst form has a very small gradient,
whereas the second form has a large gradient, leading to faster training A more direct way to ensure that the generator distribution pG(x)moves towards
the data distribution pdata(x)is to modify the error criterion to reÔ¨Çect how far apart
the two distributions are in data space This can be measured using the Wasserstein
distance, also known as the earth mover‚Äôs distance Imagine the distribution pG(x)
as a pile of earth that is transported in small increments to construct the distribution
pdata(x) The Wasserstein metric is the total amount of earth moved multiplied by
the mean distance moved Of the many ways of rearranging the pile of earth to build
pdata(x), the one that yields the smallest mean distance is the one used to deÔ¨Åne
the metric In practice, this cannot be implemented directly, and it is approximated
by using a discriminator network that has real-valued outputs and then limiting the
gradient‚àáxd(x;)of the discriminator function with respect to xby using weight
clipping, giving rise to the Wasserstein GAN (Arjovsky, Chintala, and Bottou, 2017) An improved approach is to introduce a penalty on the gradient, giving rise to the
gradient penalty Wasserstein GAN (Gulrajani et al., 2017) whose error function is
given by
EWGAN-GP (w;) =‚àí1
Nreal/summationdisplay
n
‚ààreal/bracketleftBig
lnd(xn;)‚àí/parenleftbig
/bardbl‚àáxnd(xn;)/bardbl2‚àí1/parenrightbig2/bracketrightBig
+1
Nsyn
th/summationdisplay
n‚ààsynthlnd(g(zn;w;)) (17.11)
wherecontrols the relative importance of the penalty term

============================================================

=== CHUNK 529 ===
Palavras: 379
Caracteres: 2565
--------------------------------------------------
Image GANs
The basic concept of the GAN has given rise to a huge research literature, with many
algorithmic developments and numerous applications One of the most widespread
and successful application areas for GANs is the generation of images Early GAN
models used fully connected networks for the generator and discriminator How-
ever, there are many beneÔ¨Åts to using convolutional networks, especially for images Chapter 10
of higher resolution The discriminator network takes an image as input and pro-
vides a scalar probability as output, so a standard convolutional network is appro-
priate The generator network needs to map a lower-dimensional latent space into a
high-resolution image, and so a network based on transpose convolutions is used, as Section 10.5.3
illustrated in Figure 17.4 High quality images can be obtained by progressively growing both the gener-
ator network and the discriminator network starting from a low resolution and then
successively adding new layers that model increasingly Ô¨Åne details as training pro-
gresses (Karras et al., 2017) This speeds up the training and permits the synthesis of
high-resolution images of size 1024√ó1024 starting from images of size 4√ó4 As an
example of the scale and complexity of some GAN architectures, consider the GAN
model for class-conditional image generation called BigGAN, whose architecture is
shown in Figure 17.5 17.2.1 CycleGAN
As an example of the broad variety of GANs we consider an architecture called
aCycleGAN (Zhu et al., 2017) This also illustrates how techniques in deep learning
can be adapted to solve different kinds of problems beyond traditional tasks such as
z
100
project and
reshapeconv 1conv 2conv 3conv 4
4√ó4√ó1024
8√ó8√ó512
16√ó16√ó256
32√ó32√ó128
64√ó64√ó3
Figure 17.4 Example architecture of a deep convolutional GAN showing the use of transpose convolutions to
expand the dimensionality in successive blocks of the network 540 17.GENERATIVE ADVERSARIAL NETWORKS
zSplitLinearResidual Block ConcatResidual Block ConcatNon-LocalResidual Block Concatx
c
(a)Batch NormReLUReLUUp-sampleConvolutionBatch NormReLUConvolution
LinearLinear
Up-sampleConvolutionAdd
(b)
Figure 17.5 (a) Architecture of the generative network in the BigGAN model, which has over 70 million param-
eters (b) Details of each of the residual blocks in the generative network The discriminative network, which has
88 million parameters, has a somewhat analogous structure except that it uses average pooling layers to reduce
the dimensionality, instead of using up-sampling to increase the dimensionality

============================================================

=== CHUNK 530 ===
Palavras: 377
Caracteres: 2458
--------------------------------------------------
[Based on Brock, Donahue, and
Simonyan (2018).]
classiÔ¨Åcation and density estimation Consider the problem of turning a photograph
into a Monet painting of the same scene, or vice versa In Figure 17.6 we show
examples of image pairs from a trained CycleGAN that has learned to perform such
an image-to-image translation The aim is to learn two bijective (one-to-one) mappings, one that goes from the
domainXof photographs to the domain Yof Monet paintings and one in the reverse
direction To achieve this, CycleGAN makes use of two conditional generators, gX
andgY, and two discriminators, dXanddY The generator gX(y;wX)takes as
input a sample painting y‚ààYand generates a corresponding synthetic photograph,
whereas the discriminator dX(x;X)distinguishes between synthetic and real pho-
tographs Similarly, the generator gY(x;wY)takes a photograph x‚ààXas input
and generates a synthetic painting y, and the discriminator dY(y;Y)distinguishes
between synthetic paintings and real ones The discriminator dXis therefore trained
on a combination of synthetic photographs generated by gXand real photographs,
whereasdYis trained on a combination of synthetic paintings generated by gYand
17.2 Image GANs 541
Figure 17.6 Examples of image translation
using a CycleGAN showing the synthesis of a
photographic-style image from a Monet paint-
ing (top row) and the synthesis of an image
in the style of a Monet painting from a photo-
graph (bottom row) (2017)
with permission.]
real paintings If we train this architecture using the standard GAN loss function, it would learn
to generate realistic synthetic Monet paintings and realistic synthetic photographs,
but there would be nothing to force a generated painting to look anything like the
corresponding photograph, or vice versa We therefore introduce an additional term
in the loss function called the cycle consistency error, containing two terms, whose
construction is illustrated in Figure 17.7 The goal is to ensure that when a photograph is translated into a painting and
then back into a photograph it should be close to the original photograph, thereby
ensuring that the generated painting retains sufÔ¨Åcient information about the photo-
graph to allow the photograph to be reconstructed Similarly, when a painting is
translated into a photograph and then back into a painting it should be close to the
Figure 17.7 Diagram showing how the cycle
consistency error is calculated for an example
photograph xn

============================================================

=== CHUNK 531 ===
Palavras: 356
Caracteres: 2249
--------------------------------------------------
The photograph is Ô¨Årst mapped
into the painting domain using the generator gY,
and the resulting vector is then mapped back
into the photograph domain using the genera-
torgX The discrepancy between the resulting
photograph and the original xndeÔ¨Ånes a contri-
bution to the cycle consistency error An analo-
gous process is used to calculate the contribu-
tion to the cycle consistency error from a paint-
ingynby mapping it to a photograph using gX
and then back to a painting using gY.X
photographsY
paintingsxn
gX(gY(xn)) gY(xn)Ecyc
542 17 GENERATIVE ADVERSARIAL NETWORKS
xn gYdY EGAN
gX Ecycyn
yn gX
dX EGANgY Ecyc
xn
Figure
17.8 Flow of information through a CycleGAN The total error for the data points xnandynis the sum
of the four component errors Applying this to all the photographs and paintings in the training
set then gives a cycle consistency error of the form
Ecyc(wX;wY) =1
NX/summationdisplay
n
‚ààX/bardblgX(gY(xn))‚àíxn/bardbl1
+1
NY/summationdisplay
n
‚ààY/bardblgY(gX(yn))‚àíyn/bardbl1 (17.12)
where/bardbl¬∑/bardbl 1denotes the L1 norm The cycle consistency error is added to the usual
GAN loss functions deÔ¨Åned by (17.6) to give a total error function:
EGAN(wX;X) +EGAN(wY;Y) +Ecyc(wX;wY) (17.13)
where the coefÔ¨Åcient determines the relative importance of the GAN errors and the
cycle consistency error Information Ô¨Çow through the CycleGAN when calculating
the error function for one image and one painting is shown in Figure 17.8 We have seen that GANs can perform well as generative models, but they can
also be used for representation learning in which rich statistical structure in a data Section 6.3.3
set is revealed through unsupervised learning When the deep convolutional GAN
shown in Figure 17.4 is trained on a data set of bedroom images (Radford, Metz, and
Chintala, 2015) and random samples from the latent space are propagated through
the trained network, the generated images also look like bedrooms, as expected In
addition, however, the latent space has become organized in ways that are semanti-
cally meaningful For example, if we follow a smooth trajectory through the latent
space and generate the corresponding series of images, we obtain smooth transitions
from one image to the next, as seen in Figure 17.9

============================================================

=== CHUNK 532 ===
Palavras: 355
Caracteres: 2224
--------------------------------------------------
Moreover, it is possible to identify directions in latent space that correspond
to semantically meaningful transformations For example, for faces, one direction
might correspond to changes in the orientation of the face, whereas other directions
might correspond to changes in lighting or the degree to which the face is smiling or
not These are called disentangled representations and allow new images to be syn-
thesized having speciÔ¨Åed properties Figure 17.10 is an example from a GAN trained
on face images, showing that semantic attributes such as gender or the presence of
glasses correspond to particular directions in latent space Image GANs 543
Figure 17.9 Samples generated by a deep convolutional GAN trained on images of bedrooms Each row is
generated by taking a smooth walk through latent space between randomly generated locations We see smooth
transitions, with each image plausibly looking like a bedroom In the bottom row, for example, we see a TV on
the wall gradually morph into a window [From Radford, Metz, and Chintala (2015) with permission.]
Figure 17.10 An example of vector
arithmetic in the latent space of a
trained GAN In each of the three
columns, the latent space vectors that
generated these images are averaged
and then vector arithmetic is applied
to the resulting mean vectors to cre-
ate a new vector corresponding to the
central image in the 3√ó3array on the
right Adding noise to this vector gen-
erates another eight sample images The four images on the bottom row
show that the same arithmetic applied
directly in data space simply results
in a blurred image due to misalign-
ment [From Radford, Metz, and Chin-
tala (2015) with permission.]

544 17 GENERATIVE ADVERSARIAL NETWORKS
Exercises
17.1 (???) We would like the GAN error function (17.6) to have the property that, given
sufÔ¨Åciently Ô¨Çexible neural networks, the stationary point is obtained when the gen-
erator distribution matches the true data distribution In this exercise we prove this
result for network models with inÔ¨Ånite Ô¨Çexibility by optimizing over the full space
of probability distributions pG(x)and over the full space of functions d(x)corre-
sponding to the generative and discriminative networks, respectively

============================================================

=== CHUNK 533 ===
Palavras: 377
Caracteres: 2650
--------------------------------------------------
SpeciÔ¨Åcally,
we assume that the discriminative model is optimized in an inner loop, giving rise to
an effective outer loop error function for the generative model First, show that, in
the limit of an inÔ¨Ånite number of data samples, the GAN error function (17.6) can be
rewritten in the form
E(pG;d) =‚àí/integraldisplay
pdata(x) lnd(x) d x‚àí/integraldisplay
pG(x) ln(1‚àíd(x)) d x (17.14)
wherepdata(x)is the Ô¨Åxed distribution of real data points Now consider a varia-
tional optimization over all functions d(x) Show that, for a Ô¨Åxed generative net- Appendix B
work, the solution for the discriminator d(x) that minimizes Eis given by
d?(x) =pdata(x)
pdata(x) +pG(x): (17.15)
Hence, show that the error function Ecan be written as a function of the generator
networkpG(x)in the form
C(pG) =‚àí/integraldisplay
pdata(x) ln/braceleftbiggpdata(x)
pdata(x) +pG(x)/bracerightbigg
dx
‚àí/integraldisplay
pG(x) ln/braceleftbiggpG(x)
pdata(x) +pG(x)/bracerightbigg
dx: (17.16)
Now show that this can be rewritten in the form
C(pG) =‚àíln(4) + KL/parenleftbigg
pdata/vextenddouble/vextenddouble/vextenddouble/vextenddoublepdata+pG
2/parenrightbigg
+ KL/parenleftbigg
pG/vextenddouble/vextenddouble/vextenddouble/vextenddoublepdata+pG
2/parenrightbigg
(17.17)
where the Kullback‚ÄìLeibler divergence KL(p/bardblq )is deÔ¨Åned by (2.100) Finally, us-
ing the property that KL(p/bardblq )>0with equality if, and only if, p(x) =q(x)for Section 2.5.5
allx, show that the minimum of C(pG)occurs when pG(x) =pdata(x) Note
that the sum of the two Kullback‚ÄìLeibler divergence terms in (17.17) is known as
theJensen‚ÄìShannon divergence betweenpdataandpG Like the Kullback‚ÄìLeibler
divergence, this is a non-negative quantity that vanishes if, and only if, the two dis-
tributions are equal, but unlike the KL divergence, it is symmetric with respect to the
two distributions 17.2 (???) In this exercise we explore the problems that can arise from the adversarial
nature of GAN training Consider a cost function E(a;b) =abdeÔ¨Åned over two
parametersaandb, analogous to the parameters of a generative and discriminative
Exercises 545
network, respectively Show that the point a= 0;b= 0 is a stationary point of
the cost function By considering the second derivatives along the lines b=aand
b=‚àíashow that the point a= 0;b= 0 is a saddle point Now suppose that we
optimize this error function by taking inÔ¨Ånitesimal steps, so that the variables be-
come functions of continuous time a(t),b(t)deÔ¨Åned by a continuous-time gradient
descent, in which the parameter a(t) of the generative network is updated so as to
increaseE(a;b), whereas the parameter b(t) is updated so as to decrease E(a;b)

============================================================

=== CHUNK 534 ===
Palavras: 350
Caracteres: 2276
--------------------------------------------------
Show that the evolution of the parameters is governed by the equations
da
dt=@E
@a;db
dt=‚àí@E
@b: (17.18)
Hence, show that a(t) satisÔ¨Åes the second-order differential equation
d2a
dt2=‚àí2a(t): (17.19)
Verify that the following expression is a solution of (17.19):
a(t) =Ccos(t) +Dsin(t) (17.20)
whereCandDare arbitrary constants If the system is initialized at t= 0with the
valuesa= 1,b= 0, Ô¨Ånd the values of CandDand hence show that the resulting
values ofa(t) andb(t) trace out a circle of unit radius in a;bspace centred on the
origin, and that they therefore never converge to the saddle point 17.3 (?)Consider a GAN in which the training set consists of equal numbers of cat and
dog images and in which the generator network has learned to produce high quality
images of dogs Show that, when presented with a dog image, the optimal output for
the discriminator network (trained to generate the probability that the image is real)
is1=3 18
Normalizing
Flows
We have seen how generative adversarial networks (GANs) extend the framework Chapter 17
of linear latent-variable models by using deep neural networks to represent highly
Ô¨Çexible and learnable nonlinear transformations from the latent space to the data
space However, the likelihood function is generally either intractable, because the
network function cannot be inverted, or may not even be deÔ¨Åned if the latent space
has a lower dimensionality than the data space In GANs, a second, discriminative
network was therefore introduced to facilitate adversarial training Here we discuss the second of our four approaches to training nonlinear latent Section 16.4.4
variable models that involves restricting the form of the neural network model such
that the likelihood function can be evaluated without approximation while still en-
suring that sampling from the trained model is straightforward Suppose we deÔ¨Åne
a distribution pz(z), sometimes also called a base distribution, over a latent variable
zalong with a nonlinear function x=f(z;w), given by a deep neural network, that
547 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_18    
548 18 NORMALIZING FLOWS
transforms the latent space into the data space

============================================================

=== CHUNK 535 ===
Palavras: 363
Caracteres: 2273
--------------------------------------------------
Assuming pz(z)is a simple distribu-
tion such as a Gaussian, sampling from such a model is easy as each latent sample
z?‚àºpz(z)is simply passed through the neural network to generate a corresponding
data sample x?=f(z?;w) To calculate the likelihood function for this model, we need the data-space dis-
tribution, which depends on the inverse of the neural network function We write
this as z=g(x;w), and it satisÔ¨Åes z=g(f(z;w);w) This requires that, for every
value of w, the functions f(z;w)andg(x;w)are invertible, also called bijective, so
that each value of xcorresponds to a unique value of zand vice versa We can then
use the change of variables formula to calculate the data density: Section 2.4
px(x|w ) =pz(g(x;w))|detJ(x)| (18.1)
where J(x) is the Jacobian matrix of partial derivatives whose elements are given by
Jij(x) =@gi(x;w)
@xj(18.2)
and|¬∑|denotes the modulus or absolute value We will continue to refer to zas a ‚Äòla-
tent‚Äô variable even though the deterministic mapping means that any given data value
xcorresponds to a unique value of zwhose value is therefore no longer uncertain The mapping function f(z;w)will be deÔ¨Åned in terms of a special form of neu-
ral network, whose structure we will discuss shortly One consequence of requiring
an invertible mapping is that the dimensionality of the latent space must be the same
as that of the data space, which can lead to large models for high-dimensional data
such as images Also, in general, the cost of evaluating the determinant of a D√óD
matrix isO(D3), so we will seek to impose some further restrictions on the model
in order that evaluation of the Jacobian matrix determinant is more efÔ¨Åcient If we consider a training set D={x1;:::;xN}of independent data points, the
log likelihood function is given from (18.1) by
lnp(D|w ) =N/summationdisplay
n=1lnpx(xn|w) (18.3)
=N/summationdisplay
n=1/braceleftBig
lnpz(g(xn;w)) + ln|detJ(xn)|/bracerightBig
(18.4)
and our goal is to use the likelihood function to train the neural network To be able
to model a wide range of distributions, we want the transformation function x=
f(z;w)to be highly Ô¨Çexible, and so we use a deep neural network architecture We
can ensure that the overall function is invertible if we make each layer of the network
invertible

============================================================

=== CHUNK 536 ===
Palavras: 358
Caracteres: 2253
--------------------------------------------------
To see this, consider three successive transformations, each corresponding Exercise 18.2
to one layer, of the form:
x=fA(fB(fC(z))): (18.5)
Then the inverse function is given by
z=gC(gB(gA(x))) (18.6)
18.1 Coupling Flows 549
where gA;gB, and gCare the inverse functions of fA;fB, and fC, respectively Moreover, the determinant of the Jacobian for such a layered structure is also easy
to evaluate in terms of the Jacobian determinants for each of the individual layers by
making use of the chain rule of calculus:
Jij=@zi
@xj=/summationdisplay
k/summationdisplay
l@gC
i
@gB
k@gB
k
@gA
l@gA
l
@xj: (18.7)
We recognize the right-hand side as the product of three matrices, and the determi-
nant of a product is the product of the determinants Therefore, the log determinant Appendix A
of the overall Jacobian will be the sum of the log determinants corresponding to each
layer This approach to modelling a Ô¨Çexible distribution is called a normalizing Ô¨Çow
because the transformation of a probability distribution through a sequence of map-
pings is somewhat analogous to the Ô¨Çow of a Ô¨Çuid Also, the effect of the inverse
mapping is to transform the complex data distribution into a normalized form, typ-
ically a Gaussian or normal distribution Normalizing Ô¨Çows have been reviewed by
Kobyzev, Prince, and Brubaker (2019) and Papamakarios et al Here we
discuss the core concepts from the two main classes of normalizing Ô¨Çows used in
practice: coupling Ô¨Çows andautoregressive Ô¨Çows We also look at the use of neural
differential equations to deÔ¨Åne invertible mappings, leading to continuous Ô¨Çows Coupling Flows
Our goal is to design a single invertible function layer, so that we can compose many
of them together to deÔ¨Åne a highly Ô¨Çexible class of invertible functions Consider
Ô¨Årst a linear transformation of the form
x=az+b: (18.8)
This is easy to invert, giving
z=1
a(x‚àíb): (18.9)
However, linear transformations are closed under composition, meaning that a se-
quence of linear transformations is equivalent to a single overall linear transforma-
tion Moreover, a linear transformation of a Gaussian distribution is again Gaussian Exercise 3.6
So even if we have many such ‚Äòlayers‚Äô of linear transformation, we will only ever
have a Gaussian distribution

============================================================

=== CHUNK 537 ===
Palavras: 369
Caracteres: 2419
--------------------------------------------------
The question is whether we can retain the invertability
of a linear transformation while allowing additional Ô¨Çexibility so that the resulting
distribution can be non-Gaussian One solution to this problem is given by a form of normalizing Ô¨Çow model called
real NVP (Dinh, Krueger, and Bengio, 2014; Dinh, Sohl-Dickstein, and Bengio,
2016), which is short for ‚Äòreal-valued non-volume-preserving‚Äô The idea is to par-
tition the latent-variable vector zinto two parts z= (zA;zB), so that if zhas di-
mensionDandzAhas dimension d, then zBhas dimension D‚àíd NORMALIZING FLOWS
Figure 18.1 A single layer of the real NVP nor-
malizing Ô¨Çow model Here the network NN1
computes the function exp(s(zA;w))and the
network NN2 computes the function b(zA;w) The output vector is then deÔ¨Åned by (18.10) and
(18.11).zA
zBNN1 NN2xA
xB
‚äô +exp(
s(¬∑)) b(¬∑)
partition
the output vector x= (xA;xB)where xAhas dimension dandxBhas
dimensionD‚àíd For the Ô¨Årst part of the output vector, we simply copy the input:
xA=zA: (18.10)
The second part of the vector undergoes a linear transformation, but now the coefÔ¨Å-
cients in the linear transformation are given by nonlinear functions of zA:
xB= exp( s(zA;w))‚äôzB+b(zA;w) (18.11)
where s(zA;w)andb(zA;w)are the real-valued outputs of neural networks, and
the exponential ensures that the multiplicative term is non-negative Here ‚äôdenotes
theHadamard product involving an element-wise multiplication of the two vectors Similarly, the exponential in (18.11) is taken element-wise Note that we have shown
the same vector win both network functions In practice, these may be implemented
as separate networks with their own parameters, or as one network with two sets of
outputs Due to the use of neural network functions, the value of xBcan be a very Ô¨Çexible
function of xA Nevertheless, the overall transformation is easily invertible: given a
value for x= (xA;xB)we Ô¨Årst compute
zA=xA; (18.12)
then we evaluate s(zA;w)andb(zA;w), and Ô¨Ånally we compute zBusing
zB= exp(‚àís(zA;w))‚äô(xB‚àíb(zA;w)): (18.13)
The overall transformation is illustrated in Figure 18.1 Note that there is no re-
quirement for the individual neural network functions s(zA;w)andb(zA;w)to be
invertible Now consider the evaluation of the Jacobian deÔ¨Åned by (18.2) and its determi-
nant We can divide the Jacobian matrix into blocks, corresponding to the partition-
ing of zandx, giving
J=Ô£Æ
Ô£ØÔ£ØÔ£∞Id 0
@zB
@xAdiag
(exp(‚àís))Ô£π
Ô£∫Ô£∫Ô£ª: (18.14)
18.1

============================================================

=== CHUNK 538 ===
Palavras: 376
Caracteres: 2304
--------------------------------------------------
Coupling Flows 551
zA
zBNN1 NN2 NN3 NN4
‚äô +‚äô +
Figure
18.2 By composing two layers of the form shown in Figure 18.1, we obtain a more Ô¨Çexible, but still
invertible, nonlinear layer Each sub-layer is invertible and has an easily evaluated Jacobian, and hence the
overall double layer has the same properties The top left block corresponds to the derivatives of zAwith respect to xAand hence
from (18.12) is given by the d√ódidentity matrix The top right block corresponds to
the derivatives of zAwith respect to xBand these terms vanish, again from (18.12) The bottom left block corresponds to the derivatives of zBwith respect to xA From
(18.13), these are complicated expressions involving the neural network functions Finally, the bottom right block corresponds to the derivatives of zBwith respect to
xB, which from (18.13) are given by a diagonal matrix whose diagonal elements
are given by the exponentials of the negative elements of s(zA;w) We therefore
see that the Jacobian matrix (18.14) is a lower triangular matrix, meaning that all
elements above the leading diagonal are zero For such a matrix, the determinant
is just the product of the elements along the leading diagonal, and therefore it does Appendix A
not depend on the complicated expressions in the lower left block Consequently,
the determinant of the Jacobian is simply given by the product of the elements of
exp(‚àís(z A;w)) A clear limitation of this approach is that the value of zAis unchanged by the
transformation This is easily resolved by adding another layer in which the roles
ofzAandzBare reversed, as illustrated in Figure 18.2 This double-layer structure
can then be repeated multiple times to facilitate a very Ô¨Çexible class of generative
models The overall training procedure involves creating mini-batches of data points, in
which the contribution of each data point to the log likelihood function is obtained
from (18.4) For a latent distribution of the form N(z|0;I), the log density is simply
‚àí/bardblz/bardbl2=2up to an additive constant The inverse transformation z=g(x)is cal-
culated using a sequence of inverse transformations of the form (18.13) Similarly,
the log of the Jacobian determinant is given by a sum of log determinants for each
layer where each term is itself a sum of terms of the form ‚àísi(x;w)

============================================================

=== CHUNK 539 ===
Palavras: 359
Caracteres: 2260
--------------------------------------------------
Gradients of
the log likelihood can be evaluated using automatic differentiation, and the network
parameters updated by stochastic gradient descent The real NVP model belongs to a broad class of normalizing Ô¨Çows called cou-
pling Ô¨Çows, in which the linear transformation (18.11) is replaced by a more general
552 18 NORMALIZING FLOWS
Figure 18.3 Illustration of the real NVP
normalizing Ô¨Çow model ap-
plied to the two-moons data
set showing (a) the Gaus-
sian base distribution, (b)
the distribution after a trans-
formation of the vertical axis
only, (c) the distribution after
a subsequent transformation
of the horizontal axis, (d) the
distribution after a second
transformation of the vertical
axis, (e) the distribution af-
ter a second transformation
of the horizontal axis, and
(f) the data set on which the
model was trained (a)
(b)
(c)
(d)
(e)
(f)
form:
xB=h
(zB;g(zA;w)) (18.15)
where h(zB;g)is a function of zBthat is efÔ¨Åciently invertible for any given value of
gand is called the coupling function The function g(zA;w)is called a conditioner
and is typically represented by a neural network We can illustrate the real NVP normalizing Ô¨Çow using a simple data set, some-
times known as ‚Äòtwo moons‚Äô, as shown in Figure 18.3 Here a two-dimensional
Gaussian distribution is transformed into a more complex distribution by using two
successive layers each of which consists of alternate transformations on each of the
two dimensions A
utoregressive Flows
A
related formulation of normalizing Ô¨Çows can be motivated by noting that the joint
distribution over a set of variables can always be written as the product of conditional
distributions, one for each variable We Ô¨Årst choose an ordering of the variables in Section 11.1
18.2 Autoregressive Flows 553
Figure 18.4 Illustration of two al-
ternative structures for autoregres-
sive normalizing Ô¨Çows The masked
autoregressive Ô¨Çow shown in (a) al-
lows efÔ¨Åcient evaluation of the like-
lihood function, whereas the alter-
native inverse autoregressive Ô¨Çow
shown in (b) allows for efÔ¨Åcient
sampling.z1
z2
z3
znx1
x2
x3
xn
(a)z1
z2
z3
znx1
x2
x3
xn
(b)
the
vector x, from which we can write, without loss of generality,
p(x1;:::;xD) =D/productdisplay
i=1p(xi|x1:i‚àí1) (18.16)
where x1:i‚àí1 denotesx1;:::;xi‚àí1

============================================================

=== CHUNK 540 ===
Palavras: 362
Caracteres: 2418
--------------------------------------------------
This factorization can be used to construct a
class of normalizing Ô¨Çow called a masked autoregressive Ô¨Çow, or MAF (Papamakar-
ios, Pavlakou, and Murray, 2017), given by
xi=h(zi;gi(x1:i‚àí1;wi)) (18.17)
which is illustrated in Figure 18.4(a) Here h(zi;¬∑)is the coupling function, which
is chosen to be easily invertible with respect to zi, andgiis the conditioner, which
is typically represented by a deep neural network The term masked refers to the use
of a single neural network to implement a set of equations of the form (18.17) along
with a binary mask (Germain et al., 2015) that force a subset of the network weights
to be zero to implement the autoregressive constraint (18.16) In this case the reverse calculations needed to evaluate the likelihood function
are given by
zi=h‚àí1(xi;gi(x1:i‚àí1;wi)) (18.18)
and hence can be performed efÔ¨Åciently on modern hardware since the individual
functions in (18.18) needed to evaluate z1;:::;zDcan be evaluated in parallel The
Jacobian matrix corresponding to the set of transformations (18.18) has elements
@zi=@xj, which form an upper-triangular matrix whose determinant is given by the Exercise 18.4
product of the diagonal elements and can therefore also be evaluated efÔ¨Åciently However, sampling from this model must be done by evaluating (18.17), which is
intrinsically sequential and therefore slow because the values of x1;:::;xi‚àí1must
be evaluated before xican be computed To avoid this inefÔ¨Åcient sampling, we can instead deÔ¨Åne an inverse autoregres-
sive Ô¨Çows, or IAF (Kingma et al., 2016), given by
xi=h(zi;/tildewidegi(z1:i‚àí1;wi)) (18.19)
554 18 NORMALIZING FLOWS
as illustrated in Figure 18.4(b) Sampling is now efÔ¨Åcient since, for a given choice
ofz, the evaluation of the elements x1;:::;xDusing (18.19) can be performed in
parallel However, the inverse function, which is needed to evaluate the likelihood,
requires a series of calculations of the form
zi=h‚àí1(xi;/tildewidegi(z1:i‚àí1;wi)); (18.20)
which are intrinsically sequential and therefore slow Whether a masked autoregres-
sive Ô¨Çow or an inverse autoregressive Ô¨Çow is preferred will depend on the speciÔ¨Åc
application We see that coupling Ô¨Çows and autoregressive Ô¨Çows are closely related Al-
though autoregressive Ô¨Çows introduce considerable Ô¨Çexibility, this comes with a
computational cost that grows linearly in the dimensionality Dof the data space
due to the need for sequential ancestral sampling

============================================================

=== CHUNK 541 ===
Palavras: 404
Caracteres: 2440
--------------------------------------------------
Coupling Ô¨Çows can be viewed as
a special case of autoregressive Ô¨Çows in which some of this generality is sacriÔ¨Åced
for efÔ¨Åciency by dividing the variables into two groups instead of Dgroups Contin
uous Flows
The
Ô¨Ånal approach to normalizing Ô¨Çows that we consider in this chapter will make
use of deep neural networks deÔ¨Åned in terms of an ordinary differential equation, or
ODE This can be thought of as a deep network with an inÔ¨Ånite number of layers We Ô¨Årst introduce the concept of a neural ODE then we see how this can be applied
to the formulation of a normalizing Ô¨Çow model 18.3.1 Neural differential equations
We have seen that neural networks are especially useful when they comprise
many layers of processing, and so we can ask what happens if we explore the limit
of an inÔ¨Ånitely large number of layers Consider a residual network where each layer
of processing generates an output given by the input vector with the addition of some
parameterized nonlinear function of that input vector:
z(t+1)=z(t)+f(z(t);w) (18.21)
wheret= 1;:::;T labels the layers in the network Note that we have used the
same function at each layer, with a shared parameter vector w, because this allows
us to consider an arbitrarily large number of such layers while keeping the number
of parameters bounded Imagine that we increase the number of layers while ensur-
ing that the changes introduced at each layer become correspondingly smaller In
the limit, the hidden-unit activation vector becomes a function z(t) of a continuous
variablet, and we can express the evolution of this vector through the network as a
differential equation: Exercise 18.5
dz(t)
d
t=f(z(t); w) (18.22)
wheretis often referred to as ‚Äòtime‚Äô The formulation in (18.22) is known as a neural
ordinary differential equation orneural ODE (Chen et al., 2018) Continuous Flows 555
Figure 18.5 Comparison of a conventional layered network with a neural differential equation The di-
agram on the left corresponds to a residual network with Ô¨Åve layers and shows trajectories
for several starting values of a single scalar input The diagram on the right shows the re-
sult of numerical integration of a continuous neural ODE, again for several starting values
of the scalar input, in which we see that the function is not evaluated at uniformly-spaced
time intervals, but instead the evaluation points are chosen adaptively by the numerical
solver and depend on the choice of input value

============================================================

=== CHUNK 542 ===
Palavras: 364
Caracteres: 2293
--------------------------------------------------
(2018) with permission.]
means that there is a single variable t If we denote the input to the network by
the vector z(0), then the output z(T)is obtained by integration of the differential
equation
z(T) =/integraldisplayT
0f(z(t);w) dt: (18.23)
This integral can be evaluated using standard numerical integration packages The
simplest method for solving differential equations is Euler‚Äôs forward integration
method, which corresponds to the expression (18.21) In practice, more powerful
numerical integration algorithms can adapt their function evaluation to achieve In
particular, they can adaptively choose values of tthat typically are not uniformly
spaced The number of such evaluations replaces the concept of depth in a conven-
tional layered network A comparison of a standard layered neural network and a
neural differential equation are shown in Figure 18.5 18.3.2 Neural ODE backpropagation
We now need to address the challenge of how to train a neural ODE, that is how
to determine the value of wby optimizing a loss function Let us assume that we are
given a data set comprising values of the input vector z(0) along with an associated
output target vector and a loss function L(¬∑)that depends on the output vector z(T) One approach would be to use automatic differentiation to differentiate through all Section 8.2
of the operations performed by the ODE solver during the forward pass NORMALIZING FLOWS
this is straightforward to do, it is costly from a memory perspective and is not op-
timal in terms of controlling numerical error (2018) treat the
ODE solver as a black box and use a technique called the adjoint sensitivity method,
which can be viewed as the continuous analogue of explicit backpropagation Recall
that backpropagation involves, for each data point, three successive phases: Ô¨Årst a Chapter 8
forward propagation to evaluate the activation vectors at each layer of the network,
second the evaluation of the derivatives of the loss with respect to the activations at
each layer starting at the output and propagating backwards through the network by
exploiting the chain rule of calculus, and third the evaluation of the derivatives with
respect to network parameters by forming products of activations from the forward
pass and gradients from the backward pass

============================================================

=== CHUNK 543 ===
Palavras: 351
Caracteres: 2230
--------------------------------------------------
We will see that there are analogous
steps when computing the gradients for a neural ODE To apply backpropagation to neural ODEs, we deÔ¨Åne a quantity called the ad-
joint given by
a(t) =dL
dz(t): (18.24)
We see that a(T)corresponds to the usual derivative of the loss with respect to the
output vector The adjoint satisÔ¨Åes its own differential equation given by Exercise 18.6
da(t)
dt=‚àía(t)T‚àázf(z(t); w); (18.25)
which is a continuous version of the chain rule of calculus This can be solved by
integrating backwards starting from a(T), which again can be done using a black-
box ODE solver In principle, this requires that we have stored the trajectory z(t)
computed during the forward phase, which could be problematic as the inverse solver
might wish to evaluate z(t) at different values of tcompared to the forward solver Instead we simply allow the backwards solver to recompute any required values of
z(t) by integrating (18.22) alongside (18.25) starting with the output value z(T) The third step in the backpropagation method is to evaluate derivatives of the loss
with respect to network parameters by forming appropriate products of activations
and gradients When a parameter value is shared across multiple connections in a
network, the total derivative is formed from the sum of derivatives for each of the
connections For our neural ODE, in which the same parameter vector wis shared Exercise 9.7
throughout the network, this summation becomes an integration over t, which takes
the form Exercise 18.7
‚àáwL=‚àí/integraldisplayT
0a(t)T‚àáwf(z(t); w) dt: (18.26)
The derivatives‚àázfin (18.25) and‚àáwfin (18.26) can be evaluated efÔ¨Åciently
using automatic differentiation Note that the above results can equally be applied to Section 8.2
a more general neural network function f(z(t);t; w)that has an explicit dependence
ontin addition to the implicit dependence through z(t) One beneÔ¨Åt of neural ODEs trained using the adjoint method, compared to con-
ventional layered networks, is that there is no need to store the intermediate results
of the forward propagation, and hence the memory cost is constant Continuous Flows 557
neural ODEs can naturally handle continuous-time data in which observations occur
at arbitrary times

============================================================

=== CHUNK 544 ===
Palavras: 351
Caracteres: 2197
--------------------------------------------------
If the error function Ldepends on values of z(t) other than the
output value, then multiple runs of the reverse-model solver are required, with one
run for each consecutive pair of outputs, so that the single solution is broken down
into multiple consecutive solutions in order to access the intermediate states (Chen
et al., 2018) Note that a high level of accuracy in the solver can be used during train-
ing, with a lower accuracy, and hence fewer function evaluations, during inference
in applications for which compute resources are limited 18.3.3 Neural ODE Ô¨Çows
We can make use of a neural ordinary differential equation to deÔ¨Åne an alter-
native approach to the construction of tractable normalizing Ô¨Çow models A neural
ODE deÔ¨Ånes a highly Ô¨Çexible transformation from an input vector z(0) to an output
vector z(T)in terms of a differential equation of the form
dz(t)
d
t=f(z(t); w): (18.27)
If we deÔ¨Åne a base distribution over the input vector p(z(0)) then the neural ODE
propagates this forward through time to give a distribution p(z(t)) for each value
oft, leading to a distribution over the output vector p(z(T )) (2018)
showed that for neural ODEs, the transformation of the density can be evaluated by
integrating a differential equation given by Exercise 18.8
d lnp(z(t))
d
t=‚àíTr/parenleftbigg@f
@z
(t)/parenrightbigg
(18.28)
where@f=@zrepresents the Jacobian matrix with elements @fi=@zj This integra-
tion can be performed using standard ODE solvers Likewise, samples from this
density can be obtained by sampling from the base density p(z(0)), which is chosen
to be a simple distribution such as a Gaussian, and propagating the values to the out-
put by integrating (18.27) again using the ODE solver The resulting framework is
known as a continuous normalizing Ô¨Çow and is illustrated in Figure 18.6 Continuous Exercise 18.9
normalizing Ô¨Çows can be trained using the adjoint sensitivity method used for neural Section 18.3.1
ODEs, which can be viewed as the continuous time equivalent of backpropagation Since (18.28) involves the trace of the Jacobian rather than the determinant,
which arises in discrete normalizing Ô¨Çows, it might appear to be more computation-
ally efÔ¨Åcient

============================================================

=== CHUNK 545 ===
Palavras: 360
Caracteres: 2329
--------------------------------------------------
In general, evaluating the determinant of a D√óDmatrix requires
O(D3)operations, whereas evaluating the trace requires O(D)operations How-
ever, if the determinant is lower diagonal, as in many forms of normalizing Ô¨Çow,
then the determinant is the product of the diagonal terms and therefore also involves
O(D)operations Since evaluating the individual elements of the Jacobian matrix
requires a separate forward propagation, which itself requires O(D)operations, eval-
uating the trace or the determinant (for a lower triangular matrix) takes O(D2)op-
erations overall However, the cost of evaluating the trace can be reduced to O(D)
by using Hutchinson‚Äôs trace estimator (Grathwohl et al., 2018), which for a matrix
558 18 NORMALIZING FLOWS
Figure 18.6 Illustration of a continu-
ous normalizing Ô¨Çow showing a simple
Gaussian distribution at t= 0 that is
continuously transformed into a multi-
modal distribution at t=T The Ô¨Çow
lines show how points along the z-axis
evolve as a function of t Where the
Ô¨Çow lines spread apart the density is re-
duced, and where they move together
the density is increased p(z(T))
0T
t
zp(z(0))
Atakes the form
Tr(A) = E/bracketleftbig
TA/bracketrightbig
(18.29)
whereis a random vector whose distribution has zero mean and unit covariance, for
example, a Gaussian N(0;I) For a speciÔ¨Åc , the matrix-vector product Acan be
evaluated efÔ¨Åciently in a single pass using reverse-mode automatic differentiation We can then approximate the trace using a Ô¨Ånite number of samples in the form
Tr(A)/similarequal1
MM/summationdisplay
m=1T
mAm: (18.30)
In practice we can set M= 1 and just use a single sample, which is refreshed for
each new data point Although this is a noisy estimate, this might not be too signiÔ¨Å-
cant since it forms part of a noisy stochastic gradient descent procedure Importantly
it is unbiased, meaning that the expectation of the estimator is equal to the true value Exercise 18.11
SigniÔ¨Åcant improvements in training efÔ¨Åciency for continuous normalizing Ô¨Çows
can be achieved using a technique called Ô¨Çow matching (Lipman et al., 2022) This
brings normalizing Ô¨Çows closer to diffusion models and avoids the need for back- Chapter 20
propagation through the integrator while signiÔ¨Åcantly reducing memory require-
ments and enabling faster inference and more stable training

============================================================

=== CHUNK 546 ===
Palavras: 371
Caracteres: 2352
--------------------------------------------------
Exercises 559
Exercises
18.1 (??) Consider a transformation x=f(z)along with its inverse z=g(x) By
differentiating x=f(g(x)), show that
JK=I (18.31)
where Iis the identity matrix, and JandKare matrices with elements
Jij=@gi
@xj; K ij=@fi
@zj: (18.32)
Using the result that the determinant of a product of matrices is the product of their
determinants, show that
det(J) =1
det(K): (18.33)
Hence, show that the formula (18.1) for the transformation of a density under a
change of variables can be rewritten as
px(x) =pz(g(x))|detK|‚àí1(18.34)
where Kis evaluated at z=g(x) 18.2 (?)Consider a sequence of invertible transformations of the form
x=f1(f2(¬∑¬∑¬∑fM‚àí1(fM(z))¬∑¬∑¬∑)): (18.35)
Show that the inverse function is given by
z=f‚àí1
M(f‚àí1
M‚àí1(¬∑¬∑¬∑f‚àí1
2(f‚àí1
1(x))¬∑¬∑¬∑)): (18.36)
18.3 (?)Consider a linear change of variables of the form
x=z+b: (18.37)
Show that the Jacobian of this transformation is the identity matrix Interpret this
result by comparing the volume of a small region of z-space with the volume of the
corresponding region of x-space 18.4 (??) Show that the Jacobian of the autoregressive normalizing Ô¨Çow transformation
given by (18.18) is a lower triangular matrix The determinant of such a matrix is
given by the product of the terms on the leading diagonal and is therefore easily
evaluated 18.5 (?)Consider the forward propagation equation for a residual network given by (18.21)
in which we consider a small increment in the ‚Äòtime‚Äô variable t:
z(t+)=z(t)+f(z(t);w): (18.38)
Here the additive contribution from the neural network is scaled by  Note that
(18.21) corresponds to the case = 1 By taking the limit ‚Üí0, derive the forward
propagation differential equation given by (18.22) NORMALIZING FLOWS
18.6 (??) In this exercise and the next we provide an informal derivation of the backpropa-
gation and gradient evaluation equations for a neural ODE A more formal derivation
of these results can be found in Chen et al Write down the backpropagation
equation corresponding to the forward equation (18.38) By taking the limit ‚Üí0,
derive the backward propagation equation (18.25), where a(t) is deÔ¨Åned by (18.24) 18.7 (??) By making use of the result (8.10), write down an expression for the gradient
of a loss function L(z(T ))for a multilayered residual network deÔ¨Åned by (18.38)
in which all layers share the same parameter vector w

============================================================

=== CHUNK 547 ===
Palavras: 356
Caracteres: 2147
--------------------------------------------------
By taking the limit ‚Üí0,
derive the equation (18.26) for the derivative of the loss function 18.8 (???) In this exercise we give an informal derivation of (18.28) for one-dimensional
distributions Consider a distribution q(z)at timetthat is transformed to a new
distribution p(x)at timet+tas a result of a transformation from ztox Also
consider nearby values zandz+ ‚àÜz along with corresponding values xandx+
‚àÜxas shown in Figure 18.7 First, write down an equation that expresses that the
probability mass in the interval ‚àÜzis the same as that in the interval ‚àÜx Second,
write down an equation that shows how the probability density changes in going
fromttot+t, expressed in terms of the derivative dq(t)=dt Third, write down an
equation for ‚àÜxin terms of ‚àÜzby introducing the function f(z) = dz=dt Finally,
by combining these three equations and taking the limit t‚Üí0, show that
d
d
tlnq(z) =‚àíf/prime(z); (18.39)
which is the one-dimensional version of (18.28) Figure 18.7 Schematic illustration of the
transformation of probability den-
sities used to derive the equation
for continuous normalizing Ô¨Çows
in one dimension q(
z)
tp
(x)
t+t
zx
z+
zx+
x
18.9 (
??)The Ô¨Çow lines in Figure 18.6 were plotted by taking a set of equally spaced
values and using the inverse of the cumulative distribution function at each value of t
to plot the corresponding points in z-space Show that this is equivalent to using the
differential equation (18.27) to compute the Ô¨Çow lines where fis deÔ¨Åned by (18.28) 18.10 (??) Using the differential equation (18.27) write down an expression for the base
density of a continuous normalizing Ô¨Çow in terms of the output density, expressed
as an integral over t Hence, by making use of the fact that changing the sign of a
deÔ¨Ånite integral is equivalent to swapping the limits on that integral, show that the
computational cost of inverting a continuous normalizing Ô¨Çow is the same as that
needed to evaluate the forward Ô¨Çow Exercises 561
18.11 (?)Show that the expectation of the right-hand side in the Hutchinson trace estimator
(18.30) is equal to Tr(A) for any value of M This shows that the estimator is
unbiased

============================================================

=== CHUNK 548 ===
Palavras: 361
Caracteres: 2335
--------------------------------------------------
19
Autoencoders
A central goal of deep learning is to discover representations of data that are useful
for one or more subsequent applications One well-established approach to learn-
ing internal representations is called the auto-associative neural network orautoen-
coder This consists of a neural network having the same number of output units as
inputs and which is trained to generate an output ythat is close to the input x Once
trained, an internal layer within the neural network gives a representation z(x)for
each new input Such a network can be viewed as having two parts The Ô¨Årst is an
encoder, which maps the input xinto a hidden representation z(x), and the second
is adecoder, which maps the hidden representation onto the output y(z) If an autoencoder is to Ô¨Ånd non-trivial solutions, it is necessary to introduce
some form of constraint, otherwise the network can simply copy the input values
to the outputs This constraint might be achieved, for example, by restricting the
dimensionality of zrelative to that of xor by requiring zto have a sparse represen-
563 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_19    
564 19 Alternatively, the network can be forced to discover non-trivial solutions by
modifying the training process such that the network has to learn to undo corrup-
tions to the input vectors such as additive noise or missing values These kinds of
constraint encourage the network to discover interesting structure within the data to
achieve good training performance In this chapter, we start with deterministic autoencoders and then later gener-
alize to stochastic models that learn an encoder distribution p(z|x) together with a
decoder distribution p(y|z) These probabilistic models are known as variational
autoencoders and represent the third of our four approaches to learning nonlinear
latent variable models Deterministic
Autoencoders
W
e encountered a simple form of autoencoder when we studied principal compo-
nent analysis (PCA) This is a model that makes a linear transformation of an input Section 16.1
vector onto a lower dimensional manifold, and the resulting projection can be ap-
proximately reconstructed back in the original data space, again through a linear
transformation

============================================================

=== CHUNK 549 ===
Palavras: 374
Caracteres: 2327
--------------------------------------------------
We can make use of the nonlinearity of neural networks to deÔ¨Åne a
form of nonlinear PCA in which the latent manifold is no longer a linear subspace
of the data space This is achieved by using a network having the same number of
outputs as inputs and by optimizing the weights so as to minimize some measure of
the reconstruction error between inputs and outputs with respect to a set of training
data Simple autoencoders are rarely used directly in modern deep learning, as they
do not provide semantically meaningful representations in the latent space and they
are not able directly to generate new examples from the data distribution However,
they provide an important conceptual foundation for some of the more powerful deep
generative models such as variational autoencoders Section 19.2
19.1.1 Linear autoencoders
Consider Ô¨Årst a multilayer perceptron of the form shown in Figure 19.1, having
Dinputs,Doutput units, and Mhidden units, with M < D The targets used
to train the network are simply the input vectors themselves, so that the network
attempts to map each input vector onto itself Such a network is said to form an auto-
associative mapping Since the number of hidden units is smaller than the number
of inputs, a perfect reconstruction of all input vectors is not in general possible We
therefore determine the network parameters wby minimizing an error function that
captures the degree of mismatch between the input vectors and their reconstructions In particular, we choose a sum-of-squares error of the form
E(w) =1
2N/summationdisplay
n
=1/bardbly(xn;w)‚àíxn/bardbl2: (19.1)
If the hidden units have linear activation functions, then it can be shown that the
error function has a unique global minimum and that at this minimum the network
19.1 Deterministic Autoencoders 565
Figure 19.1 An autoencoder neural network having two
layers of weights Such a network is trained
to map input vectors onto themselves by
minimizing a sum-of-squares error Even
with nonlinear units in the hidden layer,
such a network is equivalent to linear prin-
cipal component analysis Links represent-
ing bias parameters have been omitted for
clarity.xdinputs y1
performs
a projection onto the M-dimensional subspace that is spanned by the Ô¨Årst
Mprincipal components of the data (Bourlard and Kamp, 1988; Baldi and Hornik,
1989)

============================================================

=== CHUNK 550 ===
Palavras: 366
Caracteres: 2416
--------------------------------------------------
Thus, the vectors of weights that lead into the hidden units in Figure 19.1
form a basis set that spans the principal subspace Note, however, that these vectors
need not be orthogonal or normalized This result is unsurprising, since both PCA
and neural networks rely on linear dimensionality reduction and minimize the same
sum-of-squares error function It might be thought that the limitations of a linear manifold could be overcome
by using nonlinear activation functions for the hidden units in the network in Fig-
ure 19.1 However, even with nonlinear hidden units, the minimum error solution
is again given by the projection onto the principal component subspace (Bourlard
and Kamp, 1988) There is therefore no advantage in using two-layer neural net-
works to perform dimensionality reduction Standard techniques for PCA, based on
singular-value decomposition (SVD), are guaranteed to give the correct solution in
Ô¨Ånite time, and they also generate an ordered set of eigenvalues with corresponding
orthonormal eigenvectors 19.1.2 Deep autoencoders
The situation is different, however, if additional nonlinear layers are included in
the network Consider the four-layer auto-associative network shown in Figure 19.2 Again, the output units are linear, and the Munits in the second layer can also
be linear However, the Ô¨Årst and third layers have sigmoidal nonlinear activation
functions The network is again trained by minimizing the error function (19.1) We
can view this network as two successive functional mappings F1andF2, as indicated
inFigure 19.2 The Ô¨Årst mapping F1projects the original D-dimensional data onto
anM-dimensional subspace SdeÔ¨Åned by the activations of the units in the second
layer Because of the Ô¨Årst layer of nonlinear units, this mapping is very general and
is not restricted to being linear Similarly, the second half of the network deÔ¨Ånes
an arbitrary functional mapping from the M-dimensional hidden space back into the
originalD-dimensional input space This has a simple geometrical interpretation, as
indicated for D= 3andM= 2inFigure 19.3 Such a network effectively performs a nonlinear form of PCA It has the ad-
vantage of not being limited to linear transformations, although it contains standard
566 19 AUTOENCODERS
Figure 19.2 Adding extra hidden layers
of nonlinear units produces an auto-
associative network, which can perform a
nonlinear dimensionality reduction.xDinputs

============================================================

=== CHUNK 551 ===
Palavras: 372
Caracteres: 2390
--------------------------------------------------
y1
nonlinearF1 F2
PCA
as a special case However, training the network now involves a nonlinear op-
timization, since the error function (19.1) is no longer a quadratic function of the
network parameters Computationally intensive nonlinear optimization techniques
must be used, and there is the risk of Ô¨Ånding a sub-optimal local minimum of the
error function Also, the dimensionality of the subspace must be speciÔ¨Åed before
training the network 19.1.3 Sparse autoencoders
Instead of limiting the number of nodes in one of the hidden layers in the net-
work, an alternative way to constrain the internal representation is to use a regularizer
to encourage a sparse representation, leading to a lower effective dimensionality A simple choice is the L1regularizer since this encourages sparseness, giving a Section 9.2.2
x1x2
x3z1z2
y1y2
y3y
z xF2
F1
S
Figure
19.3 Geometrical interpretation of the mappings performed by the network in Figure 19.2 for a model
withD= 3inputs andM= 2units in the second layer The function F2from the latent space deÔ¨Ånes the way
in which the manifold Sis embedded within the higher-dimensional data space Since F2can be nonlinear, the
embedding ofScan be non-planar, as indicated in the Ô¨Ågure The function F1then deÔ¨Ånes a projection from the
originalD-dimensional data space into the M-dimensional latent space Deterministic Autoencoders 567
regularized error function of the form
/tildewideE(w) =E(w) +K/summationdisplay
k=1|zk| (19.2)
whereE(w)is the unregularized error, and the sum over kis taken over the acti-
vation values of all the units in one of the hidden layers Note that regularization
is usually applied to the parameters of a network, whereas here it is being used on
the unit activations The derivatives required for gradient descent training can be
evaluated using automatic differentiation, as usual 19.1.4 Denoising autoencoders
We have seen the importance of constraining the dimensionality of the latent
space layer in a simple autoencoder to avoid the model simply learning the identity
mapping An alternative approach, that also forces the model to discover interesting
internal structure in the data, is to use a denoising autoencoder (Vincent et al., 2008) The idea is to take each input vector xnand to corrupt it with noise to give a modiÔ¨Åed
vector/tildewidexnwhich is then input to an autoencoder to give an output y(/tildewidexn;w)

============================================================

=== CHUNK 552 ===
Palavras: 356
Caracteres: 2240
--------------------------------------------------
The
network is trained to reconstruct the original noise-free input vector by minimizing
an error function such as the sum-of squares given by
E(w) =N/summationdisplay
n=1/bardbly(/tildewidexn;w)‚àíxn/bardbl2: (19.3)
One form of noise involves setting a randomly chosen subset of the input variables
to zero The fraction of such inputs represents the noise level, and lies in the range
0661 An alternative approach is to add independent zero-mean Gaussian
noise to every input variable, where the scale of the noise is set by the variance of
the Gaussian By learning to denoise the input data, the network is forced to learn
aspects of the structure of that data For example, if the data comprises images,
then learning that nearby pixel values are strongly correlated allows noise-corrupted
pixels to be corrected More formally, the training of denoising autoencoders is related to score match-
ing (Vincent, 2011) where the score is deÔ¨Åned by s(x) =‚àáxlnp(x) Some intuition
for this relationship is given in Figure 19.4 The autoencoder learns to reverse the
distortion vector /tildewidexn‚àíxnand therefore learns a vector for each point in data space
that points towards the manifold and therefore towards the region of high data den-
sity The score vector ‚àálnp(x) is similarly a vector pointing towards the region of
high data density We will explore the relationship between score matching and de-
noising in more depth when we discuss diffusion models which also learn to remove
noise from noise-corrupted inputs Section 20.3
19.1.5 Masked autoencoders
We have seen that transformer models such as BERT can learn rich internal
representations of natural languages through self-supervision by masking random
568 19 AUTOENCODERS
Figure 19.4 In a denoising autoencoder, data
points, which are assumed to
live on a lower-dimensional man-
ifold in data space, are corrupted
with additive noise The autoen-
coder learns to map corrupted
data points back to their original
values and therefore learns a vec-
tor for each point in data space
that points towards the manifold corrupted data point
original
data point
data manifold
subsets
of the inputs, and it is natural to ask if a similar approach can be applied to Section 12.2
natural images

============================================================

=== CHUNK 553 ===
Palavras: 365
Caracteres: 2285
--------------------------------------------------
In a masked autoencoder (Heet al., 2021), a deep network is used
to reconstruct an image given a corrupted version of that image as input, similar to
denoising autoencoders However in this case, the form of corruption is masking, or
dropping out, part of the input image This technique is generally used in combina-
tion with a vision transformer architecture, as in this case, masking part of the input Section 12.4.1
can be easily implemented by passing only a subset of randomly selected input patch
tokens to the encoder The overall algorithm is summarized in Figure 19.5 Compared to language, images have much more redundancy along with strong
local correlations Omitting a single word from a sentence can greatly increase am-
biguity whereas removing a random patch from an image typically has little impact
on the semantics of the image Unsurprisingly, the best internal representations are
learned when a relatively high proportion of the input image is masked, typically
75% compared with the 15% masking for BERT In BERT the masked inputs are
replaced by a Ô¨Åxed mask token, whereas in the masked autoencoder the masked
patches are simply omitted By omitting a large fraction of the input patches, we can
save signiÔ¨Åcant computation, particularly as the computation required for a training
instance of a transformer scales poorly with input sequence length, thus making the
masked autoencoder a good choice for pre-training large transformer encoders As the decoder layer is also a transformer, it needs to work in the dimensionality
of the original image Since the output of a transformer has the same dimensionality
as the input, we need to restore the image dimensionality between the output of the
encoder and the input of the decoder This is achieved by reinstating the masked
patches, represented by a Ô¨Åxed mask token vector, with each patch token augmented
by positional encoding information Due to the much higher dimensionality of the
decoder representation, the decoder transformer has far fewer learnable parameters
than the encoder The output of the decoder is followed by a learnable linear layer
19.2 Variational Autoencoders 569
inputs
encoder
:::decoder
outputs
:::
predictions
targets
Figure 19.5 Architecture of a masked autoencoder during the training phase

============================================================

=== CHUNK 554 ===
Palavras: 375
Caracteres: 2356
--------------------------------------------------
Note that the target is the com-
plement of the input as the loss is only applied on masked patches After training, the decoder is discarded and
the encoder is used to map images to an internal representation for use in downstream tasks that maps the output representation into the space of pixel values, and the training
error function is simply the mean squared error averaged over the missing patches
for each image Examples of images reconstructed by a trained masked autoencoder
are shown in Figure 19.6 and demonstrate the ability of a trained autoencoder to
generate semantically plausible reconstructions However, the ultimate goal is to
learn useful internal representations for subsequent downstream tasks, for which the
decoder is discarded and the encoder is applied to the full image with no masking
and with a fresh set of output layers that are Ô¨Åne-tuned for the required application Note also that although this algorithm was initially designed for image data, it can in
theory be applied to any modality Variational Autoencoders
We have already seen that the likelihood function for a latent-variable model given
by
p(x|w) =/integraldisplay
p(x|z;w)p(z) dz; (19.4)
in whichp(x|z;w)is deÔ¨Åned by a deep neural network, is intractable because the
integral over zcannot be evaluated analytically The variational autoencoder , or
570 19 AUTOENCODERS
Figure 19.6 Four examples of images reconstructed using a trained masked autoencoder, in which 80% of
the input patches are masked In each case the masked image is on the left, the reconstructed image is in the
centre, and the original image is on the right (2021) with permission.]
VAE (Kingma and Welling, 2013; Rezende, Mohamed, and Wierstra, 2014; Doer-
sch, 2016; Kingma and Welling, 2019) instead works with an approximation to this
likelihood when training the model There are three key ideas in the V AE: (i) use of
the evidence lower bound (ELBO) to approximate the likelihood function, leading to
a close relationship to the EM algorithm, (ii) amortized inference in which a second Section 15.3
model, the encoder network, is used to approximate the posterior distributions over
latent variables in the E step, rather than evaluating the posterior distribution for each
data point exactly, and (iii) making the training of the encoder model tractable using
thereparameterization trick

============================================================

=== CHUNK 555 ===
Palavras: 359
Caracteres: 2576
--------------------------------------------------
Consider a generative model with a conditional distribution p(x|z;w)over the
D-dimensional data variable xgoverned by the output of a deep neural network
g(z;w) For example, g(z;w)might represent the mean of a Gaussian conditional
distribution Also, consider a distribution over the M-dimensional latent variable z
that is given by a zero-mean unit-variance Gaussian:
p(z) =N(z|0;I): (19.5)
To derive the V AE approximation, Ô¨Årst recall that, for an arbitrary probability distri-
butionq(z)over a space described by the latent variable z, the following relationship
holds: Section 15.4
lnp(x|w) =L(w) + KL (q(z)/bardblp(z|x;w)) (19.6)
whereLis the evidence lower bound , or ELBO, also known as the variational lower
bound , given by
L(w) =/integraldisplay
q(z) ln/braceleftbiggp(x|z;w)p(z)
q(z)/bracerightbigg
dz (19.7)
19.2 Variational Autoencoders 571
and the Kullback‚ÄìLeibler divergence KL (¬∑/bardbl¬∑)is deÔ¨Åned by
KL (q(z)/bardblp(z|x; w)) =‚àí/integraldisplay
q(z) ln/braceleftbiggp(z|x; w)
q(
z)/bracerightbigg
dz: (19.8)
Because the Kullback‚ÄìLeibler divergence satisÔ¨Åes KL (q/bardblp)>0, it follows that
lnp(x|w )>L (19.9)
and soLis a lower bound on lnp(x|w ) Although the log likelihood lnp(x|w )is
intractable, we will see how the lower bound can be evaluated using a Monte Carlo
estimate Hence it provides an approximation to the true log likelihood Now consider a set of training data points D={x1;:::;xN}, which are as-
sumed to be drawn independently from the model distribution p(x) The log likeli-
hood function for this data set is given by
lnp(D|w ) =N/summationdisplay
n=1Ln+N/summationdisplay
n=1KL (qn(zn)/bardblp(zn|xn;w)) (19.10)
where
Ln=/integraldisplay
qn(zn) ln/braceleftbiggp(xn|zn;w)p(zn)
qn(
zn)/bracerightbigg
dzn: (19.11)
Note that this introduces a separate latent variable zncorresponding to each data
vector xn, as we saw with mixture models and with the probabilistic PCA model Section 15.2
Section 16.2 Consequently, each latent variable has its own independent distribution qn(zn), each
of which can be optimized separately Since (19.10) holds for any choice of the distributions qn(z), we can choose the
distributions that maximize the bound Ln, or equivalently the distributions that min-
imize the Kullback‚ÄìLeibler divergences KL (qn(zn)/bardblp(zn|xn;w)) For the simple
Gaussian mixture and probabilistic PCA models considered previously, we were able
to evaluate these posterior distributions exactly in the E step of the EM algorithm,
which corresponds to setting each qn(zn)equal to the corresponding posterior dis-
tributionp(zn|xn;w)

============================================================

=== CHUNK 556 ===
Palavras: 354
Caracteres: 2293
--------------------------------------------------
This gives zero Kullback‚ÄìLeibler divergence, and hence the
lower bound is equal to the true log likelihood The interpretation of the posterior
distribution is illustrated in Figure 19.7 using the simple example introduced earlier
in the context of generative adversarial networks Section 16.4.1
The exact posterior distribution of znis given from Bayes‚Äô theorem by
p(zn|xn;w) =p(xn|zn;w)p(zn)
p
(xn|w): (19.12)
The numerator is straightforward to evaluate for our deep generative model How-
ever, we see that the denominator is given by the likelihood function, which as we
have already noted, is intractable We therefore need to Ô¨Ånd an approximation to
the posterior distribution In principle, we could consider a separate parameterized
model for each of the distributions qn(zn)and optimize each model numerically,
572 19 AUTOENCODERS
‚àí4‚àí2 0 2 4
zp(z)p(z|x‚ãÜ)
(a)
zx1x2
x‚ãÜ (b)
Figure
19.7 Evaluation of the posterior distribution for the same model as shown in Figure 16.13 The marginal
distribution p(x), shown in the right-most plot in (b), has a banana shape, and the speciÔ¨Åc data point x?is closer
to the horns of the shape than to the middle Consequently the posterior distribution p(z|x?), shown in (a), is
bimodal, even though the prior distribution p(z) is unimodal [Based on Prince (2020) with permission.]
but this would be computationally very expensive, especially for large data sets, and
moreover we would have to re-evaluate the distributions after every update of w Instead, we turn now to a different, more efÔ¨Åcient approximation framework based
on the introduction of a second neural network 19.2.1 Amortized inference
In the variational autoencoder, instead of trying to evaluate a separate posterior
distributionp(zn|xn;w)for each of the data points xnindividually, we train a single
neural network, called the encoder network, to approximate all these distributions This technique is called amortized inference and requires an encoder that produces
a single distribution q(z|x;)that is conditioned on x, whererepresents the pa-
rameters of the network The objective function, given by the evidence lower bound,
now has a dependence on as well as on w, and we use gradient-based optimization
methods to maximize the bound jointly with respect to both sets of parameters

============================================================

=== CHUNK 557 ===
Palavras: 413
Caracteres: 2550
--------------------------------------------------
A V AE therefore comprises two neural networks that have independent param-
eters but which are trained jointly: an encoder network that takes a data vector and
maps it to a latent space, and the original network that takes a latent space vector and
maps it back to the data space and which we can therefore interpret as a decoder net-
work This like the simple neural network autoencoder model, except that we now Section 19.1
deÔ¨Åne a probability distribution over the latent space We will see that the encoder
calculates an approximate probabilistic inverse of the decoder according to Bayes‚Äô
theorem A typical choice for the encoder is a Gaussian distribution with a diagonal co-
variance matrix whose mean and variance parameters, jand2
j, are given by the
19.2 Variational Autoencoders 573
w0wL(w0,œÜ0)L(w0,œÜ1)
lnp(x|w)
(a)
w0w1wL(w0,œÜ1)
L(w1,œÜ1)lnp(x|w)
 (b)
Figure
19.8 Illustration of the optimization of the ELBO (evidence lower bound) (a) For a given value w0of
the decoder network parameters w, we can increase the bound by optimizing the parameters of the encoder
network (b) For a given value of , we can increase the value of the ELBO function by optimizing w Note that
the ELBO function, shown by the blue curves, always lies somewhat below the log likelihood function, shown in
red, because the encoder network is generally not able to match the true posterior distribution exactly outputs of a neural network that takes xas input:
q(z|x;) =M/productdisplay
j=1N/parenleftbig
zj|j(x;);2
j(x;)/parenrightbig
: (19.13)
Note that the means j(x;)lie in the range (‚àí‚àû;‚àû), and so the corresponding
output-unit activation functions can be linear, whereas the variances 2
j(x;)must
be non-negative and so the associated output units typically use exp(¬∑) as their acti-
vation function The goal is to use gradient-based optimization to maximize the bound with re-
spect to both sets of parameters andw, typically by using stochastic gradient
descent based on mini-batches Although we optimize the parameters jointly, con-
ceptually we could imagine alternating between optimizing and optimizing w, in
the spirit of the EM algorithm, as illustrated in Figure 19.8 A key difference compared to EM is that, for a given value of w, optimizing with
respect to the parameters of the encoder does not in general reduce the Kullback-
Leibler divergence to zero, because the encoder network is not a perfect predictor
of the posterior latent distribution and so there is a residual gap between the lower
bound and the true log likelihood

============================================================

=== CHUNK 558 ===
Palavras: 358
Caracteres: 2352
--------------------------------------------------
Although the encoder is very Ô¨Çexible, since it
is based on a deep neural network, it is not expected to model the true posterior
distribution exactly because (i) the true conditional posterior distribution will not be
574 19 AUTOENCODERS
lnp(x|w)
EM
(a)
lnp(x|w)
 (b)
Figure
19.9 Comparison of the EM algorithm with ELBO optimization in a VAE (a) In the EM algorithm we
alternate between updating the variational posterior distribution in the E step, and the model parameters in the
M step When the E step is exact, the gap between the lower bound and the log likelihood is reduced to zero
after each E step (b) In the VAE we perform joint optimization of the encoder network parameters (analogous
to the E step) and the decoder network parameters w(analogous to the M step) a factorized Gaussian, (ii) even a large neural network has limited Ô¨Çexibility, and (iii)
the training process is only an approximate optimization The relation between the
EM algorithm and ELBO optimization is summarized in Figure 19.9 19.2.2 The reparameterization trick
Unfortunately, as it stands, the lower bound (19.11) is still intractable to compute
because it involves integrals over the latent variables {zn}in which the integrand has
a complicated dependence on the latent variables because of the decoder network For data point xnwe can write the contribution to the lower bound in the form
Ln(w;) =/integraldisplay
q(zn|xn;) ln/braceleftbiggp(xn|zn;w)p(zn)
q(
zn|xn;)/bracerightbigg
dzn
=/integraldisplay
q(zn|xn;) lnp(xn|zn;w) dzn‚àíKL(q (zn|xn;)/bardblp(zn)): (19.14)
The second term on the right-hand side is a Kullback‚ÄìLeibler divergence between
two Gaussian distributions and can be evaluated analytically: Exercise 2.27
KL (q(zn|xn;)/bardblp(zn)) =1
2M/summationdisplay
j=1/braceleftbig
1
+ ln2
j(xn)‚àí2
j(xn)‚àí2
j(xn)/bracerightbig
:(19.15)
19.2 Variational Autoencoders 575
xi
2
iz w p
(x|z;w)
Figure
19.10 When the ELBO is estimated by Ô¨Åxing the latent variable zto a sampled value this blocks
backpropagation of the error signal to the encoder network For the Ô¨Årst term in (19.14), we could try to approximate the integral over znwith a
simple Monte Carlo estimator:
/integraldisplay
q(zn|xn;) lnp(xn|zn;w) dzn/similarequal1
LL/summationdisplay
l=1lnp
(xn|z(l)
n;w) (19.16)
where{z(l)
n}are samples drawn from the encoder distribution q(zn|xn;)

============================================================

=== CHUNK 559 ===
Palavras: 360
Caracteres: 2271
--------------------------------------------------
This is
easily differentiated with respect to w, but the gradient with respect to is problem-
atic because changes to will change the distribution q(zn|xn;)from which the
samples are drawn and yet these samples are Ô¨Åxed values so that we do not have a
way to obtain the derivatives of these samples with respect to Conceptually, we
can think of the process of Ô¨Åxing znto a speciÔ¨Åc sample value as blocking the back-
propagation of the error signal to the encoder network, as illustrated in Figure 19.10 We can resolve this by making use of the reparameterization trick in which we
reformulate the Monte Carlo sampling procedure such that derivatives with respect
tocan be calculated explicitly First, note that if is a Gaussian random variable
with zero mean and unit variance, then the quantity
z=+ (19.17)
will have a Gaussian distribution, with mean and variance 2 We now apply this Exercise 19.2
to the samples in (19.16) in which andare deÔ¨Åned by the outputs j(xn;)
and2
j(xn;)of the encoder network, which represent the means and variances in
distribution (19.13) Instead of drawing samples of zndirectly, we draw samples for
and use (19.17) to evaluate corresponding samples for zn:
z(l)
nj=j(xn;)(l)
nj+2
j(xn;) (19.18)
wherel= 1;:::;L indexes the samples This makes the dependence on explicit
and allows gradients with respect to to be evaluated, as illustrated in Figure 19.11 The reparameterization trick can be extended to other distributions but is limited to
continuous variables There are techniques to evaluate gradients directly without the
reparameterization trick (Williams, 1992), but these estimators have high variance,
and so reparameterization can also be viewed as a variance reduction technique The full error function for the V AE, using our speciÔ¨Åc modelling assumptions,
therefore becomes
L=/summationdisplay
nÔ£±
Ô£≤
Ô£≥1
2M/summationdisplay
j=1/braceleftbig
1
+ ln2
nj‚àí2
nj‚àí2
nj/bracerightbig
+1
LL/summationdisplay
l=1lnp
(xn|z(l)
n;w)Ô£º
Ô£Ω
Ô£æ(19.19)
576 19 AUTOENCODERS
xi
2
iz
w p(x|z;w)
Figure 19.11 The reparameterization trick replaces a direct sample of zby one that is calculated from a
sample of an independent random variable , thereby allowing the error signal to be back-
propagated to the encoder network

============================================================

=== CHUNK 560 ===
Palavras: 362
Caracteres: 2218
--------------------------------------------------
The resulting model can be trained using gradient-
based optimization to learn the parameters of both the encoder and decoder networks where z(l)
nhas components z(l)
nj=nj(l)+nj, in whichnj=j(xn;)and
nj=j(xn;), and the summation over nin (19.19) is over the data points in a
mini-batch The number of samples L, for each data point xn, is typically set to 1, so
that only a single sample is used Although this gives a noisy estimate of the bound,
it forms part of the stochastic gradient optimization step, which is already noisy, and
overall leads to more efÔ¨Åcient optimization We can summarize V AE training as follows For each data point in a mini-
batch, forward propagate through the encoder network to evaluate the means and
variances of the approximate latent distribution, sample from this distribution using
the reparameterization trick, and then propagate these samples through the decoder
network to evaluate the ELBO (19.19) The gradients with respect to wandare
then evaluated using automatic differentiation V AE training is summarized in Al-
gorithm 19.1, where, for clarity, we have omitted that this would generally be done
using mini-batches Once the model is trained, the encoder network is discarded and
new data points are generated by sampling from the prior p(z) and forward propa-
gating through the decoder network to obtain samples in the data space After training we might want to assess how well the model represents a new test
point/hatwidex Since the log likelihood is intractable, we can use the lower bound Las an
approximation To estimate this we can sample from q(z|/hatwidex;)as this gives more
accurate estimates than sampling from p(z) There are many variants of V AEs When applied to image data, the encoder is
typically based on convolutions and the decoder based on transpose convolutions Section 10.5.3
In a conditional VAE both the encoder and decoder take a conditioning variable c
as an additional input For example, we might want to generate images of objects,
in which crepresents the object class The latent-space prior distribution p(z) can
again be a simple Gaussian, or it can be extended to a conditional distribution p(z|c)
given by another neural network

============================================================

=== CHUNK 561 ===
Palavras: 360
Caracteres: 2283
--------------------------------------------------
Training and testing proceed as before Note that the Ô¨Årst term in the ELBO (19.14) encourages the encoder distribution
q(z|x;)to be close to the prior p(z), and so the decoder model is encouraged to
produce realistic outputs when the trained model is run generatively by sampling
fromp(z) When training V AEs, a problem can arise in which the variational dis-
tributionq(z|x;)converges to the prior distribution p(z) and therefore becomes
uninformative because it no longer depends on x In effect the latent code is ig-
19.2 Variational Autoencoders 577
Algorithm 19.1: Variational autoencoder training
Input: Training data setD={x1;:::;xN}
Encoder network{j(xn;);2
j(xn;)}; j‚àà{1;:::;M}
Decoder network g(z;w)
Initial weight vectors w;
Learning rate 
Output: Final weight vectors w;
repeat
L‚Üê 0
forj‚àà{1;:::;M}do
nj‚àºN(0;1)
znj‚Üêj(xn;)nj+2
j(xn;)
L‚ÜêL +1
2/braceleftbig
1 + ln2
nj‚àí2
nj‚àí2
nj/bracerightbig
end for
L‚ÜêL + lnp(xn|zn;w)
w‚Üêw+‚àáwL// Update decoder weights
‚Üê+‚àáL// Update encoder weights
until converged
return w;
nored This is known as posterior collapse A symptom of this is that if we take
an input and encode it and then decode it, we get a poor reconstruction that looks
blurry In this case the Kullback‚ÄìLeibler divergence KL(q (z|x;)/bardblp(z)) is close to
zero A different problem occurs when the latent code is not compressed, which is
characterized by highly accurate reconstructions, but such that outputs generated by
samplingp(z) and passing the samples through the decoder network have poor qual-
ity and do not resemble the training data In this case the Kullback‚ÄìLeibler diver-
gence is relatively large, and because the trained system has a variational distribution
that is very different from the prior, samples from the prior do not generate realistic
outputs Both problems can be addressed by introducing a coefÔ¨Åcient in front of the Ô¨Årst
term in (19.14) to control the regularization effectiveness of the Kullback‚ÄìLeibler
divergence, where typically  > 1(Higgins et al., 2017) If the reconstructions
look poor then can be increased, whereas if the samples look poor then can be
decreased The value of can also be set to follow an annealing schedule in which
it starts with a small value and is gradually increased during training

============================================================

=== CHUNK 562 ===
Palavras: 353
Caracteres: 2249
--------------------------------------------------
Finally, note that we have considered a decoder network g(z;w)that represents
578 19 AUTOENCODERS
the mean of a Gaussian output distribution We can extend the V AE to include out-
puts representing the variance of the Gaussian or, more generally, the parameters that
characterize other more complex distributions Section 6.5
Exercises
19.1 (??) Show that, for any distribution q(z|)and any function G(z), the following
relation holds:
‚àá/integraldisplay
q(z|)G(z) d z=/integraldisplay
q(z|)G(z)‚àálnq(z|) d z: (19.20)
Hence, show that the left-hand side of (19.20) can be approximated by the following
Monte Carlo estimator:
‚àá/integraldisplay
q(z|)G(z) d z/similarequal/summationdisplay
iG(z(i))‚àálnq(z(i)|) (19.21)
where the samples {z(i)}are drawn independently from the distribution q(z|) Ver-
ify that this estimator is unbiased, i.e., that the average value of the right-hand side
of (19.21), averaged over the distribution of the samples, is equal to the left-hand
side In principle, by setting G(z) =p(x|z; w), this result would allow the gradient
of the second term on the right-hand side of (19.14) with respect to to be evalu-
ated without making use of the reparameterization trick Also, because this method
is unbiased, it will give the exact answer in the limit of an inÔ¨Ånite number of sam-
ples However, the reparameterization trick is more efÔ¨Åcient, meaning that fewer
samples are needed to get good accuracy, because it directly computes the change of
p(x|z; w)due to the change in zthat results from a change in 19.2 (?)Verify that if has a zero-mean unit-variance Gaussian distribution, then the
variablezin (19.17) will have a Gaussian distribution with mean and variance 2 19.3 (??) In this exercise we extend the diagonal covariance V AE encoder network (19.13)
to one with a general covariance matrix Consider a K-dimensional random vector
drawn from a simple Gaussian:
‚àºN(z|0;I); (19.22)
which is then linearly transformed using the relation
z=+L (19.23)
where Lis a lower-triangular matrix (i.e., a K√óKmatrix with all elements above
the leading diagonal being zero) Show that zhas a distributionN(z|;), and
write down an expression for in terms of L Explain why the diagonal elements of
Lmust be non-negative

============================================================

=== CHUNK 563 ===
Palavras: 354
Caracteres: 2310
--------------------------------------------------
Describe how andLcan be expressed as the outputs of a
neural network, and discuss suitable choices for output-unit activation functions Exercises 579
19.4 (??) Evaluate the Kullback‚ÄìLeibler divergence term in (19.14) Hence, show how
the gradients of this term with respect to wandcan be evaluated for training the
encoder and decoder networks 19.5 (?)We have seen that the ELBO given by (19.11) can be written in the form (19.14) Show that it can also be written as
Ln(w;) =/integraldisplay
q(zn|xn;) ln{p(xn|zn;w)p(zn)}dzn
‚àí/integraldisplay
q(zn|xn;) lnq(zn|xn;) dzn: (19.24)
19.6 (?)Show that the ELBO given by (19.11) can be written in the form
Ln(w;) =/integraldisplay
q(zn|xn;) lnp(zn) dzn
+/integraldisplay
q(zn|xn;) ln/braceleftbiggp(xn|zn;w)
q(zn|xn;)/bracerightbigg
dzn: (19.25)
20
Diffusion
Models
We have seen that a powerful way to construct rich generative models is to introduce
a distribution p(z)over a latent variable z, and then to transform zinto the data space
xusing a deep neural network It is sufÔ¨Åcient to use a simple, Ô¨Åxed distribution
forp(z), such as a Gaussian N(z|0;I), since the generality of the neural network
transforms this into a highly Ô¨Çexible family of distributions over x In previous
chapters we have explored several models which Ô¨Åt within this framework but which
take different approaches to deÔ¨Åning and training the deep neural network, based on
generative adversarial networks, variational autoencoders, and normalizing Ô¨Çows In this chapter we discuss a fourth class of models within this general frame- Section 16.4.4
work, known as diffusion models, also called denoising diffusion probabilistic mod-
els, or DDPMs (Sohl-Dickstein et al., 2015; Ho, Jain, and Abbeel, 2020), which
have emerged as the state of the art for many applications For illustration we will
focus on models of image data although the framework has much broader applicabil-
581 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4_20    
582 20 DIFFUSION MODELS
x
 z1
 z2
 zT
Figure 20.1 Illustration of the encoding process in a diffusion model showing an image xthat is gradually
corrupted with multiple stages of additive Gaussian noise giving a sequence of increasingly noisy images

============================================================

=== CHUNK 564 ===
Palavras: 350
Caracteres: 2163
--------------------------------------------------
After
a large number Tof steps the result is indistinguishable from a sample drawn from a Gaussian distribution A
deep neural network is then trained to reverse this process The central idea is to take each training image and to corrupt it using a multi-step
noise process to transform it into a sample from a Gaussian distribution This is il-
lustrated in Figure 20.1 A deep neural network is then trained to invert this process,
and once trained the network can then generate new images starting with samples
from a Gaussian as input Diffusion models can be viewed as a form of hierarchical variational autoen-
coder in which the encoder distribution is Ô¨Åxed, and deÔ¨Åned by the noise process, Section 19.2
and only the generative distribution is learned (Luo, 2022) They are easy to train,
they scale well on parallel hardware, and they avoid the challenges and instabilities
of adversarial training while producing results that have quality comparable to, or
better than, generative adversarial networks However, generating new samples can
be computationally expensive due to the need for multiple forward passes through
the decoder network (Dhariwal and Nichol, 2021) Forward Encoder
Suppose we take an image from the training set, which we will denote by x, and
blend it with Gaussian noise independently for each pixel to give a noise-corrupted
image z1deÔ¨Åned by
z1=/radicalbig
1‚àí1x+/radicalbig
11 (20.1)
where1‚àºN (1|0;I)and1<1is the variance of the noise distribution The
choice of coefÔ¨Åcients‚àö1‚àí1and‚àö1in (20.1) and (20.3) ensures that the mean of Exercise 20.1
the distribution of ztis closer to zero than the mean of zt‚àí1and that the variance of zt
is closer to the unit matrix than the variance of zt‚àí1 We can write the transformation
(20.1) in the form Exercise 20.2
q(z1|x) =N(z1|/radicalbig
1‚àí1x;1I): (20.2)
We then repeat the process with additional independent Gaussian noise steps to give
a sequence of increasingly noisy images z2;:::;zT Note that in the literature on
diffusion models, these latent variables are sometimes denoted x1;:::;xTand the
observed variable is denoted x0 We use the notation of zfor latent variables and x
20.1

============================================================

=== CHUNK 565 ===
Palavras: 352
Caracteres: 2251
--------------------------------------------------
Forward Encoder 583
x zt
‚àí1 zt zTq(
zt|zt‚àí1)q(zt‚àí1|zt;x)
p(zt‚àí1|zt;w)
Figure
20.2 A diffusion process represented as a probabilistic graphical model The original image xis
shown by the shaded node, since it is an observed variable, whereas the noise-corrupted
images z1;:::;zTare considered to be latent variables The noise process is deÔ¨Åned by
the forward distribution q(zt|zt‚àí1)and can be viewed as an encoder Our goal is to learn a
modelp(zt‚àí1|zt;w)that tries to reverse this noise process and which can be viewed as a
decoder As we will see later, the conditional distribution q(zt‚àí1|zt;x)plays an important
role in deÔ¨Åning the training procedure for the observed variable for consistency with the rest of the book Each successive
image is given by
zt=/radicalbig
1‚àítzt
‚àí1+/radicalbig
tt (20.3)
wheret‚àº
N(t|0;I) Again, we can write (20.3) in the form
q(zt|zt‚àí1) =N(zt|/radicalbig
1‚àítzt
‚àí1;tI): (20.4)
The sequence of conditional distributions (20.4) forms a Markov chain and can be Section 11.3
expressed as a probabilistic graphical model as shown in Figure 20.2 The values of
the variance parameters t‚àà(0;1)are set by hand and are typically chosen such
that the variance values increase through the chain according to a prescribed schedule
such that1< 2<:::< T 20.1.1 Diffusion kernel
The joint distribution of the latent variables, conditioned on the observed data
vector x, is given by
q(z1;:::;zt|x) =q(z1|x)t/productdisplay
=2q(z|z‚àí1): (20.5)
If we now marginalize over the intermediate variables z1;:::;zt‚àí1, we obtain the
diffusion kernel : Exercise 20.3
q(zt|x) =N(zt|‚àötx
;(1‚àít)I) (20.6)
where we have deÔ¨Åned
t=t/productdisplay
=1(1‚àí): (20.7)
584 20 DIFFUSION MODELS
We see that each intermediate distribution has a simple closed-form Gaussian ex-
pression from which we can directly sample, which will prove useful when training
DDPMs as it allows efÔ¨Åcient stochastic gradient descent using randomly chosen in-
termediate terms in the Markov chain without having to run the whole chain We can
also write (20.6) in the form
zt=‚àötx+‚àö
1‚àítt (20.8)
where againt‚àºN(t|0;I) Note that that now represents the total noise added to
the original image instead of the incremental noise added at this step of the Markov
chain

============================================================

=== CHUNK 566 ===
Palavras: 368
Caracteres: 2340
--------------------------------------------------
After many steps the image becomes indistinguishable from Gaussian noise, and
in the limitT‚Üí‚àû we have Exercise 20.4
q(zT|x) =N(zT|0;I) (20.9)
and therefore all information about the original image is lost The choice of coef-
Ô¨Åcients‚àö1‚àítand‚àötin (20.3) ensures that once the Markov chain converges
to a distribution with zero mean and unit covariance, further updates will leave this
unchanged Exercise 20.5
Since the right-hand side of (20.9) is independent of x, it follows that the marginal
distribution of zTis given by
q(zT) =N(zT|0;I): (20.10)
It is common to refer to the Markov chain (20.4) as the forward process, and it is
analogous to the encoder in a V AE, except that here it is Ô¨Åxed rather than learned Note, however, that the usual terminology in the literature is the opposite of that
typically used in the literature regarding normalizing Ô¨Çows, where the mapping from
latent space to data space is considered the forward process 20.1.2 Conditional distribution
Our goal is to learn to undo the noise process, and so it is natural to consider
the reverse of the conditional distribution q(zt|zt‚àí1), which we can express using
Bayes‚Äô theorem in the form
q(zt‚àí1|zt) =q(zt|zt‚àí1)q(zt‚àí1)
q(zt): (20.11)
We can write the marginal distribution q(zt‚àí1)in the form
q(zt‚àí1) =/integraldisplay
q(zt‚àí1|x)p(x) d x (20.12)
whereq(zt‚àí1|x)is given by the conditional Gaussian (20.6) This distribution is
intractable, however, because we must integrate over the unknown data density p(x) If we approximate the integration using samples from the training data set, we obtain
a complicated distribution expressed as a mixture of Gaussians Reverse Decoder 585
Instead, we consider the conditional version of the reverse distribution, condi-
tioned on the data vector x, deÔ¨Åned by q(zt‚àí1|zt;x), which as we will see shortly
turns out to be a simple Gaussian distribution Intuitively this is reasonable since,
given a noisy image, it is difÔ¨Åcult to guess which lower-noise image gave rise to it,
whereas if we also know the starting image then the problem becomes much easier We can calculate this conditional distribution using Bayes‚Äô theorem:
q(zt‚àí1|zt;x) =q(zt|zt‚àí1;x)q(zt‚àí1|x)
q(
zt|x): (20.13)
We now make use of the Markov property of the forward process to write
q(zt|zt‚àí1;x) =q(zt|zt‚àí1) (20.14)
where the right-hand side is given by (20.4)

============================================================

=== CHUNK 567 ===
Palavras: 369
Caracteres: 2383
--------------------------------------------------
As a function of zt‚àí1, this takes the
form of an exponential of a quadratic form The term q(zt‚àí1|x)in the numerator of
(20.13) is the diffusion kernel given by (20.6), which again involves the exponential
of a quadratic form with respect to zt‚àí1 We can ignore the denominator in (20.13)
since as a function of zt‚àí1it is constant Thus, we see that the right-hand side of
(20.13) takes the form of a Gaussian distribution, and we can identify its mean and
covariance using the technique of ‚Äòcompleting the square‚Äô to give Exercise 20.6
q(zt‚àí1|zt;x) =N/parenleftbig
zt‚àí1|mt(x;zt);2
tI/parenrightbig
(20.15)
where
mt(x;zt) =(1‚àít‚àí1)‚àö1‚àítzt+‚àöt
‚àí1tx
1‚àít(20.16)
2
t=t(1‚àít
‚àí1)
1‚àít(20.17)
and
we have made use of (20.7) Re
verse Decoder
W
e have seen that the forward encoder model is deÔ¨Åned by a sequence of Gaussian
conditional distributions q(zt|zt‚àí1)but that inverting this directly leads to a distri-
butionq(zt‚àí1|zt)that is intractable, as it would require integrating over all possible
values of the starting vector xwhose distribution is the unknown data distribution
p(x) that we wish to model Instead, we will learn an approximation to the reverse
distribution by using a distribution p(zt‚àí1|zt;w)governed by a deep neural network,
where wrepresents the network weights and biases This reverse step is analogous
to the decoder in a variational autoencoder and is illustrated in Figure 20.2 Once Chapter 19
the network is trained, we can sample from the simple Gaussian distribution over zT
and transform it into a sample from the data distribution p(x) through a sequence of
reverse sampling steps by repeated application of the trained network DIFFUSION MODELS
zt‚àí1
ztq(zt|zt‚àí1)
zt
zt‚àí1q(zt‚àí1|zt)
q(zt‚àí1)
Figure
20.3 Illustration of the evaluation of the reverse distribution q(zt‚àí1|zt)using Bayes‚Äô theorem (20.13) for
scalar variables The red curve on the right-hand plot shows the marginal distribution q(zt‚àí1)illustrated using a
mixture of three Gaussians, whereas the left-hand plot shows the Gaussian forward noise process q(zt|zt‚àí1)as
a distribution over ztcentred on zt‚àí1 By multiplying these together and normalizing, we obtain the distribution
q(zt‚àí1|zt)shown for a particular choice of ztby the blue curve Because the distribution on the left is relatively
broad, corresponding to a large variance t, the distribution q(zt‚àí1|zt)has a complex multimodal structure

============================================================

=== CHUNK 568 ===
Palavras: 352
Caracteres: 2244
--------------------------------------------------
Intuitively, if we keep the variances small so that t/lessmuch1then the change in the
latent vector between steps will be relatively small, and hence it should be easier to
learn to invert the transformation More speciÔ¨Åcally, if t/lessmuch1then the distribution
q(zt‚àí1|zt)will be approximately a Gaussian distribution over zt‚àí1 This can be
seen from (20.11) since the right-hand side depends on zt‚àí1throughq(zt|zt‚àí1)and
q(zt‚àí1) Ifq(zt|zt‚àí1)is a sufÔ¨Åciently narrow Gaussian then q(zt‚àí1)will vary only
a small amount over the region in which q(zt|zt‚àí1)has signiÔ¨Åcant mass, and hence
q(zt‚àí1|zt)will also be approximately Gaussian This intuition can be conÔ¨Årmed
using a simple example as shown in Figures 20.3 and20.4 However, since the
variances at each step are small, we must use a large number of steps to ensure that
the distribution over the Ô¨Ånal latent variable zTobtained from the forward noising
process will still be close to a Gaussian, and this increases the cost of generating new
samples In practice, Tmay be several thousand We can see more formally that q(zt‚àí1|zt)will be approximately Gaussian by
zt‚àí1
ztq(zt|zt‚àí1)
zt
zt‚àí1q(zt‚àí1|zt)
q(zt‚àí1)
Figure
20.4 As in Figure 20.3 but in which the Gaussian distribution q(zt|zt‚àí1)in the left-hand plot has a much
smaller variance t We see that the corresponding distribution q(zt‚àí1|zt)shown in blue on the right-hand plot
is close to being Gaussian, with a similar variance to q(zt|zt‚àí1) Reverse Decoder 587
making a Taylor series expansion of lnq(zt‚àí1|zt)around the point ztas a function
ofzt‚àí1 This also shows that for small variance, the reverse distribution q(zt|zt‚àí1) Exercise 20.7
will have a covariance that is close to the covariance tIof the forward noise process
q(zt‚àí1|zt) We therefore model the reverse process using a Gaussian distribution of
the form
p(zt‚àí1|zt;w) =N(zt‚àí1|(zt;w;t);tI) (20.18)
where(zt;w;t)is a deep neural network governed by a set of parameters w Note
that the network takes the step index texplicitly as an input so that it can account
for the variation of the variance tacross different steps of the chain This allows us
to use a single network to invert all the steps in the Markov chain, instead of having
to learn a separate network for each step

============================================================

=== CHUNK 569 ===
Palavras: 444
Caracteres: 3056
--------------------------------------------------
It is also possible to learn the covariances
of the denoising process by incorporating further outputs in the network to account
for the curvature in the distribution q(zt‚àí1)in the neighbourhood of zt(Nichol and
Dhariwal, 2021) There considerable Ô¨Çexibility in the choice of architecture for the
neural network used to model (zt;w;t)provided the output has the same dimen-
sionality as the input Given this restriction, a U-net architecture is a common choice Section 10.5.4
for image processing applications The overall reverse denoising process then takes the form of a Markov chain
given by
p(x;z1;:::;zT|w) =p(zT)/braceleftBiggT/productdisplay
t=2p(zt‚àí1|zt;w)/bracerightBigg
p(x|z 1;w): (20.19)
Herep(zT)is assumed to be the same as the distribution of q(zT)and hence is
given byN(zT|0;I) Once the model has been trained, sampling is straightforward
because we Ô¨Årst sample from the simple Gaussian p(zT)and then we sample se-
quentially from each of the conditional distributions p(zt‚àí1|zt;w)in turn, Ô¨Ånally
sampling from p(x|z 1;w)to obtain a sample xin the data space 20.2.1 Training the decoder
We next have to decide on an objective function for training the neural network The obvious choice is the likelihood function, which for data point xis given by
p(x|w ) =/integraldisplay
¬∑¬∑¬∑/integraldisplay
p(x;z1;:::;zT|w) dz1:::dzT (20.20)
in whichp(x;z1;:::;zT|w)is deÔ¨Åned by (20.19) This is an instance of the general
latent-variable model (16.81) in which the latent variables comprise z= (z1;:::;zT)
and the observed variable is x Note that the latent variables all have the same dimen-
sionality as the data space, as was the case for normalizing Ô¨Çows but not for varia-
tional autoencoders or generative adversarial networks We see from (20.20) that the
likelihood involves integrating over all possible trajectories by which noise samples
could give rise to the observed data point The integrals in (20.20) are intractable as
they involve integrating over the highly complex neural network functions DIFFUSION MODELS
20.2.2 Evidence lower bound
Since the exact likelihood is intractable, we can adopt a similar approach to that
used with variational autoencoders and maximize a lower bound on the log likelihood
called the evidence lower bound (ELBO), which we re-derive here in the context of Section 16.3
diffusion models For any choice of distribution q(z), the following relation always
holds:
lnp(x|w ) =L(w) + KL (q(z)/bardblp(z|x; w)) (20.21)
whereLis the evidence lower bound, also known as the variational lower bound,
given by
L(w) =/integraldisplay
q(z) ln/braceleftbiggp(x;z|w)
q(z)/bracerightbigg
dz (20.22)
and the Kullback‚ÄìLeibler divergence KL (f/bardblg)between two probability densities
f(z)andg(z)is deÔ¨Åned by Section 2.5.7
KL (f(z)/bardblg (z)) =‚àí/integraldisplay
f(z) ln/braceleftbiggg(z)
f(z)/bracerightbigg
dz: (20.23)
To verify the relation (20.21) Ô¨Årst note that, from the product rule of probability, we
have
p(x;z|w) =p(z|x; w)p(x|w ): (20.24)
Substituting (20.24) into (20.22) and making use of (20.23) gives (20.21)

============================================================

=== CHUNK 570 ===
Palavras: 379
Caracteres: 2597
--------------------------------------------------
The Exercise 20.8
Kullback‚ÄìLeibler divergence has the property KL (¬∑/bardbl¬∑)>0from which it follows Section 2.5.7
that
lnp(x|w )>L(w): (20.25)
Since the log likelihood function is intractable, we train the neural network by max-
imizing the lower bound L(w) To do this, we Ô¨Årst derive an explicit form for the lower bound of the diffusion
model In deÔ¨Åning the lower bound we are free to choose any form we like for q(z)as
long as it is a valid probability distribution, i.e., that it is non-negative and integrates
to1 With many applications of the ELBO, such as the variational autoencoder, we
chose a form for q(z)that has adjustable parameters, often in the form of a deep
neural network, and then we maximize the ELBO with respect to those parameters
as well as with respect to the parameters of the distribution p(x;z|w) Optimizing
the distribution q(z)encourages the bound to be tight, which brings the optimization
of the parameters in p(x;z|w)closer to that of maximum likelihood With diffusion
models, however, we chose q(z)to be given by the Ô¨Åxed distributionq(z1;:::;zT|x)
deÔ¨Åned by the Markov chain (20.5), and so the only adjustable parameters are those
in the model p(x;z1;:::;zT|w)for the reverse Markov chain Note that we are
using the Ô¨Çexibility in the choice of q(z)to select a form that depends on x We therefore substitute for q(z1;:::;zT|x)in (20.21) using (20.5), and likewise
we substitute for p(x;z1;:::;zT|w)using (20.19), which allows us to write the
20.2 Reverse Decoder 589
ELBO in the form
L(w) =EqÔ£Æ
Ô£∞lnp(zT)/braceleftBig/producttextT
t=2p(zt‚àí1|zt;w)/bracerightBig
p(x|z 1;w)
q(z1|x)/producttextT
t=2q(zt|zt‚àí1;x)Ô£π
Ô£ª
=Eq/bracketleftBigg
lnp(zT) +T/summationdisplay
t=2lnp(zt‚àí1|zt;w)
q(zt|zt‚àí1;x)‚àílnq(z1|x) + lnp(x|z 1;w)/bracketrightBigg
(20.26)
where we have deÔ¨Åned
Eq[¬∑]‚â°/integraldisplay
¬∑¬∑¬∑/integraldisplay
q(z1|x)T/productdisplay
t=2q(zt|zt‚àí1) [¬∑] dz 1:::dzT: (20.27)
The Ô¨Årst term lnp(zT)on the right-hand side of (20.26) is just the Ô¨Åxed distri-
butionN(zT|0;I) This has no trainable parameters and can therefore be omitted
from the ELBO since it represents a Ô¨Åxed additive constant Similarly, the third term
‚àílnq(z1|x)is independent of wand so again can be omitted The fourth term on the right-hand side of (20.26) corresponds to the reconstruc-
tion term from the variational autoencoder It can be evaluated by approximating the
expectation Eq[¬∑]by a Monte Carlo estimate obtained by drawing samples from the
distribution over z1deÔ¨Åned by (20.2) so that
Eq[lnp(x|z 1;w)]/similarequalL/summationdisplay
l=1lnp(x|z(l)
1;w) (20.28)
where z(l)
1‚àºN (z1|‚àö1‚àí1x; 1I)

============================================================

=== CHUNK 571 ===
Palavras: 364
Caracteres: 2548
--------------------------------------------------
Unlike with V AEs we do not need to back-
propagate an error signal through the sampled value because the q-distribution is
Ô¨Åxed and so there is no need here for the reparameterization trick Section 19.2.2
This leaves the second term on the right-hand side of (20.26), which comprises a
sum of terms each of which is dependent on a pair of adjacent latent-variable values
zt‚àí1andzt We saw earlier when we derived the diffusion kernel (20.6) that we can
sample from q(zt‚àí1|x)directly as a Gaussian distribution and we could then obtain
a corresponding sample of ztusing (20.4), which is also a Gaussian Although this
would be a correct procedure in the limit of an inÔ¨Ånite number of samples, the use of
pairs of sampled values creates very noisy estimates with high variance, so that an
unnecessarily large numbers of samples is required Instead, we rewrite the ELBO
in a form that can be estimated by sampling just one value per term 20.2.3 Rewriting the ELBO
Following our discussion of the ELBO for the variational autoencoder, our goal
here is to write the ELBO in terms of Kullback‚ÄìLeibler divergences, which we can
then subsequently express in closed form The neural network is a model of the
distribution in the reverse direction p(zt‚àí1|zt;w)whereas the q-distribution is ex-
pressed in the forward direction q(zt|zt‚àí1;x), and so we use Bayes‚Äô theorem to
590 20 DIFFUSION MODELS
reverse the conditional distribution by writing
q(zt|zt‚àí1;x) =q(zt‚àí1|zt;x)q(zt|x)
q(zt‚àí1|x): (20.29)
This allows us to write the second term in (20.26) in the form
lnp(zt‚àí1|zt;w)
q(zt|zt‚àí1;x)= lnp(zt‚àí1|zt;w)
q(zt‚àí1|zt;x)+ lnq(zt‚àí1|x)
q(zt|x): (20.30)
The second term on the right-hand side of (20.30) is independent of wand so can be
omitted Substituting (20.30) into (20.26), we then obtain
L(w) =Eq/bracketleftBiggT/summationdisplay
t=2lnp(zt‚àí1|zt;w)
q(zt‚àí1|zt;x)+ lnp(x|z 1;w)/bracketrightBigg
: (20.31)
Finally, we can rewrite (20.31) in the form Exercise 20.9
L(w) =/integraldisplay
q(z1|x) lnp(x|z 1;w) dz1
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
reconstruction term
‚àíT/summationdisplay
t=2/integraldisplay
KL(q (zt‚àí1|zt;x)/bardblp(zt‚àí1|zt;w))q(zt|x) dzt
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
consistency terms(20.32)
where we have simpliÔ¨Åed the expectation over q(z1;:::;zT|x)in the Ô¨Årst term since
z1is the only latent variable appearing in the integrand Therefore in the expectation
deÔ¨Åned by (20.27), all the conditional distributions integrate to unity leaving only
the integral over z1

============================================================

=== CHUNK 572 ===
Palavras: 406
Caracteres: 2549
--------------------------------------------------
Likewise, in the second term, each integral involves only two
adjacent latent variables zt‚àí1andzt, and all remaining variables can be integrated
out The bound (20.32) is now very similar to the ELBO for the variational autoen-
coder given by (19.14), except that there are now multiple encoder and decoder
stages The reconstruction term rewards high probability for the observed data sam-
ple and can be trained in the same way as the corresponding term in the V AE by using
the sampling approximation (20.28) The consistency terms in (20.32) are deÔ¨Åned Chapter 19
between pairs of Gaussian distributions and therefore can be expressed in closed
form, as follows The distribution q(zt‚àí1|zt;x)is given by (20.15) whereas the dis-
tributionp(zt‚àí1|zt;w)is given by (20.18) and so the Kullback‚ÄìLeibler divergence
becomes Exercise 20.11
KL(q (zt‚àí1|zt;x)/bardblp(zt‚àí1|zt;w))
=1
2t/bardblmt(x;zt)‚àí(zt;w;t)/bardbl2+ const (20.33)
20.2 Reverse Decoder 591
where mt(x;zt)is deÔ¨Åned by (20.16) and where any additive terms that are inde-
pendent of the network parameters whave been absorbed into the constant term,
which plays no role in training Each of the consistency terms in (20.32) has one
remaining integral over zt, weighted by q(zt|x) This can be approximated by draw-
ing a sample from q(zt|x), which can be done efÔ¨Åciently using the diffusion kernel
(20.6) We see that the KL divergence (20.33) takes the form of a simple squared-loss
function Since we adjust the network parameters to maximize the lower bound in
(20.32), we will be minimizing this squared error because there is a minus sign in
front of the Kullback‚ÄìLeibler divergence terms in the ELBO 20.2.4 Predicting the noise
One modiÔ¨Åcation that leads to higher quality results is to change the role of the
neural network so that instead of predicting the denoised image at each step of the
Markov chain it predicts the total noise component that was added to the original
image to create the noisy image at that step (Ho, Jain, and Abbeel, 2020) To do this
we Ô¨Årst take (20.8) and rearrange to give
x=1‚àötzt‚àí‚àö1‚àít‚àött: (20.34)
If we now substitute this into (20.16) we can rewrite the mean mt(x;zt)of the
reverse conditional distribution q(zt‚àí1|zt;x)in terms of the original data vector x
and the noiseto give Exercise 20.12
mt(x;zt) =1‚àö1‚àít/braceleftbigg
zt‚àít‚àö1‚àítt/bracerightbigg
: (20.35)
Similarly, instead of a neural network (zt;w;t)that predicts the denoised image,
we introduce a neural network g(zt;w;t)that aims to predict the total noise that was
added to xto generate zt

============================================================

=== CHUNK 573 ===
Palavras: 364
Caracteres: 2396
--------------------------------------------------
Following the same steps that led to (20.35) shows that
these two network functions are related by
(zt;w;t) =1‚àö1‚àít/braceleftbigg
zt‚àít‚àö1‚àítg(zt;w;t)/bracerightbigg
: (20.36)
We can now substitute (20.35) and (20.36) into (20.33) to give
KL(q (zt‚àí1|zt;x)/bardblp(zt‚àí1|zt;w))
=t
2(1‚àít)(1‚àít)/bardblg(zt;w;t)‚àít/bardbl2+ const
=t
2(1‚àít)(1‚àít)/vextenddouble/vextenddoubleg(‚àötx+‚àö
1‚àítt;w;t)‚àít/vextenddouble/vextenddouble2+ const (20.37)
where in the Ô¨Ånal line we have substituted for ztusing (20.8) DIFFUSION MODELS
The reconstruction term in the ELBO (20.32) can be approximated using (20.28)
with a sampled value of z1 Using the form (20.18) for p(x|z; w)we have
lnp(x|z 1;w) =‚àí1
21/bardblx‚àí(z1;w;1)/bardbl2+ const: (20.38)
If we substitute for (z1;w;1)using (20.36) and we substitute for xusing (20.1)
and then make use of 1= (1‚àí1), which follows from (20.7), we obtain Exercise 20.13
lnp(x|z 1;w) =‚àí1
2(1‚àít)/bardblg(z1;w;1)‚àí1/bardbl2+ const: (20.39)
This is precisely the same form as (20.37) for the special case t= 1, and so the
reconstruction and consistency terms can be combined Ho, Jain, and Abbeel (2020) found empirically that performance is further im-
proved simply by omitting the factor t=2(1‚àít)(1‚àít)in front of (20.37), so
that all steps in the Markov chain have equal weighting Substituting this simpliÔ¨Åed
version of (20.37) into (20.33) gives a training objective function in the form
L(w) =‚àíT/summationdisplay
t=1/vextenddouble/vextenddoubleg(‚àötx+‚àö
1‚àítt;w;t)‚àít/vextenddouble/vextenddouble2: (20.40)
The squared error on the right-hand side of (20.40) has a very simple interpretation:
for a given step tin the Markov chain and for a given training data point x, we sample
a noise vector tand use this to create the corresponding noisy latent vector ztfor
that step The loss function is then the squared difference between the predicted
noise and the actual noise Note that the network g(¬∑;¬∑;¬∑)is predicting the total noise
added to the original data vector x, not just the incremental noise added in step t When we use stochastic gradient descent, we evaluate the gradient vector of the
loss function with respect to the network parameters for a randomly selected data
pointxfrom the training set Also, for each such data point we randomly select a
steptalong the Markov chain, rather than evaluate the error for every term in the
summation over tin (20.40)

============================================================

=== CHUNK 574 ===
Palavras: 375
Caracteres: 2419
--------------------------------------------------
These gradients are accumulated over mini-batches of
data samples and then used to update the weights Also note that this loss function automatically builds in a form of data augmen-
tation, because every time a particular training sample xis used it is combined with a
fresh sampletof noise All the above relates to a single data point xfrom the train-
ing set The corresponding computation of the gradient is shown in Algorithm 20.1 20.2.5 Generating new samples
Once the network has been trained we can generate new samples in the data
space by Ô¨Årst sampling from the Gaussian distribution p(zT)and then denoising
successively through each step of the Markov chain Given a denoised sample ztat
stept, we generate a sample zt‚àí1in three steps First we evaluate the output of the
neural network given by g(zt;w;t) From this we evaluate (zt;w;t)using (20.36) Reverse Decoder 593
Algorithm 20.1: Training a denoising diffusion probabilistic model
Input: Training dataD={xn}
Noise schedule{1;:::;T}
Output: Network parameters w
fort‚àà{1;:::;T}do
t‚Üê/producttextt
=1(1‚àí)// Calculate alphas from betas
end for
repeat
x‚àºD // Sample a data point
t‚àº{1;:::;T}// Sample a point along the Markov chain
‚àºN(|0;I)// Sample a noise vector
zt‚Üê‚àötx+‚àö1‚àít// Evaluate noisy latent variable
L(w)‚Üê/bardblg (zt;w;t)‚àí/bardbl2// Compute loss term
Take optimizer step
until converged
return w
Finally we generate a sample zt‚àí1fromp(zt‚àí1|zt;w) =N(zt‚àí1|(zt;w;t);tI)
by adding noise scaled by the variance so that
zt‚àí1=(zt;w;t) +/radicalbig
t (20.41)
where‚àºN (|0;I) Note that the network g(¬∑;¬∑;¬∑)predicts the total noise added
to the original data vector xto obtain zt, but in the sampling step, we subtract off
only a fraction t=‚àö1‚àítof this noise from zt‚àí1and then add additional noise
with variance tto generate zt‚àí1 At the Ô¨Ånal step when we calculate a synthetic
data sample x, we do not add additional noise since we are aiming to generate a
noise-free output The sampling procedure is summarized in Algorithm 20.2 The main drawback of diffusion models for generating data is that they require
multiple sequential inference passes through the trained network, which can be com-
putationally expensive One way to speed up the sampling process is Ô¨Årst to convert
the denoising process to a differential equation over continuous time and then to use
alternative efÔ¨Åcient discretization methods to solve the equation efÔ¨Åciently

============================================================

=== CHUNK 575 ===
Palavras: 356
Caracteres: 2328
--------------------------------------------------
Section 20.3.4
We have assumed in this chapter that the data and latent variables are continuous
and that we can therefore use Gaussian noise models Diffusion models can also
be deÔ¨Åned for discrete spaces (Austin et al., 2021), for example, to generate new
candidate drug molecules in which part of the generation process involves choosing
atom types from a subset of chemical elements We have seen that diffusion models can be computationally intensive because
594 20 DIFFUSION MODELS
Algorithm 20.2: Sampling from a denoising diffusion probabilistic model
Input: Trained denoising network g(z;w;t)
Noise schedule{1;:::;T}
Output: Sample vector xin data space
zT‚àºN(z|0;I)// Sample from final latent space
fort‚ààT;:::; 2do
t‚Üê/producttextt
=1(1‚àí)// Calculate alpha
// Evaluate network output
(zt;w;t)‚Üê1‚àö1‚àít/braceleftBig
zt‚àít‚àö1‚àítg(zt;w;t)/bracerightBig
‚àºN(|0;I)// Sample a noise vector
zt‚àí1‚Üê(zt;w;t) +‚àöt// Add scaled noise
end for
x=1‚àö1‚àí1/braceleftBig
z1‚àí1‚àö1‚àí1g(z1;w;t)/bracerightBig
// Final denoising step
return x
they sequentially reverse a noise process that can have hundreds or thousands of
steps Song, Meng, and Ermon (2020) introduced a related technique called de-
noising diffusion implicit models that relax the Markovian assumption on the noise
process while retaining the same objective function for training This thereby allows
one or two orders of magnitude speed-up during sampling without degrading the
quality of the generated samples Score Matching
The denoising diffusion models discussed so far in this chapter are closely related to
another class of deep generative models that were developed relatively independently
and which are based on score matching (Hyv ¬®arinen, 2005; Song and Ermon, 2019) These make use of the score function orStein score, which is deÔ¨Åned as the gradient
of the log likelihood with respect to the data vector xand is given by
s(x) =‚àáxlnp(x): (20.42)
Here it is important to emphasize that the gradient is with respect to the data vector,
not with respect to any parameter vector Note that s(x) is a vector-valued function
of the same dimensionality as xand that each element si(x) =@lnp(x)=@xiis
associated with a corresponding element xiofx For example, if xis an image then
s(x) can also be represented as an image of the same dimensions with corresponding
20.3

============================================================

=== CHUNK 576 ===
Palavras: 354
Caracteres: 2117
--------------------------------------------------
Score Matching 595
Figure 20.5 Illustration of the score func-
tion, showing a distribution in
two dimensions comprising a
mixture of Gaussians repre-
sented as a heat map and the
corresponding score function
deÔ¨Åned by (20.42) plotted as
vectors on a regular grid of x-
values Figure 20.5 shows an example of a probability density in two dimensions,
along with the corresponding score function To see why the score function is useful, consider two functions q(x)andp(x)
that have the property that their scores are equal, so that ‚àáxlnq(x) =‚àáxlnp(x)
for all values of x If we integrate both sides of the equation with respect to xand
take exponentials, we obtain q(x) =Kp(x) whereKis a constant independent of
x So if we are able to learn a model s(x;w)of the score function then we have
modelled the original data density, up to a multiplicative constant 20.3.1 Score loss function
To train such a model we need to deÔ¨Åne a loss function that aims to match the
model score function s(x;w)to the score function ‚àáxlnp(x) of the distribution
p(x) that generated the data An example of such a loss function is the expected
squared error between the model score and the true score, given by
J(w) =1
2/integraldisplay
/bardbl
s(x;w)‚àí‚àá xlnp(x)/bardbl2p(x) d x: (20.43)
As we saw in the discussion of energy-based models, the score function does Section 14.3.1
not require the associated probability density to be normalized, because the normal-
ization constant is removed by the gradient operator, and so there is considerable
Ô¨Çexibility in the choice of model There are broadly two ways to represent the score
function s(x;w)using a deep neural network Each element siofscorresponds to
one of the elements xiofx, so the Ô¨Årst approach is to have a network with the same
number of outputs as inputs However, the score function is deÔ¨Åned to be the gradient
of a scalar function (the log probability density), which is a more restricted class of
functions So an alternative approach is to have a network with a single output (x) Exercise 20.14
596 20 DIFFUSION MODELS
and then to compute ‚àáx(x) using automatic differentiation

============================================================

=== CHUNK 577 ===
Palavras: 364
Caracteres: 2443
--------------------------------------------------
This second approach,
however, requires two backpropagation steps and is therefore computationally more
expensive For this reason, most applications simply adopt the Ô¨Årst approach Exercise 20.15
20.3.2 ModiÔ¨Åed score loss
One problem with the loss function (20.43) is that we cannot minimize it directly
because we do not know the true data score ‚àáxlnp(x) All we have is the Ô¨Ånite data
setD= (x1;:::;xN)from which we can construct an empirical distribution:
pD(x) =1
NN/summationdisplay
n=1(x‚àíxn): (20.44)
Here(x)is the Dirac delta function, which can be thought of informally as an
inÔ¨Ånitely tall ‚Äòspike‚Äô at x=0with the properties
(x) = 0;x/negationslash=0 (20.45)/integraldisplay
(x) dx= 1: (20.46)
Since (20.44) is not a differentiable function of x, we cannot compute its score func-
tion We can address this by introducing a noise model to ‚Äòsmear out‚Äô the data points
and give a smooth, differentiable representation of the density This is known as a
Parzen estimator orkernel density estimator and is deÔ¨Åned by Section 3.5.2
q(z) =/integraldisplay
q(z|x; )p(x) d x (20.47)
whereq(z|x; )is the noise kernel A common choice of kernel is the Gaussian
q(z|x; ) =N(z|x;2I): (20.48)
Instead of minimizing the loss function (20.43), we then use the corresponding
loss with respect to the smoothed Parzen density in the form
J(w) =1
2/integraldisplay
/bardbls(z;w)‚àí‚àá zlnq(z)/bardbl2q(z) dz: (20.49)
A key result is that by substituting (20.47) into (20.49) we can rewrite this loss
function in an equivalent form given by (Vincent, 2011) Exercise 20.17
J(w) =1
2/integraldisplay/integraldisplay
/bardbls(z;w)‚àí‚àá zlnq(z|x; )/bardbl2q(z|x; )p(x) d zdx+ const:
(20.50)
If we substitute for p(x) using the empirical density (20.44), we obtain
J(w) =1
2NN/summationdisplay
n=1/integraldisplay
/bardbls(z;w)‚àí‚àá zlnq(z|xn;)/bardbl2q(z|xn;) dz+const: (20.51)
20.3 Score Matching 597
Figure 20.6 Examples of sampling trajec-
tories obtained using Langevin
dynamics deÔ¨Åned by (14.61) for
the distribution shown in Fig-
ure 20.5, showing three trajec-
tories all starting at the centre of
the plot F
or the Gaussian Parzen kernel (20.48), the score function becomes
‚àázlnq(z|x; ) =‚àí1
 (20.52)
where=z‚àíxis
drawn fromN(z|0;I) If we consider the speciÔ¨Åc noise model
(20.6) then we obtain
‚àázlnq(z|x; ) =‚àí1‚àö1‚àít
: (20.53)
We therefore see that the score loss (20.50) measures the difference between the
neural network prediction and the noise 

============================================================

=== CHUNK 578 ===
Palavras: 385
Caracteres: 2385
--------------------------------------------------
Therefore, this loss function has the same
minimum as the form (20.37) used in the denoising diffusion model, with the score
function s(z;w)playing the same role as the noise prediction network g(z;w)up
to a constant scaling ‚àí1=‚àö1‚àít(Song
and Ermon, 2019) Minimizing (20.50) is
known as denoising score matching, and we see the close connection to denoising
diffusion models There remains the question of how to choose the noise variance
2, and we will return to this shortly Having trained a score-based model we then need to draw new samples Langevin
dynamics is well-suited to score-based models because it is based on the score func-
tion and therefore does not require a normalized probability distribution, and is il- Section 14.3
lustrated in Figure 20.6 20.3.3 Noise variance
We have seen how to learn the score function from a set of training data and how
to generate new samples from the learned distribution using Langevin sampling However, we can identify three potential problems with this approach (Song and
598 20 DIFFUSION MODELS
Ermon, 2019; Luo, 2022) First, if the data distribution lies on a manifold of lower
dimensionality than the data space, the probability density will be zero at points off Chapter 16
the manifold and here the score function is undeÔ¨Åned since lnp(x) is undeÔ¨Åned Second, in regions of low data density, the estimate of the score function may be
inaccurate since the loss function (20.43) is weighted by the density An inaccurate
score function can lead to poor trajectories when using Langevin sampling Third,
even with an accurate model of the score function, the Langevin procedure may not
sample correctly if the data distribution comprises a mixture of disjoint distributions Exercise 20.18
All three problems can be addressed by choosing a sufÔ¨Åciently large value for
the noise variance 2used in the kernel function (20.48), because this smears out the
data distribution However, too large a variance will introduce a signiÔ¨Åcant distortion
of the original distribution and this itself introduces inaccuracies in the modelling of
the score function This trade-off can be addressed by considering a sequence of
variance values 2
1< 2
2< ::: < 2
T(Song and Ermon, 2019), in which 2
1is
sufÔ¨Åciently small that the data distribution is accurately represented whereas 2
Tis
sufÔ¨Åciently large that the aforementioned problems are avoided

============================================================

=== CHUNK 579 ===
Palavras: 353
Caracteres: 2385
--------------------------------------------------
The score network
is then modiÔ¨Åed to take the variance as an additional input s(x;w;2)and is trained
by using a loss function that is a weighted sum of the loss functions of the form
(20.51) in which each term represents the error between the associated network and
the corresponding perturbed data set For a data vector xn, the loss function then
takes the form
1
2L/summationdisplay
i=1(i)/integraldisplay/vextenddouble/vextenddoubles(z;w;2
i)‚àí‚àá zlnq(z|xn;i)/vextenddouble/vextenddouble2q(z|xn;i) dz (20.54)
where(i) are weighting coefÔ¨Åcients We see that this training procedure precisely
mirrors that used to train hierarchical denoising networks Section 20.2.1
Once trained, samples can be generated by running a few steps of Langevin
sampling from each of the models for i=L;L‚àí1;:::; 2;1in turn This technique
is called annealed Langevin dynamics, and is analogous to Algorithm 20.2 used to
sample from denoising diffusion models 20.3.4 Stochastic differential equations
We have seen that it is helpful to use a large number of steps, often several
thousand, when constructing the noise process for a diffusion model It is therefore
natural to ask what happens if we consider the limit of an inÔ¨Ånite number of steps,
much as we did for inÔ¨Ånitely deep neural networks when we introduced neural dif-
ferential equations In taking such a limit, we need to ensure that the noise variance Section 18.3.1
tat each step becomes smaller in keeping with the step size This leads to a formu-
lation of diffusion models for continuous time as stochastic differential equations or
SDEs (Song et al., 2020) Both denoising diffusion probabilistic models and score
matching models can then be viewed as a discretization of a continuous-time SDE We can write a general SDE as an inÔ¨Ånitesimal update to the vector zin the form
dz=f(z;t) dt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
drift+g(t) dv/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
diffusion(20.55)
20.4 Guided Diffusion 599
where the drift term is deterministic, as in an ODE, but the diffusion term is stochas-
tic, for example given by inÔ¨Ånitesimal Gaussian steps Here the parameter tis often
called ‚Äòtime‚Äô by analogy with physical systems The forward noise process (20.3)
for a diffusion model can be written as an SDE of the form (20.55) by taking the
continuous-time limit

============================================================

=== CHUNK 580 ===
Palavras: 368
Caracteres: 2310
--------------------------------------------------
Exercise 20.19
For the SDE (20.55), there is a corresponding reverse SDE (Song et al., 2020)
given by
dz=/braceleftbig
f(z;t)‚àíg2(t)‚àá zlnp(z)/bracerightbig
dt+g(t) dv (20.56)
where we recognize ‚àázlnp(z) as the score function The SDE given by (20.55) is
to be solved in reverse from t=Ttot= 0 To solve an SDE numerically, we need to discretize the time variable The
simplest approach is to use Ô¨Åxed, equally spaced time steps, which is known as
the Euler‚ÄìMaruyama solver For the reverse SDE, we then recover a form of the
Langevin equation However, more sophisticated solvers can be employed that use Section 14.3
more Ô¨Çexible forms of discretization (Kloeden and Platen, 2013) For all diffusion processes governed by an SDE, there exists a corresponding de-
terministic process described by an ODE whose trajectories have the same marginal
probability densities p(z|t) as the SDE (Song et al., 2020) For an SDE of the form
(20.56), the corresponding ODE is given by
dz
d
t=f(z;t)‚àí1
2g2(
t)‚àázlnp(z): (20.57)
The ODE formulation allows the use of efÔ¨Åcient adaptive-step solvers to reduce the
number of function evaluations dramatically Moreover, it allows probabilistic dif-
fusion models to be related to normalizing Ô¨Çow models, from which the change- Chapter 18
of-variables formula (18.1) can be used to provide an exact evaluation of the log
likelihood Guided
Diffusion
So
far, we have considered diffusion models as a way to represent the unconditional
densityp(x) learned from a set of training examples x1;:::;xNdrawn indepen-
dently from p(x) Once the model has been trained, we can generate new samples
from this distribution We have already seen an example of unconditional sampling
from a deep generative model for face images in Figure 1.3, in that case from a GAN
model In many applications, however, we want to sample from a conditional distribu-
tionp(x|c) where the conditioning variable ccould, for example, be a class label or
a textual description of the desired content for an image This also forms the basis
for applications such as image super-resolution, image inpainting, video generation,
and many others The simplest approach to achieving this would be to treat cas an
additional input into the denoising neural network g(z;w;t;c)and then to train the
network using matched pairs {xn;cn}

============================================================

=== CHUNK 581 ===
Palavras: 390
Caracteres: 2527
--------------------------------------------------
The main limitation of this approach is that
600 20 DIFFUSION MODELS
the network can give insufÔ¨Åcient weight to, or even ignore, the conditioning vari-
ables, so we need a way to control how much weight is given to the conditioning
information and to trade this off against sample diversity This additional pressure
to match the conditioning information is called guidance There are two main ap-
proaches to guidance depending on whether or not a separate classiÔ¨Åer model is
used 20.4.1 ClassiÔ¨Åer guidance
Suppose that a trained classiÔ¨Åer p(c|x) is available, and consider a diffusion
model from the perspective of the score function Using Bayes‚Äô theorem we can
write the score function for the conditional diffusion model in the form
‚àáxlnp(x|c) =‚àáxln/braceleftbiggp(c|x)p(x)
p(c)/bracerightbigg
=‚àáxlnp(x) +‚àáxlnp(c|x) (20.58)
where we have used ‚àáxlnp(c) = 0 sincep(c) is independent of x The Ô¨Årst term
on the right-hand side of (20.58) is the usual unconditional score function, whereas
the second term pushes the denoising process towards the direction that maximizes
the probability of the given label cunder the classiÔ¨Åer model (Dhariwal and Nichol,
2021) The inÔ¨Çuence of the classiÔ¨Åer can be controlled by introducing a hyperpa-
rameter, called the guidance scale, which controls the weight given to the classiÔ¨Åer
gradient The score function used for sampling then becomes
score(x; c;) =‚àáxlnp(x) +‚àáxlnp(c|x): (20.59)
If= 0we recover the original unconditional diffusion model, whereas if = 1we
obtain the score corresponding to the conditional distribution p(x|c) For >1the
model is strongly encouraged to respect the conditioning label, and values of /greatermuch1
may be used, for example = 10 However, this comes at the expense of diversity in
the samples as the model prefers ‚Äòeasy‚Äô examples that the classiÔ¨Åer is able to classify
correctly One problem with the classiÔ¨Åer-based approach to guidance is that a separate
classiÔ¨Åer must be trained Furthermore, this classiÔ¨Åer needs to be able to classify
examples with varying degrees of noise, whereas standard classiÔ¨Åers are trained on
clean examples We therefore turn to an alternative approach that avoids the use of a
separate classiÔ¨Åer 20.4.2 ClassiÔ¨Åer-free guidance
If we use (20.58) to replace ‚àáxlnp(c|x) in (20.59), we can write the score
function in the form Exercise 20.20
score(x; c;) =‚àáxlnp(x|c) + (1‚àí)‚àá xlnp(x); (20.60)
which for 0<< 1represents a convex combination of the conditional log density
lnp(x|c) and the unconditional log density lnp(x)

============================================================

=== CHUNK 582 ===
Palavras: 354
Caracteres: 2313
--------------------------------------------------
For>1the contribution from
20.4 Guided Diffusion 601
the unconditional score becomes negative, meaning the model actively reduces the
probability of generating samples that ignore the conditioning information in favour
of samples that do Furthermore, we can avoid training separate networks to model p(x|c) andp(x)
by training a single conditional model in which the conditioning variable cis set to
a null value, for example c=0, with some probability during training, typically
around 10‚Äì20% Then p(x) is represented by p(x|c =0) This is somewhat anal-
ogous to dropout in which the conditioning inputs are collectively set to zero for a Section 9.6.1
random subset of training vectors Once trained, the score function (20.60) is then used to encourage a strong
weighting of the conditional information In practice, classiÔ¨Åer-free guidance gives
much higher quality results than classiÔ¨Åer guidance (Nichol et al., 2021; Saharia et
al., 2022) The reason is that a classiÔ¨Åer p(c|x) can ignore most of the input vector
xas long as it makes a good prediction of cwhereas classiÔ¨Åer-free guidance is based
on the conditional density p(x|c), which must assign a high probability to all aspects
ofx Text-guided diffusion models can leverage techniques from large language mod-
els to allow the conditioning input to be a general text sequence, known as a prompt, Chapter 12
and not simply a selection from a predeÔ¨Åned set of class labels This allows the text
input to inÔ¨Çuence the denoising process in two ways, Ô¨Årst by concatenating the inter-
nal representation from a transformer-based language model with the input to the de-
noising network and second by allowing cross-attention layers within the denoising
network to attend to the text token sequence ClassiÔ¨Åer-free guidance, conditioned
on a text prompt, is illustrated in Figure 20.7 Another application for conditional diffusion models is image super-resolution
in which a low-resolution image is transformed into a corresponding high-resolution
image This is intrinsically an inverse problem, and multiple high-resolution im-
ages will be consistent with a given low-resolution image Super-resolution can
be achieved by denoising a high-resolution sample from a Gaussian using the low-
resolution image as a conditioning variable (Saharia, Ho, et al., 2021)

============================================================

=== CHUNK 583 ===
Palavras: 350
Caracteres: 2241
--------------------------------------------------
Examples of
this method are shown in Figure 20.8 Such models can be cascaded to achieve very
high resolution (Ho et al., 2021), for example going from 64√ó64to256√ó256,
and then from 256√ó256to1024√ó1024 Each stage is typically represented by a
U-net architecture, with each U-net conditioned on the Ô¨Ånal denoised output of the Section 10.5.4
previous one This type of cascade can also be used with image-generation diffusion models,
in which the image denoising is performed at a lower resolution and the result is sub-
sequently up-sampled using a separate network (which may also take a text prompt
as input) to give a Ô¨Ånal high-resolution output (Nichol et al., 2021; Saharia et al.,
2022) This can signiÔ¨Åcantly reduce the computational cost compared to working
directly in a high-dimensional space since the denoising process may involve hun-
dreds of passes through the denoising network Note that these approaches still work
within the image space directly but at lower resolution A different approach to addressing the high computational cost of applying dif-
fusion models directly in the space of high-resolution images is called latent diffu-
602 20 DIFFUSION MODELS
Figure 20.7 Illustration of classiÔ¨Åer-free guidance of diffusion models, generated from a model called GLIDE
using the conditioning text A stained glass window of a panda eating bamboo Examples on the left were
generated with = 0 (no guidance, just the plain conditional model) whereas examples on the right were
generated with = 3 (2021) with permission.]
sion models (Rombach et al., 2021) Here an autoencoder is Ô¨Årst trained on noise- Section 19.1
free images to obtain a lower-dimensional representation of the images and is then
Ô¨Åxed A U-net architecture is then trained to perform the denoising within the lower-
dimensional space, which itself is not directly interpretable as an image Finally,
the denoised representation is mapped into the high-resolution image space using
the output half of the Ô¨Åxed autoencoder network This approach makes more efÔ¨Å-
cient use of the low-dimensional space, which can then focus on image semantics,
leaving the decoder to create a corresponding sharp, high-resolution image from the
denoised low-dimensional representation

============================================================

=== CHUNK 584 ===
Palavras: 360
Caracteres: 2171
--------------------------------------------------
There are many other applications of conditional image generation including
inpainting, un-cropping, restoration, image morphing, style transfer, colourization,
de-blurring, and video generation (Yang, Srivastava, and Mandt, 2022) An example
of inpainting is shown in Figure 20.9 Exercises 603
Figure 20.8 Two examples of low-
resolution images along with associ-
ated samples of corresponding high-
resolution images generated by a
diffusion model The top row shows
a16√ó16input image and the cor-
responding 128√ó128output image
along with the original image from
which the input image was gener-
ated The bottom row shows a 64√ó
64input image with a 256√ó256out-
put image, again with the original im-
age for comparison [From Saharia,
Ho,et al (2021) with permission.]
Original Input Output
Figure 20.9 Example of inpaint-
ing showing the original image on
the left, an image with sections re-
moved in the middle, and the image
with inpainting on the right [From
Saharia, Chan, Chang, et al (2021)
with permission.]
Exercises
20.1 (?)Using (20.3) write down expressions for the mean and covariance of ztin terms
of the mean and covariance of zt‚àí1 Hence, show that for 0< t<1the mean of
the distribution of ztis closer to zero than the mean of zt‚àí1and that the covariance
ofztis closer to the unit matrix Ithan the covariance of zt‚àí1 20.2 (?)Show that the transformation (20.1) can be written in the equivalent form (20.2) 20.3 (???)In this exercise we use proof by induction to show that the marginal distri-
bution of xtfor the forward process of the diffusion model, as deÔ¨Åned by (20.4), is
given by (20.6) where tis deÔ¨Åned by (20.7) First verify that (20.6) holds when
t= 1 Now assume that (20.6) is true for some particular value of tand derive the
corresponding result for the value t+ 1 To do this, it is easiest to write the for-
ward process using the representation (20.3) and to make use of the result (3.212),
which shows that the sum of two independent Gaussian random variables is itself a
Gaussian in which the means and covariances are additive 20.4 (?)By using the result (20.6), where tis deÔ¨Åned by (20.7), show that in the limit
T‚Üí‚àû we obtain (20.9)

============================================================

=== CHUNK 585 ===
Palavras: 394
Caracteres: 2375
--------------------------------------------------
DIFFUSION MODELS
20.5 (??) Consider two independent random variables aandbalong with a Ô¨Åxed scalar
 Show that
cov[a +b] = cov[ a] + cov[ b] (20.61)
cov[a] =2cov[a]: (20.62)
Use these results to show that if the distribution of zt‚àí1has zero mean and unit
covariance, then the distribution of zt, deÔ¨Åned by (20.3), will also have zero mean
and unit covariance, irrespective of the value of t 20.6 (???) In this exercise we will use the technique of completing the square to derive
the result (20.15) starting from Bayes‚Äô theorem (20.13) First note that the two terms
in the numerator on the right-hand side of (20.13), given by (20.4) and (20.6), both
take the form of exponentials of quadratic functions of zt‚àí1 The required distribu-
tion is therefore a Gaussian, and so we need only to Ô¨Ånd its mean and covariance To
do this, consider only the terms in the exponentials that depend on zt‚àí1and note that
the product of two exponentials is the exponential of the sum of the two exponents Gather together all the terms that are quadratic in zt‚àí1as well as those that are linear
inzt‚àí1and then rearrange them in the form (zt‚àí1‚àímt)TS‚àí1
t(zt‚àí1‚àímt) Then,
by inspection, Ô¨Ånd expressions for mt(x;zt)andSt Note that additive terms that
are independent of zt‚àí1can be ignored 20.7 (???) In this exercise we show that the reverse of the conditional distribution q(zt|zt‚àí1)
for the forward noise process in a diffusion model can be approximated by a Gaus-
sian when the noise variance is small Consider the inverse conditional distribution
q(zt‚àí1|zt)given by Bayes‚Äô theorem in the form (20.11) where the forward distribu-
tionq(zt|zt‚àí1)is given by (20.4) By taking the logarithm of both sides of (20.11)
and then making a Taylor expansion of q(zt‚àí1)centred on the value zt, show that,
for small values of the noise variance t, the distribution q(zt‚àí1|zt)is approximately
a Gaussian with mean ztand covariance tI Find expressions for the lowest-order
corrections to the mean and to the covariance as expansions in powers of t 20.8 (??) By substituting the product rule of probability in the form (20.24) into the deÔ¨Å-
nition (20.22) of the ELBO for the diffusion model and making use of the deÔ¨Ånition
(20.23) of the Kullback‚ÄìLeibler divergence, verify that the log likelihood function
can be written as the sum of a lower bound and a Kullback‚ÄìLeibler divergence in the
form (20.21)

============================================================

=== CHUNK 586 ===
Palavras: 378
Caracteres: 2246
--------------------------------------------------
20.9 (??) Verify that the ELBO for the diffusion model given by (20.31) can be written
in the form (20.32) where the Kullback‚ÄìLeibler divergence is deÔ¨Åned by (20.23) 20.10 (??) When we derived the ELBO for the diffusion model given by (20.32), we omit-
ted the Ô¨Årst and third terms in (20.26) because they are independent of w Similarly
we omitted the second term in the right-hand side of (20.30) because this is also in-
dependent of w Show that if all of these omitted terms are retained they lead to an
additional term in the ELBO L(x) given by
KL (q(zT|x)/bardblp(zT)): (20.63)
Exercises 605
Note that the noise process is constructed in such a way that the distribution q(zT|x)
is equal to the Gaussian N(x|0;I) Similarly, the distribution p(zT)is deÔ¨Åned to be
equal toN(x|0;I), and hence the two distributions in (20.63) are equal and so the
Kullback‚ÄìLeibler divergence vanishes 20.11 (??) By making use of (20.15) for the distribution q(zt‚àí1|zt;x)and (20.18) for the
distributionp(zt‚àí1|zt;w), show that the Kullback‚ÄìLeibler divergence appearing in
the consistency terms in (20.32) is given by (20.33) 20.12 (??) By substituting (20.34) into (20.16) rewrite the mean mt(x;zt)in terms of the
original data vector xand the noise in the form (20.35), where tis deÔ¨Åned by
(20.7) 20.13 (??) Show that the reconstruction term (20.38) in the ELBO for diffusion models can
be written in the form (20.39) To do this, substitute for (z1;w;1)using (20.36)
and substitute for xusing (20.1), and then make use of 1= (1‚àí1), which follows
from (20.7) 20.14 (?)The score function is deÔ¨Åned by s(x) =‚àáxp(x|w )and is therefore a vector of
the same dimensionality as the input vector x Consider a matrix whose elements
are given by
Mij=@si
@xj‚àí@sj
@xi: (20.64)
Show that if the score function is deÔ¨Åned by taking the gradient s=‚àáx(x) of
the output of a neural network with a single output variable (x), then all the matrix
elementsMij= 0for all pairsi;j Note that if the score function s(x) =‚àáxp(x|w )
is instead represented directly by a deep neural network with the same number of
outputs as inputs, then only the diagonal matrix elements Mii= 0, and so the output
of the network does not in general correspond to the gradient of any scalar function

============================================================

=== CHUNK 587 ===
Palavras: 371
Caracteres: 2536
--------------------------------------------------
20.15 (??) Consider a deep neural network representation s(x;w)for the score function
deÔ¨Åned by (20.42), where xandshave dimensionality D Compare the computa-
tional complexity of evaluating the score for a network with Doutputs that represents
the score function directly with one that computes a single scalar function (x;w)
in which the score function is computed indirectly through automatic differentiation Show that the latter approach is typically more computationally expensive 20.16 (???) We cannot minimize the score function (20.43) directly because we do not
know the functional form of the true data density p(x), and therefore we cannot
write down an expression for the score function ‚àáxlnp(x) However, by using
integration by parts (Hyv ¬®arinen, 2005), we can rewrite (20.43) in the form
J(w) =/integraldisplay/braceleftbigg
‚àá¬∑s(x;w) +1
2/bardbls(x; w)/bardbl2/bracerightbigg
p(x) d x+ const (20.65)
606 20 DIFFUSION MODELS
where the constant term is independent of the network parameters w, and the diver-
gence‚àá¬∑s(x;w)is deÔ¨Åned by
‚àá¬∑s=D/summationdisplay
i=1@si
@xi=D/summationdisplay
i=1@2lnp(x)
@x2
i(20.66)
in whichDis the dimensionality of x Derive the result (20.65) by Ô¨Årst expanding the
square in (20.43) and noting that the term involving /bardbls(x; w)/bardbl2already appears in
(20.43) whereas the term involving /bardblsD/bardbl2can be absorbed into the additive constant,
where we have deÔ¨Åned sD=‚àálnpD(x) Now consider the formula
d
dx{p(x)g (x)}=dp(x)
dxg(x) +p(x)dg(x)
dx(20.67)
for the derivative of the product of two functions Integrate both sides of this formula
with respect to xand rearrange to obtain the integration-by-parts formula:
/integraldisplay‚àû
‚àí‚àûdp(x)
dxg(x) dx=‚àí/integraldisplay‚àû
‚àí‚àûdg(x)
dxp(x) dx (20.68)
where we have assumed that p(‚àû) =p(‚àí‚àû) = 0 Apply this result together
with the deÔ¨Ånition sD=‚àálnp(x) to the term involving s(x;w)TsDto complete
the proof Note that the evaluation of the second derivatives in (20.66) requires a
separate backward propagation pass for each derivative and hence has an overall
computational cost that grows quadratically with the dimensionality Dof the data
space (Martens, Sutskever, and Swersky, 2012) This precludes the direct application
of this loss function to spaces of high dimensionality, and so techniques such as
sliced score matching (Song et al., 2019) have been developed to help address this
inefÔ¨Åciency 20.17 (??) In this exercise we show that the score function loss (20.50) is equivalent, up
to an additive constant, to the form (20.49)

============================================================

=== CHUNK 588 ===
Palavras: 364
Caracteres: 2193
--------------------------------------------------
To do this, Ô¨Årst expand the square in
(20.49) and by using (20.47) show that the term in sTsfrom (20.49) is the same as the
corresponding term obtained by expanding the square in (20.50) Next note that the
term in/bardbl‚àázlnq/bardbl2in (20.49) is independent of wand likewise that the corresponding
term in (20.50) is also independent of w, and so these can be viewed as additive
constants in the loss function and play no role in training Finally, consider the cross-
term in (20.49) By substituting for q(z)using (20.47), show that this is equal to the
corresponding cross-term from (20.50) Hence, show that the two loss functions are
equal up to an additive constant 20.18 (?)Consider a probability distribution that consists of a mixture of two disjoint dis-
tributions (i.e., distributions with the property that when one of them is non-zero the
other must be zero) of the form
p(x) =pA(x) + (1‚àí)pB(x): (20.69)
Exercises 607
Show that when the score function, deÔ¨Åned by (20.42), is evaluated for any given
pointx, the mixing coefÔ¨Åcient does not appear From this it follows that Langevin
dynamics deÔ¨Åned by (14.61) will not sample from the two component distributions
with the correct proportions This problem is resolved by adding noise from a broad
distribution, as discussed in the text 20.19 (??) For discrete steps, the forward noise process in a diffusion model is deÔ¨Åned by
(20.3) Here we take the continuous-time limit and convert this to an SDE We Ô¨Årst
introduce a continuously-changing variance function (t)such thatt=(t)‚àÜt By making a Taylor expansion of the square root in the Ô¨Årst term on the right-hand-
side of (20.3), show that the inÔ¨Ånitesimal update can be written in the form
dz=‚àí1
2(t)zdt+/radicalbig
(t) dv: (20.70)
We see that this is a special case of the general SDE (20.55) 20.20 (?)By using (20.58) to replace ‚àáxlnp(c|x), show that the score function in (20.59)
can be written in the form (20.60) Linear Algebra
In this appendix, we gather together some useful properties and identities involving
matrices and determinants This is not intended to be an introductory tutorial, and
it is assumed that the reader is already familiar with basic linear algebra

============================================================

=== CHUNK 589 ===
Palavras: 351
Caracteres: 2320
--------------------------------------------------
For some
results, we indicate how to prove them, whereas in more complex cases we leave
the interested reader to refer to standard textbooks on the subject In all cases, we
assume that inverses exist and that matrix dimensions are such that the formulae
are correctly deÔ¨Åned A comprehensive discussion of linear algebra can be found in
Golub and Van Loan (1996), and an extensive collection of matrix properties is given
by L ¬®utkepohl (1996) Matrix derivatives are discussed in Magnus and Neudecker
(1999) Matrix Identities
A matrix Ahas elements Aijwhereiindexes the rows and jindexes the columns We use INto denote the N√óNidentity matrix (also called the unit matrix), and
if there is no ambiguity over dimensionality, we simply use I The transpose matrix
AThas elements (AT)ij=Aji From the deÔ¨Ånition of a transpose, we have
(AB)T=BTAT; (A.1)
which can be veriÔ¨Åed by writing out the indices The inverse of A, denoted A‚àí1,
satisÔ¨Åes
AA‚àí1=A‚àí1A=I: (A.2)
Because ABB‚àí1A‚àí1=I, we have
(AB)‚àí1=B‚àí1A‚àí1: (A.3)
Also we have/parenleftbig
AT/parenrightbig‚àí1=/parenleftbig
A‚àí1/parenrightbigT; (A.4)
609 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4
610 A LINEAR ALGEBRA
which is easily proven by taking the transpose of (A.2) and applying (A.1) A useful identity involving matrix inverses is the following:
(P‚àí1+BTR‚àí1B)‚àí1BTR‚àí1=PBT(BPBT+R)‚àí1; (A.5)
which is easily veriÔ¨Åed by right-multiplying both sides by (BPBT+R) Suppose
thatPhas dimensionality N√óNand that Rhas dimensionality M√óM, so that B
isM√óN Then ifM/lessmuchN, it will be much cheaper to evaluate the right-hand side
of (A.5) than the left-hand side A special case that sometimes arises is
(I+AB)‚àí1A=A(I+BA)‚àí1: (A.6)
Another useful identity involving inverses is the following:
(A+BD‚àí1C)‚àí1=A‚àí1‚àíA‚àí1B(D +CA‚àí1B)‚àí1CA‚àí1; (A.7)
which is known as the Woodbury identity It can be veriÔ¨Åed by multiplying both sides
by(A+BD‚àí1C) This is useful, for instance, when Ais large and diagonal and
hence easy to invert, and when Bhas many rows but few columns (and conversely
forC), so that the right-hand side is much cheaper to evaluate than the left-hand
side A set of vectors{a1;:::;aN}is said to be linearly independent if the relation/summationtext
nnan= 0 holds only if all n= 0

============================================================

=== CHUNK 590 ===
Palavras: 366
Caracteres: 2676
--------------------------------------------------
This implies that none of the vectors
can be expressed as a linear combination of the remainder The rank of a matrix is
the maximum number of linearly independent rows (or equivalently the maximum
number of linearly independent columns) Traces and Determinants
Square matrices have traces and determinants The trace Tr(A) of a matrix Ais
deÔ¨Åned as the sum of the elements on the leading diagonal By writing out the
indices, we see that
Tr(AB ) =Tr(BA ): (A.8)
By applying this formula multiple times to the product of three matrices, we see that
Tr(ABC) = Tr(CAB) = Tr(BCA); (A.9)
which is known as the cyclic property of the trace operator It clearly extends to the
product of any number of matrices The determinant |A|of anN√óNmatrix Ais
deÔ¨Åned by
|A|=/summationdisplay
(¬±1)A 1i1A2i2¬∑¬∑¬∑ANiN (A.10)
in which the sum is taken over all products consisting of precisely one element from
each row and one element from each column, with a coefÔ¨Åcient +1or‚àí1according
to whether the permutation i1i2:::iNis even or odd, respectively Note that |I|= 1,
A.3 Matrix Derivatives 611
and that the determinant of a diagonal matrix is given by the product of the elements
on the leading diagonal Thus, for a 2√ó2matrix, the determinant takes the form
|A|=/vextendsingle/vextendsingle/vextendsingle/vextendsinglea11a12
a21a22/vextendsingle/vextendsingle/vextendsingle/vextendsingle=a11a22‚àía12a21: (A.11)
The determinant of a product of two matrices is given by
|AB|=|A||B| (A.12)
as can be shown from (A.10) Also, the determinant of an inverse matrix is given by
/vextendsingle/vextendsingleA‚àí1/vextendsingle/vextendsingle=1
|A|; (A.13)
which can be shown by taking the determinant of (A.2) and applying (A.12) IfAandBare matrices of size N√óM, then
/vextendsingle/vextendsingleIN+ABT/vextendsingle/vextendsingle=/vextendsingle/vextendsingleIM+ATB/vextendsingle/vextendsingle: (A.14)
A useful special case is /vextendsingle/vextendsingleIN+abT/vextendsingle/vextendsingle= 1 + aTb (A.15)
where aandbareN-dimensional column vectors Matrix Derivatives
Sometimes we need to consider derivatives of vectors and matrices with respect to
scalars The derivative of a vector awith respect to a scalar xis a vector whose
components are given by/parenleftbigg@a
@x/parenrightbigg
i=@ai
@x(A.16)
with an analogous deÔ¨Ånition for the derivative of a matrix Derivatives with respect
to vectors and matrices can also be deÔ¨Åned, for instance
/parenleftbigg@x
@a/parenrightbigg
i=@x
@ai(A.17)
and similarly/parenleftbigg@a
@b/parenrightbigg
ij=@ai
@bj: (A.18)
The following is easily proven by writing out the components:
@
@x/parenleftbig
xTa/parenrightbig
=@
@x/parenleftbig
aTx/parenrightbig
=a: (A.19)
612 A

============================================================

=== CHUNK 591 ===
Palavras: 354
Caracteres: 2194
--------------------------------------------------
LINEAR ALGEBRA
Similarly
@
@x(AB) =@A
@xB+A@B
@x: (A.20)
The derivative of the inverse of a matrix can be expressed as
@
@x/parenleftbig
A‚àí1/parenrightbig
=‚àíA‚àí1@A
@xA‚àí1(A.21)
as can be shown by differentiating the equation A‚àí1A=Iusing (A.20) and then
right-multiplying by A‚àí1 Also
@
@xln|A|=Tr/parenleftbigg
A‚àí1@A
@x/parenrightbigg
; (A.22)
which we shall prove later If we choose xto be one of the elements of A, we have
@
@AijTr(AB) =Bji (A.23)
as can be seen by writing out the matrices using index notation We can write this
result more compactly in the form
@
@ATr(AB) = BT: (A.24)
With this notation, we have the following properties:
@
@ATr/parenleftbig
ATB/parenrightbig
=B; (A.25)
@
@ATr(A) = I; (A.26)
@
@ATr(ABAT) = A(B +BT); (A.27)
which can again be proven by writing out the matrix indices We also have
@
@Aln|A|=/parenleftbig
A‚àí1/parenrightbigT; (A.28)
which follows from (A.22) and (A.24) Eigenvectors
For a square matrix Aof sizeM√óM, the eigenvector equation is deÔ¨Åned by
Aui=iui (A.29)
A.4 Eigenvectors 613
fori= 1;:::;M , where uiis an eigenvector andiis the corresponding eigenvalue This can be viewed as a set of Msimultaneous homogeneous linear equations, and
the condition for a solution is that
|A‚àíiI|= 0; (A.30)
which is known as the characteristic equation Because this is a polynomial of order
Mini, it must have Msolutions (though these need not all be distinct) The rank
ofAis equal to the number of non-zero eigenvalues Of particular interest are symmetric matrices, which arise as covariance ma-
trices, kernel matrices, and Hessians Symmetric matrices have the property that
Aij=Aji, or equivalently AT=A The inverse of a symmetric matrix is also sym-
metric, as can be seen by taking the transpose of A‚àí1A=Iand using AA‚àí1=I
together with the symmetry of I In general, the eigenvalues of a matrix are complex numbers, but for symmetric
matrices, the eigenvalues iare real This can be seen by Ô¨Årst left-multiplying (A.29)
by(u i)T, where?denotes the complex conjugate, to give
(u i)Tui: (A.31)
Next we take the complex conjugate of (A.29) and left-multiply by uT
ito give
uT
iAu i (A.32)
where we have used A?=Abecause we are considering only real matrices A

============================================================

=== CHUNK 592 ===
Palavras: 376
Caracteres: 2316
--------------------------------------------------
Taking the transpose of the second of these equations and using AT=A, we see
that the left-hand sides of the two equations are equal and hence that  i=i, and
soimust be real The eigenvectors uiof a real symmetric matrix can be chosen to be orthonormal
(i.e., orthogonal and of unit length) so that
uT
iuj=Iij (A.33)
whereIijare the elements of the identity matrix I To show this, we Ô¨Årst left-multiply
(A.29) by uT
jto give
uT
jAui=iuT
jui (A.34)
and hence, by exchanging the indices, we have
uT
iAuj=juT
iuj: (A.35)
We now take the transpose of the second equation and make use of the symmetry
property AT=A, and then subtract the two equations to give
(i‚àíj)uT
iuj= 0: (A.36)
Hence, fori/negationslash=j, we have uT
iuj= 0so that uiandujare orthogonal If the two
eigenvalues are equal, then any linear combination ui+ujis also an eigenvector
614 A LINEAR ALGEBRA
with the same eigenvalue, so we can select one linear combination arbitrarily, and
then choose the second to be orthogonal to the Ô¨Årst (it can be shown that the de-
generate eigenvectors are never linearly dependent) Hence, the eigenvectors can be
chosen to be orthogonal, and by normalizing can be set to unit length Because there
areMeigenvalues, the corresponding Morthogonal eigenvectors form a complete
set and so any M-dimensional vector can be expressed as a linear combination of
the eigenvectors We can take the eigenvectors uito be the columns of an M√óMmatrix U,
which from orthonormality satisÔ¨Åes
UTU=I: (A.37)
Such a matrix is said to be orthogonal Interestingly, the rows of this matrix are also
orthogonal, so that UUT=I To show this, note that (A.37) implies UTUU‚àí1=
U‚àí1=UTand so UU‚àí1=UUT=I Using (A.12), it also follows that |U|= 1 The eigenvector equation (A.29) can be expressed in terms of Uin the form
AU=U (A.38)
where is anM√óMdiagonal matrix whose diagonal elements are given by the
eigenvalues i If we consider a column vector xthat is transformed by an orthogonal matrix U
to give a new vector
/tildewidex=Ux (A.39)
then the length of the vector is preserved because
/tildewidexT/tildewidex=xTUTUx=xTx (A.40)
and similarly the angle between any two such vectors is preserved because
/tildewidexT/tildewidey=xTUTUy=xTy: (A.41)
Thus, multiplication by Ucan be interpreted as a rigid rotation of the coordinate
system

============================================================

=== CHUNK 593 ===
Palavras: 388
Caracteres: 2392
--------------------------------------------------
From (A.38), it follows that
UTAU= (A.42)
and because is a diagonal matrix, we say that the matrix Aisdiagonalized by the
matrix U If we left-multiply by Uand right-multiply by UT, we obtain
A=UUT: (A.43)
Taking the inverse of this equation and using (A.3) together with U‚àí1=UT, we
have
A‚àí1=U‚àí1UT: (A.44)
A.4 Eigenvectors 615
These last two equations can also be written in the form
A=M/summationdisplay
i=1iuiuT
i (A.45)
A‚àí1=M/summationdisplay
i=11
iuiuT
i: (A.46)
If we take the determinant of (A.43) and use (A.12), we obtain
|A|=M/productdisplay
i=1i: (A.47)
Similarly, taking the trace of (A.43), and using the cyclic property (A.8) of the trace
operator together with UTU=I, we have
Tr(A) =M/summationdisplay
i=1i: (A.48)
We leave it as an exercise for the reader to verify (A.22) by making use of the results
(A.33), (A.45), (A.46), and (A.47) A matrix Ais said to be positive deÔ¨Ånite, denoted by A/follows0, ifwTAw>0for
all non-zero values of the vector w Equivalently, a positive deÔ¨Ånite matrix has i>
0for all of its eigenvalues (as can be seen by setting wto each of the eigenvectors in
turn and noting that an arbitrary vector can be expanded as a linear combination of
the eigenvectors) Note that having all positive elements does not necessarily mean
that a matrix is that positive deÔ¨Ånite For example, the matrix
/parenleftbigg
1 2
3 4/parenrightbigg
(A.49)
has eigenvalues 1/similarequal5:37 and2/similarequal‚àí0:37 A matrix is said to be positive semidef-
inite ifwTAw>0holds for all values of w, which is denoted A/followsequal0and is
equivalent to i>0 The condition number of a matrix is given by
CN =/parenleftbiggmax
min/parenrightbigg1=2
(A.50)
wheremaxis the largest eigenvalue and minis the smallest eigenvalue Calculus of Variations
We can think of a function y(x)as being an operator that, for any input value x,
returns an output value y In the same way, we can deÔ¨Åne a functionalF[y]to be
an operator that takes a function y(x)and returns an output value F An example
of a functional is the length of a curve drawn in a two-dimensional plane in which
the path of the curve is deÔ¨Åned in terms of a function In the context of machine
learning, a widely used functional is the entropy H[x]for a continuous variable x
because, for any choice of probability density function p(x), it returns a scalar value
representing the entropy of xunder that density

============================================================

=== CHUNK 594 ===
Palavras: 357
Caracteres: 2274
--------------------------------------------------
Thus, the entropy of p(x)could
equally well have been written as H[p] A common problem in conventional calculus is to Ô¨Ånd a value of xthat max-
imizes (or minimizes) a function y(x) Similarly, in the calculus of variations we
seek a function y(x)that maximizes (or minimizes) a functional F[y] That is, of all
possible functions y(x), we wish to Ô¨Ånd the particular function for which the func-
tionalF[y]is a maximum (or minimum) The calculus of variations can be used, for
instance, to show that the shortest path between two points is a straight line or that
the maximum entropy distribution is a Gaussian If we were not familiar with the rules of ordinary calculus, we could evaluate
a conventional derivative dy=dxby making a small change to the variable xand
then expanding in powers of , so that
y(x+) =y(x) +dy
dx+O(2) (B.1)
and Ô¨Ånally taking the limit ‚Üí0 Similarly, for a function of several variables
y(x1;:::;xD), the corresponding partial derivatives are deÔ¨Åned by
y(x1+1;:::;xD+D) =y(x1;:::;xD) +D/summationdisplay
i=1@y
@xii+O(2): (B.2)
The analogous deÔ¨Ånition of a functional derivative arises when we consider how
much a functional F[y]changes when we make a small change (x)to the function
617 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4
618 B CALCULUS OF V ARIATIONS
Figure B.1 A functional derivative can be deÔ¨Åned by
considering how the value of a functional
F[y]changes when the function y(x)is
changed to y(x) +(x)where(x)is an
arbitrary function of x y(x)
y(x)+(
x)
x
y(
x), where(x)is an arbitrary function of x, as illustrated in Figure B.1 We denote
the functional derivative of F[y]with respect to y(x)byF=y (x)and deÔ¨Åne it by
the following relation:
F[y(x) +(x)] =F[y(x)] +/integraldisplayF

y(x)(x) dx+O(2): (B.3)
This can be seen as a natural extension of (B.2) in which F[y]now depends on a
continuous set of variables, namely the values of yat all pointsx Requiring that the
functional be stationary with respect to small variations in the function y(x)gives
/integraldisplayF

y(x)(x) dx= 0: (B.4)
Because this must hold for an arbitrary choice of (x), it follows that the functional
derivative must vanish

============================================================

=== CHUNK 595 ===
Palavras: 361
Caracteres: 2384
--------------------------------------------------
To see this, imagine choosing a perturbation (x)that is zero
everywhere except in the neighbourhood of a point /hatwidex, in which case the functional
derivative must be zero at x=/hatwidex However, because this must be true for every
choice of/hatwidex, the functional derivative must vanish for all values of x Consider a functional that is deÔ¨Åned by an integral over a function G(y;y/prime;x),
which depends on both y(x)and its derivative y/prime(x)and has a direct dependence on
x:
F[y] =/integraldisplay
G(y(x);y/prime(x);x) dx (B.5)
where the value of y(x)is assumed to be Ô¨Åxed at the boundary of the region of
integration (which might be at inÔ¨Ånity) If we now consider variations in the function
y(x), we obtain
F[y(x) +(x)] =F[y(x)] +/integraldisplay/braceleftbigg@G
@
y(x) +@G
@
y/prime/prime(x)/bracerightbigg
dx+O(2):(B.6)
We now have to cast this in the form (B.3) To do so, we integrate the second term
by parts and note that (x)must vanish at the boundary of the integral (because y(x)
is Ô¨Åxed at the boundary) This gives
F[y(x) +(x)] =F[y(x)] +/integraldisplay/braceleftbigg@G
@
y‚àíd
d
x/parenleftbigg@G
@
y/prime/parenrightbigg/bracerightbigg
(x) dx+O(2)(B.7)
B CALCULUS OF V ARIATIONS 619
from which we can read off the functional derivative by comparison with (B.3) Requiring that the functional derivative vanishes then gives
@G
@y‚àíd
dx/parenleftbigg@G
@y/prime/parenrightbigg
= 0; (B.8)
which are known as the Euler‚ÄìLagrange equations For example, if
G=y(x)2+ (y/prime(x))2(B.9)
then the Euler‚ÄìLagrange equations take the form
y(x)‚àíd2y
dx2= 0: (B.10)
This second-order differential equation can be solved for y(x)by making use of the
boundary conditions on y(x) Often, we consider functionals deÔ¨Åned by integrals whose integrands take the
formG(y;x )and that do not depend on the derivatives of y(x) In this case, station-
arity simply requires that @G=@y (x) = 0 for all values of x If we are optimizing a functional with respect to a probability distribution, then
we need to maintain the normalization constraint on the probabilities This is often
most conveniently done using a Lagrange multiplier, which then allows an uncon- Appendix C
strained optimization to be performed The extension of the above results to a multi-dimensional variable xis straight-
forward For a more comprehensive discussion of the calculus of variations, see
Sagan (1969)

============================================================

=== CHUNK 596 ===
Palavras: 364
Caracteres: 2326
--------------------------------------------------
Lagrange Multipliers
Lagrange multipliers, also sometimes called undetermined multipliers, are used to
Ô¨Ånd the stationary points of a function of several variables subject to one or more
constraints Consider the problem of Ô¨Ånding the maximum of a function f(x1;x2)subject to
a constraint relating x1andx2, which we write in the form
g(x1;x2) = 0: (C.1)
One approach would be to solve the constraint equation (C.1) and thus express x2as
a function of x1in the formx2=h(x1) This can then be substituted into f(x1;x2)
to give a function of x1alone of the form f(x1;h(x1)) The maximum with respect
tox1could then be found by differentiation in the usual way, to give the stationary
valuex 1, with the corresponding value of x2given byx One problem with this approach is that it may be difÔ¨Åcult to Ô¨Ånd an analytic
solution of the constraint equation that allows x2to be expressed as an explicit func-
tion ofx1 Also, this approach treats x1andx2differently and so spoils the natural
symmetry between these variables A more elegant, and often simpler, approach introduces a parameter called a
Lagrange multiplier We shall motivate this technique from a geometrical perspec-
tive Consider a D-dimensional variable xwith components x1;:::;xD The con-
straint equation g(x) = 0 then represents a (D‚àí1)-dimensional surface in x-space
as indicated in Figure C.1 First note that at any point on the constraint surface, the gradient ‚àág(x)of the
constraint function is orthogonal to the surface To see this, consider a point xthat
lies on the constraint surface along with a nearby point x+that also lies on the
surface If we make a Taylor expansion around x, we have
g(x+)/similarequalg(x) +T‚àág(x): (C.2)
Because both xandx+lie on the constraint surface, we have g(x) =g(x+)and
henceT‚àág(x)/similarequal0 In the limit/bardbl/bardbl‚Üí 0, we haveT‚àág(x) = 0 , and becauseis
621 ¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4
622 C LAGRANGE MULTIPLIERS
Figure C.1 A geometrical picture of the technique of Lagrange
multipliers in which we seek to maximize a func-
tionf(x), subject to the constraint g(x) = 0 If
xisDdimensional, the constraint g(x) = 0 cor-
responds to a subspace of dimensionality D‚àí1,
as indicated by the red curve

============================================================

=== CHUNK 597 ===
Palavras: 357
Caracteres: 2124
--------------------------------------------------
The problem can
be solved by optimizing the Lagrangian function
L(x; ) =f(x) +g(x) rf(x)
rg(x)xA
g(x)=
0
then
parallel to the constraint surface g(x) = 0, we see that the vector ‚àágis normal
to the surface Next we seek a point x?on the constraint surface such that f(x)is maximized Such a point must have the property that the vector ‚àáf(x)is also orthogonal to the
constraint surface, as illustrated in Figure C.1, because otherwise we could increase
the value of f(x)by moving a short distance along the constraint surface Thus, ‚àáf
and‚àágare parallel (or anti-parallel) vectors, and so there must exist a parameter 
such that
‚àáf+‚àág = 0 (C.3)
where/negationslash= 0is known as a Lagrange multiplier Note that can have either sign At this point, it is convenient to introduce the Lagrangian function deÔ¨Åned by
L(x;)‚â°f(x) +g(x): (C.4)
The constrained stationarity condition (C.3) is obtained by setting ‚àáxL= 0 Fur-
thermore, the condition @L=@ = 0leads to the constraint equation g(x) = 0 Thus, to Ô¨Ånd the maximum of a function f(x)subject to the constraint g(x) = 0 ,
we deÔ¨Åne the Lagrangian function given by (C.4) and we then Ô¨Ånd the stationary
point ofL(x;) with respect to both xand For aD-dimensional vector x, this
givesD+ 1equations that determine both the stationary point x?and the value of  If we are interested only in x?, then we can eliminate from the stationarity equa-
tions without needing to Ô¨Ånd its value (hence, the term ‚Äòundetermined multiplier‚Äô) As a simple example, suppose we wish to Ô¨Ånd the stationary point of the function
f(x1;x2) = 1‚àíx2
1‚àíx2
2subject to the constraint g(x1;x2) =x1+x2‚àí1 = 0, as
illustrated in Figure C.2 The corresponding Lagrangian function is given by
L(x;) = 1‚àíx2
1‚àíx2
2+(x1+x2‚àí1): (C.5)
The conditions for this Lagrangian to be stationary with respect to x1,x2, andgive
the following coupled equations:
‚àí2x 1+= 0 (C.6)
‚àí2x 2+= 0 (C.7)
x1+x2‚àí1 = 0: (C.8)
C LAGRANGE MULTIPLIERS 623
Figure C.2 A simple example of the use of Lagrange multipliers
in which the aim is to maximize f(x1;x2) = 1‚àí
x2
1‚àíx2
2subject to the constraint g(x1;x2) = 0 where
g(x1;x2) =x1+x2‚àí1

============================================================

=== CHUNK 598 ===
Palavras: 364
Caracteres: 2189
--------------------------------------------------
The circles show contours of
the function f(x1;x2), and the diagonal line shows
the constraint surface g(x1;x2) = 0 2)
Solving
these equations then gives the stationary point as (x 2) = (1=2; 1=2), and
the corresponding value for the Lagrange multiplier is = 1 So far, we have considered the problem of maximizing a function subject to an
equality constraint of the form g(x) = 0 We now consider the problem of maxi-
mizingf(x)subject to an inequality constraint of the formg(x)>0, as illustrated
inFigure C.3 There are now two kinds of solution possible, according to whether the con-
strained stationary point lies in the region where g(x)>0, in which case the con-
straint is inactive, or whether it lies on the boundary g(x) = 0 , in which case the
constraint is said to be active In the former case, the function g(x)plays no role
and so the stationary condition is simply ‚àáf(x) = 0 This again corresponds to
a stationary point of the Lagrange function (C.4) but this time with = 0 The
latter case, where the solution lies on the boundary, is analogous to the equality con-
straint discussed previously and corresponds to a stationary point of the Lagrange
function (C.4) with /negationslash= 0 Now, however, the sign of the Lagrange multiplier is
crucial, because the function f(x)is at a maximum only if its gradient is oriented
away from the region g(x)>0, as illustrated in Figure C.3 We therefore have
‚àáf(x) =‚àí‚àág (x)for some value of >0 For either of these two cases, the product g(x) = 0 Thus, the solution to
Figure C.3 Illustration of the problem of maximizing
f(x)subject to the inequality constraint
g(x)>0 rf(x)
rg(x)xA
xB
g(x)=
0g(x)>0
624 C LAGRANGE MULTIPLIERS
the problem of maximizing f(x)subject tog(x)>0is obtained by optimizing the
Lagrange function (C.4) with respect to xandsubject to the conditions
g(x)>0 (C.9)
>0 (C.10)
g(x) = 0: (C.11)
These are known as the Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions (Karush, 1939;
Kuhn and Tucker, 1951) Note that if we wish to minimize (rather than maximize) the function f(x)sub-
ject to an inequality constraint g(x)>0, then we minimize the Lagrangian function
L(x;) =f(x)‚àíg(x)with respect to x, again subject to >0

============================================================

=== CHUNK 599 ===
Palavras: 364
Caracteres: 2695
--------------------------------------------------
Finally, it is straightforward to extend the technique of Lagrange multipliers to
cases with multiple equality and inequality constraints Suppose we wish to maxi-
mizef(x)subject togj(x) = 0 forj= 1;:::;J , andhk(x)>0fork= 1;:::;K We then introduce Lagrange multipliers {j}and{k}, and then optimize the La-
grangian function given by
L(x;{j};{k}) =f(x) +J/summationdisplay
j=1jgj(x) +K/summationdisplay
k=1khk(x) (C.12)
subject tok>0andkhk(x) = 0 fork= 1;:::;K Extensions to constrained
functional derivatives are similarly straightforward For a more detailed discussion Appendix B
of the technique of Lagrange multipliers, see Nocedal and Wright (1999) Bibliography
Abramowitz, M., and I Handbook
of Mathematical Functions ‚ÄúOver-relaxation method for the
Monte Carlo evaluation of the partition func-
tion for multiquadratic actions.‚Äù Physical Re-
view D 23:2901‚Äì2904 Aghajanyan, Armen, Bernie Huang, Candace Ross,
Vladimir Karpukhin, Hu Xu, Naman Goyal,
Dmytro Okhonko, et al CM3: A Causal
Masked Multimodal Model of the Internet Aghajanyan, Armen, Luke Zettlemoyer, and
Sonal Gupta Intrinsic Dimension-
ality Explains the Effectiveness of Lan-
guage Model Fine-Tuning ‚ÄúA constrained EM
algorithm for principal component analysis.‚Äù
Neural Computation 15 (1): 57‚Äì65 Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, et al Flamingo: a Visual Lan-
guage Model for Few-Shot Learning ‚ÄúA
new learning algorithm for blind signal sep-
aration.‚Äù In Advances in Neural Information
Processing Systems, edited by D Neu-
rocomputing: Foundations of Research ‚ÄúAsymptotic Theory for
Principal Component Analysis.‚Äù Annals of
Mathematical Statistics 34:122‚Äì148 ‚ÄúIndependent factor analysis.‚Äù Neu-
ral Computation 11 (4): 803‚Äì851 Austin, Jacob, Daniel D Johnson, Jonathan Ho,
Daniel Tarlow, and Rianne van den Berg ‚ÄúStructured Denoising Diffusion Models in
Discrete State-Spaces.‚Äù In Advances in Neural
Information Processing Systems, 34:17981‚Äì
17993 Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E
Hinton ‚ÄúKernel Inde-
pendent Component Analysis.‚Äù Journal of Ma-
chine Learning Research 3:1‚Äì48 Badrinarayanan, Vijay, Alex Kendall, and Roberto
Cipolla SegNet: A Deep Convolu-
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4625
626 BIBLIOGRAPHY
tional Encoder-Decoder Architecture for
Image Segmentation Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua
Bengio Neural Machine Translation by
Jointly Learning to Align and Translate ‚ÄúNeural networks
and principal component analysis: learning
from examples without local minima.‚Äù Neural
Networks 2 (1): 53‚Äì58

============================================================

=== CHUNK 600 ===
Palavras: 362
Caracteres: 2633
--------------------------------------------------
Balduzzi, David, Marcus Frean, Lennox Leary,
JP Lewis, Kurt Wan-Duo Ma, and Brian
McWilliams The Shattered Gradi-
ents Problem: If resnets are the answer,
then what is the question Latent Variable Models
and Factor Analysis Basilevsky, Alexander Statistical Factor
Analysis and Related Methods: Theory and
Applications Decision Theory: An Introduction
to Dynamic Programming and Sequential De-
cisions Battaglia, Peter W., Jessica B Hamrick, Vic-
tor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tac-
chetti, et al Relational inductive biases,
deep learning, and graph networks ‚ÄúAutomatic differentiation
in machine learning: a survey.‚Äù Journal of Ma-
chine Learning Research 18:1‚Äì43 ‚ÄúImproving
the convergence of back-propagation learn-
ing with second order methods.‚Äù In Proceed-
ings of the 1988 Connectionist Models Sum-
mer School, edited by D Belkin, Mikhail, Daniel Hsu, Siyuan Ma, and
Soumik Mandal ‚ÄúReconciling mod-ern machine-learning practice and the clas-
sical bias-variance trade-off.‚Äù Proceedings of
the National Academy of Sciences 116 (32):
15849‚Äì15854 ‚ÄúAn infor-
mation maximization approach to blind sepa-
ration and blind deconvolution.‚Äù Neural Com-
putation 7 (6): 1129‚Äì1159 Adaptive Control Processes: A
Guided Tour Princeton University Press Bengio, Yoshua, Aaron Courville, and Pascal Vin-
cent Representation Learning: A Re-
view and New Perspectives Bengio, Yoshua, Nicholas L ¬¥eonard, and Aaron
Courville Estimating or Propagating
Gradients Through Stochastic Neurons for
Conditional Computation Statistical Decision Theory and
Bayesian Analysis ‚ÄúRegularization and Com-
plexity Control in Feed-forward Networks.‚Äù In
Proceedings International Conference on Ar-
tiÔ¨Åcial Neural Networks ICANN‚Äô95, edited by
F Fougelman-Soulie and P Gallinari, 1:141‚Äì
148 Bishop, Christopher M ‚ÄúExact Calculation of
the Hessian Matrix for the Multilayer Percep-
tron.‚Äù Neural Computation 4 (4): 494‚Äì501 Bishop, Christopher M ‚ÄúNovelty Detection
and Neural Network Validation.‚Äù IEE Proceed-
ings: Vision, Image and Signal Processing 141
(4): 217‚Äì222 Bishop, Christopher M Neural Networks for
Pattern Recognition Oxford University Press Bishop, Christopher M ‚ÄúTraining with noise
is equivalent to Tikhonov regularization.‚Äù Neu-
ral Computation 7 (1): 108‚Äì116 Bishop, Christopher M Pattern Recognition
and Machine Learning BIBLIOGRAPHY 627
Bommasani, Rishi, Drew A Hudson, Ehsan Adeli,
Russ Altman, Simran Arora, Sydney von Arx,
Michael S On the Op-
portunities and Risks of Foundation Models ‚ÄúLarge-scale machine learning
with stochastic gradient descent.‚Äù In Proceed-
ings COMPSTAT 2010, 177‚Äì186

============================================================

=== CHUNK 601 ===
Palavras: 359
Caracteres: 2619
--------------------------------------------------
‚ÄúAuto-
association by multilayer perceptrons and sin-
gular value decomposition.‚Äù Biological Cyber-
netics 59:291‚Äì294 ‚ÄúBagging predictors.‚Äù Machine
Learning 26:123‚Äì140 Berking, S
Haferkamp, A Weichenthal, et
al.2019 ‚ÄúDeep neural networks are superior
to dermatologists in melanoma image classiÔ¨Å-
cation.‚Äù European Journal of Cancer 119:11‚Äì
17 Brock, Andrew, Jeff Donahue, and Karen Si-
monyan ‚ÄúLarge-Scale GAN Train-
ing for High Fidelity Natural Image Syn-
thesis.‚Äù In Proceedings of the International
Conference Learning Representations (ICLR) Bronstein, Michael M., Joan Bruna, Taco Co-
hen, and Petar Velickovic Geomet-
ric Deep Learning: Grids, Groups, Graphs,
Geodesics, and Gauges Bronstein, Michael M., Joan Bruna, Yann Le-
Cun, Arthur Szlam, and Pierre Vandergheynst ‚ÄúGeometric Deep Learning: Going Be-
yond Eulcidean Data.‚Äù In IEEE Signal Pro-
cessing Magazine, vol ‚ÄúMultivari-
able functional interpolation and adaptive net-
works.‚Äù Complex Systems 2:321‚Äì355 Brown, Tom B., Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, et al Language Models are Few-Shot Learn-
ers.Technical report arXiv:2005.14165.Bubeck, S ¬¥ebastien, Varun Chandrasekaran, Ronen
Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, et al Sparks of ArtiÔ¨Åcial
General Intelligence: Early experiments with
GPT-4 ‚ÄúBlind signal separation: sta-
tistical principles.‚Äù Proceedings of the IEEE 9
(10): 2009‚Äì2025 ‚ÄúMultitask learning.‚Äù Machine
Learning 28:41‚Äì75 Statistical In-
ference ‚ÄúVari-
ational Bayesian learning of ICA with missing
data.‚Äù Neural Computation 15 (8): 1991‚Äì2011 ‚ÄúOn the geometry of feedforward neural net-
work error surfaces.‚Äù Neural Computation 5
(6): 910‚Äì927 Chen, Mark, Alec Radford, Rewon Child, Jef-
frey Wu, Heewoo Jun, David Luan, and Ilya
Sutskever ‚ÄúGenerative Pretraining From
Pixels.‚Äù Proceedings of Machine Learning Re-
search 119:1691‚Äì1703 Neural Ordinary
Differential Equations Chen, Ting, Simon Kornblith, Mohammad Norouzi,
and Geoffrey Hinton A Simple
Framework for Contrastive Learning of
Visual Representations Cho, Kyunghyun, Bart van Merrienboer, C ¬∏ aglar
G¬®ulc ¬∏ehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio Learning Phrase
Representations using RNN Encoder-Decoder
for Statistical Machine Translations ‚ÄúVaria-
tional mixture of Bayesian independent com-
ponent analyzers.‚Äù Neural Computation 15 (1):
213‚Äì252 628 BIBLIOGRAPHY
Christiano, Paul, Jan Leike, Tom B Brown, Miljan
Martic, Shane Legg, and Dario Amodei Deep reinforcement learning from human pref-
erences ‚ÄúLarge Scale Machine Learn-
ing.‚Äù PhD diss., Universit ¬¥e Paris VI

============================================================

=== CHUNK 602 ===
Palavras: 368
Caracteres: 2696
--------------------------------------------------
‚ÄúBlind
source separation, 2: problems statement.‚Äù Sig-
nal Processing 24 (1): 11‚Äì20 ‚ÄúNearest neighbor pat-
tern classiÔ¨Åcation.‚Äù IEEE Transactions on In-
formation Theory IT-11:21‚Äì27 Elements of
Information Theory ‚ÄúProbability, frequency and rea-
sonable expectation.‚Äù American Journal of
Physics 14 (1): 1‚Äì13 ‚ÄúApproximation by superposi-
tions of a sigmoidal function.‚Äù Mathematics of
Control, Signals and Systems 2:304‚Äì314 ‚ÄúConditional Independence in
Statistical Theory (with discussion).‚Äù Journal
of the Royal Statistical Society, Series B 4:1‚Äì
31 ‚ÄúConditional Independence
for Statistical Operations.‚Äù Annals of Statistics
8:598‚Äì617 Mathematics for Machine Learning Cambridge University Press ‚ÄúMaximum likelihood from incomplete
data via the EM algorithm.‚Äù Journal of the
Royal Statistical Society, B 39 (1): 1‚Äì38 Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei ‚ÄúImageNet: A large-
scale hierarchical image database.‚Äù In IEEE
Conference on Computer Vision and Pattern
Recognition Devlin, Jacob, Ming-Wei Chang, Kenton Lee,
and Kristina Toutanova BERT: Pre-
training of Deep Bidirectional Transformersfor Language Understanding Dhariwal, Prafulla, and Alex Nichol Diffu-
sion Models Beat GANs on Image Synthesis Dinh, Laurent, David Krueger, and Yoshua Ben-
gio NICE: Non-linear Independent
Components Estimation Dinh, Laurent, Jascha Sohl-Dickstein, and Samy
Bengio Density estimation using Real
NVP Dodge, Samuel, and Lina Karam A Study
and Comparison of Human and Deep Learning
Recognition Performance Under Visual Dis-
tortions Tutorial on Variational Autoen-
coders Dosovitskiy, Alexey, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, et al An Image is Worth 16√ó16Words: Trans-
formers for Image Recognition at Scale ‚ÄúHybrid Monte Carlo.‚Äù Physics
Letters B 195 (2): 216‚Äì222 ‚ÄúAdaptive
Subgradient Methods for Online Learning and
Stochastic Optimization.‚Äù Journal of Machine
Learning Research 12:2121‚Äì2159 Pattern ClassiÔ¨Å-
cation and Scene Analysis Dufter, Philipp, Martin Schmitt, and Hinrich
Sch¬®utze Position Information in Trans-
formers: An Overview Dumoulin, Vincent, and Francesco Visin A
guide to convolution arithmetic for deep learn-
ing.Technical report Hidden Markov Models: Estimation and Con-
trol BIBLIOGRAPHY 629
Esser, Patrick, Robin Rombach, and Bj ¬®orn Om-
mer Taming Transformers for High-
Resolution Image Synthesis ‚ÄúDermatologist-level classiÔ¨Åcation of
skin cancer with deep neural networks.‚Äù Na-
ture542:115‚Äì118 An Introduction to Latent Vari-
able Models Eykholt, Kevin, Ivan Evtimov, Earlence Fernan-
des, Bo Li, Amir Rahmati, Chaowei Xiao,
Atul Prakash, Tadayoshi Kohno, and Dawn
Song

============================================================

=== CHUNK 603 ===
Palavras: 355
Caracteres: 2508
--------------------------------------------------
‚ÄúRobust Physical-World Attacks
on Deep Learning Visual ClassiÔ¨Åcation.‚Äù In
Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) ‚ÄúAn introduction to ROC analy-
sis.‚Äù Pattern Recognition Letters 27:861‚Äì874 An Introduction to Probability The-
ory and its Applications Practical Methods of Optimiza-
tion Computer Vi-
sion: A Modern Approach ‚ÄúExperi-
ments with a new boosting algorithm.‚Äù In Thir-
teenth International Conference on Machine
Learning, edited by L ‚ÄúNeocognitron: A Self-
organizing Neural Network Model for a Mech-
anism of Pattern Recognition Unaffected by
Shift in Position.‚Äù Biological Cybernetics
36:193‚Äì202 ‚ÄúOn the approximate realiza-
tion of continuous mappings by neural net-
works.‚Äù Neural Networks 2 (3): 183‚Äì192 ‚ÄúWeighting and
Integrating Evidence for Stochastic Simulation
in Bayesian Networks.‚Äù In Uncertainty in Ar-
tiÔ¨Åcial Intelligence, edited by P Gatys, Leon A., Alexander S Ecker, and Matthias
Bethge A Neural Algorithm of Artistic
Style ‚ÄúStochastic re-
laxation, Gibbs distributions, and the Bayesian
restoration of images.‚Äù IEEE PAMI 6 (1): 721‚Äì
741 Gemmeke, Jort F., Daniel P Ellis, Dylan Freed-
man, Aren Jansen, Wade Lawrence, R Chan-
ning Moore, Manoj Plakal, and Marvin Ritter ‚ÄúAudio Set: An ontology and human-
labeled dataset for audio events.‚Äù In Proc Germain, Mathieu, Karol Gregor, Iain Murray, and
Hugo Larochelle MADE: Masked Au-
toencoder for Distribution Estimation ‚ÄúDerivative-free adaptive re-
jection sampling for Gibbs sampling.‚Äù In
Bayesian Statistics, edited by J Oxford University Press ‚ÄúAdaptive rejection Metropolis sampling.‚Äù Ap-
plied Statistics 44:455‚Äì472 Markov Chain Monte Carlo in Practice ‚ÄúAdaptive rejection
sampling for Gibbs sampling.‚Äù Applied Statis-
tics41:337‚Äì348 Gilmer, Justin, Samuel S Schoenholz, Patrick F Ri-
ley, Oriol Vinyals, and George E Neural Message Passing for Quantum Chem-
istry John Hopkins University
Press 630 BIBLIOGRAPHY
Gong, Yuan, Yu-An Chung, and James R AST: Audio Spectrogram Transformer Goodfellow, Ian, Yoshua Bengio, and Aaron
Courville Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi
Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio Generative Adversarial Networks Goodfellow, Ian J., Jonathon Shlens, and Chris-
tian Szegedy Explaining and Harness-
ing Adversarial Examples Grathwohl, Will, Ricky T Chen, Jesse Bet-
tencourt, Ilya Sutskever, and David Duve-
naud FFJORD: Free-form Continuous
Dynamics for Scalable Reversible Generative
Models

============================================================

=== CHUNK 604 ===
Palavras: 355
Caracteres: 2545
--------------------------------------------------
Griewank, A., and A Walther Evaluating
Derivatives: Principles and Techniques of Al-
gorithmic Differentiation Automatic Differentiation University of Toronto Improved
training of Wasserstein GANs Gutmann, Michael, and Aapo Hyv ¬®arinen ‚ÄúNoise-contrastive estimation: A new esti-
mation principle for unnormalized statistical
models.‚Äù Journal of Machine Learning Re-
search 9:297‚Äì304 Graph Representation
Learning Multiple View
Geometry in Computer Vision Cam-
bridge University Press ‚ÄúSecond order
derivatives for network pruning: optimal brain
surgeon.‚Äù In Proceedings International Con-
ference on Neural Information Processing Sys-
tems (NeurIPS), edited by S The Elements of Statistical Learning ‚ÄúMonte Carlo sampling
methods using Markov chains and their appli-
cations.‚Äù Biometrika 57:97‚Äì109 He, Kaiming, Xinlei Chen, Saining Xie, Yanghao
Li, Piotr Doll ¬¥ar, and Ross B Masked Autoencoders Are Scalable Vision
Learners He, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie,
and Ross Girshick Momentum Con-
trast for Unsupervised Visual Representation
Learning He, Kaiming, Xiangyu Zhang, Shaoqing Ren,
and Jian Sun Deep Residual Learn-
ing for Image Recognition He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun Delving Deep into Recti-
Ô¨Åers: Surpassing Human-Level Performance
on ImageNet ClassiÔ¨Åcation ‚ÄúPropagation of Uncertainty by
Logic Sampling in Bayes‚Äô Networks.‚Äù In Un-
certainty in ArtiÔ¨Åcial Intelligence, edited by
J ‚Äú -V AE: learning basic visual
concepts with a constrained variational frame-
work.‚Äù In Proceedings of the International
Conference Learning Representations (ICLR) Neural Networks for Machine
Learning Teh, and S Osin-
dero ‚ÄúA new view of ICA.‚Äù In Proceed-
ings of the International Conference on Inde-
pendent Component Analysis and Blind Signal
Separation, vol BIBLIOGRAPHY 631
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean Distilling the Knowledge in a Neural
Network ‚ÄúTraining products of ex-
perts by minimizing contrastive divergence.‚Äù
Neural Computation 14:1771‚Äì1800 Ho, Jonathan, Ajay Jain, and Pieter Abbeel Denoising Diffusion Probabilistic Mod-
els.Technical report Ho, Jonathan, Chitwan Saharia, William Chan,
David J Fleet, Mohammad Norouzi, and Tim
Salimans Cascaded Diffusion Models
for High Fidelity Image Generation Hochreiter, S., and J ‚ÄúLong
short-term Memory.‚Äù Neural Computation 9
(8): 1735‚Äì1780 ‚ÄúMean Ô¨Åeld approaches to in-
dependent component analysis.‚Äù Neural Com-
putation 14 (4): 889‚Äì918 Holtzman, Ari, Jan Buys, Maxwell Forbes, and
Yejin Choi The Curious Case of
Neural Text Degeneration

============================================================

=== CHUNK 605 ===
Palavras: 355
Caracteres: 2557
--------------------------------------------------
‚ÄúMultilayer feedforward networks are univer-
sal approximators.‚Äù Neural Networks 2 (5):
359‚Äì366 Hospedales, Timothy, Antreas Antoniou, Paul Mi-
caelli, and Amos Storkey ‚ÄúMeta-
learning in neural networks: A survey.‚Äù IEEE
Transactions on Pattern Analysis and Machine
Intelligence 44 (9): 5149‚Äì5169 ‚ÄúAnalysis of a complex of sta-
tistical variables into principal components.‚Äù
Journal of Educational Psychology 24:417‚Äì
441 ‚ÄúRelations between two sets of
variables.‚Äù Biometrika 28:321‚Äì377 Hu, Anthony, Lloyd Russell, Hudson Yeo, Zak
Murez, George Fedoseev, Alex Kendall, Jamie
Shotton, and Gianluca Corrado GAIA-1: A Generative World Model for Autonomous
Driving Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu
Wang, and Weizhu Chen LoRA: Low-
Rank Adaptation of Large Language Models ‚ÄúReceptive
Ô¨Åelds of single neurons in the cat‚Äôs striate cor-
tex.‚Äù Journal of Physiology 148:574‚Äì591 ‚ÄúEstimation of Non-
Normalized Statistical Models by Score
Matching.‚Äù Journal of Machine Learning Re-
search 6:695‚Äì709 Hyv¬®arinen, A., and E ‚ÄúA fast Ô¨Åxed-point
algorithm for independent component analy-
sis.‚Äù Neural Computation 9 (7): 1483‚Äì1492 Hyv¬®arinen, Aapo, Jarmo Hurri, and Patrick O Natural Image Statistics: A Prob-
abilistic Approach to Early Computational Vi-
sion ‚ÄúBatch normaliza-
tion.‚Äù In Proceedings of the International Con-
ference on Machine Learning (ICML), 448‚Äì
456 ‚ÄúAdaptive mixtures of local ex-
perts.‚Äù Neural Computation 3 (1): 79‚Äì87 Machine Learning: Discriminative
and Generative ‚ÄúBlocking Gibbs sampling in very large proba-
bilistic expert systems.‚Äù International Journal
of Human Computer Studies Special Issue on
Real-World Applications of Uncertain Reason-
ing.42:647‚Äì666 Principal Component Analysis Jumper, John, Richard Evans, Alexander Pritzel,
Tim Green, Michael Figurnov, and Olaf
Ronneberger ‚ÄúHighly accurate protein
structure prediction with AlphaFold.‚Äù Nature
596:583‚Äì589 632 BIBLIOGRAPHY
Jutten, C., and J ‚ÄúBlind separation
of sources, 1: An adaptive algorithm based on
neuromimetic architecture.‚Äù Signal Processing
24 (1): 1‚Äì10 Kaplan, Jared, Sam McCandlish, Tom Henighan,
Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu,
and Dario Amodei Scaling Laws for
Neural Language Models Karras, Tero, Timo Aila, Samuli Laine, and Jaakko
Lehtinen Progressive Growing of GANs
for Improved Quality, Stability, and Variation ‚ÄúMinima of functions of several
variables with inequalities as side constraints.‚Äù
Master‚Äôs thesis, Department of Mathematics,
University of Chicago

============================================================

=== CHUNK 606 ===
Palavras: 352
Caracteres: 2583
--------------------------------------------------
Khosla, Prannay, Piotr Teterwak, Chen Wang,
Aaron Sarna, Yonglong Tian, Phillip Isola,
Aaron Maschinot, Ce Liu, and Dilip Krishnan Supervised Contrastive Learning Adam: A method
for stochastic optimization ‚ÄúAuto-
encoding variational Bayes.‚Äù In Proceedings
of the International Conference on Machine
Learning (ICML) Kingma, Diederik P., and Max Welling An Introduction to Variational Autoencoders Kingma, Durk P, Tim Salimans, Rafal Jozefowicz,
Xi Chen, Ilya Sutskever, and Max Welling ‚ÄúImproved variational inference with in-
verse autoregressive Ô¨Çow.‚Äù Advances in Neural
Information Processing Systems 29 Kipf, Thomas N., and Max Welling Semi-Supervised ClassiÔ¨Åcation with Graph
Convolutional Networks Kloeden, Peter E, and Eckhard Platen Numerical solution of stochastic differentialequations Stochastic Modelling and
Applied Probability ‚ÄúNormalizing Ô¨Çows: an introduction and
review of current methods.‚Äù IEEE Transac-
tions on Pattern Analysis and Machine Intel-
ligence 43 (11): 3964‚Äì3979 Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E ‚ÄúImagenet classiÔ¨Åcation with
deep convolutional neural networks.‚Äù In Ad-
vances in Neural Information Processing Sys-
tems, vol ‚ÄúNonlin-
ear programming.‚Äù In Proceedings of the 2nd
Berkeley Symposium on Mathematical Statis-
tics and Probabilities, 481‚Äì492 University of
California Press ‚ÄúOn informa-
tion and sufÔ¨Åciency.‚Äù Annals of Mathematical
Statistics 22 (1): 79‚Äì86 Kurkov ¬¥a, V ., and P ‚ÄúFunctionally
Equivalent Feed-forward Neural Networks.‚Äù
Neural Computation 6 (3): 543‚Äì558 Lasserre, J., Christopher M ‚ÄúPrincipled hybrids of generative and
discriminative models.‚Äù In Proceedings 2006
IEEE Conference on Computer Vision and Pat-
tern Recognition, New York Oxford
University Press ‚ÄúA ModiÔ¨Åed Method of Es-
timation in Factor Analysis and Some Large
Sample Results.‚Äù In Uppsala Symposium on
Psychological Factor Analysis, 35‚Äì42 Num-
ber 3 in Nordisk Psykologi Monograph Series Uppsala: Almqvist / Wiksell Latent
Structure Analysis ‚ÄúBackpropagation Applied to Handwrit-
ten ZIP Code Recognition.‚Äù Neural Computa-
tion1 (4): 541‚Äì551 BIBLIOGRAPHY 633
LeCun,
Y ., L ‚ÄúGradient-Based Learning Applied to
Document Recognition.‚Äù Proceedings of the
IEEE 86:2278‚Äì2324 ‚ÄúOptimal Brain Damage.‚Äù In Proceedings In-
ternational Conference on Neural Information
Processing Systems (NeurIPS), edited by D LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton ‚ÄúDeep Learning.‚Äù Nature 512:436‚Äì444 LeCun, Yann, Sumit Chopra, Raia Hadsell,
Marc‚ÄôAurelio Ranzato, and Fu-Jie Huang ‚ÄúA Tutorial on Energy-Based Learn-
ing.‚Äù In Predicting Structured Data, edited by
G

============================================================

=== CHUNK 607 ===
Palavras: 351
Caracteres: 2564
--------------------------------------------------
‚ÄúFrom data distributions to regu-
larization in invariant learning.‚Äù Neural Com-
putation 7:974‚Äì981 ‚ÄúMultilayer feedforward networks with
a polynomial activation function can approx-
imate any function.‚Äù Neural Networks 6:861‚Äì
867 Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer,
and Tom Goldstein Visualizing the Loss
Landscape of Neural Nets Li, Junnan, Dongxu Li, Caiming Xiong, and
Steven Hoi BLIP: Bootstrapping
Language-Image Pre-training for UniÔ¨Åed
Vision-Language Understanding and Gener-
ation Lin, Min, Qiang Chen, and Shuicheng Yan Lin, Tianyang, Yuxin Wang, Xiangyang Liu, and
Xipeng Qiu A Survey of Transformers Lipman, Yaron, Ricky T Chen, Heli Ben-Hamu,
Maximilian Nickel, and Matt Le Flow
Matching for Generative Modeling Technical
report arXiv:2210.02747 https://arxiv.org/.Liu, Pengfei, Weizhe Yuan, Jinlan Fu, Zhengbao
Jiang, Hiroaki Hayashi, and Graham Neubig Pre-train, Prompt, and Predict: A Sys-
tematic Survey of Prompting Methods in Nat-
ural Language Processing ‚ÄúLeast squares quantization
in PCM.‚Äù IEEE Transactions on Information
Theory 28 (2): 129‚Äì137 Long, Jonathan, Evan Shelhamer, and Trevor
Darrell Fully Convolutional Networks
for Semantic Segmentation Understanding Diffusion Mod-
els: A UniÔ¨Åed Perspective ‚ÄúA Practical Bayesian
Framework for Back-propagation Networks.‚Äù
Neural Computation 4 (3): 448‚Äì472 Information Theory, In-
ference and Learning Algorithms Cambridge
University Press ‚ÄúSome methods for classiÔ¨Åca-
tion and analysis of multivariate observations.‚Äù
InProceedings of the Fifth Berkeley Sympo-
sium on Mathematical Statistics and Probabil-
ity,edited by L University of California Press Matrix Dif-
ferential Calculus with Applications in Statis-
tics and Econometrics A Wavelet Tour of Signal Process-
ing.Second Least Squares Genera-
tive Adversarial Networks Directional
Statistics Martens, James, Ilya Sutskever, and Kevin Swer-
sky ‚ÄúEstimating the Hessian by Back-
propagating Curvature.‚Äù In Proceedings of the
634 BIBLIOGRAPHY
International Conference on Machine Learn-
ing (ICML) Generalized
Linear Models ‚ÄúA Logi-
cal Calculus of the Ideas Immanent in Nervous
Activity.‚Äù Reprinted in Anderson and Rosen-
feld (1988), Bulletin of Mathematical Bio-
physics 5:115‚Äì133 The EM
Algorithm and its Extensions Finite Mixture
Models ‚ÄúMaximum
likelihood estimation via the ECM algorithm:
a general framework.‚Äù Biometrika 80:267‚Äì278 Which Training Methods for GANs
do actually Converge ‚ÄúEqua-
tion of State Calculations by Fast Computing
Machines.‚Äù Journal of Chemical Physics 21
(6): 1087‚Äì1092 Metropolis, N., and S

============================================================

=== CHUNK 608 ===
Palavras: 351
Caracteres: 2548
--------------------------------------------------
‚ÄúThe Monte
Carlo method.‚Äù Journal of the American Sta-
tistical Association 44 (247): 335‚Äì341 Mikolov, Tomas, Kai Chen, Greg Corrado, and Jef-
frey Dean EfÔ¨Åcient Estimation of Word
Representations in Vector Space Expanded edition 1990 Conditional
Generative Adversarial Nets ‚ÄúEn-
semble learning for blind source separation.‚Äù
InIndependent Component Analysis: Princi-
ples and Practice, edited by S Cambridge University Press.M√∏ller, M ‚ÄúEfÔ¨Åcient Training of Feed-
Forward Neural Networks.‚Äù PhD diss., Aarhus
University, Denmark ‚ÄúOn the number of linear re-
gions of deep neural networks.‚Äù In Proceed-
ings of the International Conference on Neural
Information Processing Systems (NeurIPS) Mordvintsev, Alexander, Christopher Olah, and
Mike Tyka Inceptionism: Going Deeper
into Neural Networks Probabilistic Machine
Learning: An introduction Probabilistic Machine
Learning: Advanced Topics http:
//probml.github.io/book2 Nakkiran, Preetum, Gal Kaplun, Yamini Bansal,
Tristan Yang, Boaz Barak, and Ilya Sutskever Deep Double Descent: Where Bigger
Models and More Data Hurt Probabilistic inference using
Markov chain Monte Carlo methods Techni-
cal report CRG-TR-93-1 Department of Com-
puter Science, University of Toronto, Canada ‚ÄúSuppressing random walks in
Markov chain Monte Carlo using ordered over-
relaxation.‚Äù In Learning in Graphical Models,
edited by Michael I ‚ÄúA new view
of the EM algorithm that justiÔ¨Åes incremental
and other variants.‚Äù In Learning in Graphical
Models, edited by M ‚ÄúGeneralized linear models.‚Äù Journal of the
Royal Statistical Society, A 135:370‚Äì384 Introductory Lectures on Convex
Optimization: A Basic Course BIBLIOGRAPHY 635
Nichol,
Alex, and Prafulla Dhariwal Im-
proved Denoising Diffusion Probabilistic
Models Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob Mc-
Grew, Ilya Sutskever, and Mark Chen GLIDE: Towards Photorealistic Image Gener-
ation and Editing with Text-Guided Diffusion
Models Numerical Op-
timization Noh, Hyeonwoo, Seunghoon Hong, and Bohyung
Han Learning Deconvolution Network
for Semantic Segmentation ‚ÄúSimplifying
neural networks by soft weight sharing.‚Äù Neu-
ral Computation 4 (4): 473‚Äì493 Essential Wavelets for Statistical
Applications and Data Analysis Oord, Aaron van den, Nal Kalchbrenner, and
Koray Kavukcuoglu Pixel Recur-
rent Neural Networks Oord, Aaron van den, Nal Kalchbrenner, Oriol
Vinyals, Lasse Espeholt, Alex Graves, and Ko-
ray Kavukcuoglu Conditional Image
Generation with PixelCNN Decoders Oord, Aaron van den, Yazhe Li, and Oriol Vinyals

============================================================

=== CHUNK 609 ===
Palavras: 370
Caracteres: 2731
--------------------------------------------------
Representation Learning with Con-
trastive Predictive Coding Oord, Aaron van den, Oriol Vinyals, and Ko-
ray Kavukcuoglu Neural Discrete
Representation Learning GPT-4 Technical Report ‚ÄúGaussian pro-
cesses and SVM: mean Ô¨Åeld theory and leave-
one-out.‚Äù In Advances in Large Margin Clas-
siÔ¨Åers, edited by A Bartlett, B.Sch¬®olkopf, and D Pavlakou, and Iain Murray ‚ÄúMasked Autoregressive Flow for Den-
sity Estimation.‚Äù In Proceedings of the Inter-
national Conference on Neural Information
Processing Systems (NeurIPS), vol Papamakarios, George, Eric Nalisnick, Danilo
Jimenez Rezende, Shakir Mohamed, and
Balaji Lakshminarayanan Normalizing
Flows for Probabilistic Modeling and Infer-
ence ‚ÄúCorrelation functions and
computer simulations.‚Äù Nuclear Physics B
180:378‚Äì384 Probabilistic Reasoning in Intelli-
gent Systems ‚ÄúFast exact multiplication
by the Hessian.‚Äù Neural Computation 6 (1):
147‚Äì160 ‚ÄúMaxi-
mum likelihood source separation: a context-
sensitive generalization of ICA.‚Äù In Advances
in Neural Information Processing Systems,
edited by M ‚ÄúOn lines and planes of clos-
est Ô¨Åt to systems of points in space.‚Äù The
London, Edinburgh and Dublin Philosophical
Magazine and Journal of Science, Sixth Series
2:559‚Äì572 Phuong, Mary, and Marcus Hutter Formal
Algorithms for Transformers Variational autoen-
coders Https://www.borealisai.com/research-
blogs/tutorial-5-variational-auto-encoders Understanding Deep
Learning Unsupervised representation learning with
deep convolutional generative adversarial net-
works 636 BIBLIOGRAPHY
Radford, Alec, Jong Wook Kim, Chris Hal-
lacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, et al Learn-
ing Transferable Visual Models From Natu-
ral Language Supervision Radford, Alec, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever Lan-
guage Models are Unsupervised Multitask
Learners Rakhimov, Ruslan, Denis V olkhonskiy, Alexey
Artemov, Denis Zorin, and Evgeny Burnaev Latent Video Transformer Searching for Activation Functions Generalized In-
verse of Matrices and Its Applications Redmon, Joseph, Santosh Kumar Divvala, Ross B Girshick, and Ali Farhadi You Only
Look Once: UniÔ¨Åed, Real-Time Object Detec-
tion Ren, Shaoqing, Kaiming He, Ross B Girshick, and
Jian Sun Faster R-CNN: Towards Real-
Time Object Detection with Region Proposal
Networks Rezende, Danilo J, Shakir Mohamed, and Daan
Wierstra ‚ÄúStochastic backpropagation
and approximate inference in deep genera-
tive models.‚Äù In Proceedings of the 31st In-
ternational Conference on Machine Learning
(ICML-14), 1278‚Äì1286 ‚ÄúLearning of word stress in a sub-optimal sec-
ond order backpropagation neural network.‚Äù In
Proceedings of the IEEE International Confer-
ence on Neural Networks, 1:355‚Äì361

============================================================

=== CHUNK 610 ===
Palavras: 355
Caracteres: 2593
--------------------------------------------------
Monte Carlo
Statistical Methods Rombach, Robin, Andreas Blattmann, Dominik
Lorenz, Patrick Esser, and Bj ¬®orn Ommer High-Resolution Image Synthesis withLatent Diffusion Models Ronneberger, Olaf, Philipp Fischer, and Thomas
Brox ‚ÄúU-Net: Convolutional Networks
for Biomedical Image Segmentation.‚Äù In Med-
ical Image Computing and Computer-Assisted
Intervention ‚Äì MICCAI, edited by N Principles of Neurodynamics:
Perceptrons and the Theory of Brain Mecha-
nisms ‚ÄúEM algorithms for PCA and
SPCA.‚Äù In Advances in Neural Information
Processing Systems, edited by M ‚ÄúA unify-
ing review of linear Gaussian models.‚Äù Neural
Computation 11 (2): 305‚Äì345 ‚ÄúEM algo-
rithms for ML factor analysis.‚Äù Psychometrika
47 (1): 69‚Äì76 ‚ÄúLearning internal representations by er-
ror propagation.‚Äù In Parallel Distributed Pro-
cessing: Explorations in the Microstructure of
Cognition, edited by D McClelland, and the PDP Research Group,
vol 1: Foundations, 318‚Äì362 Reprinted in
Anderson and Rosenfeld (1988) An introduction
to deep generative modeling Introduction to the Calculus of
Variations Saharia, Chitwan, William Chan, Huiwen Chang,
Chris A Lee, Jonathan Ho, Tim Salimans,
David J Fleet, and Mohammad Norouzi Palette: Image-to-Image Diffusion Mod-
els.Technical report Saharia, Chitwan, William Chan, Saurabh Saxena,
Lala Li, Jay Whang, Emily Denton, Seyed
Kamyar Seyed Ghasemipour, et al Photorealistic Text-to-Image Diffusion Models
BIBLIOGRAPHY 637
with Deep Language Understanding Saharia, Chitwan, Jonathan Ho, William Chan,
Tim Salimans, David J Fleet, and Moham-
mad Norouzi Image Super-Resolution
via Iterative ReÔ¨Ånement How does batch normalization help opti-
mization Satorras, Victor Garcia, Emiel Hoogeboom,
and Max Welling E(n) Equivariant
Graph Neural Networks Sch¬®olkopf, B., and A Learning with
Kernels Schuhmann, Christoph, Richard Vencu, Romain
Beaumont, Robert Kaczmarczyk, Clayton
Mullis, Aarush Katta, Theo Coombes, Jenia
Jitsev, and Aran Komatsuzaki LAION-
400M: Open Dataset of CLIP-Filtered 400
Million Image-Text Pairs Schuster, Mike, and Kaisuke Nakajima ‚ÄúJapanese and Korean voice search.‚Äù In 2012
IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP),
5149‚Äì5152 Selvaraju, Ramprasaath R., Abhishek Das, Ra-
makrishna Vedantam, Michael Cogswell, Devi
Parikh, and Dhruv Batra Grad-CAM:
Visual Explanations from Deep Networks via
Gradient-based Localization Sennrich, Rico, Barry Haddow, and Alexandra
Birch Neural Machine Translation of
Rare Words with Subword Units Sermanet, Pierre, David Eigen, Xiang Zhang,
Michael Mathieu, Rob Fergus, and Yann
LeCun

============================================================

=== CHUNK 611 ===
Palavras: 362
Caracteres: 2681
--------------------------------------------------
OverFeat: Integrated Recog-
nition, Localization and Detection using
Convolutional Networks arXiv:1312.6229.Shachter, R ‚ÄúSimulation
Approaches to General Probabilistic Inference
on Belief Networks.‚Äù In Uncertainty in ArtiÔ¨Å-
cial Intelligence, edited by P ‚ÄúA mathematical theory of
communication.‚Äù The Bell System Technical
Journal 27 (3): 379‚Äì423 and 623‚Äì656 Shen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma,
Zhewei Yao, Amir Gholami, Michael W Ma-
honey, and Kurt Keutzer Q-BERT: Hes-
sian Based Ultra Low Precision Quantization
of BERT ‚ÄúTangent prop ‚Äì a formalism for spec-
ifying selected invariances in an adaptive net-
work.‚Äù In Advances in Neural Information Pro-
cessing Systems, edited by J ‚ÄúBest practice for convolutional neural net-
works applied to visual document analysis.‚Äù
InProceedings International Conference on
Document Analysis and Recognition (ICDAR),
958‚Äì962 IEEE Computer Society Simonyan, Karen, Andrea Vedaldi, and Andrew
Zisserman ‚ÄúDeep Inside Convolu-
tional Networks: Visualising Image Clas-
siÔ¨Åcation Models and Saliency Maps.‚Äù In
Computer Vision and Pattern Recognition Simonyan, Karen, and Andrew Zisserman Very Deep Convolutional Networks for Large-
Scale Image Recognition ‚ÄúTurbulence and the Dynam-
ics of Coherent Structures.‚Äù Quarterly Applied
Mathematics 45 (3): 561‚Äì590 Sohl-Dickstein, Jascha, Eric A Weiss, Niru Ma-
heswaranathan, and Surya Ganguli Deep Unsupervised Learning using Nonequi-
638 BIBLIOGRAPHY
librium Thermodynamics Amortised MAP inference
for image super-resolution Song, Jiaming, Chenlin Meng, and Stefano Ermon Denoising Diffusion Implicit Models Song, Yang, and Stefano Ermon ‚ÄúGenera-
tive Modeling by Estimating Gradients of the
Data Distribution.‚Äù In Advances in Neural In-
formation Processing Systems, 11895‚Äì11907 Song, Yang, Sahaj Garg, Jiaxin Shi, and Stefano
Ermon ‚ÄúSliced score matching: A scal-
able approach to density and score estimation.‚Äù
InUncertainty in ArtiÔ¨Åcial Intelligence, 204 Song, Yang, and Diederik P How
to Train Your Energy-Based Models Song, Yang, Jascha Sohl-Dickstein, Diederik
P Kingma, Abhishek Kumar, Stefano Er-
mon, and Ben Poole Score-Based
Generative Modeling through Stochastic
Differential Equations ‚ÄúDropout: A Simple Way to Prevent Neural
Networks from OverÔ¨Åtting.‚Äù Journal of Ma-
chine Learning Research 15:1929‚Äì1958 Independent Component Analy-
sis: A Tutorial Introduction ‚ÄúOn the importance of initialization
and momentum in deep learning.‚Äù In Proceed-
ings of the International Conference on Ma-
chine Learning (ICML) URL: incom-
pleteideas.net/IncIdeas/BitterLesson.html.Szegedy, Christian, Wojciech Zaremba, Ilya
Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus

============================================================

=== CHUNK 612 ===
Palavras: 357
Caracteres: 2637
--------------------------------------------------
Intrigu-
ing properties of neural networks Computer Vision: Algorithms and
Applications ‚ÄúNovelty detection for the
identiÔ¨Åcation of masses in mamograms.‚Äù In
Proceedings of the Fourth IEE International
Conference on ArtiÔ¨Åcial Neural Networks,
4:442‚Äì447 Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald
Metzler EfÔ¨Åcient Transformers: A Sur-
vey.Technical report ‚ÄúRegression shrinkage and se-
lection via the lasso.‚Äù Journal of the Royal Sta-
tistical Society, B 58:267‚Äì288 E., and Christopher M Probabilistic Principal Component Analysis Technical report NCRG/97/010 Neural Com-
puting Research Group, Aston University E., and Christopher M ‚ÄúProbabilistic Principal Component Analysis.‚Äù
Journal of the Royal Statistical Society, Series
B21 (3): 611‚Äì622 The nature of statistical learn-
ing theory Vaswani, Ashish, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin Attention Is All You Need Everything is Connected:
Graph Neural Networks VeliÀáckovi ¬¥c, Petar, Guillem Cucurull, Arantxa
Casanova, Adriana Romero, Pietro Li `o, and
Yoshua Bengio Graph Attention Net-
works Statistical Modelling by
Wavelets BIBLIOGRAPHY 639
Vig, Jesse, Ali Madani, Lav R Varshney, Caiming
Xiong, Richard Socher, and Nazneen Fatema
Rajani BERTology Meets Biology: In-
terpreting Attention in Protein Language Mod-
els.Technical report ‚ÄúA connection between score
matching and denoising autoencoders.‚Äù Neural
Computation 23:1661‚Äì1674 Vincent, Pascal, Hugo Larochelle, Yoshua Bengio,
and Pierre-Antoine Manzagol ‚ÄúExtract-
ing and Composing Robust Features with De-
noising Autoencoders.‚Äù In Proceedings of the
International Conference on Machine Learn-
ing (ICML) ‚ÄúOn the asymptotic behaviour
of posterior distributions.‚Äù Journal of the Royal
Statistical Society, B 31 (1): 80‚Äì88 Wang, Chengyi, Sanyuan Chen, Yu Wu, Ziqiang
Zhang, Long Zhou, Shujie Liu, Zhuo Chen, et
al.2023 Neural Codec Language Models are
Zero-Shot Text to Speech Synthesizers CRC Concise Encyclopedia
of Mathematics Chapman / Hall, / CRC Welling, Max, and Yee Whye Teh ‚ÄúBayesian
Learning via Stochastic Gradient Langevin
Dynamics.‚Äù In Proceedings of the Inter-
national Conference on Machine Learning
(ICML) ‚ÄúUsing neural networks
to model conditional multivariate densities.‚Äù
Neural Computation 8 (4): 843‚Äì854 ‚ÄúSimple statistical gradient-
following algorithms for connectionist re-
inforcement learning.‚Äù Machine Learning
8:229‚Äì256 Model-Based Machine Learn-
ing.Www.mbmlbook.com ‚ÄúThe lack of a-priori dis-
tinctions between learning algorithms.‚Äù Neural
Computation 8:1341‚Äì1390.Wu, Zhirong, Yuanjun Xiong, Stella Yu, and Dahua
Lin

============================================================

=== CHUNK 613 ===
Palavras: 321
Caracteres: 2256
--------------------------------------------------
Unsupervised Feature Learning
via Non-Parametric Instance-level Discrimi-
nation Wu, Zonghan, Shirui Pan, Fengwen Chen,
Guodong Long, Chengqi Zhang, and Philip
S A Comprehensive Survey on
Graph Neural Networks Yan, Wilson, Yunzhi Zhang, Pieter Abbeel, and Ar-
avind Srinivas VideoGPT: Video Gener-
ation using VQ-VAE and Transformers Yang, Ruihan, Prakhar Srivastava, and Stephan
Mandt Diffusion Probabilistic Model-
ing for Video Generation Yilmaz, Fatih Furkan, and Reinhard Heckel Regularization-wise double descent: Why it
occurs and how to eliminate it Yosinski, Jason, Jeff Clune, Anh Mai Nguyen,
Thomas J Fuchs, and Hod Lipson Understanding Neural Networks Through
Deep Visualization Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang,
Ruoming Pang, James Qin, Alexander Ku,
Yuanzhong Xu, Jason Baldridge, and Yonghui
Wu Vector-quantized Image Model-
ing with Improved VQGAN Yu, Jiahui, Yuanzhong Xu, Jing Yu Koh, Thang
Luong, Gunjan Baid, Zirui Wang, Vijay Va-
sudevan, et al Scaling Autoregressive
Models for Content-Rich Text-to-Image Gener-
ation Yu, Lili, Bowen Shi, Ramakanth Pasunuru, Ben-
jamin Muller, Olga Golovneva, Tianlu Wang,
Arun Babu, et al Scaling Autore-
gressive Multi-Modal Models: Pretraining
and Instruction Tuning 640 BIBLIOGRAPHY
Zaheer, Manzil, Satwik Kottur, Siamak Ravan-
bakhsh, Barnabas Poczos, Ruslan Salakhutdi-
nov, and Alexander Smola Fundamentals of
Kalman Filtering: A Practical Approach Zeiler, Matthew D., and Rob Fergus Visu-
alizing and Understanding Convolutional Net-
works Zhang, Chiyuan, Samy Bengio, Moritz Hardt,
Benjamin Recht, and Oriol Vinyals Understanding deep learning requires re-
thinking generalization Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, et
al.2023 A Survey of Large Language Models Zhou, Jie, Ganqu Cui, Shengding Hu, Zhengyan
Zhang, Cheng Yang, Zhiyuan Liu, Lifeng
Wang, Changcheng Li, and Maosong Sun Graph Neural Networks: A Review of
Methods and Applications ‚ÄúComputation of
optic Ô¨Çow using a neural network.‚Äù In Interna-
tional Conference on Neural Networks, 71‚Äì78 Unpaired Image-to-Image Translation using
Cycle-Consistent Adversarial Networks Index
Page numbers in bold indicate the primary source of information for the corresponding topic

============================================================

=== CHUNK 614 ===
Palavras: 192
Caracteres: 1377
--------------------------------------------------
1√ó1convolution, 296
1-of-Kcoding, 68, 135, 460
acceptance criterion, 441, 445, 448
activation, 17
activation function, 17, 158, 180, 182
active constraint, 623
AdaGrad, 223
Adam optimization, 224
adaptive rejection sampling, 435
adjacency matrix, 410
adjoint sensitivity method, 556
adversarial attack, 306
aggregation, 415
aleatoric uncertainty, 23
AlexNet, 300
alpha family, 61
amortized inference, 572
ancestral sampling, 450
anchor, 191
annealed Langevin dynamics, 598
AR model, seeautoregressive model
area under the ROC curve, 149
artiÔ¨Åcial intelligence, 1
attention, 358
attention head, 366
audio data, 399
auto-associative neural network, seeautoencoderautoencoder, 188, 563
automatic differentiation, 22, 233, 244
autoregressive Ô¨Çow, 552
autoregressive model, 5, 350, 379
average pooling, 297
backpropagation, 19, 233
backpropagation through time, 381
bag of words, 378
bagging, 278
base distribution, 547
basis function, 112, 158, 172, 172
batch gradient descent, 214
batch learning, 117
batch normalization, 227
Bayes net, 326
Bayes‚Äô theorem, 28
Bayesian network, 326
Bayesian probability, 54
beam search, 386
Bernoulli distribution, 66, 94
Bernoulli mixture model, 481
BERT, 388
bi-gram model, 379
bias, 39, 125
bias parameter, 112, 132, 180
bias‚Äìvariance trade-off, 123
BigGAN, 539
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
C

============================================================

=== CHUNK 615 ===
Palavras: 350
Caracteres: 2603
--------------------------------------------------
Bishop, Deep Learning , https://doi.org/10.1007/978-3-031-45468-4641 642 INDEX bijective function, 548 binomial distribution, 67 bits, 46 blind source separation, 514 blocked path, 339, 343 boosting, 279 bootstrap, 278 bottleneck, 382 bounding box, 309 Box‚ÄìMuller method, 432 byte pair encoding, 377 canonical correlation analysis, 501 canonical link function, 164 Cauchy distribution, 432 causal attention, 384 causality, 347 central differences, 239 central limit theorem, 71 ChatGPT, 394 child node, 249, 327 Cholesky decomposition, 433 circular normal distribution, 89 classical probability, 54 classiÔ¨Åcation, 3 CLIP, 192 co-parents, 348 codebook vector, 398, 465 collider node, 341 combining models, 146 committee, 277 complete data set, 476 completing the square, 77 computer vision, 288 concave function, 52 concentration parameter, 92 condition number, 220 conditional entropy, 53 conditional expectation, 35 conditional independence, 146, 337 conditional mixture model, 199 conditional probability, 27 conditional V AE, 576 conditioner, 552 confusion matrix, 147continuous bag of words, 375 continuous normalizing Ô¨Çow, 557 contrastive divergence, 455 contrastive learning, 191 convex function, 51 convolution, 290, 322 convolutional network, 287 correlation matrix, 503 cost function, 140 coupling Ô¨Çow, 549 coupling function, 552 covariance, 35 Cox‚Äôs axioms, 54 cross attention, 390 cross-correlation, 292, 322 cross-entropy error function, 160, 162, 196 cross-validation, 14 cumulative distribution function, 32 curse of dimensionality, 172 curve Ô¨Åtting, 6 CycleGAN, 539 d-separation, 338, 343, 479 DAG, seedirected acyclic graph data augmentation, 192, 257 data compression, 465 DDIM, 594 DDPM, 581 decision, 120 decision boundary, 131, 139 decision region, 131, 139 decision surface, seedecision boundary decision theory, 120, 138 decoder, 563 deep double descent, 268 deep learning, 20 deep neural networks, 20 deep sets, 417 DeepDream, 308 degrees of freedom, 495 denoising, 581 denoising autoencoder, 567 denoising diffusion implicit model, 594 denoising diffusion probabilistic model, 581 denoising score matching, 597 INDEX 643 density estimation, 37, 65 dequantization, 526 descendant node, 341 design matrix, 116 development set, 14 diagonal covariance matrix, 75 differential entropy, 50 diffusion kernel, 583 diffusion model, 581 Dirac delta function, 34 directed acyclic graph, 329 directed cycle, 329 directed factorization, 349 directed graph, 326 directed graphical model, 326 discriminant function, 132, 143 discriminative model, 144, 157, 346 disentangled representations, 542

============================================================

=== CHUNK 616 ===
Palavras: 350
Caracteres: 2566
--------------------------------------------------
distributed representation, 187 dot-product attention, 363 double descent, 268 dropout, 279 E step, 472, 476 early stopping, 266 earth mover‚Äôs distance, 538 ECM, seeexpectation conditional maximization edge, 326, 410 edge detection, 292 ELBO, seeevidence lower bound EM, seeexpectation maximization embedding space, 188 embedding vector, 409 encoder, 563 energy function, 452 energy-based models, 452 ensemble methods, 277 entropy, 46 epistemic uncertainty, 23 epoch, 215 equality constraint, 623 equivariance, 259, 292, 296, 371, 412 erf function, 164 error backpropagation, seebackpropagation error function, 8, 55, 194, 210Euler‚ÄìLagrange equations, 619 evaluation trace, 247 evidence lower bound, 485, 516, 570, 588 expectation, 34 expectation conditional maximization, 489 expectation maximization, 470, 474, 517, 519 expectation step, seeE step expectations, 430 explaining away, 343 exploding gradient, 227, 382 exponential distribution, 34, 431 exponential family, 94, 156, 329 expression swell, 245 factor analysis, 513 factor graph, 327 factor loading, 513 false negative, 25 false positive, 25 fast gradient sign method, 306 fast R-CNN, 314 feature extraction, 20, 113 feature map, 291 features, 179 feed-forward network, 172, 193 feed-forward networks, 19 few-shot learning, 191, 394 Ô¨Ålter, 291 Ô¨Åne-tuning, 3, 22, 189, 392 Ô¨Çow matching, 558 forward kinematics, 199 forward problem, 198 forward propagation, 235 foundation model, 22, 358, 392, 409 frequentist probability, 54 fuel system, 341 fully connected graphical model, 328 fully convolutional network, 318 functional, 617 Gabor Ô¨Ålters, 302 gamma distribution, 434 GAN, seegenerative adversarial network gated recurrent unit, 382 Gaussian, 36, 70 Gaussian mixture, 86, 200, 271, 466 644 INDEX GEM, seegeneralized EM algorithm generalization, 6 generalized EM algorithm, 489 generalized linear model, 158, 165 generative adversarial network, 533 generative AI, 4 generative model, 4, 144, 346, 533 generative pre-trained transformer, 6, 383 geometric deep learning, 424 Gibbs sampling, 446 global minimum, 211 GNN, seegraph neural network GPT, seegenerative pre-trained transformer GPU, seegraphics processing unit gradient descent, 209 graph attention network, 421 graph convolutional network, 414 graph neural network, 407 graph representation learning, 409 graphical model, 326 graphical model factorization, 329 graphics processing unit, 20, 358 group theory, 256 guidance, 600 Hadamard product, 550 Hamiltonian Monte Carlo, 451 handwritten digit, 501 He initialization, 216 head-to-head path, 341

============================================================

=== CHUNK 617 ===
Palavras: 350
Caracteres: 2730
--------------------------------------------------
head-to-tail path, 340 Heaviside step function, 161 Hessian matrix, 211, 242 Hessian outer product approximation, 243 heteroscedastic, 200 hidden Markov model, 380, 480 hidden unit, 19, 180 hidden variable, seelatent variable hierarchical representation, 187 histogram density estimation, 98 history of machine learning, 16 hold-out set, 14 homogeneous Markov chain, 443 Hooke‚Äôs law, 520 Hutchinson‚Äôs trace estimator, 557hybrid Monte Carlo, 451 hyperparameter, 14 IAF, seeinverse autoregressive Ô¨Çow ICA, seeindependent component analysis identiÔ¨Åability, 470 IID, seeindependent and identically distributed image segmentation, 315 ImageNet data set, 299 importance sampling, 437, 450 importance weight, 437 improper distribution, 33 improper prior, 263 inactive constraint, 623 incomplete data set, 476 independent and identically distributed, 37, 344 independent component analysis, 514 independent factor analysis, 515 independent variables, 31 inductive bias, 19, 254 inductive learning, 409, 420 inequality constraint, 623 inference, 120, 138, 143, 336 InfoNCE, 191 information theory, 46 instance discrimination, 192 internal covariate shift, 229 internal representation, 308 intersection-over-union, 310 intrinsic dimensionality, 496 invariance, 256, 256, 297, 412 inverse autoregressive Ô¨Çow, 553 inverse kinematics, 199 inverse problem, 123, 198, 254, 346 Iris data, 173 IRLS, seeiterative reweighted least squares isotropic covariance matrix, 75 iterative reweighted least squares, 160 Jacobian matrix, 44, 240 Jensen‚Äôs inequality, 52 Jensen‚ÄìShannon divergence, 544 Knearest neighbours, 103 K-means clustering algorithm, 460, 480 Kalman Ô¨Ålter, 353, 515 INDEX 645 Karush‚ÄìKuhn‚ÄìTucker conditions, 624 kernel density estimator, 100, 596 kernel function, 101 kernel image, 291 KKT, seeKarush-Kuhn-Tucker conditions KL divergence, seeKullback‚ÄìLeibler divergence Kosambi‚ÄìKarhunen‚ÄìLo `eve transform, 497 Kullback‚ÄìLeibler divergence, 51, 486 Lagrange multiplier, 621 Lagrangian, 622 Langevin dynamics, 454 Langevin sampling, 455 language model, 382 Laplace distribution, 34 large language model, 5, 382, 390 lasso, 264 latent class analysis, 481 latent diffusion model, 601 latent variable, 76, 335, 459, 495 layer normalization, 229, 369 LDM, seelatent diffusion model LDS, seelinear dynamical system leaky ReLU, 185 learning curve, 223, 266 learning rate parameter, 214 learning to learn, 190 least-mean-squares algorithm, 118 least-squares GAN, 537 leave-one-out, 15 LeNet convolutional network, 299 Levenberg‚ÄìMarquardt approximation, 244 likelihood function, 38, 468 likelihood weighted sampling, 451 linear discriminant, 132 linear dynamical system, 515 linear independence, 610 linear regression, 6, 112 linear-Gaussian model,

============================================================

=== CHUNK 618 ===
Palavras: 350
Caracteres: 2579
--------------------------------------------------
79, 332, 332 linearly separable, 132 link, seeedge link function, 158, 165 LLM, seelarge language model LMS, seeleast-mean-squares algorithm local minimum, 211log odds, 151 logic sampling, 450 logistic regression, 159 logistic sigmoid, 95, 113, 151, 159 logit function, 151 long short-term memory, 382 LoRA, seelow-rank adaptation loss function, 120, 140 loss matrix, 142 lossless data compression, 465 lossy data compression, 465 low-rank adaptation, 392 LSGAN, seeleast-squares GAN LSTM, seelong short-term memory M step, 472, 477 macrostate, 48 MAE, seemasked autoencoder MAF, seemasked autoregressive Ô¨Çow Mahalanobis distance, 71 manifold, 177, 522 MAP, seemaximum a posteriori marginal probability, 27 Markov blanket, 347, 449 Markov boundary, seeMarkov blanket Markov chain, 351, 442 Markov chain Monte Carlo, 440 Markov model, 351 Markov random Ô¨Åeld, 327 masked attention, 384 masked autoencoder, 567 masked autoregressive Ô¨Çow, 553 max-pooling, 297 max-unpooling, 317 maximization step, seeM step maximum a posteriori, 56, 477 maximum likelihood, 38, 84, 115, 153 MCMC, seeMarkov chain Monte Carlo MDN, seemixture density network mean, 36 mean value theorem, 49 measure theory, 33 mel spectrogram, 399 message-passing, 414 message-passing neural network, 415 646 INDEX meta-learning, 190 Metropolis algorithm, 441 Metropolis‚ÄìHastings algorithm, 445 microstate, 48 mini-batches, 216 minimum risk, 145 Minkowski loss, 122 missing at random, 477, 519 missing data, 519 mixing coefÔ¨Åcient, 87 mixture component, 87 mixture density network, 198 mixture distribution, 459 mixture model, 459 mixture of Gaussians, 86, 200, 271, 466 MLP, seemultilayer perceptron MNIST data, 495 mode collapse, 536 model averaging, 277 model comparison, 9 model selection, 14 moment, 37 momentum, 220 Monte Carlo dropout, 280 Monte Carlo sampling, 429 Moore-Penrose pseudo-inverse, seepseudo-inverse MRF, seeMarkov random Ô¨Åeld multi-class logistic regression, 161 multi-head attention, 366 multilayer perceptron, 18, 172 multimodal transformer, 394 multimodality, 199 multinomial distribution, 70, 95 multiplicity, 48 multitask learning, 190 mutual information, 54 n-gram model, 379 naive Bayes model, 147, 344, 378 nats, 47 natural language processing, 374 natural parameter, 94 nearest-neighbours, 103 neocognitron, 302 Nesterov momentum, 221neural ordinary differential equation, 554 neuroscience, 302 NLP, seenatural language processing no free lunch theorem, 255 node, 326, 410 noise, 23 noiseless coding theorem, 47 noisy-OR, 354 non-identiÔ¨Åability, 513 non-max suppression, 314 nonparametric

============================================================

=== CHUNK 619 ===
Palavras: 350
Caracteres: 2684
--------------------------------------------------
methods, 66, 98 normal distribution, seeGaussian normal equations, 116 normalized exponential, seesoftmax function novelty detection, 144 object detection, 308 observed variable, 335 Old Faithful data, 86 on-hot encoding, see1-of-K encoding one-shot learning, 191 one-versus-one classiÔ¨Åer, 134 one-versus-the-rest classiÔ¨Åer, 134 online gradient descent, 215 online learning, 117 ordered over-relaxation, 449 outer product approximation, 244 outlier, 137, 144, 164 over-Ô¨Åtting, 10, 123, 470 over-relaxation, 449 over-smoothing, 422 padding, 294 parameter sharing, 270, 331 parameter shrinkage, 118 parameter tying, seeparameter sharing parent node, 247, 327 partition function, 452 Parzen estimator, seekernel density estimator Parzen window, 101 PCA, seeprincipal component analysis perceptron, 17 periodic variables, 89 permutation matrix, 411 PixelCNN, 397 PixelRNN, 397 INDEX 647 plate, 334 polynomial curve Ô¨Åtting, 6 pooling, 296 positional encoding, 371 positive deÔ¨Ånite covariance, 72 positive deÔ¨Ånite matrix, 615 posterior collapse, 577 posterior probability, 31 power method, 498 pre-activation, 17 pre-processing, 20 pre-training, 189, 392 precision matrix, 77 precision parameter, 36 predictive distribution, 42, 120 preÔ¨Åx prompt, 394 principal component analysis, 497, 506, 565 principal subspace, 497 prior, 263 prior knowledge, 19, 255 prior probability, 31, 145 probabilistic graphical model, seegraphical model probabilistic PCA, 506 probability, 25 probability density, 32 probability theory, 23 probit function, 164 probit regression, 163 product rule of probability, 26, 28, 326 prompt, 394, 601 prompt engineering, 394 proposal distribution, 433, 437, 441 pseudo-inverse, 116, 136 pseudo-random numbers, 430 quadratic discriminant, 153 radial basis functions, 179 random variable, 26 raster scan, 397 readout layer, 419 real NVP normalizing Ô¨Çow, 549 receiver operating characteristic, seeROC curve receptive Ô¨Åeld, 290, 416 recurrent neural network, 380 regression, 3regression function, 121 regularization, 12, 253 regularized least squares, 118 reject option, 142, 145 rejection sampling, 433 relative entropy, 51 reparameterization trick, 574 representation learning, 22, 188 residual block, 275 residual connection, 22, 274 residual network, 275 resnet, seeresidual network responsibility, 88, 468 RLHF, 394 RMS error, seeroot-mean-square error RMSProp, 223 RNN, seerecurrent neural network robot arm, 198 robustness, 137 ROC curve, 148 root-mean-square error, 10 saliency map, 305 same convolution, 294 sample mean, 39 sample variance, 39 sampling, 429 sampling-importance-resampling, 439 scale invariance, 256 scaled self-attention, 366 scaling hypothesis, 358

============================================================

=== CHUNK 620 ===
Palavras: 350
Caracteres: 2730
--------------------------------------------------
Schur complement, 79 score function, 455, 594 score matching, 594 self-attention, 362 self-supervised learning, 5, 375 semi-supervised learning, 420 sequential estimation, 85 sequential gradient descent, 118 sequential learning, 117 SGD, seestochastic gradient descent shared parameters, seeparameter sharing shared weights, 292 shattered gradients, 274 shrinkage, 13 648 INDEX sigmoid, seelogistic sigmoid singular value decomposition, 117 SIR, seesampling-importance-resampling skip-grams, 375 skip-layer connections, 274 sliding window, 311 smoothing parameter, 100 soft ReLU, 185 soft weight sharing, 271 softmax function, 96, 152, 197, 201, 363 softplus activation function, 185 sparse autoencoders, 566 sparse connections, 292 sparsity, 264 sphering, 504 standard deviation, 36 standardizing, 462, 503 state-space model, 352 statistical bias, seebias statistical independence, seeindependent variables steepest descent, 214 Stein score, seescore function Stirling‚Äôs approximation, 48 stochastic, 8 stochastic differential equation, 598 stochastic gradient descent, 19, 214, 215 stochastic variable, 26 strided convolution, 294 strides, 311 structured data, 287, 407 style transfer, 320 sufÔ¨Åcient statistics, 67, 69, 84, 97 sum rule of probability, 26, 28, 326 sum-of-squares error, 8, 41, 136 supervised learning, 3, 420 support vector machine, 179 SVD, seesingular value decomposition SVM, seesupport vector machine swish activation function, 205 symmetry, 256 symmetry breaking, 216 tail-to-tail path, 339 tangent propagation, 258 temperature, 387tensor, 194, 295 test set, 10, 14 text-to-speech, 400 tied parameters, seeparameter sharing token, 360 tokenization, 377 training set, 3 transductive, 409, 419 transductive learning, 420 transfer learning, 3, 189, 218, 388 transformers, 357 transition probability, 443 translation invariance, 256 transpose convolution, 318 tri-gram model, 379 TTS, seetext-to-speech U-net, 319 undetermined multiplier, seeLagrange multiplier undirected graphical model, 327 uniquenesses, 513 universal approximation theorems, 182 unobserved variable, seelatent variable unsupervised learning, 4, 188 utility function, 140 V AE, seevariational autoencoder valid convolution, 294 validation set, 14 vanishing gradient, 227, 382 variance, 35, 36, 125 variational autoencoder, 569 variational inference, 485 variational lower bound, seeevidence lower bound vector quantization, 398, 465 vertex, seenode vision transformer, 395 von Mises distribution, 89 voxel, 289 Wasserstein distance, 538 Wasserstein GAN, 538 wavelets, 114 weakly supervised, 192 weight decay, 13, 260 weight parameter, 17, 180 INDEX 649 weight sharing, seeparameter sharing weight vector, 132 weight-space symmetry, 185 WGAN,

============================================================

=== CHUNK 621 ===
Palavras: 19
Caracteres: 145
--------------------------------------------------
seeWasserstein GAN whitening, 502 Woodbury identity, 610 word embedding, 375 word2vec, 375 wrapped distribution, 94 Yellowstone National Park, 86

============================================================


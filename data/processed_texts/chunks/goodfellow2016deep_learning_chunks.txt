=== CHUNK 001 ===
Palavras: 350
Caracteres: 4675
--------------------------------------------------
Deep L ea r ni n g
I a n G o o d f e l l o w
Y o s h u a B e n g i o
A a r o n C o u r v i l l e
C on t e n t s
Website vii
Acknowledgments viii
Notation xi
1Introduction 1
1.1WhoShouldReadThisBook .8
1.2HistoricalTrendsinDeepLearning 11
IAppliedMathandMachineLearningBasics 29
2LinearAlgebra 31
2.1Scalars,Vectors,MatricesandTensors .31
2.2MultiplyingMatricesandVectors .34
2.3IdentityandInverseMatrices .36
2.4LinearDependenceandSpan 39
2.6SpecialKindsofMatricesandVectors 40
2.7Eigendecomposition 42
2.8SingularValueDecomposition .44
2.9TheMoore-PenrosePseudoinverse .45
2.10TheTraceOperator 46
2.11TheDeterminant 47
2.12Example:PrincipalComponentsAnalysis .48
3ProbabilityandInformationTheory 53
3.1WhyProbability .54
i
CO NTE NT S
3.2RandomVariables .56
3.3ProbabilityDistributions .56
3.4MarginalProbability 58
3.5ConditionalProbability .59
3.6TheChainRuleofConditionalProbabilities .59
3.7IndependenceandConditionalIndependence .60
3.8Expectation,VarianceandCovariance .60
3.9CommonProbabilityDistributions .62
3.10UsefulPropertiesofCommonFunctions .70
3.12TechnicalDetailsofContinuousVariables 71
3.13InformationTheory 73
3.14StructuredProbabilisticModels 75
4NumericalComputation 80
4.1OverﬂowandUnderﬂow .80
4.2PoorConditioning 82
4.3Gradient-BasedOptimization .82
4.4ConstrainedOptimization .93
4.5Example:LinearLeastSquares .96
5MachineLearningBasics 98
5.1LearningAlgorithms .99
5.2Capacity,OverﬁttingandUnderﬁtting .110
5.3HyperparametersandValidationSets .120
5.4Estimators,BiasandVariance .122
5.5MaximumLikelihoodEstimation .131
5.6BayesianStatistics .135
5.7SupervisedLearningAlgorithms 140
5.8UnsupervisedLearningAlgorithms .146
5.9StochasticGradientDescent 151
5.10BuildingaMachineLearningAlgorithm .153
5.11ChallengesMotivatingDeepLearning .155
IIDeepNetworks:ModernPractices 166
6DeepFeedforwardNetworks 168
6.1Example:LearningXOR .171
6.2Gradient-BasedLearning .177
i i
CO NTE NT S
6.3HiddenUnits .191
6.4ArchitectureDesign .197
6.5Back-PropagationandOtherDiﬀerentiationAlgorithms .204
6.6HistoricalNotes .224
7RegularizationforDeepLearning 228
7.1ParameterNormPenalties 230
7.2NormPenaltiesasConstrainedOptimization .237
7.3RegularizationandUnder-ConstrainedProblems .239
7.4DatasetAugmentation .240
7.5NoiseRobustness .242
7.6Semi-SupervisedLearning .243
7.7Multi-TaskLearning .244
7.8EarlyStopping .246
7.9ParameterTyingandParameterSharing 253
7.10SparseRepresentations .254
7.11BaggingandOtherEnsembleMethods .258
7.13AdversarialTraining 268
7.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer270
8OptimizationforTrainingDeepModels 274
8.1HowLearningDiﬀersfromPureOptimization 275
8.2ChallengesinNeuralNetworkOptimization .282
8.3BasicAlgorithms .294
8.4ParameterInitialization Strategies .301
8.5AlgorithmswithAdaptiveLearningRates .306
8.6ApproximateSecond-Order Methods .310
8.7Optimization StrategiesandMeta-Algorithms .317
9ConvolutionalNetworks 330
9.1TheConvolutionOperation .339
9.4ConvolutionandPoolingasanInﬁnitelyStrongPrior .345
9.5VariantsoftheBasicConvolutionFunction 347
9.6StructuredOutputs 360
9.8EﬃcientConvolutionAlgorithms .362
9.9RandomorUnsupervisedFeatures .363
i i i
CO NTE NT S
9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks ..364
9.11ConvolutionalNetworksandtheHistoryofDeepLearning .371
10 SequenceModeling:RecurrentandRecursiveNets373
10.1UnfoldingComputational Graphs .375
10.2RecurrentNeuralNetworks .378
10.3BidirectionalRNNs .394
10.4Encoder-DecoderSequence-to-SequenceArchitectures ..396
10.5DeepRecurrentNetworks .398
10.6RecursiveNeuralNetworks 400
10.7TheChallengeofLong-TermDependencies .401
10.8EchoStateNetworks .404
10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales ..406
10.10 TheLongShort-TermMemoryandOtherGatedRNNs .408
10.11 Optimization forLong-TermDependencies .413
10.12 Explicit Memory 416
11 PracticalMethodology 421
11.1PerformanceMetrics .422
11.2DefaultBaselineModels .425
11.3DeterminingWhethertoGatherMoreData 426
11.4SelectingHyperparameters .427
11.5DebuggingStrategies .436
11.6Example:Multi-DigitNumberRecognition 440
12 Applications 443
12.1Large-ScaleDeepLearning .443
12.2ComputerVision .452
12.3SpeechRecognition .458
12.4NaturalLanguageProcessing .461
12.5OtherApplications .478
IIIDeepLearningResearch 486
13 LinearFactorModels 489
13.1ProbabilisticPCAandFactorAnalysis 490
13.2IndependentComponentAnalysis(ICA) .491
13.3SlowFeatureAnalysis .493
13.4SparseCoding .496
i v
CO NTE NT S
13.5ManifoldInterpretation ofPCA .499
14 Autoencoders 502
14.1Undercomplete Autoencoders .503
14.2RegularizedAutoencoders .504
14.3RepresentationalPower,LayerSizeandDepth .508
14.4StochasticEncodersandDecoders .509
14.5DenoisingAutoencoders .510
14.6LearningManifoldswithAutoencoders

============================================================

=== CHUNK 002 ===
Palavras: 343
Caracteres: 7583
--------------------------------------------------
515
14.7ContractiveAutoencoders .521
14.8PredictiveSparseDecomposition .523
14.9ApplicationsofAutoencoders .524
15 RepresentationLearning 526
15.1GreedyLayer-WiseUnsupervisedPretraining .528
15.2TransferLearningandDomainAdaptation .536
15.3Semi-SupervisedDisentanglingofCausalFactors .541
15.4DistributedRepresentation .546
15.5ExponentialGainsfromDepth .553
15.6ProvidingCluestoDiscoverUnderlyingCauses .554
16 StructuredProbabilisticModelsforDeepLearning558
16.1TheChallengeofUnstructuredModeling .559
16.2UsingGraphstoDescribeModelStructure .563
16.3SamplingfromGraphicalModels .580
16.4AdvantagesofStructuredModeling .582
16.5LearningaboutDependencies 582
16.6InferenceandApproximateInference .584
16.7TheDeepLearningApproachtoStructuredProbabilisticModels585
17 MonteCarloMethods 590
17.1SamplingandMonteCarloMethods 590
17.2ImportanceSampling .592
17.3MarkovChainMonteCarloMethods .595
17.4GibbsSampling .599
17.5TheChallengeofMixingbetweenSeparatedModes ..599
18 ConfrontingthePartitionFunction 605
18.1TheLog-LikelihoodGradient .606
18.2StochasticMaximumLikelihoodandContrastiveDivergence .607
v
CO NTE NT S
18.3Pseudolikelihood .615
18.4ScoreMatchingandRatioMatching 617
18.5DenoisingScoreMatching .619
18.6Noise-ContrastiveEstimation .620
18.7EstimatingthePartitionFunction .623
19 ApproximateInference 631
19.1InferenceasOptimization .633
19.2ExpectationMaximization .634
19.3MAPInferenceandSparseCoding .635
19.4VariationalInferenceandLearning .638
19.5LearnedApproximateInference .651
20 DeepGenerativeModels 654
20.1BoltzmannMachines .654
20.2RestrictedBoltzmannMachines .656
20.3DeepBeliefNetworks 660
20.4DeepBoltzmannMachines .663
20.5BoltzmannMachinesforReal-ValuedData .676
20.6ConvolutionalBoltzmannMachines .683
20.7BoltzmannMachinesforStructuredorSequentialOutputs .685
20.8OtherBoltzmannMachines 686
20.9Back-PropagationthroughRandomOperations .687
20.10 DirectedGenerativeNets .692
20.11 DrawingSamplesfromAutoencoders .711
20.12 Generativ eStochasticNetworks 714
20.13 OtherGenerationSchemes 716
20.14 EvaluatingGenerativeModels .717
20.15 Conclus ion .720
Bibliography 721
Index 777
v i
W e b s i t e
www.deeplearningb ook.org
Thisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa
varietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof
mistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors vii
Acknowledgments
Thisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople Wewouldliketothankthosewhocommentedonourproposalforthebook
andhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,
ÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas
Rohée Wewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthe
bookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,Guillaume
Alain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBiçici,Matko
Bošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLuc
Carrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent
Dinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,
Nando deFreitas,Çağlar Gülçehre, Jurgen V anGael,JavierAlonso García,
JonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,
AsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf
Mathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,Roman
Novak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,
RousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,
HalisSak, CésarSalgado,GrigorySapunov,YoshinoriSasaki, MikeSchuster,
JulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,David
Sussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,
MassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent
Vanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin
Webb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan Wewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon
individualchapters:
•Notation:ZhangYuanhang •Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction
viii
CO NTE NT S
CharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescu
andAlfredoSolano •Chapter, :AmjadAlmahairi,NikolaBanić,KevinBennett, 2LinearAlgebra
PhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,
SergeyOreshkov,IstvánPetrás,DennisPrangle,ThomasRohée,Gitanjali
GulveSehgal,ColbyToland,AlessandroVitaleandBobWelland •Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory
Arulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,
AnttiRasmus,AlexeySurkovandVolkerTresp •Chapter ,  :Tran LamAnIan Fischer andHu 4NumericalComputation
Yuhuang •Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics
NikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,
PeterShepard,Kee-BongSong,ZhengSunandAndyWu •Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,
ElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKrueger
andAdityaKumarPraharaj •Chapter, :MortenKolbæk,KshitijLauria, 7RegularizationforDeepLearning
InkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury •Chapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter
Armitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,
KashifRasul,KlausStroblandNicholasTurner •Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-
stantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan
StoutandWentaoWu •Chapter,10SequenceModeling:RecurrentandRecursiveNets:Gökçen
Eraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,
DmitriySerdyuk,DongyuShiandKaiyuYang •Chapter, :DanielBeckstein 11PracticalMethodology
•Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications
Roscher •Chapter,13LinearFactorModels:JayanthKoushik i x
CO NTE NT S
•Chapter, :KunalGhosh 15RepresentationLearning
•Chapter, : MinhLê 16StructuredProbabilisticModelsforDeepLearning
andAntonVarfolom •Chapter,18ConfrontingthePartitionFunction:SamBowman 19ApproximateInference
•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,
WenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon •Bibliography:LukasMichelbacherandLeslieN.Smith Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresor
datafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptions
throughoutthetext WewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto
makethewebversionofthebook,andforoﬀeringsupporttoimprovethequality
oftheresultingHTML We would liketothank Ian’swifeDaniela FloriGoodfellowforpatiently
supportingIanduringthewritingofthebookaswellasforhelpwithproofreading WewouldliketothanktheGoogleBrainteamforprovidinganintellectual
environmentwhereIancoulddevoteatremendousamountoftimetowritingthis
bookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike
tothankIan’sformermanager,GregCorrado,andhiscurrentmanager,Samy
Bengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀrey
Hintonforencouragement whenwritingwasdiﬃcult x
N ot at i o n
Thissectionprovidesaconcisereferencedescribingthenotationusedthroughout
thisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical
concepts,wedescribemostoftheseideasinchapters2–4 Num b e r s and Ar r a y s
aAscalar(integerorreal)
aAvector
AAmatrix
AAtensor
I nIdentitymatrixwithrowsandcolumns n n
IIdentitymatrixwithdimensionalityimpliedby
context
e( ) iStandardbasisvector[0 , ,0]witha
1atposition i
diag()aAsquare,diagonalmatrixwithdiagonalentries
givenbya
aAscalarrandomvariable
aAvector-valuedrandomvariable
AAmatrix-valuedrandomvariable
xi
CO NTE NT S
Set s and G r aphs
AAset
RThesetofrealnumbers
{}01 ,Thesetcontaining0and1
{ } 01 , ,

============================================================

=== CHUNK 003 ===
Palavras: 404
Caracteres: 3078
--------------------------------------------------
, nThesetofallintegersbetweenand0 n
[] a , bTherealintervalincludingand a b
(] a , bTherealintervalexcludingbutincluding a b
A B\Setsubtraction,i.e., thesetcontainingtheele-
mentsofthatarenotin A B
GAgraph
P a G(x i)Theparentsofx iinG
I ndexing
a iElement iofvectora,withindexingstartingat1
a − iAllelementsofvectorexceptforelementa i
A i , jElementofmatrix i , jA
A i , :Rowofmatrix iA
A : , iColumnofmatrix iA
A i , j , kElementofa3-Dtensor ( ) i , j , k A
A : : , , i2-Dsliceofa3-Dtensor
a iElementoftherandomvector i a
L i near Al g e br a O p e r at i o ns
ATransposeofmatrixA
A+Moore-PenrosepseudoinverseofA
ABElement-wise(Hadamard)productofandAB
det()ADeterminantofA
x i i
CO NTE NT S
Cal c ul usd y
d xDerivativeofwithrespectto y x
∂ y
∂ xPartialderivativeofwithrespectto y x
∇ x yGradientofwithrespectto y x
∇ X yMatrixderivativesofwithrespectto y X
∇ X yTensorcontainingderivativesof ywithrespectto
X
∂ f
∂xJacobianmatrixJ∈ Rm n ×of f: Rn→ Rm
∇2
x f f f () (xorH)()xTheHessianmatrixofatinputpointx
f d()xxDeﬁniteintegralovertheentiredomainofx

Sf d()xx x Deﬁniteintegralwithrespecttoovertheset S
P r o babil i t y and I nf o r m at i o n T heor y
abTherandomvariablesaandbareindependent ⊥
abcTheyareconditionallyindependentgivenc ⊥|
P()aAprobabilitydistributionoveradiscretevariable
p()aAprobabilitydistributionoveracontinuousvari-
able,oroveravariablewhosetypehasnotbeen
speciﬁed
a Randomvariableahasdistribution ∼ P P
E x ∼ P[()] () () () f xor E f xExpectationof f xwithrespectto Px
Var(()) f xVarianceofunderx f x() P()
Cov(()()) f x , g xCovarianceofandunderx f x() g x() P()
H()xShannonentropyoftherandomvariablex
D K L( ) P QKullback-LeiblerdivergenceofPandQ
N(; )xµ ,ΣGaussiandistributionoverxwithmeanµand
covarianceΣ
x i i i
CO NTE NT S
F unc t i o ns
f f : A B→Thefunctionwithdomainandrange A B
f g f g ◦Compositionofthefunctionsand
f(;)xθAfunctionofxparametrized byθ (Sometimes
wewrite f(x)andomittheargumentθtolighten
notation)
log x x Naturallogarithmof
σ x()Logisticsigmoid,1
1+exp()− x
ζ x x () log(1+exp( Softplus, ))
||||x p Lpnormofx
||||x L2normofx
x+Positivepartof,i.e., x max(0) , x
1 c o ndi t i o nis1iftheconditionistrue,0otherwise
Sometimesweuseafunction fwhoseargumentisascalarbutapplyittoa
vector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f
tothearrayelement-wise Forexample,if C= σ( X),then C i , j , k= σ( X i , j , k)forall
validvaluesof,and i j k
D at aset s and D i st r i but i o n s
p da t aThedatageneratingdistribution
ˆ p da t aTheempiricaldistributiondeﬁnedbythetraining
set
XAsetoftrainingexamples
x( ) iThe-thexample(input)fromadataset i
y( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-
ing
XThe m n×matrixwithinputexamplex( ) iinrow
X i , :
x i v
C h a p t e r 1
I n t ro d u ct i on
Inventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates
backtoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,
Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and
Galatea,Talos,andPandoramayallberegardedasartiﬁciallife( , OvidandMartin
2004Sparkes1996Tandy1997 ;,;,)

============================================================

=== CHUNK 004 ===
Palavras: 350
Caracteres: 9795
--------------------------------------------------
Whenprogrammable computerswereﬁrstconceived,peoplewonderedwhether
suchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas
built(Lovelace1842,).Today, ar t i ﬁc i al i n t e l l i g e nc e(AI)isathrivingﬁeldwith
manypracticalapplicationsandactiveresearchtopics.Welooktointelligent
softwaretoautomateroutinelabor,understandspeechorimages,makediagnoses
inmedicineandsupportbasicscientiﬁcresearch Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolved
problemsthatareintellectually diﬃcultforhumanbeingsbutrelativelystraight-
forwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-
ematicalrules Thetruechallengetoartiﬁcialintelligenceprovedtobesolving
thetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe
formally—probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing
spokenwordsorfacesinimages Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis
toallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa
hierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimpler
concepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed
forhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer
needs.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts
bybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese
1
CHAPTER1.INTRODUCTION
conceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For
thisreason,wecallthisapproachtoAI deep l e ar ni ng
ManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal
environmentsanddidnotrequirecomputerstohavemuchknowledgeabout
theworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworld
championGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002
world,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove
inonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis a
tremendousaccomplishment, butthechallengeisnotduetothediﬃcultyof
describingthesetofchesspiecesandallowablemovestothecomputer.Chess
canbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily
providedaheadoftimebytheprogrammer Ironically,abstractandformaltasksthatareamongthemostdiﬃcultmental
undertakings forahumanbeingareamongtheeasiestforacomputer.Computers
havelongbeenabletodefeateventhebesthumanchessplayer,butareonly
recentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects
orspeech.Aperson’severydayliferequiresanimmenseamountofknowledge
abouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore
diﬃculttoarticulateinaformalway.Computersneedtocapturethissame
knowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin
artiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabout
theworldinformallanguages.Acomputercanreasonaboutstatementsinthese
formallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe
k no wl e dge baseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasled
toamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha
1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage
calledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisan
unwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity
toaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory
aboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992
enginedetectedaninconsistencyinthestory: itknewthatpeopledonothave
electricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe
entity“FredWhileShaving”containedelectricalparts.Itthereforeaskedwhether
Fredwasstillapersonwhilehewasshaving Thediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest
thatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting
patternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The
2
CHAPTER1.INTRODUCTION
introductionofmachinelearningallowedcomputerstotackleproblemsinvolving
knowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple
machinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto
recommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning
algorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail Theperformanceofthesesimplemachinelearningalgorithmsdependsheavily
onthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic
regressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine
thepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant
information, suchasthepresenceorabsenceofauterinescar.Eachpieceof
informationincludedintherepresentationofthepatientisknownasa f e at ur e Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith
variousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesare
deﬁnedinanyway IflogisticregressionwasgivenanMRIscanofthepatient,
ratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeuseful
predictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany
complications thatmightoccurduringdelivery Thisdependenceonrepresentationsisageneralphenomenon thatappears
throughoutcomputerscienceandevendailylife.Incomputerscience,opera-
tionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif
thecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform
arithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuch
moretime-consuming Itisnotsurprisingthatthechoiceofrepresentationhasan
enormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimple
visualexample,seeﬁgure.1.1
Manyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetof
featurestoextractforthattask,thenprovidingthesefeaturestoasimplemachine
learningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfrom
soundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrong
clueastowhetherthespeakerisaman,woman,orchild However,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted Forexample,supposethatwewouldliketowriteaprogramtodetectcarsin
photographs Weknowthatcarshavewheels,sowemightliketousethepresence
ofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactlywhata
wheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut
itsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀ
themetalpartsofthewheel,thefenderofthecaroranobjectintheforeground
obscuringpartofthewheel,andsoon 3
CHAPTER1.INTRODUCTION
                
                
Figure1.1:Exampleofdiﬀerentrepresentations:supposewewanttoseparatetwo
categoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,
werepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot
ontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto
solvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley Onesolutiontothisproblemistousemachinelearningtodiscovernotonly
themappingfromrepresentationtooutputbutalsotherepresentationitself Thisapproachisknownas r e pr e se n t at i o n l e ar ni ng Learnedrepresentations
oftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed
representations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with
minimalhumanintervention.Arepresentationlearningalgorithmcandiscovera
goodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto
months.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof
humantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers Thequintessentialexampleofarepresentationlearningalgorithmisthe au-
t o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat
convertstheinputdataintoadiﬀerentrepresentation,anda dec o derfunction
thatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders
aretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun
throughtheencoderandthenthedecoder,butarealsotrainedtomakethenew
representationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimto
achievediﬀerentkindsofproperties Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually
toseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis
context,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;
thefactorsareusuallynotcombinedbymultiplication Suchfactorsareoftennot
4
CHAPTER1.INTRODUCTION
quantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved
objectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying
explanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas
conceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’s
age,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing
animageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,
andtheangleandbrightnessofthesun Amajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplications
isthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweare
abletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose
toblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle Mostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e
onesthatwedonotcareabout Ofcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeatures
fromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,
canbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingof
thedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvethe
originalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus D e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-
ducingrepresentationsthatareexpressedintermsofother,simplerrepresentations

============================================================

=== CHUNK 005 ===
Palavras: 350
Caracteres: 9503
--------------------------------------------------
Deeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-
cepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2
animageofapersonbycombiningsimplerconcepts,suchascornersandcontours,
whichareinturndeﬁnedintermsofedges Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeep
networkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta
mathematical functionmappingsomesetofinputvaluestooutputvalues.The
functionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach
applicationofadiﬀerentmathematical functionasprovidinganewrepresentation
oftheinput Theideaoflearningtherightrepresentationforthedataprovidesoneperspec-
tiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe
computertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation
canbethoughtofasthestateofthecomputer’smemoryafterexecutinganother
setofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore
instructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselater
5
CHAPTER1.INTRODUCTION
Visible layer
(input pixels)1st hidden layer
(edges)2nd hidden layer
(corners and
contours)3rd hidden layer
(object parts)CARPERSONANIMALOutput
(object identity)
Figure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstand
themeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection
ofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery
complicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoa
seriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.The
inputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat
weareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract
featuresfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiven
inthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining
therelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind
offeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasily
identifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthidden
layer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand
extendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden
layer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer
candetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursand
corners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan
beusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission
fromZeilerandFergus2014() 6
CHAPTER1.INTRODUCTION
x 1 x 1σ
w 1 w 1×
x 2 x 2 w 2 w 2×+El e me n t
S e t
+
×
σ
xx wwEl e me n t
S e t
L ogi s t i c
R e gr e s s i onL ogi s t i c
R e gr e s s i on
Figure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere
eachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto
outputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,
σ ( wTx ),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand
logisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth
three.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone instructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis
viewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarily
encodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores
stateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput Thisstateinformationcouldbeanalogoustoacounterorpointerinatraditional
computerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,
butithelpsthemodeltoorganizeitsprocessing Therearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewis
basedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate
thearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough
aﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgiven
itsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengths
dependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay
bedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionswe
allowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis 1.3
choiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa
modelasbeingnotthedepthofthecomputational graphbutthedepthofthe
graphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth
7
CHAPTER1.INTRODUCTION
oftheﬂowchartofthecomputations neededtocomputetherepresentationof
eachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves Thisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁned
giveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem
observinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye Afterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably
presentaswell Inthiscase,thegraphofconceptsonlyincludestwolayers—a
layerforeyesandalayerforfaces—butthegraphofcomputations includes 2n
layersifwereﬁneourestimateofeachconceptgiventheothertimes n
Becauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthe
computational graph,orthedepthoftheprobabilisticmodelinggraph—ismost
relevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelements
fromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe
depthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof
acomputerprogram Nor isthereaconsensusabouthowmuchdepthamodel
requirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthe
studyofmodelsthateitherinvolveagreateramountofcompositionoflearned
functionsorlearnedconceptsthantraditionalmachinelearningdoes Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI Speciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputer
systemstoimprovewithexperienceanddata Accordingtotheauthorsofthis
book,machinelearningistheonlyviableapproachtobuildingAIsystemsthat
canoperateincomplicated,real-worldenvironments.Deeplearningisaparticular
kindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningto
representtheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedin
relationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms
oflessabstractones.Figureillustratestherelationshipbetweenthesediﬀerent 1.4
AIdisciplines.Figuregivesahigh-levelschematicofhoweachworks 1 Wh o S h ou l d R ead T h i s Bo ok Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain
targetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents
(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho
arebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.The
othertargetaudienceissoftwareengineerswhodonothaveamachinelearning
orstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep
learningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin
8
CHAPTER1.INTRODUCTION
AIMachine learningRepresentation learningDeep learning
Example:
Knowledge
basesExample:
Logistic
regressionExample:
Shallow
autoencoders Example:
MLPs
Figure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,
whichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches
toAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology 9
CHAPTER1.INTRODUCTION
InputHand-
designed 
programOutput
InputHand-
designed 
featuresMapping from 
featuresOutput
InputFeaturesMapping from 
featuresOutput
InputSimple 
featuresMapping from 
featuresOutput
Additional 
layers of more 
abstract 
features
Rule-based
systemsClassic
machine
learning Representation
learningDeep
learning
Figure1.5: FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeach
otherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto
learnfromdata 1 0
CHAPTER1.INTRODUCTION
manysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,
naturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,
searchengines,onlineadvertisingandﬁnance Thisbookhasbeenorganizedintothreepartsinordertobestaccommodatea
varietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I
concepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II
essentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III
widelybelievedtobeimportantforfutureresearchindeeplearning Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests
orbackground Readersfamiliarwithlinearalgebra,probability,andfundamental
machinelearningconceptscanskippart,forexample,whilereaderswhojustwant I
toimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II
chapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization 1.6
ofthebook Wedoassumethatallreaderscomefromacomputersciencebackground We
assumefamiliaritywithprogramming, abasicunderstandingofcomputational
performanceissues,complexitytheory,introductory levelcalculusandsomeofthe
terminologyofgraphtheory 2 Hi s t or i c a l T ren d s i n D eep L earni n g
Itiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan
providingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:
•Deeplearninghashadalongandrichhistory,buthasgonebymanynames
reﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedin
popularity •Deeplearninghasbecomemoreusefulastheamountofavailabletraining
datahasincreased

============================================================

=== CHUNK 006 ===
Palavras: 364
Caracteres: 5496
--------------------------------------------------
•Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure
(bothhardwareandsoftware)fordeeplearninghasimproved •Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing
accuracyovertime 1 1
CHAPTER1.INTRODUCTION
1 Introduction
Part I: Applied Math and Machine Learning Basics
2 Probability and 
Information Theory
4 Numerical 
Computation5 Machine Learning 
Basics
Part II: Deep Networks: Modern Practices
6 Deep Feedforward 
Networks
7 Practical 
Methodology12 Applications
Part III: Deep Learning Research
13 Linear Factor 
Models14 Representation 
Learning
16 Structured 
Probabilistic Models17 Monte Carlo 
Methods
18 Partition 
Function19 Deep Generative 
Models
Figure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother
indicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter 1 2
CHAPTER1.INTRODUCTION
1 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -
w o rks
Weexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan
excitingnewtechnology,andaresurprisedtoseeamentionof“history”inabook
aboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deep
learningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral
yearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany
diﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeld
hasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchers
anddiﬀerentperspectives Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook However,somebasiccontextisusefulforunderstandingdeeplearning.Broadly
speaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep
learning known as c y b e r net i c sin the 1940s–1960s, deep learning knownas
c o nnec t i o n i s minthe1980s–1990s,andthecurrentresurgenceunderthename
deeplearningbeginningin2006.Thisisquantitativelyillustratedinﬁgure.1.7
Someoftheearliestlearningalgorithmswerecognizetodaywereintended
tobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning
happensorcouldhappeninthebrain Asaresult,oneofthenamesthatdeep
learninghasgonebyis ar t i ﬁc i al neur al net w o r k s(ANNs).Thecorresponding
perspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired
bythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal) Whilethekindsofneuralnetworksusedformachinelearninghavesometimes
beenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991
generallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural
perspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat
thebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda
conceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe
computational principlesbehindthebrainandduplicateitsfunctionality.Another
perspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe
principlesthatunderliehumanintelligence,somachinelearningmodelsthatshed
lightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolve
engineeringapplications Themodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspective
onthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral
principleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine
learningframeworksthatarenotnecessarilyneurallyinspired 1 3
CHAPTER1.INTRODUCTION
1940 1950 1960 1970 1980 1990 2000
Year0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase
c y b e r n e t i c s
( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )
Figure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnets
research,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or
“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The
ﬁrstwavestartedwithcyberneticsinthe1940s–1960s, withthedevelopmentoftheories
ofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949
theﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle
neuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,
withback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a
hiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton
e t a l ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a
formasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan
thecorrespondingscientiﬁcactivityoccurred 1 4
CHAPTER1.INTRODUCTION
Theearliestpredecessorsofmoderndeeplearningweresimplelinearmodels
motivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedto
takeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y Thesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput
f ( x w, ) =x 1w 1 + · · · +x nw n.Thisﬁrstwaveofneuralnetworksresearchwas
knownascybernetics,asillustratedinﬁgure.1.7
TheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943
ofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesof
inputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel
tocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobe
setcorrectly.Theseweightscouldbesetbythehumanoperator Inthe1950s,
theperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearn
theweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory The adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame
time,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow
andHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata

============================================================

=== CHUNK 007 ===
Palavras: 364
Caracteres: 7682
--------------------------------------------------
Thesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-
chinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE
wasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly
modiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominant
trainingalgorithmsfordeeplearningmodelstoday Modelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled
l i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine
learningmodels,thoughinmanycasestheyare t r a i ne dindiﬀerentwaysthanthe
originalmodelsweretrained Linearmodelshavemanylimitations.Mostfamously,theycannotlearnthe
XORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0
andf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseﬂawsinlinearmodelscaused
abacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,
1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks Today,neuroscienceisregardedasanimportantsourceofinspirationfordeep
learningresearchers,butitisnolongerthepredominant guidefortheﬁeld Themainreasonforthediminishedrole ofneuroscienceindeeplearning
researchtodayisthatwesimplydonothaveenoughinformationaboutthebrain
touseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused
bythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery
least)thousandsofinterconnectedneuronssimultaneously.Becausewearenot
abletodothis,wearefarfromunderstandingevensomeofthemostsimpleand
1 5
CHAPTER1.INTRODUCTION
well-studiedpartsofthebrain( ,) OlshausenandField2005
Neurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm
cansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto
“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired
tosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat
muchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe
diﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning
researchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudying
naturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,
theseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning
researchgroupstostudymanyorevenalloftheseapplicationareassimultaneously Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof
havingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions
witheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)
introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired
bythestructureofthemammalianvisualsystemandlaterbecamethebasis
forthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b
section.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10
the r e c t i ﬁed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced
amorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain
function.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrom
manyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a
neuroscienceasaninﬂuence,and ()citingmoreengineering- Jarrett e t a l .2009
orientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,it
neednotbetakenasarigidguide.Weknowthatactualneuronscomputevery
diﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealism
hasnotyetledtoanimprovementinmachinelearningperformance.Also,while
neurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we
donotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuch
guidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain Whileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan
inﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernel
machinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt
tosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,
especiallyappliedmathfundamentalslikelinearalgebra,probability,information
theory,andnumericaloptimization Whilesomedeeplearningresearcherscite
neuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith
1 6
CHAPTER1.INTRODUCTION
neuroscienceatall Itis worth notingthat theeﬀorttounderstandhowthe brainworkson
an algorithmic lev el is alive andwell.This endeavor is primarily knownas
“computational neuroscience”andisaseparateﬁeldofstudyfromdeeplearning Itiscommonforresearcherstomovebackandforthbetweenbothﬁelds.The
ﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems
thatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldof
computational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate
modelsofhowthebrainactuallyworks Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat
partviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-
i ng( ,; ,) Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995
thecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach
tounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.During
theearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning Despitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsof
howthebraincouldactuallyimplementthemusingneurons.Theconnectionists
begantostudymodelsofcognitionthatcouldactuallybegroundedinneural
implementations(TouretzkyandMinton1985,),revivingmanyideasdatingback
totheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949
Thecentralideainconnectionism isthatalargenumberofsimplecomputational
unitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight
appliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin
computational models Severalkeyconceptsaroseduringtheconnectionism movementofthe1980s
thatremaincentraltotoday’sdeeplearning Oneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,
1986).Thisistheideathateachinputtoasystemshouldberepresentedby
manyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany
possibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize
cars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway
ofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit
thatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red
bird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuron
mustindependentlylearntheconceptofcolorandobjectidentity.Onewayto
improveonthissituationistouseadistributedrepresentation,withthreeneurons
describingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires
onlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto
1 7
CHAPTER1.INTRODUCTION
learnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages
ofonespeciﬁccategoryofobjects Theconceptofdistributedrepresentationis
centraltothisbook,andwillbedescribedingreaterdetailinchapter.15
Anothermajoraccomplishmentoftheconnectionistmovementwasthesuc-
cessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-
sentationsandthepopularization oftheback-propagation algorithm(Rumelhart
e t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987
butasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels Duringthe1990s,researchersmadeimportantadvancesinmodelingsequences
withneuralnetworks.()and ()identiﬁedsomeof Hochreiter1991Bengio e t a l .1994
thefundamentalmathematical diﬃcultiesinmodelinglongsequences,describedin
section.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term
memoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTM
iswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage
processingtasksatGoogle

============================================================

=== CHUNK 008 ===
Palavras: 357
Caracteres: 10018
--------------------------------------------------
Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-
turesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-
callyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁll
theseunreasonableexpectations,investorsweredisappointed.Simultaneously,
otherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l 1992CortesandVapnik1995Schölkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(
dan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors
ledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007 Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance
onsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001
forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive
viaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHinton
atUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann
LeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada
multi-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman
andcomputervision Atthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃcult
totrain Wenowknowthatalgorithmsthathaveexistedsincethe1980swork
quitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat
thesealgorithmsweretoocomputationally costlytoallowmuchexperimentation
withthehardwareavailableatthetime Thethirdwaveofneuralnetworksresearchbeganwithabreakthrough in
1 8
CHAPTER1.INTRODUCTION
2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbelief
networkcouldbeeﬃcientlytrainedusingastrategycalledgreedylayer-wisepre-
training( ,),whichwillbedescribedinmoredetailinsection Hinton e t a l .2006 15.1
TheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategy
couldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007
Ranzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on
testexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe
term“deeplearning”toemphasizethatresearcherswerenowabletotraindeeper
neuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe
theoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio
2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural
networksoutperformedcompetingAIsystemsbasedonothermachinelearning
technologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity
ofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep
learningresearchhaschangeddramatically withinthetimeofthiswave.The
thirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe
abilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis
moreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep
modelstoleveragelargelabeleddatasets 2 In creasin g D a t a s et S i zes
Onemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa
crucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswere
conductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial
applicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan
atechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue
thatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm Fortunately,theamountofskillrequiredreducesastheamountoftrainingdata
increases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks
todayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy
problemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave
undergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost
importantnewdevelopmentisthattodaywecanprovidethesealgorithmswith
theresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8
datasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing
digitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,
moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly
networkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem
1 9
CHAPTER1.INTRODUCTION
intoadatasetappropriateformachinelearningapplications.Theageof“Big
Data”hasmademachinelearningmucheasierbecausethekeyburdenofstatistical
estimation—generalizingwelltonewdataafterobservingonlyasmallamount
ofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumb
isthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable
performancewitharound5,000labeledexamplespercategory,andwillmatchor
exceedhumanperformancewhentrainedwithadatasetcontainingatleast10
millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis
animportantresearcharea,focusinginparticularonhowwecantakeadvantage
oflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised
learning 3 In creasin g Mo d el S i zes
Anotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying
comparativelylittlesuccesssincethe1980sisthatwehavethecomputational
resourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-
ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful Biologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10
ourmachinelearningmodelshavehadanumberofconnectionsperneuronthat
waswithinanorderofmagnitudeofevenmammalianbrainsfordecades Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly
smalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden 1.11
units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This
growthisdrivenbyfastercomputerswithlargermemoryandbytheavailability
oflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore
complextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies
allowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberof
neuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay
representmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiological
neuralnetworksmaybeevenlargerthanthisplotportrays Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer
neuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-
lems.Eventoday’snetworks,whichweconsiderquitelargefromacomputational
systemspointofview,aresmallerthanthenervoussystemofevenrelatively
primitivevertebrateanimalslikefrogs Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,
2 0
CHAPTER1.INTRODUCTION
1900 1950 198520002015
Year100101102103104105106107108109Datasetsize(numberexamples)
IrisMNISTPublicSVHN
ImageNet
CIFAR-10ImageNet10k
ILSVRC  2014Sports-1M
RotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard
WMT
Figure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians
studieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson
1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers
ofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such
aslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand
demonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(Widrow
andHoﬀ1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning
becamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens
ofthousandsofexamplessuchastheMNISTdataset(showninﬁgure)ofscans 1.9
ofhandwrittennumbers( ,).Intheﬁrstdecadeofthe2000s,more LeCun e t a l .1998b
sophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand
Hinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout
theﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousands
totensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning ThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l 2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky
e t a l ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014
graph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructed
fromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990
dataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes 2 1
CHAPTER1.INTRODUCTION
Figure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNational
InstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata The“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewith
machinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits
andassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimple
classiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning
research.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve GeoﬀreyHintonhasdescribeditas“the d r o s o p h i l aofmachinelearning,”meaningthat
itallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory
conditions,muchasbiologistsoftenstudyfruitﬂies 2 2
CHAPTER1.INTRODUCTION
theadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2
connectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof
themostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally
expectedtocontinuewellintothefuture 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct
Sincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide
accuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen
appliedwithsuccesstobroaderandbroadersetsofapplications Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightly
cropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a
beenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern
objectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot
havearequirementthatthephotobecroppedneartheobjecttoberecognized
( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012
twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof
object),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerent
categoriesofobjects

============================================================

=== CHUNK 009 ===
Palavras: 360
Caracteres: 5506
--------------------------------------------------
ThelargestcontestinobjectrecognitionistheImageNet
LargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic
momentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork
wonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthe
state-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012
meaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories
foreachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthis
listforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare
consistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin
deeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,
asshowninﬁgure.1.12
Deeplearninghasalsohadadramaticimpactonspeechrecognition.After
improvingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated
startinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng
e t a l ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a
inasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore
thishistoryinmoredetailinsection.12.3
Deepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand
imagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,
2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan
2 3
CHAPTER1.INTRODUCTION
1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5
Y e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n
12
34
567
89
1 0
F r ui t ﬂyMo useC a tH um a n
Figure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneural
networkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween
neuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyas
manyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks
tohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman
braindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural
networksizesfrom () Wikipedia2015
1.Adaptivelinearelement( ,) WidrowandHoﬀ1960
2.Neocognitron(Fukushima1980,)
3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006
4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)
5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009
6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010
7.Distributedautoencoder(,) Le e t al.2012
8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012
9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013
10.GoogLeNet( ,) Szegedy e t al.2014a
2 4
CHAPTER1.INTRODUCTION
e t a l .,).2012
Atthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,
sohasthecomplexityofthetasksthattheycansolve () Goodfellow e t a l .2014d
showedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters
transcribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,
itwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual
elementsofthesequence( ,).Recurrentneuralnetworks, GülçehreandBengio2013
suchastheLSTMsequencemodelmentionedabove,arenowusedtomodel
relationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustﬁxedinputs Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing
anotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,
2015) Thistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion
withtheintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn
toreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such
neuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For
example,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand
sortedsequences.Thisself-programming technologyisinitsinfancy,butinthe
futurecouldinprinciplebeappliedtonearlyanytask Anothercrowningachievementofdeeplearningisitsextensiontothedomainof
r e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous
agentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom
thehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem
basedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching
human-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015
alsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics
(,) Finn e t a l .2015
Manyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearning
isnowused bymanytoptechnologycompanies includi ngGoogle, Microsoft,
Facebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftware
infrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien
e t a l ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b
DistBelief(,),Caﬀe(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015
TensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015
commercialproducts Deeplearninghasalsomadecontributionsbacktoothersciences.Modern
convolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing
2 5
CHAPTER1.INTRODUCTION
thatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013
toolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin
scientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract
inordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014
tosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014
microscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-
Barley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁc
ﬁeldsinthefuture

============================================================

=== CHUNK 010 ===
Palavras: 350
Caracteres: 2944
--------------------------------------------------
Insummary,deeplearningisanapproachtomachinelearningthathasdrawn
heavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit
developedoverthepastseveraldecades.Inrecentyears,ithasseentremendous
growthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-
puters,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead
arefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand
bringittonewfrontiers 2 6
CHAPTER1.INTRODUCTION
1950 198520002015 2056
Year10− 210− 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)
123
456
78
91011
121314
151617
181920
SpongeRoundwormLeechAntBeeFrogOctopusHuman
Figure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubled
insizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom () Wikipedia2015
1.Perceptron(,,) Rosenblatt19581962
2.Adaptivelinearelement( ,) WidrowandHoﬀ1960
3.Neocognitron(Fukushima1980,)
4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b
5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)
6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991
7.Meanﬁeldsigmoidbeliefnetwork(,) Saul e t al.1996
8.LeNet-5( ,) LeCun e t al.1998b
9.Echostatenetwork( ,) JaegerandHaas2004
10.Deepbeliefnetwork( ,) Hinton e t al.2006
11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006
12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)
13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009
14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009
15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010
16.OMP-1network( ,) CoatesandNg2011
17.Distributedautoencoder(,) Le e t al.2012
18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012
19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013
20.GoogLeNet( ,) Szegedy e t al.2014a
2 7
CHAPTER1.INTRODUCTION
2010 2011 2012 2013 2014 2015
Year000 .005 .010 .015 .020 .025 .030 .ILSVRC  classiﬁcationerrorrate
Figure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet
LargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition
everyyear,andyieldedlowerandlowererrorrateseachtime DatafromRussakovsky
e t a l ()and2014b He().2015
2 8
P a rt I
AppliedMathandMachine
LearningBasics
29
This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o
unders t an d deep learning W e b e gin with general ideas f r om applied math t hat
allo w us t o deﬁne f unctions of many v ariables , ﬁ nd t he highes t and low e s t p oints
on t hes e f unctions and q uantify degrees of b e lief N e x t , w e des c r ib e t he f undamen t al goals of machine learning W e des c r ibe how
t o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,
des igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with
r e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction

============================================================

=== CHUNK 011 ===
Palavras: 352
Caracteres: 5227
--------------------------------------------------
This e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning
algorithms , including approac hes t o machine learning t hat are not deep In t he
s ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his
f r amew ork 3 0
C h a p t e r 2
L i n e ar A l ge b ra
Linearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience
andengineering.However,becauselinearalgebraisaformofcontinuousrather
thandiscretemathematics,manycomputerscientistshavelittleexperiencewithit Agoodunderstandingoflinearalgebraisessentialforunderstandingandworking
withmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We
thereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof
thekeylinearalgebraprerequisites Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If
youhavepreviousexperiencewiththeseconceptsbutneedadetailedreference
sheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand
Pedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter
willteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso
consultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas
Shilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra
topicsthatarenotessentialforunderstandingdeeplearning 2.1Scalars,Vectors,MatricesandTensors
Thestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:
•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother
objectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames Whenweintroducethem,wespecifywhatkindofnumbertheyare.For
31
CHAPTER2.LINEARALGEBRA
example,wemightsay“Let s∈ Rbetheslopeoftheline,”whiledeﬁninga
real-valuedscalar,or“Let n∈ Nbethenumberofunits,”whiledeﬁninga
naturalnumberscalar •Vectors: Avectorisanarrayofnumbers.Thenumbersarearrangedin
order.Wecanidentifyeachindividualnumberbyitsindexinthatordering Typicallywegivevectorslowercasenameswritteninboldtypeface,such
asx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalic
typeface,withasubscript.Theﬁrstelementofxis x 1,thesecondelement
is x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin
thevector.Ifeachelementisin R,andthevectorhas nelements,thenthe
vectorliesinthesetformedbytakingtheCartesianproductof R ntimes,
denotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,
wewritethemasacolumnenclosedinsquarebrackets:
x=
x 1
x 2 (2.1)
Wecanthinkofvectorsasidentifyingpointsinspace,witheachelement
givingthecoordinatealongadiﬀerentaxis Sometimesweneedtoindexasetofelementsofavector.Inthiscase,we
deﬁneasetcontainingtheindicesandwritethesetasasubscript.For
example,toaccess x 1, x 3and x 6,wedeﬁnetheset S={1 ,3 ,6}andwrite
x S.Weusethe−signtoindexthecomplementofaset.Forexamplex − 1is
thevectorcontainingallelementsofxexceptfor x 1,andx − Sisthevector
containingalloftheelementsofexceptforx x 1, x 3and x 6 •Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁed
bytwoindicesinsteadofjustone.Weusuallygivematricesupper-case
variablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas
aheightof mandawidthof n,thenwesaythatA∈ Rm n × Weusually
identifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,
andtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe
upperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall
ofthenumberswithverticalcoordinate ibywritinga“”forthehorizontal :
coordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith
verticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis
3 2
CHAPTER2.LINEARALGEBRA
A =
A 1 1 , A 1 2 ,
A 2 1 , A 2 2 ,
A 3 1 , A 3 2 ,
 ⇒ A=A 1 1 , A 2 1 , A 3 1 ,
A 1 2 , A 2 2 , A 3 2 ,
Figure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe
maindiagonal the-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA
amatrix,wewritethemasanarrayenclosedinsquarebrackets:
A 1 1 , A 1 2 ,
A 2 1 , A 2 2 , (2.2)
Sometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust
asingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo
notconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)
ofthematrixcomputedbyapplyingthefunctionto fA
•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha
variablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”
withthistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)
bywriting A i , j , k Oneimportantoperationonmatricesisthetranspose Thetransposeofa
matrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain
diagonal,runningdownandtotheright,startingfromitsupperleftcorner.See
ﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1
matrixasAA,anditisdeﬁnedsuchthat
(A) i , j= A j , i (2.3)
Vectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The
transposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe
3 3
CHAPTER2.LINEARALGEBRA
deﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,
thenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,
x= [ x 1 , x 2 , x 3] Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we
canseethatascalarisitsowntranspose: a a= 

============================================================

=== CHUNK 012 ===
Palavras: 353
Caracteres: 5192
--------------------------------------------------
Wecanaddmatricestoeachother,aslongastheyhavethesameshape,just
byaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just
byperformingthatoperationoneachelementofamatrix:D= a·B+ cwhere
D i , j= a B· i , j+ c Inthecontextofdeeplearning,wealsousesomelessconventionalnotation Weallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,
where C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe
matrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedinto
eachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations
iscalled .broadcasting
2.2MultiplyingMatricesandVectors
Oneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo
matrices.ThematrixproductofmatricesAandBisathirdmatrixC.In
orderforthisproducttobedeﬁned,Amusthavethesamenumberofcolumnsas
Bhasrows.IfAisofshape m n×andBisofshape n p×,thenCisofshape
m p×.Wecanwritethematrixproductjustbyplacingtwoormorematrices
together,e.g (2.4)
Theproductoperationisdeﬁnedby
C i , j=
kA i , k B k, j (2.5)
Notethatthestandardproductoftwomatricesisjustamatrixcontaining not
theproductoftheindividualelements.Suchanoperationexistsandiscalledthe
element-wiseproductHadamardproduct or ,andisdenotedas.AB
Thedotproductbetweentwovectorsxandyofthesamedimensionality
isthematrixproductxy.WecanthinkofthematrixproductC=ABas
computing C i , jasthedotproductbetweenrowofandcolumnof iA jB
3 4
CHAPTER2.LINEARALGEBRA
Matrixproductoperationshavemanyusefulpropertiesthatmakemathematical
analysis ofmatrices moreconvenient.For example, matrix m ultiplication is
distributive:
ABCABAC (+) = + (2.6)
Itisalsoassociative:
ABCABC ( ) = ( ) (2.7)
Matrixmultiplication iscommutative(thecondition not AB=BAdoesnot
alwayshold),unlikescalarmultiplication However,thedotproductbetweentwo
vectorsiscommutative:
xyy= x (2.8)
Thetransposeofamatrixproducthasasimpleform:
( )AB= BA (2.9)
Thisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8
ofsuchaproductisascalarandthereforeequaltoitsowntranspose:
xy=
xy
= yx (2.10)
Sincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto
developacomprehensivelistofusefulpropertiesofthematrixproducthere,but
thereadershouldbeawarethatmanymoreexist Wenowknowenoughlinearalgebranotationtowritedownasystemoflinear
equations:
Axb= (2.11)
whereA∈ Rm n ×isaknownmatrix,b∈ Rmisaknownvector,andx∈ Rnisa
vectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone
oftheseunknownvariables.EachrowofAandeachelementofbprovideanother
constraint.Wecanrewriteequationas:2.11
A 1 : ,x= b 1 (2.12)
A 2 : ,x= b 2 (2.13) (2.14)
A m , :x= b m (2.15)
or,evenmoreexplicitly,as:
A 1 1 , x 1+A 1 2 , x 2+ +···A 1 , n x n= b 1 (2.16)
3 5
CHAPTER2.LINEARALGEBRA

100
010
001

Figure2.2:Exampleidentitymatrix:ThisisI 3 A 2 1 , x 1+A 2 2 , x 2+ +···A 2 , n x n= b 2 (2.17) (2.18)
A m , 1 x 1+A m , 2 x 2+ +···A m , n x n= b m (2.19)
Matrix-vectorproductnotationprovidesamorecompactrepresentationfor
equationsofthisform 2.3IdentityandInverseMatrices
Linearalgebraoﬀersapowerfultoolcalledmatrixinversionthatallowsusto
analyticallysolveequationformanyvaluesof 2.11 A
Todescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentity
matrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe
multiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves
n-dimensionalvectorsasI n.Formally,I n∈ Rn n ×,and
∀∈x Rn,I nxx= (2.20)
Thestructureoftheidentitymatrixissimple:alloftheentriesalongthemain
diagonalare1,whilealloftheotherentriesarezero.Seeﬁgureforanexample.2.2
ThematrixinverseofAisdenotedasA− 1,anditisdeﬁnedasthematrix
suchthat
A− 1AI= n (2.21)
Wecannowsolveequationbythefollowingsteps: 2.11
Axb= (2.22)
A− 1AxA= − 1b (2.23)
I nxA= − 1b (2.24)
3 6
CHAPTER2.LINEARALGEBRA
xA= − 1b (2.25)
Ofcourse,thisprocessdependsonitbeingpossibletoﬁndA− 1.Wediscuss
theconditionsfortheexistenceofA− 1inthefollowingsection WhenA− 1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform Intheory,thesameinversematrixcanthenbeusedtosolvetheequationmany
timesfordiﬀerentvaluesofb.However,A− 1isprimarilyusefulasatheoretical
tool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications BecauseA− 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,
algorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate
estimatesof.x
2.4LinearDependenceandSpan
InorderforA− 1toexist,equationmusthaveexactlyonesolutionforevery 2.11
valueofb.However,itisalsopossibleforthesystemofequationstohaveno
solutionsorinﬁnitelymanysolutionsforsomevaluesofb Itisnotpossibleto
havemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;if
bothandaresolutionsthen xy
zxy = α+(1 )− α (2.26)
isalsoasolutionforanyreal α
Toanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns
ofAasspecifyingdiﬀerentdirectionswecantravelfromtheorigin(thepoint
speciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareof
reachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelin
eachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof
column: i
Ax=
ix iA : , i (2.27)
Ingeneral,thiskindofoperationiscalledalinearcombination.Formally,a
linearcombinationofsomesetofvectors{v( 1 ),

============================================================

=== CHUNK 013 ===
Palavras: 357
Caracteres: 8345
--------------------------------------------------
,v( ) n}isgivenbymultiplying
eachvectorv( ) ibyacorrespondingscalarcoeﬃcientandaddingtheresults:

ic iv( ) i (2.28)
Thespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination
oftheoriginalvectors 3 7
CHAPTER2.LINEARALGEBRA
DeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb
isinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn
spacerangeortheof.A
InorderforthesystemAx=btohaveasolutionforallvaluesofb∈ Rm,
wethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm
isexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas
nosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies
immediately thatAmusthaveatleast mcolumns,i.e., n m≥ Otherwise, the
dimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera
3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx
atbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution
ifandonlyifliesonthatplane.b
Having n m≥isonlyanecessaryconditionforeverypointtohaveasolution Itisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnsto
beredundant.Considera2×2matrixwherebothofthecolumnsareidentical Thishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthe
replicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto
encompassallof R2,eventhoughtherearetwocolumns Formally,thiskindofredundancyisknownaslineardependence.Asetof
vectorsislinearlyindependentifnovectorinthesetisalinearcombination
oftheothervectors Ifweaddavectortoasetthatisalinearcombinationof
theothervectorsintheset,thenewvectordoesnotaddanypointstotheset’s
span.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,
thematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This
conditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor 2.11
everyvalueofb.Notethattherequirementisforasettohaveexactly mlinear
independentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave
morethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan
mcolumnsmayhavemorethanonesuchset Inorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat
equationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto
ensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone
wayofparametrizing eachsolution Together,thismeansthatthematrixmustbesquare,thatis,werequirethat
m= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix
withlinearlydependentcolumnsisknownas.singular
IfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe
equation.However,wecannotusethemethodofmatrixinversiontoﬁndthe
3 8
CHAPTER2.LINEARALGEBRA
solution Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis
alsopossibletodeﬁneaninversethatismultipliedontheright:
AA− 1= I (2.29)
Forsquarematrices,theleftinverseandrightinverseareequal 2.5Norms
Sometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually
measurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm
isgivenby
||||x p=
i| x i|p 1
p
(2.30)
for p , p ∈ R≥1
Norms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative
values.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom
theorigintothepointx.Morerigorously,anormisanyfunction fthatsatisﬁes
thefollowingproperties:
• ⇒ f() = 0 xx= 0
• ≤ f(+) xy f f ()+x ()y(thetriangleinequality)
•∀∈ || α R , f α(x) = α f()x
The L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe
Euclideandistancefromtheorigintothepointidentiﬁedbyx.The L2normis
usedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with
thesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2
thesquared L2norm,whichcanbecalculatedsimplyasxx Thesquared L2normismoreconvenienttoworkwithmathematically and
computationally thanthe L2normitself.Forexample,thederivativesofthe
squared L2normwithrespecttoeachelementofxeachdependonlyonthe
correspondingelementofx,whileallofthederivativesofthe L2normdepend
ontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable
becauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning
3 9
CHAPTER2.LINEARALGEBRA
applications,itisimportanttodiscriminatebetweenelementsthatareexactly
zeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction
thatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:
the L1norm.The L1normmaybesimpliﬁedto
||||x 1=
i| x i| (2.31)
The L1normiscommonlyusedinmachinelearningwhenthediﬀerencebetween
zeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves
awayfrom0by,the  L1normincreasesby 
Wesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero
elements.Someauthorsrefertothisfunctionasthe“ L0norm,”butthisisincorrect
terminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because
scalingthevectorby αdoesnotchangethenumberofnonzeroentries The L1
normisoftenusedasasubstituteforthenumberofnonzeroentries Oneothernormthatcommonlyarisesinmachinelearningisthe L∞norm,
alsoknownasthemaxnorm.Thisnormsimpliﬁestotheabsolutevalueofthe
elementwiththelargestmagnitudeinthevector,
||||x ∞= max
i| x i| (2.32)
Sometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext
ofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure
Frobeniusnorm:
|||| A F=
i , jA2
i , j , (2.33)
whichisanalogoustothe L2normofavector Thedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,
xyx= |||| 2||||y 2cos θ (2.34)
whereistheanglebetweenand θ xy
2.6SpecialKindsofMatricesandVectors
Somespecialkindsofmatricesandvectorsareparticularlyuseful Diagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong
themaindiagonal Formally,amatrixDisdiagonalifandonlyif D i , j=0for
4 0
CHAPTER2.LINEARALGEBRA
all i= j Wehavealreadyseenoneexampleofadiagonalmatrix: theidentity
matrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare
diagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix
isverycomputationally eﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeach
element x iby v i.Inotherwords,diag(v)x=vx.Invertingasquarediagonal
matrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,
andinthatcase,diag(v)− 1=diag([1 /v 1 , ,1 /v n]).Inmanycases,wemay
derivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,
butobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome
matricestobediagonal Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular
diagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill
possibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the
productDxwillinvolvescalingeachelementofx,andeitherconcatenating some
zerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast
elementsofthevectorifiswiderthanitistall D
A matrixisanymatrixthatisequaltoitsowntranspose: symmetric
AA=  (2.35)
Symmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof
twoargumentsthatdoesnotdependontheorderofthearguments.Forexample,
ifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint
itopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric A isavectorwith : unitvectorunitnorm
||||x 2= 1 (2.36)
Avectorxandavectoryareorthogonaltoeachotherifxy= 0.Ifboth
vectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach
other.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm Ifthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem
orthonormal Anorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-
malandwhosecolumnsaremutuallyorthonormal:
AAAA= = I (2.37)
4 1
CHAPTER2.LINEARALGEBRA
Thisimpliesthat
A− 1= A, (2.38)
soorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute Paycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,
theirrowsarenotmerelyorthogonalbutfullyorthonormal Thereisnospecial
termforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal 2.7Eigendecomposition
Manymathematical objectscanbeunderstoodbetterbybreakingtheminto
constituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused
bythewaywechoosetorepresentthem Forexample,integerscanbedecomposedintoprimefactors.Thewaywe
representthenumberwillchangedependingonwhetherwewriteitinbaseten 12
orinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentation
wecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5
integermultipleofwillbedivisibleby

============================================================

=== CHUNK 014 ===
Palavras: 352
Caracteres: 6320
--------------------------------------------------
12 3
Muchaswecandiscoversomethingaboutthetruenatureofanintegerby
decomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat
showusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe
representationofthematrixasanarrayofelements Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-
decomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand
eigenvalues AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-
plicationbyaltersonlythescaleof: A v
Avv= λ (2.39)
Thescalar λisknownastheeigenvaluecorrespondingtothiseigenvector.(One
canalsoﬁndalefteigenvectorsuchthatvA= λv, butweareusually
concernedwithrighteigenvectors) IfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s ∈ R= 0 Moreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook
foruniteigenvectors SupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), ,
v( ) n},withcorrespondingeigenvalues { λ 1 , , λ n}.Wemayconcatenateallofthe
4 2
CHAPTER2.LINEARALGEBRA
         
    
                     
         

   
   
                                                     
Figure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehave
amatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue λ 1andv( 2 )with
eigenvalue λ 2 ( L e f t )Weplotthesetofallunitvectorsu∈ R2asaunitcircle ( R i g h t )We
plotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we
canseethatitscalesspaceindirectionv( ) iby λ i eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), ,
v( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , ,
λ n].The ofisthengivenby eigendecompositionA
AVλV = diag()− 1 (2.40)
Wehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-
torsallowsustostretchspaceindesireddirections Ho wever,weoftenwantto
decomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp
ustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger
intoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome
4 3
CHAPTER2.LINEARALGEBRA
cases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassof
matricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetric
matrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors
andeigenvalues:
AQQ = Λ, (2.41)
whereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Λisa
diagonalmatrix.TheeigenvalueΛ i , iisassociatedwiththeeigenvectorincolumn i
ofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas
scalingspaceby λ iindirectionv( ) i.Seeﬁgureforanexample.2.3
WhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-
tion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors
sharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan
arealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ
usingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Λ
indescendingorder.Underthisconvention,theeigendecompositionisuniqueonly
ifalloftheeigenvaluesareunique Theeigendecompositionof amatrix tellsus many usefulfactsabout the
matrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero Theeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize
quadraticexpressionsoftheform f(x) =xAxsubjectto||||x 2= 1.Wheneverx
isequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue Themaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue
anditsminimumvaluewithintheconstraintregionistheminimumeigenvalue Amatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.A
matrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁ-
nite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,and
ifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positive
semideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx ,Ax≥0 PositivedeﬁnitematricesadditionallyguaranteethatxAxx = 0 ⇒ = 0 2.8SingularValueDecomposition
Insection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues 2.7
Thesingularvaluedecomposition(SVD)providesanotherwaytofactorize
amatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto
discoversomeofthesamekindofinformationastheeigendecomposition.However,
4 4
CHAPTER2.LINEARALGEBRA
theSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue
decomposition,butthesameisnottrueoftheeigenvaluedecomposition.For
example,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwe
mustuseasingularvaluedecompositioninstead RecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover
amatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewrite
Aas
AVλV = diag()− 1 (2.42)
Thesingularvaluedecompositionissimilar,exceptthistimewewillwriteA
asaproductofthreematrices:
AUDV =  (2.43)
SupposethatAisan m n×matrix.ThenUisdeﬁnedtobean m m×matrix,
D V tobeanmatrix,and m n× tobeanmatrix n n×
Eachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesU
andVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobe
adiagonalmatrix.Notethatisnotnecessarilysquare D
TheelementsalongthediagonalofDareknownasthesingularvaluesof
thematrixA.ThecolumnsofUareknownastheleft-singularvectors.The
columnsofareknownasasthe V right-singularvectors WecanactuallyinterpretthesingularvaluedecompositionofAintermsof
theeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe
eigenvectorsofAA.Theright-singularvectorsofAaretheeigenvectorsofAA Thenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAA PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially
generalizematrixinversiontonon-squarematrices,aswewillseeinthenext
section 2.9TheMoore-PenrosePseudoinverse
Matrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewant
tomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA
Axy= (2.44)
4 5
CHAPTER2.LINEARALGEBRA
byleft-multiplyingeachsidetoobtain
xBy= (2.45)
Dependingonthestructureoftheproblem,itmaynotbepossibletodesigna
uniquemappingfromto.AB
IfAistallerthanitiswide, thenitispossibleforthisequationtohave
nosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible
solutions

============================================================

=== CHUNK 015 ===
Palavras: 391
Caracteres: 6756
--------------------------------------------------
TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin
thesecases.Thepseudoinverseofisdeﬁnedasamatrix A
A+=lim
α  0(AAI+ α)− 1A (2.46)
Practicalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-
tion,butrathertheformula
A+= VD+U, (2.47)
whereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse
D+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero
elementsthentakingthetransposeoftheresultingmatrix WhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe
pseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovides
thesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible
solutions WhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution Inthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas
possibletointermsofEuclideannorm y ||−||Axy 2 2.10TheTraceOperator
Thetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:
Tr() =A
iA i , i (2.48)
Thetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare
diﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing
4 6
CHAPTER2.LINEARALGEBRA
matrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides
analternativewayofwritingtheFrobeniusnormofamatrix:
|||| A F=
Tr(AA) (2.49)
Writinganexpressionintermsofthetraceoperatoropensupopportunitiesto
manipulatetheexpressionusingmanyusefulidentities Forexample,thetrace
operatorisinvarianttothetransposeoperator:
Tr() = Tr(AA) (2.50)
Thetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto
movingthelastfactorintotheﬁrstposition,iftheshapesofthecorresponding
matricesallowtheresultingproducttobedeﬁned:
Tr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)
ormoregenerally,
Tr(n
i = 1F( ) i) = Tr(F( ) nn − 1
i = 1F( ) i) (2.52)
Thisinvariancetocyclicpermutationholdseveniftheresultingproducthasa
diﬀerentshape.Forexample,forA∈ Rm n ×andB∈ Rn m ×,wehave
Tr( ) = Tr( )ABBA (2.53)
eventhoughAB∈ Rm m ×andBA∈ Rn n × Anotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a) 2.11TheDeterminant
Thedeterminant ofa squarematrix, denoted det(A), isa functionmapping
matricesto realscalars.Thedeterminant isequal totheproductof allthe
eigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought
ofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts
space.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast
onedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then
thetransformationpreservesvolume 4 7
CHAPTER2.LINEARALGEBRA
2.12Example:PrincipalComponentsAnalysis
Onesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA
canbederivedusingonlyknowledgeofbasiclinearalgebra Supposewehaveacollectionof mpoints{x( 1 ), ,x( ) m}in Rn.Supposewe
wouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans
storingthepointsinawaythatrequireslessmemorybutmaylosesomeprecision Wewouldliketoloseaslittleprecisionaspossible Onewaywecanencodethesepointsistorepresentalower-dimensionalversion
ofthem.Foreachpointx( ) i∈ Rnwewillﬁndacorrespondingcodevectorc( ) i∈ Rl If lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe
originaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecode
foraninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed
inputgivenitscode, .xx ≈ g f(())
PCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethe
decoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback
into Rn.Let,where g() = cDcD∈ Rn l ×isthematrixdeﬁningthedecoding Computingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.To
keeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal
toeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unless
l n= )
Withtheproblemasdescribedsofar,manysolutionsarepossible,becausewe
canincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive
theproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD
norm Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrst
thingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗for
eachinputpointx.Onewaytodothisistominimizethedistancebetweenthe
inputpointxanditsreconstruction, g(c∗).Wecanmeasurethisdistanceusinga
norm.Intheprincipalcomponentsalgorithm,weusethe L2norm:
c∗= argmin
c||− ||x g()c 2 (2.54)
Wecanswitchtothesquared L2norminsteadofthe L2normitself,because
bothareminimizedbythesamevalueofc.Bothareminimizedbythesame
valueofcbecausethe L2normisnon-negative andthesquaringoperationis
4 8
CHAPTER2.LINEARALGEBRA
monotonically increasingfornon-negative arguments c∗= argmin
c||− ||x g()c2
2 (2.55)
Thefunctionbeingminimizedsimpliﬁesto
( ())x− gc( ())x− gc (2.56)
(bythedeﬁnitionofthe L2norm,equation)2.30
= xxx−g g ()c−()cxc+( g)g()c (2.57)
(bythedistributiveproperty)
= xxx−2g g ()+c ()cg()c (2.58)
(becausethescalar g()cxisequaltothetransposeofitself) Wecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,
sincethistermdoesnotdependon:c
c∗= argmin
c−2xg g ()+c ()cg .()c (2.59)
Tomakefurtherprogress,wemustsubstituteinthedeﬁnitionof: g()c
c∗= argmin
c−2xDcc+DDc (2.60)
= argmin
c−2xDcc+I lc (2.61)
(bytheorthogonalityandunitnormconstraintson)D
= argmin
c−2xDcc+c (2.62)
Wecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3
youdonotknowhowtodothis):
∇ c(2−xDcc+c) = 0 (2.63)
−2Dxc+2= 0 (2.64)
cD= x (2.65)
4 9
CHAPTER2.LINEARALGEBRA
Thismakesthealgorithmeﬃcient: wecanoptimallyencodexjustusinga
matrix-vectoroperation.Toencodeavector,weapplytheencoderfunction
f() = xDx (2.66)
Usingafurthermatrixmultiplication, wecanalsodeﬁnethePCAreconstruction
operation:
r g f () = x (()) = xDDx (2.67)
Next,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea
ofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill
usethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe
pointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix
oferrorscomputedoveralldimensionsandallpoints:
D∗= argmin
D
i , j
x( ) i
j− r(x( ) i) j2
subjecttoDDI= l(2.68)
ToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecase
where l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67
intoequationandsimplifyinginto,theproblemreducesto 2.68 Dd
d∗= argmin
d
i||x( ) i−ddx( ) i||2
2subjectto||||d 2= 1 .(2.69)
Theaboveformulationisthemostdirectwayofperformingthesubstitution,
butisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe
scalarvaluedx( ) iontherightofthevectord.Itismoreconventionaltowrite
scalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywrite
suchaformulaas
d∗= argmin
d
i||x( ) i−dx( ) id||2
2subjectto||||d 2= 1 ,(2.70)
or,exploitingthefactthatascalarisitsowntranspose,as
d∗= argmin
d
i||x( ) i−x( ) i dd||2
2subjectto||||d 2= 1 .(2.71)
Thereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements

============================================================

=== CHUNK 016 ===
Palavras: 372
Caracteres: 10849
--------------------------------------------------
5 0
CHAPTER2.LINEARALGEBRA
Atthispoint,itcanbehelpfultorewritetheproblemintermsofasingle
designmatrixofexamples,ratherthanasasumoverseparateexamplevectors Thiswillallowustousemorecompactnotation.LetX∈ Rm n ×bethematrix
deﬁnedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i Wecannowrewritetheproblemas
d∗= argmin
d||−XXdd||2
Fsubjecttodd= 1 .(2.72)
Disregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm
portionasfollows:
argmin
d||−XXdd||2
F (2.73)
= argmin
dTr
XXdd −
XXdd −
(2.74)
(byequation)2.49
= argmin
dTr(XXX−Xdd−ddXXdd+XXdd)(2.75)
= argmin
dTr(XX)Tr(−XXdd)Tr(−ddXX)+Tr(ddXXdd)
(2.76)
= argmin
d−Tr(XXdd)Tr(−ddXX)+Tr(ddXXdd)(2.77)
(becausetermsnotinvolvingdonotaﬀectthe) d argmin
= argmin
d−2Tr(XXdd)+Tr(ddXXdd)(2.78)
(becausewecancycletheorderofthematricesinsideatrace,equation)2.52
= argmin
d−2Tr(XXdd)+Tr(XXdddd)(2.79)
(usingthesamepropertyagain)
Atthispoint,were-introducetheconstraint:
argmin
d−2Tr(XXdd)+Tr(XXdddd)subjecttodd= 1(2.80)
= argmin
d−2Tr(XXdd)+Tr(XXdd)subjecttodd= 1(2.81)
(duetotheconstraint)
= argmin
d−Tr(XXdd)subjecttodd= 1(2.82)
5 1
CHAPTER2.LINEARALGEBRA
= argmax
dTr(XXdd)subjecttodd= 1(2.83)
= argmax
dTr(dXXdd )subjecttod= 1(2.84)
Thisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,
theoptimaldisgivenbytheeigenvectorofXXcorrespondingtothelargest
eigenvalue Thisderivationisspeciﬁctothecaseof l=1andrecoversonlytheﬁrst
principalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal
components,thematrixDisgivenbythe leigenvectorscorrespondingtothe
largesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend
writingthisproofasanexercise Linearalgebraisoneofthefundamentalmathematical disciplinesthatis
necessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis
ubiquitousinmachinelearningisprobabilitytheory,presentednext 5 2
C h a p t e r 3
ProbabilityandInformation
Theory
Inthischapter,wedescribeprobabilitytheoryandinformationtheory Probabilitytheoryisamathematical frameworkforrepresentinguncertain
statements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving
newuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobability
theoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems
shouldreason,sowedesignouralgorithmstocomputeorapproximate various
expressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand
statisticstotheoreticallyanalyzethebehaviorofproposedAIsystems Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand
engineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis
primarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan
understandthematerialinthisbook Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin
thepresenceofuncertainty,informationtheoryallowsustoquantifytheamount
ofuncertaintyinaprobabilitydistribution Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you
maywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14
graphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If
youhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould
besuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo
suggestthatyouconsultanadditionalresource,suchasJaynes2003() 53
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
3.1WhyProbability Manybranchesofcomputersciencedealmostlywithentitiesthatareentirely
deterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill
executeeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butare
rareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount
forthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina
relativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning
makesheavyuseofprobabilitytheory Thisisbecausemachinelearningmustalwaysdealwithuncertainquantities,
andsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities Uncertaintyandstochasticitycanarisefrommanysources.Researchershavemade
compellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast
the1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired
byPearl1988() Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty Infact,beyondmathematical statementsthataretruebydeﬁnition,itisdiﬃcult
tothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely
guaranteedtooccur Therearethreepossiblesourcesofuncertainty:
1.Inherentstochasticityinthesystembeingmodeled.Forexample,most
interpretationsofquantummechanicsdescribethedynamicsofsubatomic
particlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat
wepostulatetohaverandomdynamics,suchasahypothetical cardgame
whereweassumethatthecardsaretrulyshuﬄedintoarandomorder 2.Incompleteobservability.Evendeterministicsystemscanappearstochastic
whenwecannotobserveallofthevariablesthatdrivethebehaviorofthe
system.Forexample,intheMontyHallproblem,agameshowcontestantis
askedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen
door.Twodoorsleadtoagoatwhileathirdleadstoacar Theoutcome
giventhecontestant’schoiceisdeterministic,butfromthecontestant’spoint
ofview,theoutcomeisuncertain 3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof
the information wehave observed, the discarded i nformationresults in
uncertaintyinthemodel’spredictions Forexample,supposewebuilda
robotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe
54
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
robotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,
thenthediscretizationmakestherobotimmediatelybecomeuncertainabout
theprecisepositionofobjects: eachobjectcouldbeanywherewithinthe
discretecellthatitwasobservedtooccupy Inmanycases,itismorepracticaltouseasimplebutuncertainrulerather
thanacomplexbutcertainone,evenifthetrueruleisdeterministicandour
modelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,the
simplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearule
oftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedto
ﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirds
includingthecassowary,ostrichandkiwi...” isexpensivetodevelop,maintainand
communicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure Whileitshouldbeclearthatweneedameansofrepresentingandreasoning
aboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide
allofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheory
wasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee
howprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof
cardsinagameofpoker.Thesekindsofeventsareoftenrepeatable Whenwe
saythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated
theexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportion
poftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot
seemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor
analyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,
thismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasof
thepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatient
wouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In
thecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta
degr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheﬂu
and0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu The
formerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is
knownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels
ofcertainty,isknownas B ay e si an pr o babili t y Ifwelistseveralpropertiesthatweexpectcommonsensereasoningabout
uncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat
Bayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker
gamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas
aswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe
55
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
hascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense
assumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,
see() Ramsey1926
Probabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic
providesasetofformalrulesfordeterminingwhatpropositionsareimpliedto
betrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue
orfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe
likelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions 3.2RandomVariables
A r andom v ar i abl eisavariablethatcantakeondiﬀerentvaluesrandomly.We
typicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,
andthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2
arebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued
variables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On
itsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it
mustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachof
thesestatesare Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable
isonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthese
statesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat
arenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis
associatedwitharealvalue 3.3ProbabilityDistributions
A pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor
setofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe
describeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor
continuous 3.3.1DiscreteVariablesandProbabilityMassFunctions
Aprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-
bi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith
acapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability
56
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
massfunctionandthereadermustinferwhichprobabilitymassfunctiontouse
basedontheidentityoftherandomvariable,ratherthanthenameofthefunction;
P P ()xisusuallynotthesameas()y Theprobabilitymassfunctionmapsfromastateofarandomvariableto
theprobabilityofthatrandomvariabletakingonthatstate.Theprobability
thatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis
certainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes
todisambiguatewhichPMFtouse,wewritethenameoftherandomvariable
explicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationto
specifywhichdistributionitfollowslater:xx ∼P()
Probabilitymassfunctionscanactonmanyvariablesatthesametime.Such
aprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y
di st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y
simultaneously.Wemayalsowrite forbrevity

============================================================

=== CHUNK 017 ===
Palavras: 352
Caracteres: 5176
--------------------------------------------------
Px,y()
Tobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust
satisfythefollowingproperties:
•Thedomainofmustbethesetofallpossiblestatesofx P
•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan 0 
belessprobablethanthat.Likewise,aneventthatisguaranteedtohappen
hasprobability,andnostatecanhaveagreaterchanceofoccurring 1
•
x ∈ xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without
thisproperty,wecouldobtainprobabilities greaterthanonebycomputing
theprobabilityofoneofmanyeventsoccurring Forexample,considerasinglediscreterandomvariablexwithkdiﬀerent
states.Wecanplacea uni f o r m di st r i but i o nonx—thatis,makeeachofits
statesequallylikely—bysettingitsprobabilitymassfunctionto
Px (= x i) =1
k(3.1)
foralli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction Thevalue1
kispositivebecauseisapositiveinteger.Wealsoseethat k

iPx (= x i) =
i1
k=k
k= 1, (3.2)
sothedistributionisproperlynormalized 57
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
3.3.2ContinuousVariablesandProbabilityDensityFunctions
Whenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-
butionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability
massfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe
followingproperties:
•Thedomainofmustbethesetofallpossiblestatesofx p
•∀∈ ≥ ≤ xx,px() 0 () p Notethatwedonotrequirex 1 Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁc
statedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith
volumeisgivenby δx pxδx()
Wecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofa
setofpoints.Speciﬁcally,theprobabilitythatxliesinsomeset Sisgivenbythe
integralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx
liesintheintervalisgivenby []a,b
[ ] a , bpxdx() Foranexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁc
probabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-
tiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),
whereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans
“parametrized by”;weconsiderxtobetheargumentofthefunction,whileaand
bareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobability
massoutsidetheinterval,wesayu(x;a,b)=0forallx∈[a,b] [.Withina,b],
uxa,b (;) =1
b a −.Wecanseethatthisisnonnegativeeverywhere.Additionally,it
integratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]
bywritingx ∼Ua,b()
3.4MarginalProbability
Sometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant
toknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability
distributionoverthesubsetisknownasthe distribution m ar g i nal pr o babili t y
Forexample,supposewehavediscreterandomvariablesxandy,andweknow
P,(xy.Wecanﬁndxwiththe : ) P() sum r ul e
∀∈xxx,P(= ) =x
yPx,y (= xy= ) (3.3)
58
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
Thename“marginalprobability”comesfromtheprocessofcomputingmarginal
probabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith
diﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturalto
sumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto
therightoftherow Forcontinuousvariables,weneedtouseintegrationinsteadofsummation:
px() =
px,ydy () (3.4)
3.5ConditionalProbability
Inmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome
othereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote
theconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This
conditionalprobabilitycanbecomputedwiththeformula
Pyx (= y |x= ) =Py,x (= yx= )
Px (= x ) (3.5)
TheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcompute
theconditionalprobabilityconditionedonaneventthatneverhappens Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat
wouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat
apersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif
arandomlyselectedpersonistaughttospeakGerman,theircountryoforigin
doesnotchange.Computingtheconsequencesofanactioniscalledmakingan
i n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,
whichwedonotexploreinthisbook 3.6TheChainRuleofConditionalProbabilities
Anyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed
intoconditionaldistributionsoveronlyonevariable:
P(x( 1 ),...,x( ) n) = (Px( 1 ))Πn
i = 2P(x( ) i|x( 1 ),...,x( 1 ) i −).(3.6)
Thisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability Itfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.5
59
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
Forexample,applyingthedeﬁnitiontwice,weget
P,,P,P, (abc)= (ab|c)(bc)
P,PP (bc)= ( )bc| ()c
P,,P,PP (abc)= (ab|c)( )bc| ()c
3.7IndependenceandConditionalIndependence
Tworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution
canbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving
onlyy:
∀∈ ∈xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y Tworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom
variableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis
wayforeveryvalueofz:
∀∈ ∈ ∈ | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z

============================================================

=== CHUNK 018 ===
Palavras: 364
Caracteres: 5994
--------------------------------------------------
(3.8)
We candenoteindependence andconditionalindependence with compact
notation:xy⊥meansthatxandyareindependent,whilexyz ⊥|meansthatx
andyareconditionallyindependentgivenz 3.8Expectation,VarianceandCovariance
The e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa
probabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx
isdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P
E x ∼ P[()] =fx
xPxfx, ()() (3.9)
whileforcontinuousvariables,itiscomputedwithanintegral:
E x ∼ p[()] =fx
pxfxdx ()() (3.10)
60
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
Whentheidentityofthedistributionisclearfromthecontext,wemaysimply
writethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)] Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthe
subscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[·]averagesover
thevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis
noambiguity,wemayomitthesquarebrackets Expectationsarelinear,forexample,
E x[()+ ()] = αfxβgxα E x[()]+fxβ E x[()]gx, (3.11)
whenandarenotdependenton αβ x
The v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom
variablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:
Var(()) = fx E
(() [()]) fx− Efx2 (3.12)
Whenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The
squarerootofthevarianceisknownasthe st andar d dev i at i o n
The c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated
toeachother,aswellasthescaleofthesevariables:
Cov(()()) = [(() [()])(() [()])] fx,gy Efx− Efxgy− Egy.(3.13)
Highabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch
andarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe
covarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues
simultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto
takeonarelativelyhighvalueatthetimesthattheothertakesonarelatively
lowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe
contributionofeachvariableinordertomeasureonlyhowmuchthevariablesare
related,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables Thenotionsofcovarianceanddependencearerelated,butareinfactdistinct
concepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero
covariance,andtwovariablesthathavenon-zerocovariancearedependent.How-
ever,independence isadistinctpropertyfromcovariance.Fortwovariablestohave
zerocovariance,theremustbenolineardependencebetweenthem.Independence
isastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes
nonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave
zerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfroma
uniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable
61
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
s.Withprobability1
2,wechoosethevalueofstobe.Otherwise,wechoose 1
thevalueofstobe−1.Wecanthengeneratearandomvariableybyassigning
y=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines
themagnitudeof.However,y Cov() = 0x,y The c o v ar i anc e m at r i xofarandomvector x∈ Rnisannn×matrix,such
that
Cov() x i , j= Cov(x i,x j) (3.14)
Thediagonalelementsofthecovariancegivethevariance:
Cov(x i,x i) = Var(x i) (3.15)
3.9CommonProbabilityDistributions
Severalsimpleprobabilitydistributionsareusefulinmanycontextsinmachine
learning 3.9.1BernoulliDistribution
The B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable Itiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityofthe
randomvariablebeingequalto1.Ithasthefollowingproperties:
P φ (= 1) = x (3.16)
P φ (= 0) = 1x − (3.17)
Pxφ (= x ) = x(1 )−φ1 − x(3.18)
E x[] = xφ (3.19)
Var x() = (1 )xφ−φ (3.20)
3.9.2MultinoulliDistribution
The m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete
variablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis
1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby
Murphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution Amultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany
timeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying
thattheyreferonlytothecase n= 1
62
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
parametrized byavector p∈[0,1]k − 1,wherep igivestheprobabilityofthei-th
state.Theﬁnal,k-thstate’sprobabilityisgivenby1− 1p.Notethatwemust
constrain 1p≤1.Multinoullidistributionsareoftenusedtorefertodistributions
overcategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical
value1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation
orvarianceofmultinoulli-dis tributedrandomvariables TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-
butionovertheirdomain They areabletodescribeanydistributionovertheir
domainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause
theirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto
enumerateallofthestates.Whendealingwithcontinuousvariables,thereare
uncountablymanystates,soanydistributiondescribedbyasmallnumberof
parametersmustimposestrictlimitsonthedistribution 3.9.3GaussianDistribution
Themostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-
t i o n,alsoknownasthe : G aussian di st r i but i o n
N(;xµ,σ2) =
1
2πσ2exp
−1
2σ2( )xµ−2
.(3.21)
Seeﬁgureforaplotofthedensityfunction 3.1
Thetwoparameters µ∈ Randσ∈(0,∞)controlthenormaldistribution Theparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof
thedistribution: E[x] =µ.Thestandarddeviationofthedistributionisgivenby
σ,andthevariancebyσ2 WhenweevaluatethePDF,weneedtosquareandinvertσ.Whenweneedto
frequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientway
ofparametrizing thedistributionistouseaparameterβ∈(0,∞)tocontrolthe
pr e c i si o norinversevarianceofthedistribution:
N(;xµ,β− 1) =
β
2πexp
−1
2βxµ (−)2

============================================================

=== CHUNK 019 ===
Palavras: 352
Caracteres: 5122
--------------------------------------------------
(3.22)
Normaldistributionsareasensiblechoiceformanyapplications.Intheabsence
ofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould
take,thenormaldistributionisagooddefaultchoicefortwomajorreasons 63
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
− − − − 20 x000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x µ
Inﬂectionpointsat
x µ σ = ±
Figure3.1:Thenormaldistribution:ThenormaldistributionN(x;µ,σ2)exhibits
aclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,and
thewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormal
distribution,withand µ= 0σ= 1
First,manydistributionswewishtomodelaretrulyclosetobeingnormal
distributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-
dentrandomvariablesisapproximatelynormallydistributed.Thismeansthat
inpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally
distributednoise,evenifthesystemcanbedecomposedintopartswithmore
structuredbehavior Second,outofallpossibleprobabilitydistributionswiththesamevariance,
thenormaldistributionencodesthemaximumamountofuncertaintyoverthe
realnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone
thatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping
andjustifyingthisidearequiresmoremathematical tools,andispostponedto
section.19.4.2
Thenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe
m ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive
deﬁnitesymmetricmatrix: Σ
N(; ) = x µ, Σ
1
(2)πndet() Σexp
−1
2( ) x µ−Σ− 1( ) x µ−
.(3.23)
64
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
Theparameter µstillgivesthemeanofthedistribution,thoughnowitis
vector-valued.Theparameter Σgivesthecovariancematrixofthedistribution Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor
manydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationally
eﬃcientwaytoparametrizethedistribution,sinceweneedtoinvert Σtoevaluate
thePDF.Wecaninsteadusea : pr e c i si o n m at r i x β
N(; x µ β,− 1) =
det() β
(2)πnexp
−1
2( ) x µ−β x µ (−)
.(3.24)
Weoftenﬁxthecovariancematrixtobeadiagonalmatrix.Anevensimpler
versionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar
timestheidentitymatrix 3.9.4ExponentialandLaplaceDistributions
Inthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution
withasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al
di st r i but i o n:
pxλλ (;) = 1 x ≥ 0exp( )−λx (3.25)
Theexponentialdistributionusestheindicatorfunction 1 x ≥ 0toassignprobability
zerotoallnegativevaluesof.x
Acloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak
ofprobabilitymassatanarbitrarypointistheµ L apl ac e di st r i but i o n
Laplace(;) =xµ,γ1
2γexp
−|−|xµ
γ (3.26)
3.9.5TheDiracDistributionandEmpiricalDistribution
Insomecases,wewishtospecifythatallofthemassinaprobabilitydistribution
clustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusing
theDiracdeltafunction,:δx()
pxδxµ () = (−) (3.27)
TheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept
0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat
associateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindof
65
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
mathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeﬁnedintermsofits
propertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe
limitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother
thanzero Bydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowand
inﬁnitelyhighpeakofprobabilitymasswhere.xµ= 
AcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l
di st r i but i o n,
ˆp() = x1
mm
i = 1δ( x x−( ) i) (3.28)
whichputsprobabilitymass1
moneachofthempoints x( 1 ),..., x( ) mforminga
givendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary
todeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,
thesituationissimpler:anempiricaldistributioncanbeconceptualized asa
multinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue
thatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset Wecanviewtheempiricaldistributionformedfromadatasetoftraining
examplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel
onthisdataset Anotherimportantperspectiveontheempiricaldistributionis
thatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata
(seesection).5.5
3.9.6MixturesofDistributions
Itisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimpler
probabilitydistributions.Onecommon wayof combining distributionsis to
constructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral
componentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution
generatesthesampleisdeterminedbysamplingacomponentidentityfroma
multinoullidistribution:
P() =x
iPiPi (= c )( = xc| ) (3.29)
wherecisthemultinoullidistributionovercomponentidentities P()
Wehavealreadyseenoneexampleofamixturedistribution:theempirical
distributionoverreal-valuedvariablesisamixturedistributionwithoneDirac
componentforeachtrainingexample

============================================================

=== CHUNK 020 ===
Palavras: 361
Caracteres: 5602
--------------------------------------------------
66
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
Themixturemodelisonesimplestrategyforcombiningprobabilitydistributions
tocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16
probabilitydistributionsfromsimpleonesinmoredetail Themixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeof
paramountimportancelater—the l at e n t v ar i abl e.Alatentvariableisarandom
variablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe
mixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough
thejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)
overthelatentvariableandthedistributionP(xc|)relatingthelatentvariables
tothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough
itispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent
variablesarediscussedfurtherinsection.16.5
Averypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e
model,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas
aseparatelyparametrized mean µ( ) iandcovariance Σ( ) i.Somemixturescanhave
moreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents
viatheconstraint Σ( ) i= Σ,i∀.AswithasingleGaussiandistribution,themixture
ofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe
diagonalorisotropic Inadditiontothemeansandcovariances,theparametersofaGaussianmixture
specifythe pr i o r pr o babili t yα i=P(c=i) giventoeachcomponenti.Theword
“prior”indicatesthatitexpressesthemodel’sbeliefsaboutc b e f o r eithasobserved
x.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed
a f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r
ofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany
speciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenough
components FigureshowssamplesfromaGaussianmixturemodel 3.2
3.10UsefulPropertiesofCommonFunctions
Certainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially
theprobabilitydistributionsusedindeeplearningmodels Oneofthesefunctionsisthe : l o g i st i c si g m o i d
σx() =1
1+exp()−x (3.30)
ThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoulli
67
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
x 1x 2
Figure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethree
components.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,
meaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal
covariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned
direction.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The
thirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance
separatelyalonganarbitrarybasisofdirections distributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues
fortheφparameter.Seeﬁgureforagraphofthesigmoidfunction.The 3.3
sigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,
meaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinits
input Anothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l () = log(1+exp()) (3.31)
Thesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormal
distributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulating
expressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe
factthatitisasmoothedor“softened”versionof
x+= max(0),x (3.32)
Seeﬁgureforagraphofthesoftplusfunction 3.4
Thefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize
them:
68
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
− − 1 0 5 0 5 1 0
x0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .σ x ( )
Figure3.3:Thelogisticsigmoidfunction − − 1 0 5 0 5 1 0
x024681 0ζ x ( )
Figure3.4:Thesoftplusfunction 69
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
σx() =exp()x
exp()+exp(0)x(3.33)
d
dxσxσxσx () = ()(1−()) (3.34)
1 () = () −σxσ−x (3.35)
log() = () σx −ζ−x (3.36)
d
dxζxσx () = () (3.37)
∀∈x(01),,σ− 1() = logxx
1−x
(3.38)
∀x>,ζ0− 1() = log(exp()1) x x− (3.39)
ζx() =x
− ∞σydy() (3.40)
ζxζxx ()−(−) = (3.41)
Thefunctionσ− 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely
usedinmachinelearning Equationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus 3.41
functionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=
max{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t
function,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothe
negativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepart
andnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverx
usingthesamerelationshipbetweenand,asshowninequation ζx()ζx(−) 3.41
3.11Bayes’Rule
WeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknow
P(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity
using B a y e s’ r ul e:
P( ) =xy|PP()x( )yx|
P()y (3.42)
NotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute
P() =y
xPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y 70
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
Bayes’ruleis straightforwardto derivefrom thedeﬁnitionofconditional
probability,butitisusefultoknowthenameofthisformulasincemanytexts
refertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrst
discoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas
independentlydiscoveredbyPierre-SimonLaplace 3.12TechnicalDetailsofContinuousVariables
Aproperformalunderstandingofcontinuousrandomvariablesandprobability
densityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof
mathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof
thistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryis
employedtoresolve

============================================================

=== CHUNK 021 ===
Palavras: 369
Caracteres: 7347
--------------------------------------------------
Insection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x
lyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices
ofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets
S 1and S 2suchthatp( x∈ S 1) +p( x∈ S 2)>1but S 1∩ S 2=∅.Thesesets
aregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofreal
numbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedby
transformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure
theoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe
probabilityofwithoutencounteringparadoxes Inthisbook,weonlyintegrate
oversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever
becomesarelevantconcern Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat
applytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory
providesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such
asetissaidtohave m e asur e z e r o.Wedonotformallydeﬁnethisconceptinthis
textbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthataset
ofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,
within R2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets
thateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational
numbershasmeasurezero,forinstance) Anotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty
thatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof
2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets 71
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
measurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they
canbesafelyignoredformanyapplications.Someimportantresultsinprobability
theoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuous
values Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous
randomvariablesthataredeterministicfunctionsofoneanother.Supposewehave
tworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-
tinuous,diﬀerentiabletransformation.Onemightexpectthatp y( y) =p x(g− 1( y)) Thisisactuallynotthecase Asasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose
y=x
2andx∼U(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0
everywhereexcepttheinterval[0,1
2] 1 ,anditwillbeonthisinterval.Thismeans

p y()=ydy1
2, (3.43)
whichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake Theproblemwiththisapproachisthatitfailstoaccountforthedistortionof
spaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan
inﬁnitesimallysmallregionwithvolumeδ xisgivenbyp( x)δ x.Sincegcanexpand
orcontractspace,theinﬁnitesimalvolumesurrounding xin xspacemayhave
diﬀerentvolumeinspace y
Toseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto
preservetheproperty
|p y(())= gxdy||p x()xdx.| (3.44)
Solvingfromthis,weobtain
p y() = yp x(g− 1())y∂x
∂y(3.45)
orequivalently
p x() = xp y(())gx∂gx()
∂x (3.46)
Inhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an
m at r i x—thematrixwithJ i , j=∂ x i
∂ y j.Thus,forreal-valuedvectorsand, x y
p x() = xp y(())g xdet∂g() x
∂ x  (3.47)
72
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
3.13InformationTheory
Informationtheory isa branchof appliedmathematics thatrevolvesaround
quantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented
tostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas
communicationviaradiotransmission.Inthiscontext,informationtheorytellshow
todesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom
speciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof
machinelearning,wecanalsoapplyinformationtheorytocontinuousvariables
wheresomeofthesemessagelengthinterpretations donotapply.Thisﬁeldis
fundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis
textbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize
probabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions Formoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or
().2003
Thebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely
eventhas occurredismoreinformativethanlearningthata likely eventhas
occurred.Amessagesaying“thesunrosethismorning”issouninformative as
tobeunnecessarytosend,butamessagesaying“therewasasolareclipsethis
morning”isveryinformative Wewouldliketoquantifyinformationinawaythatformalizesthisintuition Speciﬁcally,
•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,
eventsthatareguaranteedtohappenshouldhavenoinformationcontent
whatsoever •Lesslikelyeventsshouldhavehigherinformationcontent •Independenteventsshouldhaveadditiveinformation Forexample,ﬁnding
outthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas
muchinformationasﬁndingoutthatatossedcoinhascomeupasheads
once Inordertosatisfyallthreeoftheseproperties,wedeﬁnethe se l f - i nf o r m a t i o n
ofaneventxtobe = x
IxPx () = log− () (3.48)
Inthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our
deﬁnitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof
73
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
informationgainedbyobservinganeventofprobability1
e.Othertextsusebase-2
logarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis
justarescalingofinformationmeasuredinnats Whenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,
butsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent
withunitdensitystillhaszeroinformation, despitenotbeinganeventthatis
guaranteedtooccur Self-information dealsonlywithasingleoutcome.Wecanquantifytheamount
ofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:
H() = x E x ∼ P[()] = Ix − E x ∼ P[log()]Px (3.49)
alsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe
expectedamountofinformationinaneventdrawnfromthatdistribution.Itgives
alowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits
arediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)
havelowentropy;distributionsthatareclosertouniformhavehighentropy.See
ﬁgureforademonstration.When 3.5 xiscontinuous,theShannonentropyis
knownasthe di ﬀ e r e n t i al e nt r o p y IfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame
randomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusing
the K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:
D K L( ) = PQ E x ∼ P
logPx()
Qx()
= E x ∼ P[log()log()] Px−Qx.(3.50)
Inthecaseofdiscretevariables,itistheextraamountofinformation(measured
inbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2
andthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn
fromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize
thelengthofmessagesdrawnfromprobabilitydistribution.Q
TheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-
negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin
thecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuous
variables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerence
betweentwodistributions,itisoftenconceptualized asmeasuringsomesortof
distancebetweenthesedistributions.However,itisnotatruedistancemeasure
becauseitisnotsymmetric:D K L(PQ)=D K L(QP)forsomePandQ

============================================================

=== CHUNK 022 ===
Palavras: 359
Caracteres: 7075
--------------------------------------------------
This
74
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
0 0 0 2 0 4 0 6 0 8 1 0 p0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s
Figure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow
ShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy Onthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal
to.Theentropyisgivenby 1 (p−1)log(1−p)−pplog.Whenpisnear0,thedistribution
isnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,
thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1 Whenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo
outcomes asymmetrymeansthatthereareimportantconsequencestothechoiceofwhether
touseD K L( )PQorD K L( )QP.Seeﬁgureformoredetail.3.6
AquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y
H(P,Q) =H(P)+D K L(PQ),whichissimilartotheKLdivergencebutlacking
thetermontheleft:
HP,Q( ) = − E x ∼ Plog()Qx (3.51)
Minimizingthecross-entropywithrespecttoQisequivalenttominimizingthe
KLdivergence,becausedoesnotparticipateintheomittedterm Q
Whencomputingmanyofthesequantities,itiscommontoencounterexpres-
sionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we
treattheseexpressionsaslim x → 0xxlog= 0 3.14StructuredProbabilisticModels
Machinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery
largenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve
directinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto
75
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
xProbability Densityq∗= argminq D K L() p q 
p x()
q∗() x
xProbability Densityq∗= argminq D K L() q p 
p() x
q∗() x
Figure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and
wishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing
eitherD KL(pq)orD KL(qp).Weillustratetheeﬀectofthischoiceusingamixtureof
twoGaussiansforp,andasingleGaussianforq Thechoiceofwhichdirectionofthe
KLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation
thatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh
probability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh
probabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe
directionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeach
application ( L e f t )TheeﬀectofminimizingD KL(pq).Inthiscase,weselectaqthathas
highprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto
blurthemodestogether,inordertoputhighprobabilitymassonallofthem ( R i g h t )The
eﬀectofminimizingD KL(qp).Inthiscase,weselectaqthathaslowprobabilitywhere
phaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,
asinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto
avoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we
illustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave
achievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes
arenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionofthe
KLdivergencecanstillchoosetoblurthemodes 76
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
describetheentirejointprobabilitydistributioncanbeveryineﬃcient(both
computationally andstatistically) Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,we
cansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether Forexample,supposewehavethreerandomvariables:a,bandc.Supposethat
ainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcare
independentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree
variablesasaproductofprobabilitydistributionsovertwovariables:
p,,ppp (abc) = ()a( )ba|( )cb| (3.52)
Thesefactorizationscangreatlyreducethenumberofparametersneeded
todescribethedistribution.Eachfactorusesanumberofparametersthatis
exponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly
reducethecostofrepresentingadistributionifweareabletoﬁndafactorization
intodistributionsoverfewervariables Wecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword
“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach
otherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution
withagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del Therearetwomainkindsofstructuredprobabilisticmodels:directedand
undirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode
inthegraphcorrespondstoarandomvariable, and anedgeconnectingtwo
randomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect
interactionsbetweenthosetworandomvariables D i r e c t e dmodelsuse graphswithdirectededges, andtheyrepresentfac-
torizationsintoconditionalprobabilitydistributions,asintheexampleabove Speciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin
thedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i
giventheparentsofx i,denotedPa G(x i):
p() = x
ip(x i|Pa G(x i)) (3.53)
Seeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability 3.7
distributionsitrepresents U ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent
factorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions
77
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
aa
ccbb
eedd
Figure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph
correspondstoprobabilitydistributionsthatcanbefactoredas
p,,,,ppp,pp (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)
Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a
andcinteractdirectly,butaandeinteractonlyindirectlyviac areusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall
connectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected
modelisassociatedwithafactorφ( ) i(C( ) i).Thesefactorsarejustfunctions,not
probabilitydistributions.Theoutputofeachfactormustbenon-negative, but
thereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability
distribution Theprobabilityofaconﬁgurationofrandomvariablesis pr o p o r t i o naltothe
productofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare
morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We
thereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegral
overallstatesoftheproductoftheφfunctions,inordertoobtainanormalized
probabilitydistribution:
p() = x1
Z
iφ( ) i
C( ) i (3.55)
Seeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8
probabilitydistributionsitrepresents Keep inmind thatthese graphicalrepresentationsof factorizations are a
languagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive
familiesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty
ofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa
78
CHAPTER3.PROBABILITYANDINFORMATIONTHEORY
aa
ccbb
eedd
Figure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This
graphcorrespondstoprobabilitydistributionsthatcanbefactoredas
p,,,, (abcde) =1
Zφ( 1 )( )abc,,φ( 2 )()bd,φ( 3 )()ce,

============================================================

=== CHUNK 023 ===
Palavras: 364
Caracteres: 6692
--------------------------------------------------
(3.56)
Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a
andcinteractdirectly,butaandeinteractonlyindirectlyviac probabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth
ways Throughoutpartsandofthisbook,wewillusestructuredprobabilistic III
modelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships
diﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding
ofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,
inpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III
detail Thischapterhasreviewedthebasicconceptsofprobabilitytheorythatare
mostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools
remains:numericalmethods 79
C h a p t e r 4
NumericalComputation
Machinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-
tation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby
methodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan
analyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-
lution.Commonoperationsincludeoptimization (ﬁndingthevalueofanargument
thatminimizesormaximizesafunction)andsolvingsystemsoflinearequations Evenjustevaluatingamathematical functiononadigitalcomputercanbediﬃcult
whenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely
usingaﬁniteamountofmemory 1 O v erﬂ o w an d Un d erﬂ o w
Thefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputer
isthatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumber
ofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursome
approximationerrorwhenwerepresentthenumberinthecomputer.Inmany
cases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen
itcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin
theorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof
roundingerror Oneformofroundingerrorthatisparticularlydevastatingis under ﬂo w Underﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions
behavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmall
positivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some
80
CHAPTER4.NUMERICALCOMPUTATION
softwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna
resultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this
isusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformany
furtherarithmeticoperations) Anotherhighlydamagingformofnumericalerroris o v e r ﬂo w.Overﬂowoccurs
whennumberswithlargemagnitudeareapproximatedas∞or−∞.Further
arithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowand
overﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe
probabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis
deﬁnedtobe
softmax() x i=exp( x i)n
j = 1exp( x j) (4.1)
Considerwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,
wecanseethatalloftheoutputsshouldbeequalto1
n.Numerically,thismay
notoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will
underﬂow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheﬁnal
resultisundeﬁned.When cisverylargeandpositive,exp( c)willoverﬂow,again
resultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃculties
canberesolvedbyinsteadevaluating softmax( z)where z= x−max i x i.Simple
algebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby
addingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults
inthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout
thepossibilityofunderﬂowinthedenominator leadingtoadivisionbyzero Thereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcause
theexpressionasawholetoevaluatetozero.Thismeansthatifweimplement
logsoftmax( x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresultto
thelogfunction,wecoulderroneouslyobtain −∞.Instead,wemustimplement
aseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The
logsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize
thefunction softmax
Forthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations
involvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers
oflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing
deeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-
levellibrariesthatprovidestableimplementations .Insomecases,itispossible
toimplementanewalgorithmandhavethenewimplementation automatically
8 1
CHAPTER4.NUMERICALCOMPUTATION
stabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012
ofasoftwarepackagethatautomatically detectsandstabilizesmanycommon
numericallyunstableexpressionsthatariseinthecontextofdeeplearning 2 P o or C on d i t i o n i n g
Conditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges
initsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly
canbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputs
canresultinlargechangesintheoutput Considerthefunction f( x)= A− 1x.When A∈ Rn n ×hasaneigenvalue
decomposition,its c o ndi t i o n num beris
max
i , jλ i
λ j (4.2)
Thisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When
thisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput Thissensitivityisanintrinsicpropertyofthematrixitself,nottheresult
ofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify
pre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the
errorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself 3 Gradi en t - Bas e d O p t i m i z a t i o n
Mostdeeplearningalgorithmsinvolveoptimization ofsomesort Optimization
referstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering
x Weusuallyphrasemostoptimization problemsintermsofminimizing f( x) Maximization maybeaccomplishedviaaminimization algorithmbyminimizing
− f() x Thefunctionwewanttominimizeormaximizeiscalledthe o b j e c t i v e f unc -
t i o nor c r i t e r i o n.Whenweareminimizingit, wemayalsocallitthe c o st
f unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n Inthisbook,weusetheseterms
interchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning
tosomeoftheseterms Weoftendenotethevaluethatminimizesormaximizesafunctionwitha
superscript.Forexample,wemightsay ∗ x∗= argmin() f x 8 2
CHAPTER4.NUMERICALCOMPUTATION
− − − − 20 x−20.−15.−10.−05.00.05.10.15.20 Globalminimumat= 0.x
Sincef() = 0,gradient x
descent haltshere For 0,wehave x< f() 0,x<
sowecandecreasebyf
moving rightward.For 0,wehave x> f() 0,x>
sowecandecreasebyf
moving leftward

============================================================

=== CHUNK 024 ===
Palavras: 388
Caracteres: 4720
--------------------------------------------------
f x() =1
2x2
f() = x x
Figure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa
functioncanbeusedtofollowthefunctiondownhilltoaminimum Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief
reviewofhowcalculusconceptsrelatetooptimization here Supposewehaveafunction y= f( x),whereboth xand yarerealnumbers The der i v at i v eofthisfunctionisdenotedas f( x)orasd y
d x.Thederivative f( x)
givestheslopeof f( x)atthepoint x.Inotherwords,itspeciﬁeshowtoscale
asmallchangeintheinputinordertoobtainthecorrespondingchangeinthe
output: f x  f x  f (+) ≈()+() x Thederivativeisthereforeusefulforminimizingafunctionbecauseittells
ushowtochange xinordertomakeasmallimprovementin y.Forexample,
weknowthat f( x −sign( f( x)))islessthan f( x)forsmallenough .Wecan
thusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative Thistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeﬁgureforan4.1
exampleofthistechnique When f( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection
tomove.Pointswhere f( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y
p o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring
points,soitisnolongerpossibletodecrease f( x)bymakinginﬁnitesimalsteps A l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,
8 3
CHAPTER4.NUMERICALCOMPUTATION
Minimum Maximum Saddlepoint
Figure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis
apointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan
theneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or
asaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself soitisnotpossibletoincrease f( x)bymakinginﬁnitesimalsteps.Somecritical
pointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See
ﬁgureforexamplesofeachtypeofcriticalpoint 4.2
Apointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um Itispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof
thefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally
optimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave
manylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby
veryﬂatregions.Allofthismakesoptimization verydiﬃcult,especiallywhenthe
inputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndinga
valueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See
ﬁgureforanexample.4.3
Weoftenminimizefunctionsthathavemultipleinputs: f: Rn→ R.Forthe
conceptof“minimization” to makesense,theremuststillbeonlyone(scalar)
output Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al
der i v at i v e s.Thepartialderivative∂
∂ x if( x)measureshow fchangesasonlythe
variable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative
tothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe
vectorcontainingallofthepartialderivatives,denoted ∇ x f( x).Element iofthe
gradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,
8 4
CHAPTER4.NUMERICALCOMPUTATION
xf x()
Ideally,wewouldlike
toarriveattheglobal
minimum, butthis
might notbepossible.Thislocalminimum
performsnearlyaswellas
theglobalone,
soitisanacceptable
haltingpoint Thislocalminimumperforms
poorlyandshouldbeavoided Figure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhenthereare
multiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally
acceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond
tosigniﬁcantlylowvaluesofthecostfunction criticalpointsarepointswhereeveryelementofthegradientisequaltozero The di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u
function findirection u.Inotherwords,thedirectionalderivativeisthederivative
ofthefunction f( x+ α u)withrespectto α,evaluatedat α= 0.Usingthechain
rule,wecanseethat∂
∂ αf α (+ x u)evaluatesto u∇ x f α () xwhen = 0 Tominimize f,wewouldliketoﬁndthedirectioninwhich fdecreasesthe
fastest.Wecandothisusingthedirectionalderivative:
min
u u , u = 1u∇ x f() x (4.3)
=min
u u , u = 1|||| u 2||∇ x f() x|| 2cos θ (4.4)
where θistheanglebetween uandthegradient.Substitutingin|||| u 2= 1and
ignoringfactorsthatdonotdependon u,thissimpliﬁestomin ucos θ.Thisis
minimizedwhen upointsintheoppositedirectionasthegradient.Inother
words,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly
downhill.Wecandecrease fbymovinginthedirectionofthenegativegradient m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt
Steepestdescentproposesanewpoint
x= x−∇  x f() x (4.5)
8 5
CHAPTER4.NUMERICALCOMPUTATION
where isthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep

============================================================

=== CHUNK 025 ===
Palavras: 352
Caracteres: 4757
--------------------------------------------------
Wecanchoose inseveraldiﬀerentways.Apopularapproachistoset toasmall
constant.Sometimes,wecansolveforthestepsizethatmakesthedirectional
derivativevanish.Anotherapproachistoevaluate f  ( x−∇ x f()) xforseveral
valuesof andchoosetheonethatresultsinthesmallestobjectivefunctionvalue Thislaststrategyiscalleda l i ne se ar c h Steepestdescentconvergeswheneveryelementofthegradientiszero(or,in
practice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis
iterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe
equation ∇ x f() = 0 xfor x
Althoughgradientdescentislimitedtooptimization incontinuousspaces,the
generalconceptofrepeatedlymakingasmallmove(thatisapproximately thebest
smallmove)towardsbetterconﬁgurations canbegeneralizedtodiscretespaces Ascendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng
( ,) RusselandNorvig2003
4 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces
Sometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinput
andoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis
knownasa J ac o bi an m at r i x.Speciﬁcally,ifwehaveafunction f: Rm→ Rn,
thentheJacobianmatrix J∈ Rn m ×ofisdeﬁnedsuchthat f J i , j=∂
∂ x jf() x i Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknown
asa se c o nd der i v at i v e.Forexample,forafunction f: Rn→ R,thederivative
withrespectto x iofthederivativeof fwithrespectto x jisdenotedas∂2
∂ x i ∂ x jf Inasingledimension,wecandenoted2
d x2 fby f ( x).Thesecondderivativetells
ushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportant
becauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement
aswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond
derivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many
functionsthatariseinpracticearenotquadraticbutcanbeapproximated well
asquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,
thenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredicted
usingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 
alongthenegativegradient,andthecostfunctionwilldecreaseby .Ifthesecond
derivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill
actuallydecreasebymorethan .Finally,ifthesecondderivativeispositive,the
functioncurvesupward,sothecostfunctioncandecreasebylessthan .See
8 6
CHAPTER4.NUMERICALCOMPUTATION
xf x()N e g a t i v e c u r v a t u r e
xf x()N o c u r v a t u r e
xf x()P o s i t i v e c u r v a t u r e
Figure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow
quadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost
functionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient
stepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster
thanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease
correctly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected
andeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe
functioninadvertently ﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween 4.4
thevalueofthecostfunctionpredictedbythegradientandthetruevalue Whenourfunctionhasmultipleinputdimensions,therearemanysecond
derivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe
Hessian m at r i x.TheHessianmatrix isdeﬁnedsuchthat H x()( f)
H x()( f) i , j=∂2
∂ x i ∂ x jf .() x (4.6)
Equivalently,theHessianistheJacobianofthegradient Anywherethatthesecondpartialderivativesarecontinuous,thediﬀerential
operatorsarecommutative,i.e.theirordercanbeswapped:
∂2
∂ x i ∂ x jf() = x∂2
∂ x j ∂ x if .() x (4.7)
Thisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric
Hessianalmosteverywhere Because theHessianmatrixisrealandsymmetric,
wecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof
8 7
CHAPTER4.NUMERICALCOMPUTATION
eigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunit
vector disgivenby dH d.When disaneigenvectorof H,thesecondderivative
inthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof
d,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,
withweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d
receivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond
derivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative The(directional)secondderivativetellsushowwellwecanexpectagradient
descentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation
tothefunction aroundthecurrentpoint f() x x( 0 ):
f f () x≈( x( 0 ))+( x x−( 0 ))g+1
2( x x−( 0 ))H x x (−( 0 )) .(4.8)
where gisthegradientand HistheHessianat x( 0 )

============================================================

=== CHUNK 026 ===
Palavras: 395
Caracteres: 7303
--------------------------------------------------
Ifweusealearningrate
of ,thenthenewpoint xwillbegivenby x( 0 )−  g.Substitutingthisintoour
approximation,weobtain
f( x( 0 )− ≈  g) f( x( 0 ))−  gg+1
22gH g (4.9)
Therearethree termshere:theoriginalvalue ofthefunction, the expected
improvementduetotheslopeofthefunction,andthecorrectionwemustapply
toaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the
gradientdescentstepcanactuallymoveuphill.When gH giszeroornegative,
theTaylorseriesapproximationpredictsthatincreasing foreverwilldecrease f
forever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge ,so
onemustresorttomoreheuristicchoicesof inthiscase.When gH gispositive,
solvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of
thefunctionthemostyields
∗=gg
gH g (4.10)
Intheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe
maximaleigenvalue λ m a x,thenthisoptimalstepsizeisgivenby1
λmax.Tothe
extentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic
function,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning
rate Thesecondderivativecanbeusedtodeterminewhetheracriticalpointis
alocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical
point, f( x) = 0.Whenthesecondderivative f ( x) >0,theﬁrstderivative f( x)
increasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans
8 8
CHAPTER4.NUMERICALCOMPUTATION
f( x −) <0and f( x+ ) >0forsmallenough .Inotherwords,aswemove
right,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope
beginstopointuphilltotheleft Thus,when f( x)=0and f ( x) >0,wecan
concludethat xisalocalminimum.Similarly,when f( x) = 0and f ( x) <0,we
canconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e
t e st.Unfortunately,when f ( x) = 0,thetestisinconclusive.Inthiscase xmay
beasaddlepoint,orapartofaﬂatregion Inmultipledimensions,weneedtoexamineallofthesecondderivativesofthe
function.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize
thesecondderivativetesttomultipledimensions.Atacriticalpoint,where
∇ x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether
thecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe
Hessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocal
minimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative
inanydirectionmustbepositive,andmakingreferencetotheunivariatesecond
derivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvalues
arenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually
possibletoﬁndpositiveevidenceofsaddlepointsinsomecases Whenatleast
oneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat
xisalocalmaximumononecrosssectionof fbutalocalminimumonanother
crosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond 4.5
derivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis
inconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat
leastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis
inconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue Inmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirection
atasinglepoint.TheconditionnumberoftheHessianatthispointmeasures
howmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasa
poorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone
direction,thederivativeincreasesrapidly,whileinanotherdirection,itincreases
slowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot
knowthatitneedstoexplorepreferentially inthedirectionwherethederivative
remainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize Thestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing
uphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe
stepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithless
curvature.Seeﬁgureforanexample.4.6
ThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide
8 9
CHAPTER4.NUMERICALCOMPUTATION
        
     
Figure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction
inthisexampleis f( x)= x2
1− x2
2.Alongtheaxiscorrespondingto x 1,thefunction
curvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue Alongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan
eigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfrom
thesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction
withasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue
of0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative
eigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal
maximumwithinonecrosssectionandalocalminimumwithinanothercrosssection 9 0
CHAPTER4.NUMERICALCOMPUTATION
− − − 3 0 2 0 1 0 0 1 0 2 0
x 1− 3 0− 2 0− 1 001 02 0x 2
Figure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe
Hessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose
Hessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature
hasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost
curvatureisinthedirection[1 ,1]andtheleastcurvatureisinthedirection[1 ,−1].The
redlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic
functionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending
canyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat
toolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto
descendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue
oftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat
thisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon
theHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch
directioninthiscontext 9 1
CHAPTER4.NUMERICALCOMPUTATION
thesearch.Thesimplestmethodfordoingsoisknownas Newt o n’ s m e t ho d Newton’smethodisbasedonusingasecond-orderTaylorseriesexpansionto
approximatenearsomepoint f() x x( 0 ):
f f () x≈( x( 0 ))+( x x−( 0 ))∇ x f( x( 0 ))+1
2( x x−( 0 ))H x()( f( 0 ))( x x−( 0 )) .(4.11)
Ifwethensolveforthecriticalpointofthisfunction,weobtain:
x∗= x( 0 )− H x()( f( 0 ))− 1∇ x f( x( 0 )) (4.12)
When fisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsof
applyingequationoncetojumptotheminimumofthefunctiondirectly 4.12
When fisnottrulyquadraticbutcanbelocallyapproximatedasapositive
deﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12
times Iterativelyupdatingtheapproximation andjumpingtotheminimumof
theapproximation canreachthecriticalpointmuchfasterthangradientdescent
would.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful
propertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis 8.2.3
onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues
oftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle
pointsunlessthegradientpointstowardthem Optimization algorithmsthatuseonlythegradient,suchasgradientdescent,
arecalled ﬁr st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat
alsousetheHessianmatrix,suchasNewton’smethod,arecalled se c o nd-or d e r
o pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,)

============================================================

=== CHUNK 027 ===
Palavras: 380
Caracteres: 4448
--------------------------------------------------
The optimization algorithms employedin mostcontextsin this book are
applicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees Deeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions
usedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominant
approachtooptimization istodesignoptimization algorithmsforalimitedfamily
offunctions Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-
ingourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz
continuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate
ofchangeisboundedbya L i psc hi t z c o nst antL:
∀∀| − |≤L||−|| x , y , f() x f() y x y 2 (4.13)
Thispropertyisusefulbecauseitallowsustoquantifyourassumptionthata
smallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave
9 2
CHAPTER4.NUMERICALCOMPUTATION
asmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,
andmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous
withrelativelyminormodiﬁcations Perhapsthemostsuccessfulﬁeldofspecializedoptimization is c o n v e x o p-
t i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore
guaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare
applicableonlytoconvexfunctions—functionsforwhichtheHessianispositive
semideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle
pointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most
problemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization Convexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms Ideasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving
theconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance
ofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For
moreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()
orRockafellar1997() 4 C on s t ra i n ed O p t i m i z a t i o n
Sometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall
possible values of x.Insteadwemay wishto ﬁnd themaximal or minimal
value of f( x)for valuesof xinsome set S.Thisis known as c o nst r ai n e d
o pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin
constrainedoptimization terminology Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon
approachinsuchsituationsistoimposeanormconstraint,suchas ||||≤ x 1
Onesimpleapproachtoconstrainedoptimization issimplytomodifygradient
descenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize ,
wecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse
alinesearch,wecansearchonlyoverstepsizes thatyieldnew xpointsthatare
feasible,orwecanprojecteachpointonthelinebackintotheconstraintregion Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradient
intothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning
thelinesearch(,).Rosen1960
Amoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-
mizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,
constrainedoptimization problem.Forexample,ifwewanttominimize f( x)for
9 3
CHAPTER4.NUMERICALCOMPUTATION
x∈ R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize
g( θ) = f([cossin θ , θ])withrespectto θ,thenreturn[cossin θ , θ]asthesolution
totheoriginalproblem.Thisapproachrequirescreativity;thetransformation
betweenoptimization problemsmustbedesignedspeciﬁcallyforeachcasewe
encounter The K ar ush– K u h n – T uc k e r(KKT)approach1providesaverygeneralso-
lutiontoconstrainedoptimization WiththeKKTapproach,weintroducea
newfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange
f unc t i o n TodeﬁnetheLagrangian,weﬁrstneedtodescribe Sintermsofequations
andinequalities W ewantadescriptionof Sintermsof mfunctions g( ) iand n
functions h( ) jsothat S={|∀ x i , g( ) i( x) = 0and∀ j , h( ) j( x)≤0}.Theequations
involving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving
h( ) jarecalled i neq ual i t y c o nst r ai n t s
Weintroducenewvariables λ iand α jforeachconstraint,thesearecalledthe
KKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedas
L , , f ( x λ α) = ()+ x
iλ i g( ) i()+ x
jα j h( ) j() x .(4.14)
Wecannowsolveaconstrainedminimization problemusingunconstrained
optimization ofthegeneralizedLagrangian.Observethat,solongasatleastone
feasiblepointexistsandisnotpermittedtohavevalue,then f() x ∞
min
xmax
λmax
α α , ≥ 0L , ,

============================================================

=== CHUNK 028 ===
Palavras: 355
Caracteres: 3445
--------------------------------------------------
( x λ α) (4.15)
hasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x
min
x ∈ Sf .() x (4.16)
Thisfollowsbecauseanytimetheconstraintsaresatisﬁed,
max
λmax
α α , ≥ 0L , , f , ( x λ α) = () x (4.17)
whileanytimeaconstraintisviolated,
max
λmax
α α , ≥ 0L , , ( x λ α) = ∞ (4.18)
1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y
c o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s 9 4
CHAPTER4.NUMERICALCOMPUTATION
Thesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe
optimumwithinthefeasiblepointsisunchanged Toperformconstrainedmaximization, wecanconstructthegeneralizedLa-
grangefunctionof,whichleadstothisoptimization problem: − f() x
min
xmax
λmax
α α , ≥ 0− f()+ x
iλ i g( ) i()+ x
jα j h( ) j() x .(4.19)
Wemayalsoconvertthistoaproblemwithmaximization intheouterloop:
max
xmin
λmin
α α , ≥ 0f()+ x
iλ i g( ) i() x−
jα j h( ) j() x .(4.20)
Thesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneit
withadditionorsubtractionaswewish,becausetheoptimization isfreetochoose
anysignforeach λ i Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint
h( ) i( x)is ac t i v eif h( ) i( x∗) = 0.Ifaconstraintisnotactive,thenthesolutionto
theproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif
thatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes
othersolutions.Forexample,aconvexproblemwithanentireregionofglobally
optimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthis
regioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal
stationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,
thepointfoundatconvergenceremainsastationarypointwhetherornotthe
inactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then
thesolutiontomin xmax λmax α α , ≥ 0 L( x λ α , ,)willhave α i=0.Wecanthus
observethatatthesolution, α h( x)= 0.Inotherwords,forall i,weknow
thatatleastoneoftheconstraints α i≥0and h( ) i( x)≤0mustbeactiveatthe
solution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution
isontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier
toinﬂuencethesolutionto x,ortheinequalityhasnoinﬂuenceonthesolution
andwerepresentthisbyzeroingoutitsKKTmultiplier Asimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-
mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)
conditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,
butnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:
•ThegradientofthegeneralizedLagrangianiszero •AllconstraintsonbothandtheKKTmultipliersaresatisﬁed x
9 5
CHAPTER4.NUMERICALCOMPUTATION
•Theinequalityconstraintsexhibit“complementary slackness”: α h( x) = 0 FormoreinformationabouttheKKTapproach,seeNocedalandWright2006() 5 E x am p l e: L i n ear L eas t S q u are s
Supposewewanttoﬁndthevalueofthatminimizes x
f() = x1
2||−|| A x b2
2 (4.21)
Therearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently However,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as
asimpleexampleofhowthesetechniqueswork First,weneedtoobtainthegradient:
∇ x f() = x A( ) = A x b− AA x A−b (4.22)
Wecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1
fordetails 1Analgorithmtominimize f( x) =1
2||−|| A x b2
2withrespectto x
usinggradientdescent,startingfromanarbitraryvalueof

============================================================

=== CHUNK 029 ===
Palavras: 362
Caracteres: 11347
--------------------------------------------------
x
Setthestepsize()andtolerance()tosmall,positivenumbers  δ
whi l e|| AA x A−b|| 2 > δ do
x x← − 
AA x A−b
e nd whi l e
OnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,because
thetruefunctionisquadratic,thequadraticapproximation employedbyNewton’s
methodisexact,andthealgorithmconvergestotheglobalminimuminasingle
step Nowsuppose we wishto minimizethesame function,butsubjectto the
constraint xx≤1.Todoso,weintroducetheLagrangian
L , λ f λ ( x) = ()+ x
xx−1 (4.23)
Wecannowsolvetheproblem
min
xmax
λ , λ ≥ 0L , λ ( x) (4.24)
9 6
CHAPTER4.NUMERICALCOMPUTATION
Thesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe
foundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,
thenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda
solutionwheretheconstraintisactive.Bydiﬀerentiating theLagrangianwith
respectto,weobtaintheequation x
AA x A−b x+2 λ= 0 (4.25)
Thistellsusthatthesolutionwilltaketheform
x A= (A I+2 λ)− 1Ab (4.26)
Themagnitudeof λmustbechosensuchthattheresultobeystheconstraint.We
canﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ
∂
∂ λL , λ( x) = xx−1 (4.27)
Whenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative
uphillandincreasetheLagrangianwithrespectto λ,weincrease λ.Becausethe
coeﬃcientonthe xxpenaltyhasincreased,solvingthelinearequationfor xwill
nowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation
andadjusting λcontinuesuntil xhasthecorrectnormandthederivativeon λis
0 Thisconcludesthemathematical preliminaries thatweusetodevelopmachine
learningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged
learningsystems 9 7
C h a p t e r 5
Mac h i n e L e ar n i n g B asics
Deeplearningisaspeciﬁckindofmachinelearning.Inordertounderstand
deeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof
machinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral
principlesthatwillbeappliedthroughouttherestofthebook.Novicereadersor
thosewhowantawiderperspectiveareencouragedtoconsidermachinelearning
textbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy
()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006
feelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11
on traditional machinelearning techniques thathavestrongly inﬂuenced the
developmentofdeeplearningalgorithms Webeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentan
example:thelinearregressionalgorithm W ethenproceedtodescribehowthe
challengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatterns
thatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings
calledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm
itself;wediscusshowtosettheseusingadditionaldata.Machinelearningis
essentiallyaformofappliedstatisticswithincreasedemphasisontheuseof
computerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis
onprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthe
twocentralapproachestostatistics:frequentistestimatorsandBayesianinference Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised
learningandunsupervisedlearning;wedescribethesecategoriesandgivesome
examplesofsimplelearningalgorithmsfromeachcategory Mostdeeplearning
algorithmsare basedonan optimization algorithmcalled stochasticgradient
descent.Wedescribehowtocombinevariousalgorithmcomponentssuchas
98
CHAPTER5.MACHINELEARNINGBASICS
anoptimization algorithm,acostfunction,amodel,andadatasettobuilda
machinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11
factorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat
overcometheseobstacles 5.1LearningAlgorithms
Amachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But
whatdowemeanbylearning?Mitchell1997()providesthedeﬁnition“Acomputer
programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT
andperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,
improveswithexperienceE.”Onecanimagineaverywidevarietyofexperiences
E,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis
booktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe
diﬀerentkindsoftasks,performance measuresandexperiencesthatcanbeused
toconstructmachinelearningalgorithms 5.1.1TheTask, T
Machinelearningallowsustotackletasksthataretoodiﬃculttosolvewith
ﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcand
philosophicalpointofview,machinelearningisinterestingbecausedevelopingour
understandingofmachinelearningentailsdevelopingourunderstandingofthe
principlesthatunderlieintelligence Inthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearning
itselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe
task.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite
aprogramthatspeciﬁeshowtowalkmanually Machinelearningtasksareusuallydescribedintermsofhowthemachine
learningsystemshouldprocessanexample.Anexampleisacollectionoffeatures
thathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant
themachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa
vectorx∈ Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,
thefeaturesofanimageareusuallythevaluesofthepixelsintheimage 9 9
CHAPTER5.MACHINELEARNINGBASICS
Manykindsoftaskscanbesolvedwithmachinelearning.Someofthemost
commonmachinelearningtasksincludethefollowing:
•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecify
whichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning
algorithmisusuallyaskedtoproduceafunctionf: Rn→{1,...,k}.When
y=f(x),themodelassignsaninputdescribedbyvectorxtoacategory
identiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcation
task,forexample,wherefoutputsaprobabilitydistributionoverclasses Anexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinput
isanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe
outputisanumericcodeidentifyingtheobjectintheimage.Forexample,
theWillowGaragePR2robotisabletoactasawaiterthatcanrecognize
diﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-
fellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith
deeplearning( ,; ,).Object Krizhevskyetal.2012IoﬀeandSzegedy2015
recognitionisthesamebasictechnologythatallowscomputerstorecognize
faces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople
inphotocollectionsandallowcomputerstointeractmorenaturallywith
theirusers •Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechal-
lengingifthecomputerprogramisnotguaranteedthateverymeasurement
initsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcation
task,thelearningalgorithmonlyhastodeﬁneafunctionmapping single
fromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay
bemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearning
algorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set
fyingxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituation
arisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests
areexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargeset
offunctionsistolearnaprobabilitydistributionoveralloftherelevant
variables,thensolvetheclassiﬁcationtaskbymarginalizing outthemissing
variables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁ-
cationfunctionsneededforeachpossiblesetofmissinginputs,butweonly
needtolearnasinglefunctiondescribingthejointprobabilitydistribution SeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel
appliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis
sectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcation
withmissinginputsisjustoneexampleofwhatmachinelearningcando 1 0 0
CHAPTER5.MACHINELEARNINGBASICS
•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta
numericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm
isaskedtooutputafunctionf: Rn→ R.Thistypeoftaskissimilarto
classiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleof
aregressiontaskisthepredictionoftheexpectedclaimamountthatan
insuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction
offuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor
algorithmictrading •Transcription:Inthistypeoftask,themachinelearningsystemisasked
toobservearelativelyunstructuredrepresentationofsomekindofdataand
transcribeitintodiscrete,textualform.Forexample,inopticalcharacter
recognition,thecomputerprogramisshownaphotographcontainingan
imageoftextandisaskedtoreturnthistextintheformofasequence
ofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses
deeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal 2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram
isprovidedanaudiowaveformandemitsasequenceofcharactersorword
IDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep
learningisacrucialcomponentofmodernspeechrecognitionsystemsused
atmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal •Machinetranslation:Inamachinetranslationtask,theinputalready
consistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram
mustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis
commonlyappliedtonaturallanguages,suchastranslatingfromEnglishto
French.Deeplearninghasrecentlybeguntohaveanimportantimpacton
thiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,) •Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe
outputisavector(orotherdatastructurecontainingmultiplevalues)with
importantrelationshipsbetweenthediﬀerentelements.Thisisabroad
category,andsubsumesthetranscriptionandtranslationtasksdescribed
above,butalsomanyothertasks.Oneexampleisparsing—mappinga
naturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure
andtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon See ()foranexampleofdeeplearningappliedtoaparsing Collobert2011
task.Anotherexampleispixel-wisesegmentationofimages, wherethe
computerprogramassignseverypixelinanimagetoaspeciﬁccategory.For
1 0 1
CHAPTER5.MACHINELEARNINGBASICS
example,deeplearningcanbeusedtoannotatethelocationsofroadsin
aerialphotographs(MnihandHinton2010,).Theoutputneednothaveits
formmirrorthestructureoftheinputascloselyasintheseannotation-style
tasks.Forexample,inimagecaptioning,thecomputerprogramobservesan
imageandoutputsanaturallanguagesentencedescribingtheimage(Kiros
etal ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;
KarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare
calledstructuredoutputtasksbecausetheprogrammustoutputseveral
valuesthatarealltightlyinter-related.Forexample,thewordsproducedby
animagecaptioningprogrammustformavalidsentence •Anomalydetection:Inthistypeoftask,thecomputerprogramsifts
throughasetofeventsorobjects,andﬂagssomeofthemasbeingunusual
oratypical.Anexampleofananomalydetectiontaskiscreditcardfraud
detection.Bymodelingyourpurchasinghabits,acreditcardcompanycan
detectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard
information,thethief’spurchaseswilloftencomefromadiﬀerentprobability
distributionoverpurchasetypesthanyourown.Thecreditcardcompany
canpreventfraudbyplacingaholdonanaccountassoonasthatcardhas
beenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009
surveyofanomalydetectionmethods

============================================================

=== CHUNK 030 ===
Palavras: 354
Caracteres: 13986
--------------------------------------------------
•Synthesisandsampling:Inthistypeoftask,themachinelearningal-
gorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe
trainingdata Synthesisandsamplingviamachinelearningcanbeuseful
formediaapplicationswhereitcanbeexpensiveorboringforanartistto
generatelargevolumesofcontentbyhand.Forexample,videogamescan
automatically generatetexturesforlargeobjectsorlandscapes,ratherthan
requiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013
cases,wewantthesamplingorsynthesisproceduretogeneratesomespeciﬁc
kindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we
provideawrittensentenceandasktheprogramtoemitanaudiowaveform
containingaspokenversionofthatsentence Thisisakindofstructured
outputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrect
outputforeachinput,andweexplicitlydesirealargeamountofvariationin
theoutput,inorderfortheoutputtoseemmorenaturalandrealistic •Imputationofmissingvalues:Inthistypeoftask,themachinelearning
algorithmisgivenanewexamplex∈ Rn,butwithsomeentriesx iofx
missing.Thealgorithmmustprovideapredictionofthevaluesofthemissing
entries 1 0 2
CHAPTER5.MACHINELEARNINGBASICS
•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin
inputacorruptedexample˜x∈ Rnobtainedbyanunknowncorruptionprocess
fromacleanexamplex∈ Rn.Thelearnermustpredictthecleanexample
xfromitscorruptedversion˜x,ormoregenerallypredicttheconditional
probabilitydistributionp(x|˜x) •Densityestimationorprobabilitymassfunctionestimation:In
thedensityestimationproblem,themachinelearningalgorithmisasked
tolearnafunctionpmodel: Rn→ R,wherepmodel(x)canbeinterpreted
asaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass
function(if xisdiscrete)onthespacethattheexamplesweredrawnfrom Todosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe
discussperformancemeasuresP),thealgorithmneedstolearnthestructure
ofthedataithasseen.Itmustknowwhereexamplesclustertightlyand
wheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire
thelearningalgorithmtoatleastimplicitlycapturethestructureofthe
probabilitydistribution.Densityestimationallowsustoexplicitlycapture
thatdistribution.Inprinciple,wecanthenperformcomputations onthat
distributioninordertosolvetheothertasksaswell.Forexample,ifwe
haveperformeddensityestimationtoobtainaprobabilitydistributionp(x),
wecanusethatdistributiontosolvethemissingvalueimputationtask.If
avaluex iismissingandalloftheothervalues,denotedx − i,aregiven,
thenweknowthedistributionoveritisgivenbyp(x i|x − i).Inpractice,
densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,
becauseinmanycasestherequiredoperationsonp(x)arecomputationally
intractable Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks
welisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan
do,nottodeﬁnearigidtaxonomyoftasks 5.1.2ThePerformanceMeasure, P
Inordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign
aquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis
speciﬁctothetaskbeingcarriedoutbythesystem T
Fortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-
scription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe
proportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan
1 0 3
CHAPTER5.MACHINELEARNINGBASICS
alsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion
ofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto
theerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0
ifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,
itdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1
loss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodel
acontinuous-valuedscoreforeachexample.Themostcommonapproachisto
reporttheaveragelog-probabilit ythemodelassignstosomeexamples Usuallyweareinterestedinhowwellthemachinelearningalgorithmperforms
ondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen
deployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing
atestsetofdatathatisseparatefromthedatausedfortrainingthemachine
learningsystem Thechoiceofperformancemeasuremayseemstraightforwardandobjective,
butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswellto
thedesiredbehaviorofthesystem Insomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy
ofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grained
performancemeasurethatgivespartialcreditforgettingsomeelementsofthe
sequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe
systemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes
verylargemistakes?Thesekindsofdesignchoicesdependontheapplication Inothercases,weknowwhatquantitywewouldideallyliketomeasure,but
measuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof
densityestimation.Manyofthebestprobabilisticmodelsrepresentprobability
distributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto
aspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one
mustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,
ordesignagoodapproximationtothedesiredcriterion 5.1.3TheExperience, E
Machinelearningalgorithmscanbebroadlycategorizedasunsupervisedor
supervisedbywhatkindofexperiencetheyareallowedtohaveduringthe
learningprocess Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed
toexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as
1 0 4
CHAPTER5.MACHINELEARNINGBASICS
deﬁnedinsection.Sometimeswewillalsocallexamples 5.1.1 datapoints
Oneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-
searchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936
diﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample Thefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe
plant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset
alsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesare
representedinthedataset Unsupervisedlearningalgorithmsexperienceadatasetcontainingmany
features,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext
ofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat
generatedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor
taskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms
performotherroles,likeclustering,whichconsistsofdividingthedatasetinto
clustersofsimilarexamples Supervisedlearningalgorithmsexperienceadatasetcontainingfeatures,
buteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris
datasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning
algorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree
diﬀerentspeciesbasedontheirmeasurements Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples
ofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-
bilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while
supervisedlearninginvolvesobservingseveralexamplesofarandomvector xand
anassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby
estimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof
thetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine
learningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror
teacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide Unsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan
beusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates
thatforavector x∈ Rn,thejointdistributioncanbedecomposedas
p() = xn
i=1p(x i|x1,...,x i −1) (5.1)
Thisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof
modelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we
1 0 5
CHAPTER5.MACHINELEARNINGBASICS
cansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional
unsupervised learningtechnologiesto learn thejointdistributionp( x,y)and
inferring
py(| x) =p,y( x)
yp,y( x) (5.2)
Thoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor
distinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith
machinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcation
andstructuredoutputproblemsassupervisedlearning.Densityestimationin
supportofothertasksisusuallyconsideredunsupervisedlearning Othervariantsofthelearningparadigmarepossible.Forexample,insemi-
supervisedlearning,someexamplesincludeasupervisiontargetbutothersdo
not.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas
containingornotcontaininganexampleofaclass,buttheindividualmembers
ofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning
withdeepmodels,seeKotzias 2015etal.() Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For
example,reinforcementlearningalgorithmsinteractwithanenvironment,so
thereisafeedbackloopbetweenthelearningsystemanditsexperiences Such
algorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998
orBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,
and ()forthedeeplearningapproachtoreinforcementlearning Mnihetal.2013
Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan
bedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,
whichareinturncollectionsoffeatures Onecommonwayofdescribingadatasetiswitha .Adesign designmatrix
matrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthe
matrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains
150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent
thedatasetwithadesignmatrixX∈ R1504 ×,whereX i ,1isthesepallengthof
planti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning
algorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto
describeeachexampleasavector,andeachofthesevectorsmustbethesamesize Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs
withdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerent
numbersofpixels,sonotallofthephotographs maybedescribedwiththesame
lengthofvector.Sectionandchapterdescribehowtohandlediﬀerent 9.7 10
1 0 6
CHAPTER5.MACHINELEARNINGBASICS
typesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe
datasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:
{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors
x() iandx() jhavethesamesize Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetas
wellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm
toperformobjectrecognitionfromphotographs, weneedtospecifywhichobject
appearsineachofthephotos.Wemightdothiswithanumericcode,with0
signifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking
withadatasetcontainingadesignmatrixoffeatureobservationsX,wealso
provideavectoroflabels,withyy iprovidingthelabelforexample.i
Ofcourse,sometimesthelabelmaybemorethanjustasinglenumber.For
example,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire
sentences,thenthelabelforeachexamplesentenceisasequenceofwords Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,
thereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere
covermostcases,butitisalwayspossibletodesignnewonesfornewapplications 5.1.4Example:LinearRegression
Ourdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapable
ofimprovingacomputerprogram’sperformanceatsometaskviaexperienceis
somewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa
simplemachinelearningalgorithm:linearregression.Wewillreturntothis
examplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto
understanditsbehavior Asthenameimplies,linearregressionsolvesaregressionproblem Inother
words,thegoalistobuildasystemthatcantakeavectorx∈ Rnasinputand
predictthevalueofascalary∈ Rasitsoutput.Inthecaseoflinearregression,
theoutputisalinearfunctionoftheinput.Letˆybethevaluethatourmodel
predictsshouldtakeon.Wedeﬁnetheoutputtobe y
ˆy= wx (5.3)
wherew∈ Rnisavectorof .parameters
Parametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis
thecoeﬃcientthatwemultiplybyfeaturex ibeforesummingupthecontributions
fromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow
eachfeatureaﬀectstheprediction If afeaturex ireceivesapositiveweightw i,
1 0 7
CHAPTER5.MACHINELEARNINGBASICS
thenincreasingthevalueofthatfeatureincreasesthevalueofourprediction ˆy Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature
decreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,
thenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasno
eﬀectontheprediction WethushaveadeﬁnitionofourtaskT: topredictyfromxbyoutputting
ˆy= wx.Nextweneedadeﬁnitionofourperformancemeasure,.P
Supposethatwehaveadesignmatrixofmexampleinputsthatwewillnot
usefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave
avectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese
examples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest
set.WerefertothedesignmatrixofinputsasX()testandthevectorofregression
targetsasy()test Onewayofmeasuringtheperformanceofthemodelistocomputethemean
squarederrorofthemodelonthetestset.Ifˆy()testgivesthepredictionsofthe
modelonthetestset,thenthemeansquarederrorisgivenby
MSEtest=1
m
i(ˆy()test−y()test)2
i (5.4)
Intuitively,onecanseethatthiserrormeasuredecreasesto0when ˆy()test=y()test Wecanalsoseethat
MSEtest=1
m||ˆy()test−y()test||2
2, (5.5)
sotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions
andthetargetsincreases Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat
willimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm
isallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One
intuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1
minimizethemeansquarederroronthetrainingset,MSEtrain

============================================================

=== CHUNK 031 ===
Palavras: 359
Caracteres: 8658
--------------------------------------------------
TominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0
∇ wMSEtrain= 0 (5.6)
⇒∇ w1
m||ˆy()train−y()train||2
2= 0 (5.7)
⇒1
m∇ w||X()trainwy−()train||2
2= 0 (5.8)
1 0 8
CHAPTER5.MACHINELEARNINGBASICS
− − 1 0 x1− 3− 2− 10123yL i n ea r r eg r es s i o n ex a m p l e
0 5 1 0 1 5 w10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w
Figure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,
eachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw
containsonlyasingleparametertolearn,w 1 ( L e f t )Observethatlinearregressionlearns
tosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe
trainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal
equations,whichwecanseeminimizesthemeansquarederroronthetrainingset ⇒∇ w
X()trainwy−()train
X()trainwy−()train
= 0(5.9)
⇒∇ w
wX()train X()trainww−2X()train y()train+y()train y()train
= 0
(5.10)
⇒2X()train X()trainwX−2()train y()train= 0(5.11)
⇒w=
X()train X()train−1
X()train y()train(5.12)
Thesystemofequationswhosesolutionisgivenbyequationisknownas 5.12
thenormalequations.Evaluatingequationconstitutesasimplelearning 5.12
algorithm.Foranexampleofthelinearregressionlearningalgorithminaction,
seeﬁgure.5.1
Itisworthnotingthatthetermlinearregressionisoftenusedtoreferto
aslightlymoresophisticatedmodelwithoneadditionalparameter—an intercept
term.Inthismodelb
ˆy= wx+b (5.13)
sothemappingfromparameterstopredictionsisstillalinearfunctionbutthe
mappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensionto
aﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikea
line,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter
1 0 9
CHAPTER5.MACHINELEARNINGBASICS
b,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan
extraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1
playstheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”when
referringtoaﬃnefunctionsthroughoutthisbook Theintercepttermbisoftencalledthebiasparameteroftheaﬃnetransfor-
mation.Thisterminologyderivesfromthepointofviewthattheoutputofthe
transformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm
isdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimation
algorithm’sexpectedestimateofaquantityisnotequaltothetruequantity Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,
butitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent
sectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm
designanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated
learningalgorithms 5.2Capacity,OverﬁttingandUnderﬁtting
Thecentralchallengeinmachinelearningisthatwemustperformwellonnew,
previouslyunseeninputs—notjustthoseonwhichourmodelwastrained The
abilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization Typically,whentrainingamachinelearningmodel,wehaveaccesstoatraining
set,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining
error,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply
anoptimization problem.Whatseparatesmachinelearningfromoptimization is
thatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas
well Thegeneralization errorisdeﬁnedastheexpectedvalueoftheerrorona
newinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawn
fromthedistributionofinputsweexpectthesystemtoencounterinpractice Wetypicallyestimatethegeneralization errorofamachinelearningmodelby
measuringitsperformanceonatestsetofexamplesthatwerecollectedseparately
fromthetrainingset Inourlinearregressionexample,wetrainedthemodelbyminimizingthe
trainingerror,
1
m()train||X()trainwy−()train||2
2, (5.14)
butweactuallycareaboutthetesterror,1
m()test||X()testwy−()test||2
2 Howcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythe
1 1 0
CHAPTER5.MACHINELEARNINGBASICS
trainingset?Theﬁeldofstatisticallearningtheoryprovidessomeanswers.If
thetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan
do.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest
setarecollected,thenwecanmakesomeprogress Thetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets
calledthedatageneratingprocess.Wetypicallymakeasetofassumptions
knowncollectivelyasthei.i.d Theseassumptionsarethatthe
examplesineachdatasetareindependentfromeachother,andthatthetrain
setandtestsetareidenticallydistributed,drawnfromthesameprobability
distributionaseachother Thisassumptionallowsustodescribethedatagen-
eratingprocesswithaprobabilitydistributionoverasingleexample.Thesame
distributionisthenusedtogenerateeverytrainexampleandeverytestexample Wecallthatsharedunderlyingdistributionthedatageneratingdistribution,
denotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto
mathematically studytherelationshipbetweentrainingerrorandtesterror Oneimmediateconnectionwecanobservebetweenthetrainingandtesterror
isthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe
expectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution
p(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest
set.Forsomeﬁxedvaluew,theexpectedtrainingseterrorisexactlythesameas
theexpectedtestseterror,becausebothexpectationsareformedusingthesame
datasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthe
nameweassigntothedatasetwesample Ofcourse, when weuseamachinelearning algorithm, w edonotﬁxthe
parametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,
thenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe
testset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto
theexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine
learningalgorithmwillperformareitsabilityto:
1 Makethetrainingerrorsmall Makethegapbetweentrainingandtesterrorsmall Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:
underﬁttingandoverﬁtting.Underﬁttingoccurswhenthemodelisnotableto
obtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhen
thegapbetweenthetrainingerrorandtesterroristoolarge Wecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyaltering
itscapacity.Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof
1 1 1
CHAPTER5.MACHINELEARNINGBASICS
functions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Models
withhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdo
notservethemwellonthetestset Onewaytocontrolthecapacityofalearningalgorithmisbychoosingits
hypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto
selectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe
setofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize
linearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits
hypothesisspace.Doingsoincreasesthemodel’scapacity Apolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe
arealreadyfamiliar,withprediction
ˆybwx = + (5.15)
Byintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we
canlearnamodelthatisquadraticasafunctionof:x
ˆybw = +1xw+2x2 (5.16)
Thoughthismodelimplementsaquadraticfunctionofits,theoutputis input
stillalinearfunctionoftheparameters,sowecanstillusethenormalequations
totrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas
additionalfeatures,forexampletoobtainapolynomialofdegree9:
ˆyb= +9
i=1w ixi (5.17)
Machinelearningalgorithmswillgenerallyperformbestwhentheircapacity
isappropriateforthetruecomplexityofthetasktheyneedtoperformandthe
amountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacity
areunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex
tasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey
mayoverﬁt Figureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2
anddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlying
functionisquadratic Thelinearfunctionisunabletocapturethecurvaturein
thetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableof
representingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitely
manyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe
1 1 2
CHAPTER5.MACHINELEARNINGBASICS
havemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing
asolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.In
thisexample,thequadraticmodelisperfectlymatchedtothetruestructureof
thetasksoitgeneralizeswelltonewdata           
                  
          
Figure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawas
generatedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically
byevaluatingaquadraticfunction

============================================================

=== CHUNK 032 ===
Palavras: 486
Caracteres: 9011
--------------------------------------------------
( L e f t )Alinearfunctionﬁttothedatasuﬀersfrom
underﬁtting—itcannotcapturethecurvaturethatispresentinthedata A ( C e n t e r )
quadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfrom
asigniﬁcantamountofoverﬁttingorunderﬁtting.Apolynomialofdegree9ﬁtto ( R i g h t )
thedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolve
theunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining
pointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue
underlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue
functiondecreasesinthisarea Sofarwehavedescribedonlyonewayofchangingamodel’scapacity:by
changingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew
parametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging
amodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The
modelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom
whenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled
therepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebest
functionwithinthisfamilyisaverydiﬃcultoptimization problem.Inpractice,
thelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyone
thatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchas
1 1 3
CHAPTER5.MACHINELEARNINGBASICS
theimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm’s
eﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodel
family Ourmodernideasaboutimprovingthegeneralization ofmachinelearning
modelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearly
asPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow
mostwidelyknownasOccam’srazor(c.1287-1347).Thisprinciplestatesthat
amongcompetinghypothesesthatexplainknownobservationsequallywell,one
shouldchoosethe“simplest”one.Thisideawasformalizedandmademoreprecise
inthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand
Chervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,) Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity Amongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or
VCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.The
VCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthere
existsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily Quantifyingthecapacityofthemodelallowsstatisticallearningtheoryto
makequantitativepredictions.Themostimportantresultsinstatisticallearning
theoryshowthatthediscrepancybetweentrainingerrorandgeneralization error
isboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut
shrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,
1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide
intellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyare
rarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin
partbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite
diﬃculttodeterminethecapacityofdeeplearningalgorithms Theproblemof
determiningthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausethe
eﬀectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and
wehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization
problemsinvolvedindeeplearning Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize
(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea
suﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,training
errordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel
capacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,
generalization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis
illustratedinﬁgure.5.3
Toreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce
1 1 4
CHAPTER5.MACHINELEARNINGBASICS
0 O pti m a l C a pa c i t y
C a pa c i t yE r r o rU nde r ﬁtti ng z o ne O v e r ﬁtti ng z o ne
G e ne r a l i z a t i o n g a pT r a i n i n g e r r o r
G e n e r a l i z a t i o n e r r o r
Figure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror
behavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror
arebothhigh.Thisistheunderﬁttingregime.Asweincreasecapacity,trainingerror
decreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,
thesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁtting
regime,wherecapacityistoolarge,abovetheoptimalcapacity theconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric
models,suchaslinearregression.Parametricmodelslearnafunctiondescribed
byaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved Non-parametric modelshavenosuchlimitation Sometimes,non-parametric modelsarejusttheoreticalabstractions(suchas
analgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot
beimplemented inpractice.However,wecanalsodesignpracticalnon-parametric
modelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample
ofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,
whichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodel
simplystorestheXandyfromthetrainingset Whenaskedtoclassifyatest
pointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe
associatedregressiontarget.Inotherwords,ˆy=y iwherei=argmin||X i ,:−||x2
2 ThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,
suchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005
allowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,
thenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which
mightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerent
outputs)onanyregressiondataset Finally,wecanalsocreateanon-parametric learningalgorithmbywrappinga
1 1 5
CHAPTER5.MACHINELEARNINGBASICS
parametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber
ofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning
thatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa
polynomialexpansionoftheinput Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution
thatgeneratesthedata Evensuchamodelwillstillincursomeerroronmany
problems,becausetheremaystillbesomenoiseinthedistribution.Inthecase
ofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,
orymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose
includedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue
distributioniscalledthe p,y(x)Bayeserror Trainingandgeneralization errorvaryasthesizeofthetrainingsetvaries Expectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples
increases.Fornon-parametric models,moredatayieldsbettergeneralization until
thebestpossibleerrorisachieved.Anyﬁxedparametricmodelwithlessthan
optimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See
ﬁgureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4
capacityandyetstillhavealargegapbetweentrainingandgeneralization error Inthissituation,wemaybeabletoreducethisgapbygatheringmoretraining
examples 5.2.1TheNoFreeLunchTheorem
Learningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom
aﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof
logic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,
isnotlogicallyvalid Tologicallyinferaruledescribingeverymemberofaset,
onemusthaveinformationabouteverymemberofthatset Inpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,
ratherthantheentirelycertainrulesusedinpurelylogicalreasoning Machine
learningpromisestoﬁndrulesthatareprobably most correctaboutmembersof
thesettheyconcern Unfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree
lunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover
allpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthe
sameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,
insomesense,nomachinelearningalgorithmisuniversallyanybetterthanany
other.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage
1 1 6
CHAPTER5.MACHINELEARNINGBASICS
      
                                                    
                
              
                     
                       
      
                                                     
Figure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellas
ontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon
addingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,
andthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40
diﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals

============================================================

=== CHUNK 033 ===
Palavras: 372
Caracteres: 12136
--------------------------------------------------
( T o p )TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,
andamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.For
thequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases Thisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,
becausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic
modeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto
ahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The
trainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm
tomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,
thetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoat
leasttheBayeserror Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )
(shownhereasthedegreeoftheoptimalpolynomialregressor)increases Theoptimal
capacityplateausafterreachingsuﬃcientcomplexitytosolvethetask 1 1 7
CHAPTER5.MACHINELEARNINGBASICS
performance(overallpossibletasks)asmerelypredictingthateverypointbelongs
tothesameclass Fortunately,theseresultsholdonlywhenweaverageoverpossibledata all
generatingdistributions.Ifwemakeassumptionsaboutthekindsofprobability
distributionsweencounterinreal-worldapplications,thenwecandesignlearning
algorithmsthatperformwellonthesedistributions Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversal
learningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto
understandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAI
agentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon
datadrawnfromthekindsofdatageneratingdistributionswecareabout 5.2.2Regularization
Thenofreelunchtheoremimpliesthatwemustdesignourmachinelearning
algorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetof
preferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith
thelearningproblemsweaskthealgorithmtosolve,itperformsbetter Sofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed
concretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyadding
orremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm
isabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthe
degreeofapolynomialforaregressionproblem.Theviewwehavedescribedso
farisoversimpliﬁed Thebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewe
makethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentity
ofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,
hasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These
linearfunctionscanbeveryusefulforproblemswheretherelationshipbetween
inputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems
thatbehaveinaverynonlinearfashion.Forexample,linearregressionwould
notperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus
controltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe
allowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese
functions Wecanalsogivealearningalgorithmapreferenceforonesolutioninits
hypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone
ispreferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetraining
1 1 8
CHAPTER5.MACHINELEARNINGBASICS
datasigniﬁcantlybetterthanthepreferredsolution Forexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude
weightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum
comprisingboththemeansquarederroronthetrainingandacriterionJ(w)that
expressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,
J() = wMSEtrain+λww, (5.18)
whereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference
forsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcesthe
weightstobecomesmaller MinimizingJ(w)resultsinachoiceofweightsthat
makeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesus
solutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan
exampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweight
decay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvalues
of.Seeﬁgurefortheresults λ 5.5
           
         
                       
         
          
    
Figure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingset
fromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9 5.2
Wevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting ( L e f t )Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeat
all.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )
mediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape λ
Eventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated
shape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller
coeﬃcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )
pseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the
degree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2
1 1 9
CHAPTER5.MACHINELEARNINGBASICS
Moregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)by
addingapenaltycalledaregularizertothecostfunction.Inthecaseofweight
decay,theregularizerisΩ(w) =ww.Inchapter,wewillseethatmanyother 7
regularizersarepossible Expressingpreferencesforonefunctionoveranotherisamoregeneralway
ofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthe
hypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas
expressinganinﬁnitelystrongpreferenceagainstthatfunction Inourweightdecayexample,weexpressedourpreferenceforlinearfunctions
deﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionwe
minimize.Thereare many otherwaysof expressing preferencesfor diﬀerent
solutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproaches
areknownasregularization Regularizationisanymodiﬁcationwemaketoa
learningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits
trainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachine
learning,rivaledinitsimportanceonlybyoptimization Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachine
learningalgorithm,and,inparticular,nobestformofregularization Instead
wemustchooseaformofregularizationthatiswell-suitedtotheparticulartask
wewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin
particularisthataverywiderangeoftasks(suchasalloftheintellectualtasks
thatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeforms
ofregularization 5.3HyperparametersandValidationSets
Mostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol
thebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-
ters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm
itself(thoughwecan designa nestedlearning procedure where one learning
algorithmlearnsthebesthyperparametersforanotherlearningalgorithm) Inthepolynomialregressionexamplewesawinﬁgure,thereisasingle 5.2
hyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-
parameter.Theλvalueusedtocontrolthestrengthofweightdecayisanother
exampleofahyperparameter Sometimesasettingischosentobeahyperparameter thatthelearningal-
gorithmdoesnotlearnbecauseitisdiﬃculttooptimize.Morefrequently,the
1 2 0
CHAPTER5.MACHINELEARNINGBASICS
settingmustbeahyperparameter becauseitisnotappropriatetolearnthat
hyperparameteronthetrainingset.Thisappliestoallhyperparameters that
controlmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would
alwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refer
toﬁgure).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher 5.3
degreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalower
degreepolynomialandapositiveweightdecaysetting Tosolvethisproblem,weneedavalidationsetofexamplesthatthetraining
algorithmdoesnotobserve Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom
thesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization
errorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe
testexamplesarenotusedinanywaytomakechoicesaboutthemodel,including
itshyperparameters Forthisreason,noexamplefromthetestsetcanbeused
inthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe
trainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.One
ofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation
set,usedtoestimatethegeneralization errorduringoraftertraining,allowing
forthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto
learntheparametersisstilltypicallycalledthetrainingset,eventhoughthis
maybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess Thesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe
validationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand
20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters ,
thevalidationseterrorwillunderestimatethegeneralization error,thoughtypically
byasmalleramountthanthetrainingerror.Afterallhyperparameter optimization
iscomplete,thegeneralization errormaybeestimatedusingthetestset Inpractice, when thesametestsethasbeenusedrepeatedlytoevaluate
performanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsider
alltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-
the-artperformanceonthattestset,weenduphavingoptimisticevaluationswith
thetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthe
trueﬁeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto
moveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets 1 2 1
CHAPTER5.MACHINELEARNINGBASICS
5.3.1Cross-Validation
Dividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematic
ifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty
aroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithm
Aworksbetterthanalgorithmonthegiventask B
Whenthedatasethashundredsofthousandsofexamplesormore,thisisnota
seriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone
tousealloftheexamplesintheestimationofthemeantesterror,atthepriceof
increasedcomputational cost.Theseproceduresarebasedontheideaofrepeating
thetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplits
oftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation
procedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1
splittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated
bytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe
dataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One
problemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage
errorestimators(BengioandGrandvalet2004,),butapproximationsaretypically
used 5.4Estimators,BiasandVariance
Theﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine
learninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize Foundationalconceptssuchasparameterestimation,biasandvarianceareuseful
toformallycharacterizenotionsofgeneralization, underﬁttingandoverﬁtting 5.4.1PointEstimation
Pointestimationistheattempttoprovidethesingle“best”predictionofsome
quantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter
oravectorofparametersinsomeparametricmodel,suchastheweightsinour
linearregressionexampleinsection,butitcanalsobeawholefunction 5.1.4
Inordertodistinguishestimatesofparametersfromtheirtruevalue,our
conventionwillbetodenoteapointestimateofaparameterbyθ ˆθ Let{x(1),...,x() m}beasetofmindependentandidenticallydistributed
1 2 2
CHAPTER5.MACHINELEARNINGBASICS
Algorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate
generalization errorofalearningalgorithmAwhenthegivendataset Distoo
smallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof
generalization error,becausethemeanofalossLonasmalltestsetmayhavetoo
highvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for
thei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)
inthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase
ofunsupervisedlearning

============================================================

=== CHUNK 034 ===
Palavras: 363
Caracteres: 5438
--------------------------------------------------
The algorithmreturnsthevectoroferrorseforeach
examplein D,whosemeanistheestimatedgeneralization error Theerrorson
individualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean
(equation) Whiletheseconﬁdenceintervalsarenotwell-justiﬁedafterthe 5.47
useofcross-validation,itisstillcommonpracticetousethemtodeclarethat
algorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerror
ofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm
B DeﬁneKFoldXV(): D,A,L,k
Require: D,thegivendataset,withelementsz() i
Require:A,thelearningalgorithm,seenasafunctionthattakesadatasetas
inputandoutputsalearnedfunction
Require:L,thelossfunction,seenasafunctionfromalearnedfunctionfand
anexamplez() i∈ ∈ Dtoascalar R
Require:k,thenumberoffolds
Splitintomutuallyexclusivesubsets Dk D i,whoseunionis D
fordoikfromto1
f i= (A D D\ i)
forz() jin D ido
e j= (Lf i,z() j)
endfor
endfor
Returne
1 2 3
CHAPTER5.MACHINELEARNINGBASICS
(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic
ˆθ m= (gx(1),...,x() m) (5.19)
Thedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrue
θoreventhattherangeofgisthesameasthesetofallowablevaluesofθ Thisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofan
estimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,
agoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthat
generatedthetrainingdata Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume
thatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimate
ˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any
functionofthedataisrandom.Therefore ˆθisarandomvariable Pointestimationcanalsorefertotheestimationoftherelationshipbetween
inputandtargetvariables.Werefertothesetypesofpointestimatesasfunction
estimators FunctionEstimationAswementionedabove,sometimesweareinterestedin
performingfunctionestimation(orfunctionapproximation).Herewearetryingto
predictavariableygivenaninputvectorx.Weassumethatthereisafunction
f(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,
wemayassumethaty=f(x)+,wherestandsforthepartofythatisnot
predictablefromx Infunctionestimation,weareinterestedinapproximating
fwithamodelorestimate ˆf.Functionestimationisreallyjustthesameas
estimatingaparameterθ;thefunctionestimator ˆfissimplyapointestimatorin
functionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4
thepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2
scenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating
afunction ˆf y mappingfromtox Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand
discusswhattheytellusabouttheseestimators 5.4.2Bias
Thebiasofanestimatorisdeﬁnedas:
bias(ˆθ m) = ( Eˆθ m)−θ (5.20)
1 2 4
CHAPTER5.MACHINELEARNINGBASICS
wheretheexpectationisoverthedata(seenassamplesfromarandomvariable)
andθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistri-
bution.Anestimator ˆθ missaidtobeunbiasedifbias(ˆθ m) = 0,whichimplies
that E(ˆθ m)=θ.Anestimator ˆθ missaidtobeasymptoticallyunbiasedif
lim m → ∞bias(ˆθ m) = 0,whichimpliesthatlim m → ∞ E(ˆθ m) = θ Example:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}
thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-
butionwithmean:θ
Px(() i;) = θθx() i(1 )−θ(1 − x() i) (5.21)
Acommonestimatorfortheθparameterofthisdistributionisthemeanofthe
trainingsamples:
ˆθ m=1
mm
i=1x() i (5.22)
Todeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22
intoequation:5.20
bias(ˆθ m) = [ Eˆθ m]−θ (5.23)
= E
1
mm
i=1x() i
−θ (5.24)
=1
mm
i=1E
x() i
−θ (5.25)
=1
mm
i=11
x() i=0
x() iθx() i(1 )−θ(1 − x() i)
−θ(5.26)
=1
mm
i=1()θ−θ (5.27)
= = 0θθ− (5.28)
Since bias(ˆθ) = 0,wesaythatourestimator ˆθisunbiased Example:GaussianDistributionEstimatoroftheMeanNow,consider
asetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed
accordingtoaGaussiandistributionp(x() i) =N(x() i;µ,σ2),wherei∈{1,...,m} 1 2 5
CHAPTER5.MACHINELEARNINGBASICS
RecallthattheGaussianprobabilitydensityfunctionisgivenby
px(() i;µ,σ2) =1√
2πσ2exp
−1
2(x() i−µ)2
σ2
.(5.29)
AcommonestimatoroftheGaussianmeanparameterisknownasthesample
mean:
ˆµ m=1
mm
i=1x() i(5.30)
Todeterminethebiasofthesamplemean,weareagaininterestedincalculating
itsexpectation:
bias(ˆµ m) = [ˆ Eµ m]−µ (5.31)
= E
1
mm
i=1x() i
−µ (5.32)
=
1
mm
i=1E
x() i
−µ (5.33)
=
1
mm
i=1µ
−µ (5.34)
= = 0µµ− (5.35)
ThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmean
parameter Example:EstimatorsoftheVarianceofaGaussianDistributionAsan
example,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofa
Gaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased Theﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:
ˆσ2
m=1
mm
i=1
x() i−ˆµ m2
, (5.36)
where ˆµ misthesamplemean,deﬁnedabove.Moreformally,weareinterestedin
computing
bias(ˆσ2
m) = [ˆ Eσ2
m]−σ2(5.37)
1 2 6
CHAPTER5.MACHINELEARNINGBASICS
Webeginbyevaluatingtheterm E[ˆσ2
m]:
E[ˆσ2
m] = E
1
mm
i=1
x() i−ˆµ m2
(5.38)
=m−1
mσ2(5.39)
Returningtoequation,weconcludethatthebiasof 5.37 ˆσ2
mis−σ2/m.Therefore,
thesamplevarianceisabiasedestimator Theunbiasedsamplevarianceestimator
˜σ2
m=1
m−1m
i=1
x() i−ˆµ m2
(5.40)
providesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased Thatis,weﬁndthat E[˜σ2
m] = σ2:
E[˜σ2
m] = E
1
m−1m
i=1
x() i−ˆµ m2
(5.41)
=m
m−1E[ˆσ2
m] (5.42)
=m
m−1m−1
mσ2
(5.43)
= σ2

============================================================

=== CHUNK 035 ===
Palavras: 374
Caracteres: 9256
--------------------------------------------------
(5.44)
Wehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased
estimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswe
willseeweoftenusebiasedestimatorsthatpossessotherimportantproperties 5.4.3VarianceandStandardError
Anotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch
weexpectittovaryasafunctionofthedatasample.Justaswecomputedthe
expectationoftheestimatortodetermineitsbias,wecancomputeitsvariance Thevarianceofanestimatorissimplythevariance
Var(ˆθ) (5.45)
wheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe
varianceiscalledthe ,denotedstandarderror SE(ˆθ) 1 2 7
CHAPTER5.MACHINELEARNINGBASICS
Thevarianceorthestandarderrorofanestimatorprovidesameasureofhow
wewouldexpecttheestimatewecomputefromdatatovaryasweindependently
resamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe
mightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively
lowvariance Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimate
ofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave
obtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave
beendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceof
errorthatwewanttoquantify Thestandarderrorofthemeanisgivenby
SE(ˆµ m) =Var
1
mm
i=1x() i
=σ√m, (5.46)
whereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften
estimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootof
thesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance
provideanunbiasedestimateofthestandarddeviation.Bothapproachestend
tounderestimatethetruestandarddeviation,butarestillusedinpractice.The
squarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate Forlarge,theapproximation isquitereasonable m
Thestandarderrorofthemeanisveryusefulinmachinelearningexperiments Weoftenestimatethegeneralization errorbycomputingthesamplemeanofthe
erroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe
accuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which
tellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,
wecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation
fallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredon
themean ˆµ mis
(ˆµ m−196SE( ˆ.µ m)ˆ,µ m+196SE( ˆ.µ m)), (5.47)
underthenormaldistributionwithmean ˆµ mandvariance SE(ˆµ m)2.Inmachine
learningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm
Biftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAis
lessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithm
B 1 2 8
CHAPTER5.MACHINELEARNINGBASICS
Example: BernoulliDistributionWeonceagainconsiderasetofsamples
{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution
(recallP(x() i;θ) =θx() i(1−θ)(1 − x() i)).Thistimeweareinterestedincomputing
thevarianceoftheestimator ˆθ m=1
mm
i=1x() i Var
ˆθ m
= Var
1
mm
i=1x() i
(5.48)
=1
m2m
i=1Var
x() i
(5.49)
=1
m2m
i=1θθ (1−) (5.50)
=1
m2mθθ (1−) (5.51)
=1
mθθ (1−) (5.52)
Thevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples
inthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill
returntowhenwediscussconsistency(seesection).5.4.5
5.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquared
Error
Biasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Bias
measurestheexpecteddeviationfromthetruevalueofthefunctionorparameter Varianceontheotherhand,providesameasureofthedeviationfromtheexpected
estimatorvaluethatanyparticularsamplingofthedataislikelytocause Whathappenswhenwearegivenachoicebetweentwoestimators,onewith
morebiasandonewithmorevariance?Howdowechoosebetweenthem?For
example,imaginethatweareinterestedinapproximating thefunctionshownin
ﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand 5.2
onethatsuﬀersfromlargevariance.Howdowechoosebetweenthem Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-
natively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:
MSE = [( Eˆθ m−θ)2] (5.53)
= Bias(ˆθ m)2+Var(ˆθ m) (5.54)
1 2 9
CHAPTER5.MACHINELEARNINGBASICS
TheMSEmeasurestheoverallexpecteddeviation—in asquarederrorsense—
betweentheestimatorandthetruevalueoftheparameterθ.Asisclearfrom
equation,evaluatingtheMSEincorporatesboththebiasandthevariance 5.54
DesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat
managetokeepboththeirbiasandvariancesomewhatincheck C apac i t yB i as Ge ne r al i z at i on
e r r orV ar i anc e
O pt i m al
c apac i t yO v e r ﬁt t i ng z o n e U nde r ﬁt t i ng z o n e
Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance
(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold
curve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁtting
whenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationship
issimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedin
sectionandﬁgure 5.2 5.3
Therelationshipbetweenbiasandvarianceistightlylinkedtothemachine
learningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-
eralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful
componentsofgeneralization error),increasingcapacitytendstoincreasevariance
anddecreasebias.Thisisillustratedinﬁgure,whereweseeagaintheU-shaped 5.6
curveofgeneralization errorasafunctionofcapacity 5.4.5Consistency
Sofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof
ﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe
amountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber
ofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue
1 3 0
CHAPTER5.MACHINELEARNINGBASICS
valueofthecorrespondingparameters.Moreformally,wewouldlikethat
plimm → ∞ˆθ m= θ (5.55)
Thesymbolplimindicatesconvergenceinprobability,meaningthatforany>0,
P(|ˆθ m−|θ>)→0asm→∞.Theconditiondescribedbyequationis5.55
knownasconsistency.Itissometimesreferredtoasweakconsistency,with
strongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almost
sureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex
occurswhenp(lim m → ∞ x() m= ) = 1x Consistencyensuresthatthebiasinducedbytheestimatordiminishesasthe
numberofdataexamplesgrows.However,thereverseisnottrue—asymptotic
unbiasednessdoesnotimplyconsistency Forexample,considerestimatingthe
meanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsisting
ofmsamples:{x(1),...,x() m}.Wecouldusetheﬁrstsamplex(1)ofthedataset
asanunbiasedestimator:ˆθ=x(1).Inthatcase, E(ˆθ m)=θsotheestimator
isunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies
thattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent
estimatorasitisthecasethat not ˆθ m→ →∞θmas 5.5MaximumLikelihoodEstimation
Previously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzed
theirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing
thatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand
variance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁc
functionsthataregoodestimatorsfordiﬀerentmodels Themostcommonsuchprincipleisthemaximumlikelihoodprinciple Considerasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom
thetruebutunknowndatageneratingdistributionpdata() x Letpmodel( x;θ)beaparametricfamilyofprobabilitydistributionsoverthe
samespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationx
toarealnumberestimatingthetrueprobabilitypdata()x Themaximumlikelihoodestimatorforisthendeﬁnedas θ
θML= argmax
θpmodel(;) Xθ (5.56)
= argmax
θm
i=1pmodel(x() i;)θ (5.57)
1 3 1
CHAPTER5.MACHINELEARNINGBASICS
Thisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons Forexample,itispronetonumericalunderﬂow.Toobtainamoreconvenient
butequivalentoptimization problem,weobservethattakingthelogarithmofthe
likelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct
intoasum:
θML= argmax
θm
i=1logpmodel(x() i;)θ (5.58)
Becausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan
dividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation
withrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:
θML= argmax
θE x ∼ˆ pdatalogpmodel(;)xθ (5.59)
Onewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing
thedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetraining
setandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo
measuredbytheKLdivergence.TheKLdivergenceisgivenby
DKL(ˆpdatapmodel) = E x ∼ˆ pdata[log ˆpdata()logx−pmodel()]x.(5.60)
Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthe
model.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we
needonlyminimize
− E x ∼ˆ pdata[logpmodel()]x (5.61)
whichisofcoursethesameasthemaximization inequation.5.59
MinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-
entropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”to
identifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,
butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-
entropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthe
probabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhe
cross-entropybetweentheempiricaldistributionandaGaussianmodel

============================================================

=== CHUNK 036 ===
Palavras: 380
Caracteres: 10952
--------------------------------------------------
Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis-
tributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatch
thetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis
distribution Whiletheoptimalθisthesameregardlessofwhetherwearemaximizingthe
likelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions
1 3 2
CHAPTER5.MACHINELEARNINGBASICS
arediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction Maximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood
(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof
maximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase
becausetheKLdivergencehasaknownminimumvalueofzero.Thenegative
log-likelihoodcanactuallybecomenegativewhenisreal-valued.x
5.5.1ConditionalLog-LikelihoodandMeanSquaredError
Themaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere
ourgoalistoestimateaconditionalprobabilityP( y x|;θ)inordertopredict y
given x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor
mostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved
targets,thentheconditionalmaximumlikelihoodestimatoris
θML= argmax
θP ( ;)YX|θ (5.62)
Iftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto
θML= argmax
θm
i=1log(Py() i|x() i;)θ (5.63)
Example:LinearRegressionasMaximumLikelihoodLinearregression,
introducedearlierinsection,maybejustiﬁedasamaximumlikelihood 5.1.4
procedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns
totakeaninputxandproduceanoutputvalue ˆy.Themappingfromxtoˆyis
chosentominimizemeansquarederror,acriterionthatweintroducedmoreorless
arbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum
likelihoodestimation.Insteadofproducingasingleprediction ˆy,wenowthink
ofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine
thatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexamples
withthesameinputvaluexbutdiﬀerentvaluesofy Thegoalofthelearning
algorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvalues
thatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm
weobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunction ˆy(x;w)
givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat
thevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthis
choiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation
proceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe
1 3 3
CHAPTER5.MACHINELEARNINGBASICS
examplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63
givenby
m
i=1log(py() i|x() i;)θ (5.64)
= log −mσ−m
2log(2)π−m
i=1ˆy() i−y() i2
2σ2,(5.65)
where ˆy() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe
numberofthetrainingexamples.Comparingthelog-likelihoodwiththemean
squarederror,
MSEtrain=1
mm
i=1||ˆy() i−y() i||2, (5.66)
weimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields
thesameestimateoftheparameterswasdoesminimizingthemeansquarederror Thetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.This
justiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe
willsee,themaximumlikelihoodestimatorhasseveraldesirableproperties 5.5.2PropertiesofMaximumLikelihood
Themainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto
bethebestestimatorasymptotically,asthenumberofexamplesm→∞,interms
ofitsrateofconvergenceasincreases.m
Underappropriate conditions, the maximumlikelihood estimatorhas the
propertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5
oftrainingexamplesapproachesinﬁnity,themaximumlikelihoodestimateofa
parameterconvergestothetruevalueoftheparameter.Theseconditionsare:
•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ) Otherwise,noestimatorcanrecoverpdata •Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-
wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable
todeterminewhichvalueofwasusedbythedatageneratingprocessing θ
Thereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-
tor,manyofwhichsharethepropertyofbeingconsistentestimators However,
1 3 4
CHAPTER5.MACHINELEARNINGBASICS
consistentestimatorscandiﬀerintheirstatisticeﬃciency,meaningthatone
consistentestimatormayobtainlowergeneralization errorforaﬁxednumberof
samplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelof
generalization error Statisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinear
regression)whereourgoalistoestimatethevalueofaparameter(andassuming
itispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto
measurehowclosewearetothetrueparameterisbytheexpectedmeansquared
error,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter
values,wheretheexpectationisovermtrainingsamplesfromthedatagenerating
distribution.Thatparametricmeansquarederrordecreasesasmincreases,and
formlarge,theCramér-Raolowerbound(,;,)showsthatno Rao1945Cramér1946
consistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood
estimator Forthesereasons(consistencyandeﬃciency),maximumlikelihoodisoften
consideredthepreferredestimatortouseformachinelearning.Whenthenumber
ofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategies
suchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood
thathaslessvariancewhentrainingdataislimited 5.6BayesianStatistics
Sofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-
ingasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatone
estimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakinga
prediction.ThelatteristhedomainofBayesianstatistics Asdiscussed insection , the frequen tist perspective isthat thetrue 5.4.1
parametervalueθisﬁxedbutunknown,whilethepointestimate ˆθisarandom
variableonaccountofitbeingafunctionofthedataset(whichisseenasrandom) TheBayesianperspectiveonstatisticsisquitediﬀerent The Bayesianuses
probabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetis
directlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθ
isunknownoruncertainandthusisrepresentedasarandomvariable Beforeobservingthedata,werepresentourknowledgeofθusingtheprior
probabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”) Generally,themachinelearningpractitionerselectsapriordistributionthatis
quitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthe
1 3 5
CHAPTER5.MACHINELEARNINGBASICS
valueofθbeforeobservinganydata.Forexample,onemightassume that apriori
θliesinsomeﬁniterangeorvolume,withauniformdistribution Manypriors
insteadreﬂectapreferencefor“simpler” solutions(suchassmallermagnitude
coeﬃcients,orafunctionthatisclosertobeingconstant) Nowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan
recovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihood
px((1),...,x() m|θ)withthepriorviaBayes’rule:
px(θ|(1),...,x() m) =px((1),...,x() m|θθ)(p)
px((1),...,x() m)(5.67)
InthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa
relativelyuniformorGaussiandistributionwithhighentropy,andtheobservation
ofthedatausuallycausestheposteriortoloseentropyandconcentratearounda
fewhighlylikelyvaluesoftheparameters Relativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwo
importantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakes
predictionsusingapointestimateofθ,theBayesianapproachistomakepredictions
usingafulldistributionoverθ.Forexample,afterobservingmexamples,the
predicteddistributionoverthenextdatasample,x(+1) m,isgivenby
px((+1) m|x(1),...,x() m) =
px((+1) m| |θθ)(px(1),...,x() m)d.θ(5.68)
Hereeachvalueofθwithpositiveprobabilitydensitycontributestotheprediction
ofthenextexample,withthecontributionweightedbytheposteriordensityitself Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe
valueofθ,thenthisuncertaintyisincorporated directlyintoanypredictionswe
mightmake Insection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4
taintyinagivenpointestimateofθbyevaluatingitsvariance.Thevarianceof
theestimatorisanassessmentofhowtheestimatemightchangewithalternative
samplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal
withtheuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto
protectwellagainstoverﬁtting Thisintegralisofcoursejustanapplicationof
thelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe
frequentistmachineryforconstructinganestimatorisbasedontheratheradhoc
decisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint
estimate ThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimation
andthemaximumlikelihoodapproachisduetothecontributionoftheBayesian
1 3 6
CHAPTER5.MACHINELEARNINGBASICS
priordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensity
towardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori
theprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman
judgmentimpactingthepredictions Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata
isavailable,buttypicallysuﬀerfromhighcomputational costwhenthenumberof
trainingexamplesislarge Example:BayesianLinearRegressionHereweconsidertheBayesianesti-
mationapproachtolearningthelinearregressionparameters.Inlinearregression,
welearnalinearmappingfromaninputvectorx∈ Rntopredictthevalueofa
scalar.Thepredictionisparametrized bythevector y∈ R w∈ Rn:
ˆy= wx (5.69)
Givenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction
ofovertheentiretrainingsetas: y
ˆy()train= X()trainw (5.70)
ExpressedasaGaussianconditionaldistributionony()train,wehave
p(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)
∝exp
−1
2(y()train−X()trainw)(y()train−X()trainw)
,
(5.72)
wherewefollowthestandardMSEformulationinassumingthattheGaussian
varianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto
(X()train,y()train) ( ) assimplyXy, Todeterminetheposteriordistributionoverthemodelparametervectorw,we
ﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebelief
aboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnatural
toexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe
typicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty
aboutθ Forreal-valuedparametersitiscommontouseaGaussianasaprior
distribution:
p() = (;w Nwµ0, Λ0) exp∝
−1
2(wµ−0)Λ−1
0(wµ−0)
,(5.73)
1 3 7
CHAPTER5.MACHINELEARNINGBASICS
whereµ0and Λ0arethepriordistributionmeanvectorandcovariancematrix
respectively.1
Withthepriorthusspeciﬁed,wecannowproceedindeterminingtheposterior
distributionoverthemodelparameters p,p,p (wX|y) ∝(yX|w)()w (5.74)
∝exp
−1
2( )yXw−( )yXw−
exp
−1
2(wµ−0)Λ−1
0(wµ−0)
(5.75)
∝exp
−1
2
−2yXww+XXww+Λ−1
0wµ−2
0 Λ−1
0w (5.76)
Wenowdeﬁne Λ m=
XX+ Λ−1
0 −1andµ m= Λ m
Xy+ Λ−1
0µ0
.Using
thesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussian
distribution:
p, (wX|y) exp∝
−1
2(wµ− m)Λ−1
m(wµ− m)+1
2µ
m Λ−1
mµ m
(5.77)
∝exp
−1
2(wµ− m)Λ−1
m(wµ− m)

============================================================

=== CHUNK 037 ===
Palavras: 351
Caracteres: 8974
--------------------------------------------------
(5.78)
Alltermsthatdonotincludetheparametervectorwhavebeenomitted;they
areimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1
EquationshowshowtonormalizeamultivariateGaussiandistribution 3.23
Examiningthisposteriordistributionallowsustogainsomeintuitionforthe
eﬀectofBayesianinference.Inmostsituations,wesetµ0to 0.Ifweset Λ0=1
αI,
thenµ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha
weightdecaypenaltyofαww.OnediﬀerenceisthattheBayesianestimateis
undeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearning
processwithaninﬁnitelywideprioronw.Themoreimportantdiﬀerenceisthat
theBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe
diﬀerentvaluesofare,ratherthanprovidingonlytheestimate w µ m 5.6.1Maximum (MAP)Estimation A P o s t e ri o ri
WhilethemostprincipledapproachistomakepredictionsusingthefullBayesian
posteriordistributionovertheparameterθ,itisstilloftendesirabletohavea
1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a
d i a g o n a l c o v a ria n c e m a t rix Λ0= diag( λ0) 1 3 8
CHAPTER5.MACHINELEARNINGBASICS
singlepointestimate Onecommonreasonfordesiringapointestimateisthat
mostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare
intractable,andapointestimateoﬀersatractableapproximation.Ratherthan
simplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof
thebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoice
ofthepointestimate.Onerationalwaytodothisistochoosethemaximum
aposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof
maximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon
caseofcontinuous):θ
θMAP= argmax
θp( ) = argmaxθx|
θlog( )+log() pxθ|pθ.(5.79)
Werecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-
likelihoodterm,and,correspondingtothepriordistribution log()pθ
Asanexample,consideralinearregressionmodelwithaGaussianprioron
theweightsw.IfthispriorisgivenbyN(w; 0,1
λI2),thenthelog-priortermin
equationisproportional tothefamiliar 5.79 λwwweightdecaypenalty,plusa
termthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAP
BayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight
decay AswithfullBayesianinference,MAPBayesianinferencehastheadvantageof
leveraginginformationthatisbroughtbythepriorandcannotbefoundinthe
trainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe
MAPpointestimate(incomparisontotheMLestimate).However,itdoessoat
thepriceofincreasedbias Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearning
regularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-
tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof
addinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).Not
allregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,
someregularizertermsmaynotbethelogarithmofaprobabilitydistribution Otherregularizationtermsdependonthedata,whichofcourseapriorprobability
distributionisnotallowedtodo MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated
yetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty
termcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian
distribution,astheprior(NowlanandHinton1992,) 1 3 9
CHAPTER5.MACHINELEARNINGBASICS
5.7SupervisedLearningAlgorithms
Recallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3
learningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena
trainingsetofexamplesofinputsxandoutputsy Inmanycasestheoutputs
ymaybediﬃculttocollectautomatically andmustbeprovidedbyahuman
“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswere
collectedautomatically 5.7.1ProbabilisticSupervisedLearning
Most supervised learning algorithms inthis book are based on estimating a
probabilitydistributionp(y|x).Wecandothissimplybyusingmaximum
likelihoodestimationtoﬁndthebestparametervectorθforaparametricfamily
ofdistributions .py(|xθ;)
Wehavealreadyseenthatlinearregressioncorrespondstothefamily
pyy (| Nxθ;) = (;θxI,) (5.80)
Wecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁninga
diﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and
class1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The
probabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues
mustaddupto1 Thenormaldistributionoverreal-valuednumbersthatweusedforlinear
regressionisparametrized intermsofamean.Anyvaluewesupplyforthismean
isvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because
itsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse
thelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe
interval(0,1)andinterpretthatvalueasaprobability:
py σ (= 1 ;) = |xθ (θx) (5.81)
Thisapproachisknownaslogisticregression(asomewhatstrangenamesince
weusethemodelforclassiﬁcationratherthanregression) Inthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsby
solvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.There
isnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor
thembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative
log-likelihood(NLL)usinggradientdescent 1 4 0
CHAPTER5.MACHINELEARNINGBASICS
Thissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,
bywritingdownaparametricfamilyofconditionalprobabilitydistributionsover
therightkindofinputandoutputvariables 5.7.2SupportVectorMachines
Oneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvector
machine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto
logisticregressioninthatitisdrivenbyalinearfunctionwx+b.Unlikelogistic
regression,thesupportvectormachinedoesnotprovideprobabilities, butonly
outputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen
wx+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen
wx+bisnegative Onekeyinnovationassociatedwithsupportvectormachinesisthekernel
trick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms
canbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,
itcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan
bere-writtenas
wx+= +bbm
i=1α ixx() i(5.82)
wherex() iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthe
learningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature
functionφ(x) andthedotproductwithafunctionk(xx,() i) =φ(x)·φ(x() i) called
akernel.The ·operatorrepresentsaninnerproductanalogoustoφ(x)φ(x() i) Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In
someinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for
example,innerproductsbasedonintegrationratherthansummation.Acomplete
developmentofthesekindsofinnerproductsisbeyondthescopeofthisbook Afterreplacingdotproductswithkernelevaluations,wecanmakepredictions
usingthefunction
fb () = x +
iα ik,(xx() i) (5.83)
Thisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)
andf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.The
kernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying
φ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace Thekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels
thatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare
1 4 1
CHAPTER5.MACHINELEARNINGBASICS
guaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedand
optimizeonlyα,i.e.,theoptimization algorithmcanviewthedecisionfunction
asbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmits
animplementationthatissigniﬁcantlymorecomputational eﬃcientthannaively
constructingtwovectorsandexplicitlytakingtheirdotproduct φ()x
Insomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultin
aninﬁnitecomputational costforthenaive,explicitapproach.Inmanycases,
k(xx,)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As
anexampleofaninﬁnite-dimens ionalfeaturespacewithatractablekernel,we
constructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethat
thismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros Wecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent
tothecorrespondinginﬁnite-dimens ionaldotproduct ThemostcommonlyusedkernelistheGaussiankernel
k, ,σ (uvuv ) = (N −;02I) (5.84)
where N(x;µ, Σ)isthestandardnormaldensity.Thiskernelisalsoknownas
theradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines
invspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot
productinaninﬁnite-dimens ionalspace,butthederivationofthisspaceisless
straightforwardthaninourexampleofthekernelovertheintegers min
WecanthinkoftheGaussiankernelasperformingakindoftemplatematch-
ing.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate
forclassy.WhenatestpointxisnearxaccordingtoEuclideandistance,the
Gaussiankernelhasalargeresponse,indicatingthatxisverysimilartothex
template.Themodelthenputsalargeweightontheassociatedtraininglabely

============================================================

=== CHUNK 038 ===
Palavras: 386
Caracteres: 12266
--------------------------------------------------
Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe
similarityofthecorrespondingtrainingexamples Supportvectormachinesarenottheonlyalgorithmthatcanbeenhanced
usingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The
categoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines
orkernelmethods( ,; WilliamsandRasmussen1996Schölkopf1999etal.,) Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision
functionislinearinthenumberoftrainingexamples,becausethei-thexample
contributesatermα ik(xx,() i)tothedecisionfunction.Supportvectormachines
areabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor
thetrainingexamplesthathavenon-zeroα i.Thesetrainingexamplesareknown
1 4 2
CHAPTER5.MACHINELEARNINGBASICS
assupportvectors Kernelmachinesalsosuﬀerfromahighcomputational costoftrainingwhen
thedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9
generickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11
modernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof
kernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal ()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006
ontheMNISTbenchmark 5.7.3OtherSimpleSupervisedLearningAlgorithms
Wehavealreadybrieﬂyencounteredanothernon-probabilis ticsupervisedlearning
algorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis
afamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asa
non-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxed
numberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm
asnothavinganyparameters,butratherimplementingasimplefunctionofthe
trainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess Instead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,
weﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe
averageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially
anykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.In
thecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithc y= 1
andc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese
one-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric
learningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,
supposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1
loss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1
numberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayes
errorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally
distantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsx
willhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe
algorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone
ofthem,theprocedureconvergestotheBayeserrorrate Thehighcapacityof
k-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset However,itdoessoathighcomputational cost,anditmaygeneralizeverybadly
givenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatit
cannotlearnthatonefeatureismorediscriminativethananother.Forexample,
imaginewehavearegressiontaskwithx∈ R100drawnfromanisotropicGaussian
1 4 3
CHAPTER5.MACHINELEARNINGBASICS
distribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose
furtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall
cases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern Thenearestneighborofmostpointsxwillbedeterminedbythelargenumberof
featuresx2throughx100,notbythelonefeaturex1 Thustheoutputonsmall
trainingsetswillessentiallyberandom 1 4 4
CHAPTER5.MACHINELEARNINGBASICS
0
101
1110 1
011
1111 1110110
10010
001110 11111101001 00
010 01111
111
11
Figure5.7:Diagramsdescribinghowadecisiontreeworks ( T o p )Eachnodeofthetree
choosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon
theright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis
displayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtained
byappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom) ( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree
mightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode
drawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe
centeroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,
withonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitis
notpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe
numberoftrainingexamples 1 4 5
CHAPTER5.MACHINELEARNINGBASICS
Anothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions
andhasseparateparametersforeachregionisthedecisiontree( , Breimanetal 1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision 5.7
treeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat
regionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned
cut) Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one
correspondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps
everypointinitsinputregiontothesameoutput.Decisiontreesareusually
trainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The
learningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree
ofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints
thatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare
typicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,
struggletosolvesomeproblemsthatareeasyevenforlogisticregression.For
example,ifwehaveatwo-classproblemandthepositiveclassoccurswherever
x2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus
needtoapproximatethedecisionboundarywithmanynodes,implementingastep
functionthatconstantlywalksbackandforthacrossthetruedecisionfunction
withaxis-alignedsteps Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany
limitations.Nonetheless,theyareusefullearningalgorithmswhencomputational
resourcesareconstrained.Wecanalsobuildintuitionformoresophisticated
learningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetween
sophisticatedalgorithmsand-NNordecisiontreebaselines k
See (), (),  ()orothermachine Murphy2012Bishop2006Hastieetal.2001
learningtextbooksformorematerialontraditionalsupervisedlearningalgorithms 5.8UnsupervisedLearningAlgorithms
Recallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3
only“features”butnotasupervisionsignal.Thedistinctionbetweensupervised
andunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisno
objectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby
asupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract
informationfromadistributionthatdonotrequirehumanlabortoannotate
examples.Thetermisusuallyassociatedwithdensityestimation,learningto
drawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,
ﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof
1 4 6
CHAPTER5.MACHINELEARNINGBASICS
relatedexamples Aclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthe
data.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelooking
forarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile
obeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler
moreaccessiblethanitself.x
Therearemultiplewaysofdeﬁningarepresentation.Threeofthe simpler 
mostcommonincludelowerdimensionalrepresentations,sparserepresentations
andindependentrepresentations.Low-dimensionalrepresentationsattemptto
compressasmuchinformationaboutxaspossibleinasmallerrepresentation Sparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand
Ghahramani1997,)embedthedatasetintoarepresentationwhoseentriesare
mostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires
increasingthedimensionalityoftherepresentation,sothattherepresentation
becomingmostlyzeroesdoesnotdiscardtoomuchinformation Thisresultsinan
overallstructureoftherepresentationthattendstodistributedataalongtheaxes
oftherepresentationspace.Independentrepresentationsattempttodisentangle
thesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions
oftherepresentationarestatisticallyindependent Of coursethese three criteriaare certainly notmutuallyexclusive.Low-
dimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-
pendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto
reducethesizeofarepresentationistoﬁndandremoveredundancies.Identifying
andremovingmoreredundancyallowsthedimensionalityreductionalgorithmto
achievemorecompressionwhilediscardinglessinformation Thenotionofrepresentationisoneofthecentralthemesofdeeplearningand
thereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome
simpleexamplesofrepresentationlearningalgorithms.Together,theseexample
algorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe
remainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat
developthesecriteriaindiﬀerentwaysorintroduceothercriteria 5.8.1PrincipalComponentsAnalysis
Insection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12
ameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning
algorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon
twoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa
1 4 7
CHAPTER5.MACHINELEARNINGBASICS
− − 2 0 1 0 0 1 0 2 0
x 1− 2 0− 1 001 02 0x 2
− − 2 0 1 0 0 1 0 2 0
z 1− 2 0− 1 001 02 0z 2
Figure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance
withtheaxesofthenewspace ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis
space,thevariancemightoccuralongdirectionsthatarenotaxis-aligned ( R i g h t )The
transformeddataz=xWnowvariesmostalongtheaxisz 1.Thedirectionofsecond
mostvarianceisnowalongz 2 representationthathaslowerdimensionalitythantheoriginalinput.Italsolearns
arepresentationwhoseelementshavenolinearcorrelationwitheachother.This
isaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsare
statisticallyindependent.Toachievefullindependence,arepresentationlearning
algorithmmustalsoremovethenonlinearrelationshipsbetweenvariables PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan
inputxtoarepresentationzasshowninﬁgure.Insection,wesawthat 5.8 2.12
wecouldlearnaone-dimensional representationthatbestreconstructstheoriginal
data(inthesenseofmeansquarederror)andthatthisrepresentationactually
correspondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCA
asasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuch
oftheinformationinthedataaspossible(again,asmeasuredbyleast-squares
reconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation
decorrelatestheoriginaldatarepresentation.X
Letusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethat
thedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily
becenteredbysubtractingthemeanfromallexamplesinapreprocessingstep Theunbiasedsamplecovariancematrixassociatedwithisgivenby:X
Var[] =x1
m−1XX (5.85)
1 4 8
CHAPTER5.MACHINELEARNINGBASICS
PCAﬁndsarepresentation(throughlineartransformation)z=xWwhere
Var[]zisdiagonal Insection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X
aregivenbytheeigenvectorsofXX.Fromthisview,
XXWW = Λ (5.86)
Inthissection,weexploitanalternativederivationoftheprincipalcomponents.The
principalcomponentsmayalsobeobtainedviathesingularvaluedecomposition Speciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe
rightsingularvectorsinthedecompositionX=UW Σ Wethenrecoverthe
originaleigenvectorequationwithastheeigenvectorbasis: W
XX=
UW Σ
UW Σ= W Σ2W.(5.87)
TheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe
SVDof,wecanexpressthevarianceofas: X X
Var[] =x1
m−1XX (5.88)
=1
m−1(UW Σ)UW Σ(5.89)
=1
m−1W ΣUUW Σ(5.90)
=1
m−1W Σ2W, (5.91)
whereweusethefactthatUU=IbecausetheUmatrixofthesingularvalue
decompositionisdeﬁnedtobeorthogonal.Thisshowsthatifwetakez=xW,
wecanensurethatthecovarianceofisdiagonalasrequired: z
Var[] =z1
m−1ZZ (5.92)
=1
m−1WXXW (5.93)
=1
m−1WW Σ2WW (5.94)
=1
m−1Σ2, (5.95)
wherethistimeweusethefactthatWW=I,againfromthedeﬁnitionofthe
SVD

============================================================

=== CHUNK 039 ===
Palavras: 351
Caracteres: 12253
--------------------------------------------------
1 4 9
CHAPTER5.MACHINELEARNINGBASICS
Theaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear
transformationW,theresultingrepresentationhasadiagonalcovariancematrix
(asgivenby Σ2)whichimmediatelyimpliesthattheindividualelementsofzare
mutuallyuncorrelated ThisabilityofPCAtotransformdataintoarepresentationwheretheelements
aremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple
exampleofarepresentationthatattemptstodisentangletheunknownfactorsof
variationunderlyingthedata InthecaseofPCA,thisdisentanglingtakesthe
formofﬁndingarotationoftheinputspace(describedbyW)thatalignsthe
principalaxesofvariancewiththebasisofthenewrepresentationspaceassociated
with.z
Whilecorrelationisanimportantcategoryofdependencybetweenelementsof
thedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore
complicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat
canbedonewithasimplelineartransformation 5.8.2-meansClustering k
Anotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering Thek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclusters
ofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas
providingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx
belongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare
zero Theone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse
representation,becausethemajorityofitsentriesarezeroforeveryinput.Later,
wewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,
wheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes
areanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁts
ofadistributedrepresentation.Theone-hotcodestillconferssomestatistical
advantages(itnaturallyconveystheideathatallexamplesinthesameclusterare
similartoeachother)anditconfersthecomputational advantagethattheentire
representationmaybecapturedbyasingleinteger Thek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ() k}
todiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence Inonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof
thenearestcentroidµ() i.Intheotherstep,eachcentroidµ() iisupdatedtothe
meanofalltrainingexamplesx() jassignedtocluster.i
1 5 0
CHAPTER5.MACHINELEARNINGBASICS
Onediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherently
ill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella
clusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof
theclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe
membersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct
thetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe
clusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there
maybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyof
therealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebut
obtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.For
example,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof
imagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray
cars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmay
ﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterof
redvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering
algorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign
theexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This
newclusteringnowatleastcapturesinformationaboutbothattributes,butithas
lostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgray
cars,justastheyareinadiﬀerentclusterfromgraytrucks Theoutputofthe
clusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars
thantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisall
weknow Theseissuesillustratesomeofthereasonsthatwemaypreferadistributed
representationtoaone-hotrepresentation.Adistributedrepresentationcouldhave
twoattributesforeachvehicle—onerepresentingitscolorandonerepresenting
whetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal
distributedrepresentationis(howcanthelearningalgorithmknowwhetherthe
twoattributesweareinterestedinarecolorandcar-versus-truckratherthan
manufacturerandage?)buthavingmanyattributesreducestheburdenonthe
algorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure
similaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributes
insteadofjusttestingwhetheroneattributematches 5.9StochasticGradientDescent
Nearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic
gradientdescentorSGD.Stochasticgradientdescentisanextensionofthe
1 5 1
CHAPTER5.MACHINELEARNINGBASICS
gradientdescentalgorithmintroducedinsection.4.3
Arecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary
forgoodgeneralization, butlargetrainingsetsarealsomorecomputationally
expensive Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa
sumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the
negativeconditionallog-likelihoodofthetrainingdatacanbewrittenas
J() = θ E x ,y ∼ˆ pdataL,y,(xθ) =1
mm
i=1L(x() i,y() i,θ)(5.96)
whereistheper-exampleloss L L,y,py (xθ) = log− (|xθ;)
Fortheseadditivecostfunctions,gradientdescentrequirescomputing
∇ θJ() =θ1
mm
i=1∇ θL(x() i,y() i,.θ) (5.97)
Thecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto
billionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively
long Theinsightofstochasticgradientdescentisthatthegradientisanexpectation Theexpectationmaybeapproximately estimatedusingasmallsetofsamples Speciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamples
B={x(1),...,x( m)}drawnuniformlyfromthetrainingset.Theminibatchsize
mistypicallychosentobearelativelysmallnumberofexamples,rangingfrom
1toafewhundred.Crucially,misusuallyheldﬁxedasthetrainingsetsizem
grows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputed
ononlyahundredexamples Theestimateofthegradientisformedas
g=1
m∇ θm
i=1L(x() i,y() i,.θ) (5.98)
usingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B
thenfollowstheestimatedgradientdownhill:
θθg ← −, (5.99)
whereisthelearningrate 
1 5 2
CHAPTER5.MACHINELEARNINGBASICS
Gradientdescentingeneralhasoftenbeenregardedassloworunreliable.In
thepast,theapplicationofgradientdescenttonon-convexoptimization problems
wasregardedasfoolhardyorunprincipled Today,weknowthatthemachine
learningmodelsdescribedinpartworkverywellwhentrainedwithgradient II
descent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena
localminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue
ofthecostfunctionquicklyenoughtobeuseful Stochasticgradientdescenthasmanyimportantusesoutsidethecontextof
deeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge
datasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe
trainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize
increases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach
convergenceusuallyincreaseswithtrainingsetsize However,asmapproaches
inﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore
SGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot
extendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest
error.Fromthispointofview,onecanarguethattheasymptoticcostoftraining
amodelwithSGDisasafunctionof O(1) m
Priortotheadventofdeeplearning,themainwaytolearnnonlinearmodels
wastousethekerneltrickincombinationwithalinearmodel.Manykernellearning
algorithmsrequireconstructinganmm×matrixG i , j=k(x() i,x() j).Constructing
thismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets
with billions of examples In academia, starting in2006,deep learning was
initiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter
thancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof
thousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin
industry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge
datasets Stochasticgradientdescentandmanyenhancements toitaredescribedfurther
inchapter.8
5.10BuildingaMachineLearningAlgorithm
Nearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof
afairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,an
optimization procedureandamodel Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof
1 5 3
CHAPTER5.MACHINELEARNINGBASICS
Xyand,thecostfunction
J,b(w) = − E x ,y ∼ˆ pdatalogpmodel( )y|x, (5.100)
themodelspeciﬁcationpmodel(y|x) =N(y;xw+b,1),and,inmostcases,the
optimization algorithmdeﬁnedbysolvingforwherethegradientofthecostiszero
usingthenormalequations Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently
fromtheothers,wecanobtainaverywidevarietyofalgorithms Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearning
processtoperformstatisticalestimation.Themostcommoncostfunctionisthe
negativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum
likelihoodestimation Thecostfunctionmayalsoincludeadditionalterms,suchasregularization
terms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction
toobtain
J,bλ (w) = ||||w2
2− E x ,y ∼ˆ pdatalogpmodel( )y|x.(5.101)
Thisstillallowsclosed-formoptimization Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger
beoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical
optimization procedure,suchasgradientdescent Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,and
optimization algorithmssupportsbothsupervisedandunsupervisedlearning.The
linearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised
learningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandproviding
anappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrst
PCAvectorbyspecifyingthatourlossfunctionis
J() = w E x ∼ˆ pdata||− ||xr(;)xw2
2 (5.102)
whileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunction
r() = xwxw Insomecases,thecostfunctionmaybeafunctionthatwecannotactually
evaluate,forcomputational reasons.Inthesecases,wecanstillapproximately
minimizeitusingiterativenumericaloptimization solongaswehavesomewayof
approximatingitsgradients Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot
immediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor
1 5 4
CHAPTER5.MACHINELEARNINGBASICS
hand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some
modelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause
theircostfunctionshaveﬂatregionsthatmaketheminappropriate forminimization
bygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms
canbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofa
taxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather
thanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations 5.11ChallengesMotivatingDeepLearning
Thesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon
awidevarietyofimportantproblems.However,theyhavenotsucceededinsolving
thecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects Thedevelopmentofdeeplearningwasmotivatedinpartbythefailureof
traditionalalgorithmstogeneralizewellonsuchAItasks Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes
exponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow
themechanismsusedtoachievegeneralization intraditionalmachinelearning
areinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such
spacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto
overcometheseandotherobstacles 5.11.1TheCurseofDimensionality
Manymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumber
ofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof
dimensionality.Ofparticularconcernisthatthenumberofpossibledistinct
conﬁgurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables
increases 1 5 5
CHAPTER5.MACHINELEARNINGBASICS
Figure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto
right),thenumberofconﬁgurationsofinterestmaygrowexponentially ( L e f t )Inthis
one-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10
regionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion
correspondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly

============================================================

=== CHUNK 040 ===
Palavras: 356
Caracteres: 14445
--------------------------------------------------
Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin
eachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )
dimensionsitismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable Weneed
tokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplesto
coverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat
leastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach
axis,weseemtoneedO(vd)regionsandexamples Thisisaninstanceofthecurseof
dimensionality.FiguregraciouslyprovidedbyNicolasChapados Thecurseofdimensionalityarisesinmanyplacesincomputerscience,and
especiallysoinmachinelearning Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge Asillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof 5.9
possibleconﬁgurations ofxismuchlargerthanthenumberoftrainingexamples Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa
grid,likeintheﬁgure.Wecandescribelow-dimensional spacewithalownumber
ofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata
point,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples
thatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability
densityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin
thesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining
examplesinthesamecell Ifwearedoingregressionwecanaveragethetarget
valuesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich
wehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof
conﬁgurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell
hasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething
1 5 6
CHAPTER5.MACHINELEARNINGBASICS
meaningfulaboutthesenewconﬁgurations Manytraditionalmachinelearning
algorithmssimplyassumethattheoutputatanewpointshouldbeapproximately
thesameastheoutputatthenearesttrainingpoint 5.11.2LocalConstancyandSmoothnessRegularization
Inordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior
beliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen
thesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions
overparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas
directlyinﬂuencingtheitselfandonlyindirectlyactingontheparameters function
viatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsas
beingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing
someclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed
(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour
degreeofbeliefinvariousfunctions Amongthemostwidelyusedoftheseimplicit“priors” isthesmoothness
priororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn
shouldnotchangeverymuchwithinasmallregion Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and
asaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-
leveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces
additional(explicit andimplicit)priorsinorder toreducethegeneralization
erroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis
insuﬃcientforthesetasks Therearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbelief
thatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerent
methodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗that
satisﬁesthecondition
f∗() x≈f∗(+)x (5.103)
formostconﬁgurationsxandsmallchange.Inotherwords,ifweknowagood
answerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat
answerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers
insomeneighborhoodwewouldcombinethem(bysomeformofaveragingor
interpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas
possible Anextremeexampleofthelocalconstancyapproachisthek-nearestneighbors
familyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach
1 5 7
CHAPTER5.MACHINELEARNINGBASICS
regioncontainingallthepointsxthathavethesamesetofknearestneighborsin
thetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore
thanthenumberoftrainingexamples Whilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining
examples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated
withnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal
kernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther
apartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction
thatperformstemplatematching,bymeasuringhowcloselyatestexamplex
resembleseachtrainingexamplex() i Muchofthemodernmotivationfordeep
learningisderivedfromstudyingthelimitationsoflocaltemplatematchingand
howdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails
( ,) Bengioetal.2006b
Decisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-based
learningbecausetheybreaktheinputspaceintoasmanyregionsasthereare
leavesanduseaseparateparameter(orsometimesmanyparametersforextensions
ofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat
leastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare
requiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatistical
conﬁdenceinthepredictedoutput Ingeneral,todistinguishO(k)regionsininputspace,allofthesemethods
requireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters
associatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,
whereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustrated
inﬁgure.5.10
Isthereawaytorepresentacomplexfunctionthathasmanymoreregions
tobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming
onlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat For example, imagine that thetargetfunctionis akind ofcheckerboard.A
checkerboardcontainsmanyvariationsbutthereisasimplestructuretothem Imaginewhathappenswhenthenumberoftrainingexamplesissubstantially
smallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based
ononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould
beguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame
checkerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner
couldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo
notcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan
exampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe
1 5 8
CHAPTER5.MACHINELEARNINGBASICS
Figure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace
intoregions Anexample(representedherebyacircle)withineachregiondeﬁnesthe
regionboundary(representedherebythelines).Theyvalueassociatedwitheachexample
deﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion The
regionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoi
diagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber
oftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighbor
algorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe
localsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample
onlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately
surroundingthatexample 1 5 9
CHAPTER5.MACHINELEARNINGBASICS
entirecheckerboardrightistocovereachofitscellswithatleastoneexample Thesmoothnessassumptionandtheassociatednon-parametric learningalgo-
rithmsworkextremelywellsolongasthereareenoughexamplesforthelearning
algorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys
ofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe
functiontobelearnedissmoothenoughandvariesinfewenoughdimensions Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina
diﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerently
indiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetof
trainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge
numberofregionscomparedtothenumberofexamples),isthereanyhopeto
generalizewell Theanswertobothofthesequestions—whetheritispossibletorepresent
acomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimated
functiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthatavery
largenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solong
asweintroducesomedependenciesbetweentheregionsviaadditionalassumptions
abouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually
generalizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c
diﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare
reasonableforabroadrangeofAItasksinordertocapturetheseadvantages Otherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-
sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding
theassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch
strong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralize
toamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo
complextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,
sowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby
thecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-
gorithms Theseapparentlymildassumptionsallowanexponentialgaininthe
relationshipbetweenthenumberofexamplesandthenumberofregionsthatcan
bedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections
6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,
distributedrepresentationscountertheexponentialchallengesposedbythecurse
ofdimensionality 1 6 0
CHAPTER5.MACHINELEARNINGBASICS
5.11.3ManifoldLearning
Animportantconceptunderlyingmanyideasinmachinelearningisthatofa
manifold Amanifoldisaconnected region Mathematically , it isasetofpoints,
associatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the
manifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience
thesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin
3-Dspace Thedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistence
oftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition
toaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecan
walknorth,south,east,orwest Althoughthereisaformalmathematical meaningtotheterm“manifold,”in
machinelearningittendstobeusedmorelooselytodesignateaconnectedset
ofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof
degreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each
dimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan5.11
exampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-
dimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality
ofthemanifoldtovaryfromonepointtoanother This oftenhappenswhena
manifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingle
dimensioninmostplacesbuttwodimensionsattheintersectionatthecenter 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 .− 1 0 .− 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually
concentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates
theunderlyingmanifoldthatthelearnershouldinfer 1 6 1
CHAPTER5.MACHINELEARNINGBASICS
Manymachinelearningproblemsseemhopelessifweexpectthemachine
learningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn Manifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost
of Rnconsistsofinvalidinputs, andthatinterestinginputsoccuronlyalong
acollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting
variationsintheoutputofthelearnedfunctionoccurringonlyalongdirections
thatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe
movefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase
ofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis
probabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe
supervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis
highlyconcentrated Theassumptionthatthedataliesalongalow-dimensional manifoldmaynot
alwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas
thosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis
atleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists
oftwocategoriesofobservations Theﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-
bilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis
highlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs
fromthesedomains Figureshowshow,instead,uniformlysampledpoints 5.12
looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal
isavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat
random,whatistheprobabilitythatyouwillgetameaningfulEnglish-language
text?Almostzero,again,becausemostofthelongsequencesoflettersdonot
correspondtoanaturallanguagesequence:thedistributionofnaturallanguage
sequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters 1 6 2
CHAPTER5.MACHINELEARNINGBASICS
Figure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel
accordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-
zeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered
inAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests
thattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe
volumeofimagespace Ofcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshow
thatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso
establishthattheexamplesweencounterareconnectedtoeachotherbyother
1 6 3
CHAPTER5.MACHINELEARNINGBASICS
examples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat
maybereachedbyapplyingtransformationstotraversethemanifold.Thesecond
argumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch
neighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we
cancertainlythinkofmanypossibletransformationsthatallowustotraceouta
manifoldinimagespace:wecangraduallydimorbrightenthelights,gradually
moveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof
objects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost
applications.Forexample,themanifoldofimagesofhumanfacesmaynotbe
connectedtothemanifoldofimagesofcatfaces

============================================================

=== CHUNK 041 ===
Palavras: 351
Caracteres: 10174
--------------------------------------------------
Thesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-
tuitivereasonssupportingit.Morerigorousexperiments (Cayton2005Narayanan,;
andMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,
2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;
andSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof
interestinAI Whenthedataliesonalow-dimensional manifold,itcanbemostnatural
formachinelearningalgorithmstorepresentthedataintermsofcoordinateson
themanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan
thinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto
speciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notinterms
ofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,
butholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral
principleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13
adatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe
methodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee 20.6
howamachinelearningalgorithmcansuccessfullyaccomplishthisgoal Thisconcludespart,whichhasprovidedthebasicconceptsinmathematics I
andmachinelearningwhichareemployedthroughouttheremainingpartsofthe
book.Youarenowpreparedtoembarkuponyourstudyofdeeplearning 1 6 4
CHAPTER5.MACHINELEARNINGBASICS
Figure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000
forwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional
manifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe
abletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6
feat 1 6 5
P a rt I I
D e e p N e t w orks: Mo d e rn
Practices
166
Thispartofthebooksummarizesthestateofmoderndeeplearningasitis
usedtosolvepracticalapplications Deeplearninghasalonghistoryandmanyaspirations.Severalapproaches
havebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals
haveyettoberealized.Theseless-developedbranchesofdeeplearningappearin
theﬁnalpartofthebook Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-
nologiesthatarealreadyusedheavilyinindustry Modern deeplearning provides avery powerful framework forsupervised
learning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan
representfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan
inputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can
beaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃciently
largedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed
asassociatingonevectortoanother,orthatarediﬃcultenoughthataperson
wouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remain
beyondthescopeofdeeplearningfornow Thispartofthebookdescribesthecoreparametricfunctionapproximation
technologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning We begin by describingthe feedforward deepnetworkmodelthatisusedto
representthesefunctions.Next,wepresentadvancedtechniquesforregularization
andoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh
resolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce
theconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural
networkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines
forthepracticalmethodologyinvolvedindesigning,building,andconﬁguringan
applicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep
learning Thesechaptersarethemostimportantforapractitioner—someone whowants
tobeginimplementingandusingdeeplearningalgorithmstosolvereal-world
problemstoday 1 6 7
C h a p t e r 6
D e e p F e e d f orw ard N e t w orks
Deepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,
ormultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels Thegoalofafeedforwardnetworkistoapproximatesomefunction f∗.Forexample,
foraclassiﬁer, y= f∗(x)mapsaninputxtoacategory y.Afeedforwardnetwork
deﬁnesamappingy= f(x;θ)andlearnsthevalueoftheparametersθthatresult
inthebestfunctionapproximation Thesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthe
functionbeingevaluatedfromx,throughtheintermediate computations usedto
deﬁne f,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhich
outputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks
areextendedtoincludefeedbackconnections,theyarecalledrecurrentneural
networks,presentedinchapter.10
Feedforwardnetworksareofextremeimportancetomachinelearningpracti-
tioners.Theyformthebasisofmanyimportantcommercialapplications.For
example,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea
specializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual
steppingstoneonthepathtorecurrentnetworks,whichpowermanynatural
languageapplications Feedforwardneuralnetworksarecallednetworksbecausetheyaretypically
representedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-
ciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed
together.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected
inachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost
commonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheﬁrst
layerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall
168
CHAPTER6.DEEPFEEDFORWARDNETWORKS
lengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat
thename“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalled
theoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f∗(x) Thetrainingdataprovidesuswithnoisy,approximateexamplesof f∗(x) evaluated
atdiﬀerenttrainingpoints.Eachexamplexisaccompanied byalabel y f≈∗(x) Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint
x;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis
notdirectlyspeciﬁedbythetrainingdata Thelearningalgorithmmustdecide
howtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes
notsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust
decidehowtousetheselayerstobestimplementanapproximation of f∗.Because
thetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these
layersarecalledhiddenlayers Finally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby
neuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The
dimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each
elementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,
wecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,
eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin
thesensethatitreceivesinputfrommanyotherunitsandcomputesitsown
activationvalue Theideaofusingmanylayersofvector-valuedrepresentation
isdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute
theserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsabout
thefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork
researchisguidedbymanymathematical andengineeringdisciplines,andthe
goalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof
feedforwardnetworksasfunctionapproximation machinesthataredesignedto
achievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe
knowaboutthebrain,ratherthanasmodelsofbrainfunction Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodels
andconsiderhowtoovercometheirlimitations Linearmodels,suchaslogistic
regressionandlinearregression,areappealingbecausetheymaybeﬁteﬃciently
andreliably,eitherinclosedformorwithconvexoptimization Linearmodelsalso
havetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so
themodelcannotunderstandtheinteractionbetweenanytwoinputvariables Toextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply
thelinearmodelnottoxitselfbuttoatransformedinput φ(x),where φisa
1 6 9
CHAPTER6.DEEPFEEDFORWARDNETWORKS
nonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin
section,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2
the φmapping.Wecanthinkof φasprovidingasetoffeaturesdescribingx,or
asprovidinganewrepresentationfor.x
Thequestionisthenhowtochoosethemapping φ
1.Oneoptionistouseaverygeneric φ,suchastheinﬁnite-dimens ional φthat
isimplicitlyusedbykernelmachinesbasedontheRBFkernel If φ(x)is
ofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthe
trainingset,butgeneralization tothetestsetoftenremainspoor.Very
genericfeaturemappingsareusuallybasedonlyontheprincipleoflocal
smoothnessanddonotencodeenoughpriorinformationtosolveadvanced
problems 2.Anotheroptionistomanuallyengineer φ.Untiltheadventofdeeplearning,
thiswasthedominantapproach.Thisapproachrequiresdecadesofhuman
eﬀortfor eachseparate task, withpractitioners specializing in diﬀerent
domainssuchasspeech recognition or computer vision, and with little
transferbetweendomains 3.Thestrategyofdeeplearningistolearn φ.Inthisapproach,wehaveamodel
y= f(x;θw ,) = φ(x;θ)w.Wenowhaveparametersθthatweusetolearn
φfromabroadclassoffunctions,andparameterswthatmapfrom φ(x)to
thedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with
φdeﬁningahiddenlayer Thisapproachistheonlyoneofthethreethat
givesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweigh
theharms.Inthisapproach,weparametrizetherepresentationas φ(x;θ)
andusetheoptimization algorithmtoﬁndtheθthatcorrespondstoagood
representation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrst
approachbybeinghighlygeneric—wedosobyusingaverybroadfamily
φ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach Humanpractitioners canencodetheirknowledgetohelpgeneralization by
designingfamilies φ(x;θ)thattheyexpectwillperformwell.Theadvantage
isthatthehumandesigneronlyneedstoﬁndtherightgeneralfunction
familyratherthanﬁndingpreciselytherightfunction Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond
thefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep
learningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook Feedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic
1 7 0
CHAPTER6.DEEPFEEDFORWARDNETWORKS
mappingsfromxtoythatlackfeedbackconnections

============================================================

=== CHUNK 042 ===
Palavras: 350
Caracteres: 6640
--------------------------------------------------
Othermodelspresented
laterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions
withfeedback,andlearningprobabilitydistributionsoverasinglevector Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,
weaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign
decisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost
function,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based
learning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique
tofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa
hiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill
beusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture
ofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese
layersshould beconnectedto each other, and howmanyunitsshould bein
eachlayer.Learningindeepneuralnetworksrequirescomputingthegradients
ofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits
moderngeneralizations ,whichcanbeusedtoeﬃcientlycomputethesegradients Finally,weclosewithsomehistoricalperspective 1 E x am p l e: L earni n g X O R
Tomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan
exampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning
theXORfunction TheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues, x 1
and x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1
returns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1
y= f∗(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;θ)and
ourlearningalgorithmwilladapttheparametersθtomake fassimilaraspossible
to f∗ Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0],[0 ,1],
[1 ,0],and[1 ,1]} Wewilltrainthenetworkonallfourofthesepoints The
onlychallengeistoﬁtthetrainingset Wecantreatthisproblemasaregressionproblemanduseameansquared
errorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis
exampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan
1 7 1
CHAPTER6.DEEPFEEDFORWARDNETWORKS
appropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches
aredescribedinsection.6.2.2.2
Evaluatedonourwholetrainingset,theMSElossfunctionis
J() =θ1
4
x∈ X( f∗() (;))x− fxθ2 (6.1)
Nowwemustchoosetheformofourmodel, f(x;θ).Supposethatwechoose
alinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobe θw b
f , b (;xw) = xw+ b (6.2)
Wecanminimize J(θ)inclosedformwithrespecttowand busingthenormal
equations Aftersolvingthenormalequations,weobtainw= 0and b=1
2 Thelinear
modelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1
howalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve
thisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhicha
linearmodelisabletorepresentthesolution Speciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithone
hiddenlayercontainingtwohiddenunits.Seeﬁgureforanillustrationof 6.2
thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare
computedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen
usedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe
network.Theoutputlayerisstilljustalinearregressionmodel,butnowitis
appliedtohratherthantox.Thenetworknowcontainstwofunctionschained
together:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing
f , , , b f (;xWcw) = ( 2 )( f( 1 )())x Whatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,
anditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were
linear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof
itsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =Wx
and f( 2 )(h) =hw.Then f(x) =wWx.Wecouldrepresentthisfunctionas
f() = xxwwherew= Ww Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural
networksdosousinganaﬃnetransformationcontrolledbylearnedparameters,
followedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethat
strategyhere,bydeﬁningh= g(Wx+c) ,whereWprovidestheweightsofa
lineartransformationandcthebiases.Previously,todescribealinearregression
1 7 2
CHAPTER6.DEEPFEEDFORWARDNETWORKS
0 1
x 101x 2O r i g i n a l s p a c e x
0 1 2
h 101h 2L e a r n e d s p a c e h
Figure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers
printedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint ( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR
function.When x1= 0,themodel’soutputmustincreaseas x2increases.When x1= 1,
themodel’soutputmustdecreaseas x 2increases.Alinearmodelmustapplyaﬁxed
coeﬃcient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange
thecoeﬃcienton x 2andcannotsolvethisproblem ( R i g h t )Inthetransformedspace
representedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve
theproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1
collapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave
mappedbothx= [1 ,0]andx= [0 ,1]toasinglepointinfeaturespace,h= [1 ,0] Thelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2 Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel
capacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned
representationscanalsohelpthemodeltogeneralize 1 7 3
CHAPTER6.DEEPFEEDFORWARDNETWORKS
yy
hh
x xWwyy
h 1 h 1
x 1 x 1h 2 h 2
x 2 x 2
Figure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,
thisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden
layercontainingtwounits ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph Thisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample
itcanconsumetoomuchspace Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )
eachentirevectorrepresentingalayer’sactivations Thisstyleismuchmorecompact Sometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat
describetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes
themappingfromxtoh,andavectorwdescribesthemappingfromhto y.We
typicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind
ofdrawing model,weusedavectorofweightsandascalarbiasparametertodescribean
aﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe
anaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbias
parametersisneeded.Theactivationfunction gistypicallychosentobeafunction
thatisappliedelement-wise,with h i= g(xW : , i+ c i).Inmodernneuralnetworks,
thedefaultrecommendation istousetherectiﬁedlinearunitorReLU(Jarrett
e t a l

============================================================

=== CHUNK 043 ===
Palavras: 354
Caracteres: 9937
--------------------------------------------------
,; ,; 2009NairandHinton2010Glorot,)deﬁnedbytheactivation 2011a
function depictedinﬁgure g z , z () = max0{} 6.3
Wecannowspecifyourcompletenetworkas
f , , , b (;xWcw) = wmax0{ ,Wxc+}+ b (6.3)
WecannowspecifyasolutiontotheXORproblem.Let
W=11
11
, (6.4)
c=
0
−1
, (6.5)
1 7 4
CHAPTER6.DEEPFEEDFORWARDNETWORKS
0
z0g z ( ) = m a x 0{ , z}
Figure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefault
activationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying
thisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation However,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear
functionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,they
preservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-
basedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels
generalizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild
complicatedsystemsfromminimalcomponents MuchasaTuringmachine’smemory
needsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator
fromrectiﬁedlinearfunctions 1 7 5
CHAPTER6.DEEPFEEDFORWARDNETWORKS
w=1
−2
, (6.6)
and b= 0
Wecannowwalkthroughthewaythatthemodelprocessesabatchofinputs LetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,
withoneexampleperrow:
X=
00
01
10
11
 (6.7)
Theﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrst
layer’sweightmatrix:
XW=
00
11
11
22
 (6.8)
Next,weaddthebiasvector,toobtainc

0 1−
10
10
21
 (6.9)
Inthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1
thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto 0 1 0
Alinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalue
offoreachexample,weapplytherectiﬁedlineartransformation: h

00
10
10
21
 (6.10)
Thistransformationhaschangedtherelationshipbetweentheexamples.Theyno
longerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea 6.1
linearmodelcansolvetheproblem Weﬁnishbymultiplyingbytheweightvector:w

0
1
1
0
 (6.11)
1 7 6
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Theneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained
zeroerror Inarealsituation,theremightbebillionsofmodelparametersand
billionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid
here.Instead,agradient-basedoptimization algorithmcanﬁndparametersthat
produceverylittleerror.ThesolutionwedescribedtotheXORproblemisata
globalminimumofthelossfunction,sogradientdescentcouldconvergetothis
point.ThereareotherequivalentsolutionstotheXORproblemthatgradient
descentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsonthe
initialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot
ﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented
here 2 Gradi en t - Bas e d L earni n g
Designingandtraininganeuralnetworkisnotmuchdiﬀerentfromtrainingany
othermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10
howtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,
acostfunction,andamodelfamily Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneural
networksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss
functionstobecomenon-convex.Thismeansthatneuralnetworksareusually
trainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost
functiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain
linearregressionmodelsortheconvexoptimization algorithmswithglobalconver-
genceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization
convergesstartingfromanyinitialparameters(intheory—inpracticeitisvery
robustbutcanencounternumericalproblems).Stochasticgradientdescentapplied
tonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive
tothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis
importanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe
initializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-
mizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep
modelswillbedescribedindetailinchapter,withparameterinitialization in 8
particulardiscussedinsection.Forthemoment,itsuﬃcestounderstandthat 8.4
thetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe
costfunctioninonewayoranother The speciﬁcalgorithmsareimprovements
andreﬁnementsontheideasofgradientdescent,introducedinsection,and,4.3
1 7 7
CHAPTER6.DEEPFEEDFORWARDNETWORKS
morespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescent
algorithm,introducedinsection.5.9
Wecanofcourse,trainmodelssuchaslinearregressionandsupportvector
machineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining
setisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot
muchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightly
morecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly Sectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5
algorithmandmoderngeneralizations oftheback-propagationalgorithm Aswithothermachinelearningmodels,toapplygradient-basedlearningwe
mustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof
themodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison
theneuralnetworksscenario 6.2.1CostFunctions
Animportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe
costfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless
thesameasthoseforotherparametricmodels,suchaslinearmodels Inmostcases,ourparametricmodeldeﬁnesadistribution p(yx|;θ)and
wesimplyuse theprinciple ofmaximumlikelihood.Thismeansweusethe
cross-entropybetweenthetrainingdataandthemodel’spredictionsasthecost
function Sometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete
probabilitydistributionovery,wemerelypredictsomestatisticofyconditioned
on.Specializedlossfunctionsallowustotrainapredictoroftheseestimates x
Thetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone
oftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave
alreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin
section.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2
applicabletodeepneuralnetworksandisamongthemostpopularregularization
strategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe
describedinchapter.7
6.2.1.1LearningConditionalDistributionswithMaximumLikelihood
Mostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans
thatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed
1 7 8
CHAPTER6.DEEPFEEDFORWARDNETWORKS
asthecross-entropybetweenthetrainingdataandthemodeldistribution.This
costfunctionisgivenby
J() = θ − E x y ,∼ ˆ pdatalog p m o de l( )yx| (6.12)
Thespeciﬁcformofthecostfunctionchangesfrommodeltomodel,depending
onthespeciﬁcformoflog p m o de l.Theexpansionoftheaboveequationtypically
yieldssometermsthatdonotdependonthemodelparametersandmaybedis-
carded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;θ) ,I),
thenwerecoverthemeansquarederrorcost,
J θ() =1
2E x y ,∼ ˆ pdata||− ||y f(;)xθ2+const , (6.13)
uptoascalingfactorof1
2andatermthatdoesnotdependon.Thediscardedθ
constantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase
wechosenottoparametrize Previously,wesawthattheequivalencebetween
maximumlikelihoodestimationwithanoutputdistributionandminimization of
meansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds
regardlessoftheusedtopredictthemeanoftheGaussian f(;)xθ
Anadvantageofthisapproachofderivingthecostfunctionfrommaximum
likelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel Specifyingamodel p(yx|)automatically determinesacostfunction log p(yx|) Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientof
thecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide
forthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)undermine
thisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases
thishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe
hiddenunitsortheoutputunitssaturate Thenegativelog-likelihoodhelpsto
avoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction
thatcansaturatewhenitsargumentisverynegative.The logfunctioninthe
negativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill
discusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin
section.6.2.2
Oneunusualpropertyofthecross-entropycostusedtoperformmaximum
likelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied
tothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most
modelsareparametrized insuchawaythattheycannotrepresentaprobability
ofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression
isanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel
1 7 9
CHAPTER6.DEEPFEEDFORWARDNETWORKS
cancontrolthedensityoftheoutputdistribution(forexample,bylearningthe
varianceparameterofaGaussianoutputdistribution)thenitbecomespossible
toassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin
cross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribed
inchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso 7
thatthemodelcannotreapunlimitedrewardinthisway 6.2.1.2LearningConditionalStatistics
Insteadoflearningafullprobabilitydistribution p(yx|;θ)weoftenwanttolearn
justoneconditionalstatisticofgiven.yx
Forexample,wemayhaveapredictor f(x;θ) thatwewishtopredictthemean
of.y
Ifweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneural
networkasbeingabletorepresentanyfunction ffromawideclassoffunctions,
withthisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness
ratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,we
canviewthecostfunctionasbeingafunctionalratherthanjustafunction.A
functionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof
learningaschoosingafunctionratherthanmerelychoosingasetofparameters

============================================================

=== CHUNK 044 ===
Palavras: 351
Caracteres: 8368
--------------------------------------------------
Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc
functionwedesire.Forexample,wecandesignthecostfunctionaltohaveits
minimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical
toolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2
tounderstandcalculusofvariationstounderstandthecontentofthischapter.At
themoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe
usedtoderivethefollowingtworesults Ourﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-
tionproblem
f∗= argmin
fE x y ,∼ pdata||− ||y f()x2(6.14)
yields
f∗() = x E y∼ pdata ( ) y x|[]y , (6.15)
solongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe
couldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,
minimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe
meanofforeachvalueof y x
1 8 0
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Diﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusing
calculusofvariationsisthat
f∗= argmin
fE x y ,∼ pdata||− ||y f()x 1 (6.16)
yieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha
functionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost
functioniscommonlycalled meanabsoluteerror
Unfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor
resultswhenusedwithgradient-basedoptimization Someoutputunitsthat
saturateproduceverysmallgradientswhencombinedwiththesecostfunctions Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmean
squarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean
entiredistribution p( )yx|
6.2.2OutputUnits
Thechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most
ofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe
modeldistribution Thechoiceofhowtorepresenttheoutputthendetermines
theformofthecross-entropyfunction Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe
usedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe
model,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits
withadditionaldetailabouttheiruseashiddenunitsinsection.6.3
Throughoutthissection,wesupposethatthefeedforwardnetworkprovidesa
setofhiddenfeaturesdeﬁnedbyh= f(x;θ).Theroleoftheoutputlayeristhen
toprovidesomeadditionaltransformationfromthefeaturestocompletethetask
thatthenetworkmustperform 6.2.2.1LinearUnitsforGaussianOutputDistributions
Onesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformation
withnononlinearity.Theseareoftenjustcalledlinearunits Givenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=Wh+b Linearoutputlayersareoftenusedtoproducethemeanofaconditional
Gaussiandistribution:
p( ) = (;yx| NyˆyI ,) (6.17)
1 8 1
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Maximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared
error Themaximumlikelihoodframeworkmakesitstraightforwardtolearnthe
covarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea
functionoftheinput.However,thecovariancemustbeconstrainedtobeapositive
deﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinear
outputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4
Becauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-
basedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization
algorithms 6.2.2.2SigmoidUnitsforBernoulliOutputDistributions
Manytasksrequirepredictingthevalueofabinaryvariable y.Classiﬁcation
problemswithtwoclassescanbecastinthisform Themaximum-likelihoodapproachistodeﬁneaBernoullidistributionover y
conditionedon.x
ABernoullidistributionisdeﬁnedbyjustasinglenumber.Theneuralnet
needstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it
mustlieintheinterval[0,1] Satisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposewewere
tousealinearunit,andthresholditsvaluetoobtainavalidprobability:
P y(= 1 ) = max |x
0min ,
1 ,wh+ b
.(6.18)
Thiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeable
totrainitveryeﬀectivelywithgradientdescent.Anytimethatwh+ bstrayed
outsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto
itsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe
learningalgorithmnolongerhasaguideforhowtoimprovethecorresponding
parameters Instead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysa
stronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased
onusingsigmoidoutputunitscombinedwithmaximumlikelihood Asigmoidoutputunitisdeﬁnedby
ˆ y σ= 
wh+ b
(6.19)
1 8 2
CHAPTER6.DEEPFEEDFORWARDNETWORKS
whereisthelogisticsigmoidfunctiondescribedinsection σ 3.10
Wecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it
usesalinearlayertocompute z=wh+ b.Next,itusesthesigmoidactivation
functiontoconvertintoaprobability z
Weomitthedependenceonxforthemomenttodiscusshowtodeﬁnea
probabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated
byconstructinganunnormalized probabilitydistribution˜ P( y),whichdoesnot
sumto1.Wecanthendividebyanappropriateconstanttoobtainavalid
probabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log
probabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized
probabilities WethennormalizetoseethatthisyieldsaBernoullidistribution
controlledbyasigmoidaltransformationof: z
log˜ P y y z () = (6.20)
˜ P y y z () = exp() (6.21)
P y() =exp() y z1
y= 0exp( yz)(6.22)
P y σ y z () = ((2−1)) (6.23)
Probabilitydistributionsbasedonexponentiationandnormalization arecommon
throughoutthestatisticalmodelingliterature.The zvariabledeﬁningsucha
distributionoverbinaryvariablesiscalleda.logit
Thisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse
withmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum
likelihoodis−log P( y|x),theloginthecostfunctionundoestheexpofthe
sigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-
based learningfrom makinggoodprogress.Theloss functionfor maximum
likelihoodlearningofaBernoulliparametrized byasigmoidis
J P y () = logθ − (|x) (6.24)
= log((2 1)) − σ y− z (6.25)
= ((12)) ζ − y z (6.26)
Thisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10
thelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen
(1−2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready
hastherightanswer—when y= 1and zisverypositive,or y= 0and zisvery
negative.When zhasthewrongsign,theargumenttothesoftplusfunction,
1 8 3
CHAPTER6.DEEPFEEDFORWARDNETWORKS
(1−2 y) z,maybesimpliﬁedto|| z.As|| zbecomeslargewhile zhasthewrongsign,
thesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The
derivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely
incorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty
isveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly
correctamistaken z
Whenweuseotherlossfunctions,suchasmeansquarederror,thelosscan
saturateanytime σ( z)saturates.Thesigmoidactivationfunctionsaturatesto0
when zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive Thegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,
whetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,
maximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid
outputunits Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,because
thesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing
theentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,
toavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa
functionof z,ratherthanasafunctionofˆ y= σ( z).Ifthesigmoidfunction
underﬂowstozero,thentakingthelogarithmofˆ yyieldsnegativeinﬁnity 6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions
Anytimewewishtorepresentaprobabilitydistributionoveradiscretevariable
with npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa
generalization ofthesigmoidfunctionwhichwasusedtorepresentaprobability
distributionoverabinaryvariable Softmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresent
theprobabilitydistributionover ndiﬀerentclasses.Morerarely,softmaxfunctions
canbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof
ndiﬀerentoptionsforsomeinternalvariable Inthecaseofbinaryvariables,wewishedtoproduceasinglenumber
ˆ y P y

============================================================

=== CHUNK 045 ===
Palavras: 358
Caracteres: 8656
--------------------------------------------------
= (= 1 )|x (6.27)
Becausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1
logarithmofthenumbertobewell-behavedforgradient-basedoptimization of
thelog-likelihood,wechosetoinsteadpredictanumber z=log˜ P( y=1|x) ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe
sigmoidfunction 1 8 4
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Togeneralizetothecaseofadiscretevariablewith nvalues,wenowneed
toproduceavectorˆy,with ˆ y i= P( y= i|x).Werequirenotonlythateach
elementofˆ y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1
itrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor
theBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear
layerpredictsunnormalized logprobabilities:
zW= hb+ , (6.28)
where z i=log˜ P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand
normalizetoobtainthedesired z ˆy.Formally,thesoftmaxfunctionisgivenby
softmax()z i=exp( z i)
jexp( z j) (6.29)
Aswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen
trainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In
thiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deﬁningthe
softmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo
theofthesoftmax: exp
logsoftmax()z i= z i−log
jexp( z j) (6.30)
Theﬁrsttermofequationshowsthattheinput 6.30 z ialwayshasadirect
contributiontothecostfunction.Becausethistermcannotsaturate,weknow
thatlearningcanproceed,evenifthecontributionof z itothesecondtermof
equationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst 6.30
termencourages z itobepushedup,whilethesecondtermencouragesallofztobe
pusheddown.Togainsomeintuitionforthesecondterm,log
jexp( z j),observe
thatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is
basedontheideathatexp( z k) isinsigniﬁcantforany z kthatisnoticeablylessthan
max j z j.Theintuitionwecangainfromthisapproximation isthatthenegative
log-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect
prediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then
the− z itermandthelog
jexp( z j)≈max j z j= z itermswillroughlycancel Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe
dominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed Sofarwehavediscussedonlyasingleexample.Overall,unregularized maximum
likelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict
1 8 5
CHAPTER6.DEEPFEEDFORWARDNETWORKS
thefractionofcountsofeachoutcomeobservedinthetrainingset:
softmax((;))zxθ i≈m
j = 1 1y() j= i , x() j= xm
j = 1 1x() j = x (6.31)
Becausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen
solongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In
practice,limitedmodelcapacityandimperfectoptimization willmeanthatthe
modelisonlyabletoapproximatethesefractions Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell
withthesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogto
undotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes
verynegative,causingthegradienttovanish.Inparticular,squarederrorisa
poorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits
output,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle
1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine
thesoftmaxfunctionitself Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas
asingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely
positive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These
outputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecome
extreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax
alsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction Toseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,
observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits
inputs:
softmax() = softmax(+) zz c (6.32)
Usingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:
softmax() = softmax( max zz−
iz i) (6.33)
Thereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical
errorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-
aminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven
bytheamountthatitsargumentsdeviatefrommax i z i Anoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1
( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput
softmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis
muchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and
1 8 6
CHAPTER6.DEEPFEEDFORWARDNETWORKS
cancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedto
compensateforit Theargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput
everyelementofz,asdescribedaboveusingthelinearlayerz=Wh+b.While
straightforward,thisapproachactuallyoverparametrizes thedistribution.The
constraintthatthe noutputsmustsumtomeansthatonly 1 n−1parametersare
necessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe
ﬁrst n−1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement
ofzbeﬁxed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly
whatthesigmoidunitdoes.Deﬁning P( y= 1|x) = σ( z)isequivalenttodeﬁning
P( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n−1
argumentandthe nargumentapproachestothesoftmaxcandescribethesame
setofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,
thereisrarelymuchdiﬀerencebetweenusingtheoverparametrized versionorthe
restrictedversion,anditissimplertoimplementtheoverparametrized version Fromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxas
awaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the
softmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily
correspondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral
inhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe
extreme(whenthediﬀerencebetweenthemaximal a iandtheothersislargein
magnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1
andtheothersarenearly0) Thename“softmax”canbesomewhatconfusing.Thefunctionismoreclosely
relatedtotheargmaxfunctionthanthemaxfunction Theterm“soft”derives
fromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable The
argmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous
ordiﬀerentiable Thesoftmaxfunctionthusprovidesa“softened”versionofthe
argmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)z Itwouldperhapsbebettertocallthesoftmaxfunction“softargmax,” butthe
currentnameisanentrenchedconvention 6.2.2.4OtherOutputTypes
Thelinear, sigmoid, andsoftmaxoutputunitsdescribedabovearethemost
common.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat
wewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign
1 8 7
CHAPTER6.DEEPFEEDFORWARDNETWORKS
agoodcostfunctionfornearlyanykindofoutputlayer Ingeneral,ifwedeﬁneaconditionaldistribution p(yx|;θ),theprincipleof
maximumlikelihoodsuggestsweuse asourcostfunction − | log( pyxθ;)
Ingeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;θ) Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,
f(x;θ) =ωprovidestheparametersforadistributionover y.Ourlossfunction
canthenbeinterpretedas −log(;()) p yωx
Forexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,
given x.Inthesimplecase,wherethevariance σ2isaconstant,thereisaclosed
formexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe
empiricalmeanofthesquareddiﬀerencebetweenobservations yandtheirexpected
value.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting
special-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe
distribution p( y|x)thatiscontrolledbyω= f(x;θ).Thenegativelog-likelihood
−log p(y;ω(x))willthenprovideacostfunctionwiththeappropriateterms
necessarytomakeouroptimization procedureincrementally learnthevariance.In
thesimplecasewherethestandarddeviationdoesnotdependontheinput,we
canmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnew
parametermightbe σitselforcouldbeaparameter vrepresenting σ2oritcould
beaparameter βrepresenting1
σ2,dependingonhowwechoosetoparametrize
thedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvariance
in yfordiﬀerentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe
heteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneof
thevaluesoutputby f( x;θ).AtypicalwaytodothisistoformulatetheGaussian
distributionusingprecision,ratherthanvariance,asdescribedinequation.3.22
Inthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix
diag (6.34) ()β

============================================================

=== CHUNK 046 ===
Palavras: 350
Caracteres: 3667
--------------------------------------------------
Thisformulationworkswellwithgradientdescentbecausetheformulaforthe
log-likelihoodoftheGaussiandistributionparametrized byβinvolvesonlymul-
tiplicationby β iandadditionoflogβ i.Thegradientofmultiplication, addition,
andlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the
outputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction
becomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,
arbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the
outputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,
andwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation
canvanishnearzero,makingitdiﬃculttolearnparametersthataresquared 1 8 8
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Regardlessofwhetherweusestandarddeviation,variance,orprecision,wemust
ensurethatthecovariancematrixoftheGaussianispositivedeﬁnite Because
theeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof
thecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis
positivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,
thentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity Ifwesupposethataistherawactivationofthemodelusedtodeterminethe
diagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision
vector:β= ζ(a) .Thissamestrategyappliesequallyifusingvarianceorstandard
deviationratherthanprecisionorifusingascalartimesidentityratherthan
diagonalmatrix Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethan
diagonal Ifthecovarianceisfullandconditional,thenaparametrization must
bechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix Thiscanbeachievedbywriting Σ() = ()xBxB()x,whereBisanunconstrained
squarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe
likelihoodisexpensive,witha d d×matrixrequiring O( d3)computationforthe
determinantandinverseof Σ(x)(orequivalently,andmorecommonlydone,its
eigendecompositionorthatof).Bx()
Weoftenwanttoperformmultimodalregression,thatis,topredictrealvalues
thatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiﬀerent
peaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis
anaturalrepresentationfortheoutput( ,;,) Jacobs e t a l .1991Bishop1994
NeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture
densitynetworks.AGaussianmixtureoutputwith ncomponentsisdeﬁnedby
theconditionalprobabilitydistribution
p( ) =yx|n
i = 1p i (= c |Nx)(;yµ( ) i()x , Σ( ) i())x .(6.35)
Theneuralnetworkmusthavethreeoutputs:avectordeﬁning p(c= i|x),a
matrixprovidingµ( ) i(x)forall i,andatensorproviding Σ( ) i(x)forall i.These
outputsmustsatisfydiﬀerentconstraints:
1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution
overthe ndiﬀerentcomponentsassociatedwithlatentvariable1c,andcan
1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t
y , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t
w e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a
ra n d o m v a ria b l e 1 8 9
CHAPTER6.DEEPFEEDFORWARDNETWORKS
typicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee
thattheseoutputsarepositiveandsumto1 2.Meansµ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th
Gaussiancomponent,andareunconstrained(typicallywithnononlinearity
atallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput
an n d×matrixcontainingall nofthese d-dimensionalvectors

============================================================

=== CHUNK 047 ===
Palavras: 356
Caracteres: 9248
--------------------------------------------------
Learning
thesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan
learningthemeansofadistributionwithonlyoneoutputmode.Weonly
wanttoupdatethemeanforthecomponentthatactuallyproducedthe
observation.Inpractice,wedonotknowwhichcomponentproducedeach
observation.Theexpressionforthenegativelog-likelihoodnaturallyweights
eachexample’scontributiontothelossforeachcomponentbytheprobability
thatthecomponentproducedtheexample 3.Covariances Σ( ) i(x):thesespecifythecovariancematrixforeachcomponent
i.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal
matrixtoavoidneedingtocomputedeterminants Aswithlearningthemeans
ofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign
partialresponsibilityforeachpointtoeachmixturecomponent.Gradient
descentwillautomatically followthecorrectprocessifgiventhecorrect
speciﬁcationofthenegativelog-likelihoodunderthemixturemodel Ithasbeenreportedthatgradient-basedoptimization ofconditionalGaussian
mixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone
getsdivisions(bythevariance)whichcanbenumericallyunstable(whensome
variancegetstobesmallforaparticularexample,yieldingverylargegradients) Onesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1
thegradientsheuristically( ,) MurrayandLarochelle2014
Gaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsof
speech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The
mixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput
modesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining
ahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture
densitynetworkisshowninﬁgure.6.4
Ingeneral,wemaywishtocontinuetomodellargervectorsycontainingmore
variables,andtoimposericherandricherstructuresontheseoutputvariables.For
example,wemaywishforourneuralnetworktooutputasequenceofcharacters
thatformsasentence.Inthese cases,wemaycontinuetousetheprinciple
ofmaximumlikelihoodappliedtoourmodel p(y;ω(x)),butthemodelweuse
1 9 0
CHAPTER6.DEEPFEEDFORWARDNETWORKS
xy
Figure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer Theinput xissampledfromauniformdistributionandtheoutput yissampledfrom
p m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto
theparametersoftheoutputdistribution.Theseparametersincludetheprobabilities
governingwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe
parametersforeachmixturecomponent.EachmixturecomponentisGaussianwith
predictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto
varywithrespecttotheinput,andtodosoinnonlinearways x
todescribeybecomescomplexenoughtobebeyondthescopeofthischapter Chapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels 10
oversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III
probabilitydistributions 3 Hi d d en Un i t s
Sofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat
arecommontomostparametricmachinelearningmodelstrainedwithgradient-
basedoptimization Nowweturntoanissuethatisuniquetofeedforwardneural
networks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe
model Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot
yethavemanydeﬁnitiveguidingtheoreticalprinciples Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother
typesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentouse
whichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice) We
1 9 1
CHAPTER6.DEEPFEEDFORWARDNETWORKS
describeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits Theseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually
impossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists
oftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen
traininganetworkwiththatkindofhiddenunitandevaluatingitsperformance
onavalidationset Someofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableat
allinputpoints.Forexample,therectiﬁedlinearfunction g( z) =max{0 , z}isnot
diﬀerentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-
basedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough
forthesemodelstobeusedformachinelearningtasks Thisisinpartbecause
neuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof
thecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshownin
ﬁgure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8
expecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable
fortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient Hiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiable atonlya
smallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeﬁned
bytheslopeofthefunctionimmediately totheleftof zandarightderivative
deﬁnedbytheslopeofthefunctionimmediately totherightof z.Afunction
isdiﬀerentiableat zonlyifboththeleftderivativeandtherightderivativeare
deﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneural
networksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthe
caseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative
is.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1
theone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedor
raisinganerror Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-
basedoptimization onadigitalcomputerissubjecttonumericalerroranyway Whenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying
valuetrulywas.Instead,itwaslikelytobesomesmallvalue 0 thatwasrounded
to.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but 0
theseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat
inpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunit
activationfunctionsdescribedbelow Unlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting
avectorofinputsx,computinganaﬃnetransformationz=Wx+b,and
thenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare
distinguishedfromeachotheronlybythechoiceoftheformoftheactivation
function g()z
1 9 2
CHAPTER6.DEEPFEEDFORWARDNETWORKS
6.3.1RectiﬁedLinearUnitsandTheirGeneralizations
Rectiﬁedlinearunitsusetheactivationfunction g z , z () = max0{}
Rectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear
units.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitis
thatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain This makesthe
derivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive Thegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe
rectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0
operationiseverywherethattheunitisactive.Thismeansthatthegradient 1
directionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions
thatintroducesecond-ordereﬀects Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:
hW= ( gxb+) (6.36)
Wheninitializingtheparametersoftheaﬃnetransformation,itcanbeagood
practicetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes
itverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputs
inthetrainingsetandallowthederivativestopassthrough Severalgeneralizations ofrectiﬁedlinearunitsexist.Mostofthesegeneral-
izationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperform
better Onedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-
based methods onexamples for which their activ ation iszero.Avariety of
generalizations ofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-
where Threegeneralizations ofrectiﬁedlinearunitsarebasedonusinganon-zero
slope α iwhen z i <0: h i= g(zα ,) i=max(0 , z i)+ α imin(0 , z i).Absolutevalue
rectiﬁcationﬁxes α i=−1toobtain g( z) =|| z.Itisusedforobjectrecognition
fromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009
invariantunderapolarityreversaloftheinputillumination Othergeneralizations
ofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l 2013)ﬁxes α itoasmallvaluelike0.01whileaparametricReLUorPReLU
treats α iasalearnableparameter(,) He e t a l .2015
Maxoutunits( ,)generalizerectiﬁedlinearunits Goodfellow e t a l .2013a
further.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez
intogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof
1 9 3
CHAPTER6.DEEPFEEDFORWARDNETWORKS
oneofthesegroups:
g()z i=max
j∈ G() iz j (6.37)
where G( ) iisthesetofindicesintotheinputsforgroup i,{( i−1) k+1 , Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple
directionsintheinputspace.x
Amaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces Maxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather
thanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan
learntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,
amaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe
inputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolute
valuerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearnto
implementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcourse
beparametrized diﬀerentlyfromanyoftheseotherlayertypes,sothelearning
dynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthe
samefunctionofasoneoftheotherlayertypes

============================================================

=== CHUNK 048 ===
Palavras: 354
Caracteres: 9439
--------------------------------------------------
x
Eachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,
somaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.They
canworkwellwithoutregularizationifthetrainingsetislargeandthenumberof
piecesperunitiskeptlow(,) Cai e t a l .2013
Maxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-
tisticalandcomputational advantagesbyrequiringfewerparameters.Speciﬁcally,
ifthefeaturescapturedby ndiﬀerentlinearﬁlterscanbesummarizedwithout
losinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext
layercangetbywithtimesfewerweights k
Becauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-
dancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting
inwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin
thepast( ,) Goodfellow e t a l .2014a
Rectiﬁedlinearunitsandallofthesegeneralizations ofthemarebasedonthe
principlethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization
alsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan
learnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining
them,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch
easierwhensomelinearcomputations (withsomedirectionalderivativesbeingof
magnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork
1 9 4
CHAPTER6.DEEPFEEDFORWARDNETWORKS
architectures,theLSTM,propagatesinformationthroughtimeviasummation—a
particularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther
insection.10.10
6.3.2LogisticSigmoidandHyperbolicTangent
Priortotheintroduction ofrectiﬁedlinearunits,mostneuralnetworksusedthe
logisticsigmoidactivationfunction
g z σ z () = () (6.38)
orthehyperbolictangentactivationfunction
g z z () = tanh( ) (6.39)
Theseactivationfunctionsarecloselyrelatedbecause tanh( ) = 2(2)1 z σ z−
We havealready seen sigmoid unitsasoutput units, usedto predictthe
probabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1
unitssaturateacrossmostoftheirdomain—they saturatetoahighvaluewhen
zisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly
stronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof
sigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,
theiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse
asoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan
appropriatecostfunctioncanundothesaturationofthesigmoidintheoutput
layer Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangent
activationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles
theidentityfunctionmoreclosely,inthesensethattanh(0) = 0while σ(0) =1
2 Becausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0
networkˆ y=wtanh(Utanh(Vx))resemblestrainingalinearmodelˆ y=
wUVxsolongastheactivationsofthenetworkcanbekeptsmall.This
makestrainingthenetworkeasier tanh
Sigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-
forwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome
autoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise
linearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe
drawbacksofsaturation 1 9 5
CHAPTER6.DEEPFEEDFORWARDNETWORKS
6.3.3OtherHiddenUnits
Manyothertypesofhiddenunitsarepossible,butareusedlessfrequently Ingeneral,awidevarietyofdiﬀerentiable functionsperformperfectlywell Manyunpublishedactivationfunctionsperformjustaswellasthepopularones Toprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing
h=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan
1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation
functions.Duringresearchanddevelopmentofnewtechniques,itiscommon
totestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationson
standardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit
typesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcant
improvement.Newhiddenunittypesthatperformroughlycomparablytoknown
typesaresocommonastobeuninteresting Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared
intheliterature.Wehighlightafewespeciallyusefulanddistinctiveones Onepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof
thisasusingtheidentityfunctionastheactivationfunction.Wehavealready
seenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay
alsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly
lineartransformations,thenthenetworkasawholewillbelinear.However,it
isacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider
aneuralnetworklayerwith ninputsand poutputs,h= g(Wx+b).Wemay
replacethiswithtwolayers,withonelayerusingweightmatrixUandtheother
usingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehave
essentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The
factoredapproachistocomputeh= g(VUx+b).IfUproduces qoutputs,
thenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p
parameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It
comesatthecostofconstrainingthelineartransformationtobelow-rank,but
theselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeran
eﬀectivewayofreducingthenumberofparametersinanetwork Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as
describedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3
unitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k
possiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden
unitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto
manipulatememory,describedinsection.10.12
1 9 6
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Afewotherreasonablycommonhiddenunittypesinclude:
•RadialbasisfunctionorRBFunit: h i=exp
−1
σ2
i||W : , i−||x2
.This
functionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit
saturatestoformost,itcanbediﬃculttooptimize 0x
•Softplus: g( a) = ζ( a) =log(1+ ea).Thisisasmoothversionoftherectiﬁer,
introducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair
andHinton2010()fortheconditionaldistributionsofundirectedprobabilistic
models ()comparedthesoftplusandrectiﬁerandfound Glorot e t a l .2011a
betterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged Thesoftplusdemonstratesthattheperformanceofhiddenunittypescan
beverycounterintuitive—onemightexpectittohaveanadvantageover
therectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatingless
completely,butempiricallyitdoesnot •Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlike
thelatter,itisbounded, g( a)=max(−1 ,min(1 , a)).Itwasintroduced
by() Collobert2004
Hiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden
unittypesremaintobediscovered 4 A rc h i t ec t u re D es i gn
Anotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture Thewordarchitecturereferstotheoverallstructureofthenetwork:howmany
unitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother Mostneuralnetworksareorganizedintogroupsofunitscalledlayers Most
neuralnetworkarchitectures arrangetheselayersinachainstructure,witheach
layerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayer
isgivenby
h( 1 )= g( 1 )
W( 1 )xb+( 1 )
, (6.40)
thesecondlayerisgivenby
h( 2 )= g( 2 )
W( 2 )h( 1 )+b( 2 )
, (6.41)
andsoon 1 9 7
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Inthesechain-basedarchitectures,themainarchitecturalconsiderationsare
tochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,
anetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deeper
networksoftenareabletousefarfewerunitsperlayerandfarfewerparameters
andoftengeneralizetothetestset,butarealsooftenhardertooptimize The
idealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby
monitoringthevalidationseterror 6.4.1UniversalApproximationPropertiesandDepth
Alinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can
bydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto
trainbecausemanylossfunctionsresultinconvexoptimization problemswhen
appliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires
designingaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-
mationframework.Speciﬁcally,theuniversalapproximationtheorem(Hornik
e t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989
layerandatleastonehiddenlayerwithany“squashing”activationfunction(such
asthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable
functionfromoneﬁnite-dimensional spacetoanotherwithanydesirednon-zero
amountoferror,providedthatthenetworkisgivenenoughhiddenunits.The
derivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe
functionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990
isbeyondthescopeofthisbook; forourpurposesitsuﬃcestosaythatany
continuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable
andthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay
alsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespace
toanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswith
activationfunctionsthatsaturatebothforverynegativeandforverypositive
arguments,universalapproximation theoremshavealsobeenprovedforawider
classofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinear
unit( ,)

============================================================

=== CHUNK 049 ===
Palavras: 401
Caracteres: 7639
--------------------------------------------------
Leshno e t a l .1993
Theuniversalapproximationtheoremmeansthatregardlessofwhatfunction
wearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis
function.However,wearenotguaranteedthatthetrainingalgorithmwillbeable
to l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning
canfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining
1 9 8
CHAPTER6.DEEPFEEDFORWARDNETWORKS
maynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesired
function.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto
overﬁtting.Recallfromsectionthatthe“nofreelunch”theoremshowsthat 5.2.1
thereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks
provideauniversalsystemforrepresentingfunctions,inthesensethat,givena
function,thereexistsafeedforwardnetworkthatapproximatesthefunction.There
isnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesand
choosingafunctionthatwillgeneralizetopointsnotinthetrainingset Theuniversalapproximationtheoremsaysthatthereexistsanetworklarge
enoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot
sayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993
sizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly
withonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobe
distinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the
numberofpossiblebinaryfunctionsonvectorsv∈{0 ,1}nis22nandselecting
onesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof
freedom Insummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresent
anyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand
generalizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe
numberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe
amountofgeneralization error Thereexistfamiliesoffunctionswhichcanbeapproximated eﬃcientlybyan
architecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger
modelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber
ofhiddenunitsrequiredbytheshallowmodelisexponentialin n Suchresults
wereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiable
neuralnetworksusedformachinelearning,buthavesincebeenextendedtothese
models.Theﬁrstresultswereforcircuitsoflogicgates(,).Later Håstad1986
workextendedtheseresultstolinearthresholdunitswithnon-negativeweights
( ,; ,),andthentonetworkswith HåstadandGoldmann1991Hajnal e t a l .1993
continuous-valuedactivations(,; ,) Manymodern Maass1992Maass e t a l .1994
neuralnetworksuserectiﬁedlinearunits ()demonstrated Leshno e t a l .1993
thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,
includingrectiﬁedlinearunits,haveuniversalapproximation properties,butthese
resultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythat
asuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufar e t a l 1 9 9
CHAPTER6.DEEPFEEDFORWARDNETWORKS
()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014
anexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained
fromrectiﬁernonlinearities ormaxoutunits)canrepresentfunctionswithanumber
ofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5
anetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunction
computedontopofsomehiddenunit,withrespecttotheinputofthathidden
unit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreate
mirrorresponses(onbothsidesoftheabsolutevaluenonlinearity) Bycomposing
thesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise
linearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper
rectiﬁernetworksformallyby () Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectiﬁcation
unithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis
ofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.A
functioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage
ofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )
byfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )
befoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry
(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith
permissionfrom () Montufar e t a l .2014
Moreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014
numberoflinearregionscarvedoutbyadeeprectiﬁernetworkwith dinputs,
depth,andunitsperhiddenlayer,is l n
On
dd l (− 1 )
nd
, (6.42)
i.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersper l k
unit,thenumberoflinearregionsis
O
k( 1 ) + l− d (6.43)
2 0 0
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Ofcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin
applicationsofmachinelearning(andinparticularforAI)sharesuchaproperty Wemayalsowanttochooseadeepmodelforstatisticalreasons Anytime
wechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsome
setofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould
learn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe
wanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe
interpretedfromarepresentationlearningpointofviewassayingthatwebelieve
thelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation
thatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof
variation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing
abeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof
multiplesteps,whereeachstepmakesuseofthepreviousstep’soutput These
intermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe
analogoustocountersorpointersthatthenetworkusestoorganizeitsinternal
processing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization
forawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009
Mesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,
2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;
e t a l ,;2014dSzegedy ,).Seeﬁgureandﬁgureforexamplesof 2014a 6.6 6.7
someoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes
indeedexpressausefulprioroverthespaceoffunctionsthemodellearns 6.4.2OtherArchitecturalConsiderations
Sofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe
mainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer Inpractice,neuralnetworksshowconsiderablymorediversity Manyneuralnetworkarchitectures havebeendevelopedforspeciﬁctasks Specializedarchitecturesforcomputervisioncalledconvolutionalnetworksare
describedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9
recurrentneuralnetworksforsequenceprocessing,describedinchapter,which10
havetheirownarchitecturalconsiderations Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe
mostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra
architecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer
i+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfrom
outputlayerstolayersnearertheinput 2 0 1
CHAPTER6.DEEPFEEDFORWARDNETWORKS
3 4 5 6 7 8 9 1 0 1 1
N u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )
Figure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused
totranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow
e t a l .()

============================================================

=== CHUNK 050 ===
Palavras: 382
Caracteres: 6761
--------------------------------------------------
Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth See 2014d
ﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7
donotyieldthesameeﬀect Anotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta
pairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear
transformationviaamatrixW,everyinputunitisconnectedtoeveryoutput
unit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so
thateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin
theoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce
thenumberofparametersandtheamountofcomputationrequiredtoevaluate
thenetwork,butareoftenhighlyproblem-dependent Forexample,convolutional
networks,describedinchapter,usespecializedpatternsofsparseconnections 9
thatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃcult
togivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneural
network.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat
havebeenfoundtoworkwellfordiﬀerentapplicationdomains 2 0 2
CHAPTER6.DEEPFEEDFORWARDNETWORKS
0 0 0 2 0 4 0 6 0 8 1 0 N u m b e r o f p a r a m e t e r s × 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional
3,fullyconnected
11,convolutional
Figure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis
larger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber
ofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot
nearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof
networkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof
theconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis
contextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhaving
over60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover
thespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthe
functionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult
eitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,
cornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependent
steps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize
them) 2 0 3
CHAPTER6.DEEPFEEDFORWARDNETWORKS
6 5 Bac k - Prop a g a t i o n an d O t h er D i ﬀ eren t i at i on A l go-
ri t h m s
Whenweuseafeedforwardneuralnetworktoacceptaninputxandproducean
output ˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovide
theinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer
andﬁnallyproduces ˆy.Thisiscalledforwardpropagation.Duringtraining,
forwardpropagationcancontinueonwarduntilitproducesascalarcost J(θ) Theback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a
backprop,allowstheinformationfromthecosttothenﬂowbackwardsthrough
thenetwork,inordertocomputethegradient Computingananalyticalexpressionforthegradientisstraightforward,but
numericallyevaluatingsuchanexpressioncanbecomputationally expensive.The
back-propagationalgorithmdoessousingasimpleandinexpensiveprocedure Thetermback-propagation isoften misunders toodasmeaningthewhole
learningalgorithmformulti-layerneuralnetworks.Actually,back-propagation
refersonlytothemethodforcomputingthegradient,whileanotheralgorithm,
suchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient Furthermore,back-propagation isoftenmisunderstoodasbeingspeciﬁctomulti-
layerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction
(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe
functionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient
∇ x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives
aredesired,andyisanadditionalsetofvariablesthatareinputstothefunction
butwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost
oftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,
∇ θ J(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,either
aspartof thelearning process, or to analyzethelearned model The back-
propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted
tocomputingthegradientofthecostfunctionwithrespecttotheparameters.The
ideaofcomputingderivativesbypropagatinginformationthroughanetworkis
verygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction
fwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly
usedcasewherehasasingleoutput f
2 0 4
CHAPTER6.DEEPFEEDFORWARDNETWORKS
6.5.1ComputationalGraphs
Sofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea
moreprecise language computationalgraph
Manywaysofformalizingcomputationasgraphsarepossible Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay
beascalar,vector,matrix,tensor,orevenavariableofanothertype Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage
isaccompanied byasetofallowableoperations.Functionsmorecomplicated
thantheoperationsinthissetmaybedescribedbycomposingmanyoperations
together Withoutlossofgenerality, wedeﬁneanoperationtoreturnonlyasingle
outputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave
multipleentries,suchasavector.Softwareimplementationsofback-propagation
usuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour
descriptionbecauseitintroducesmanyextradetailsthatarenotimportantto
conceptualunderstanding Ifavariable yiscomputedbyapplyinganoperationtoavariable x,then
wedrawadirectededgefrom xto y Wesometimesannotatetheoutputnode
withthenameoftheoperationapplied,andothertimesomitthislabelwhenthe
operationisclearfromcontext Examplesofcomputational graphsareshowninﬁgure.6.8
6.5.2ChainRuleofCalculus
Thechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is
usedtocomputethederivativesoffunctionsformedbycomposingotherfunctions
whosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe
chainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient Let xbearealnumber,andlet fand gbothbefunctionsmappingfromareal
numbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then
thechainrulestatesthatd z
d x=d z
d yd y
d x (6.44)
Wecangeneralizethisbeyondthescalarcase.Supposethatx∈ Rm,y∈ Rn,
2 0 5
CHAPTER6.DEEPFEEDFORWARDNETWORKS
z z
xx yy
( a)×
x x ww
( b)u( 1 )u( 1 )
d o t
bbu( 2 )u( 2 )
+ˆ y ˆ y
σ
( c )XXWWU( 1 )U( 1 )
m a t m u l
bbU( 2 )U( 2 )
+HH
r e l u
xx ww
( d)ˆ yˆ y
d o t
λ λu( 1 )u( 1 )
s q ru( 2 )u( 2 )
s u mu( 3 )u( 3 )
×
Figure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) ×operationto
compute z= x y.Thegraphforthelogisticregressionprediction ( b ) ˆ y= σ
xw+ b

============================================================

=== CHUNK 051 ===
Palavras: 355
Caracteres: 5439
--------------------------------------------------
Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpression
butneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )
computationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign
matrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch
ofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit ( d )
ispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat
appliesmorethanoneoperationtotheweightswofalinearregressionmodel.The
weightsareusedtomakeboththepredictionˆ yandtheweightdecaypenalty λ
iw2
i 2 0 6
CHAPTER6.DEEPFEEDFORWARDNETWORKS
gmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then
∂ z
∂ x i=
j∂ z
∂ y j∂ y j
∂ x i (6.45)
Invectornotation,thismaybeequivalentlywrittenas
∇ x z=∂y
∂x
∇ y z , (6.46)
where∂ y
∂ xistheJacobianmatrixof n m× g
Fromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying
aJacobianmatrix∂ y
∂ xbyagradient∇ y z.Theback-propagation algorithmconsists
ofperformingsuchaJacobian-gradient productforeachoperationinthegraph Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,
butrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe
sameasback-propagation withvectors.Theonlydiﬀerenceishowthenumbers
arearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensor
intoavectorbeforewerunback-propagation,computingavector-valuedgradient,
andthenreshapingthegradientbackintoatensor.Inthisrearrangedview,
back-propagationisstilljustmultiplyingJacobiansbygradients Todenotethegradientofavalue zwithrespecttoatensor X,wewrite ∇ X z,
justasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates—for
example,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway
byusingasinglevariable itorepresentthecompletetupleofindices.Forall
possibleindextuples i,(∇ X z) igives∂ z
∂ X i.Thisisexactlythesameashowforall
possibleintegerindices iintoavector,(∇ x z) igives∂ z
∂ x i.Usingthisnotation,we
canwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y
∇ X z=
j(∇ X Y j)∂ z
∂ Y j (6.47)
6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop
Usingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor
thegradientofascalarwithrespecttoanynodeinthecomputational graphthat
producedthatscalar.However,actuallyevaluatingthatexpressioninacomputer
introducessomeextraconsiderations Speciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithinthe
overallexpressionforthegradient.Anyprocedurethatcomputesthegradient
2 0 7
CHAPTER6.DEEPFEEDFORWARDNETWORKS
willneedtochoosewhethertostorethesesubexpressionsortorecomputethem
severaltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin
ﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9
bewasteful Forcomplicatedgraphs,therecanbeexponentiallymanyofthese
wastedcomputations, makinganaiveimplementation ofthechainruleinfeasible Inothercases,computingthesamesubexpressiontwicecouldbeavalidwayto
reducememoryconsumptionatthecostofhigherruntime Weﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁesthe
actualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1
associatedforwardcomputation), intheorderitwillactuallybedoneandaccording
totherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese
computations orviewthedescriptionofthealgorithmasasymbolicspeciﬁcation
ofthecomputational graphforcomputingtheback-propagation However,this
formulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe
symbolicgraphthatperformsthegradientcomputation Such aformulationis
presentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5
nodesthatcontainarbitrarytensors Firstconsideracomputational graphdescribinghowtocomputeasinglescalar
u( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose
gradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ) In
otherwordswewishtocompute∂ u() n
∂ u() iforall i∈{1 ,2 , , n i}.Intheapplication
ofback-propagationtocomputinggradientsforgradientdescentoverparameters,
u( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )
correspondtotheparametersofthemodel Wewillassumethatthenodesofthegraphhavebeenorderedinsuchaway
thatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and
goingupto u( ) n.Asdeﬁnedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan
operation f( ) iandiscomputedbyevaluatingthefunction
u( ) i= ( f A( ) i) (6.48)
where A( ) iisthesetofallnodesthatareparentsof u( ) i Thatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecould
putinagraph G.Inordertoperformback-propagation, wecanconstructa
computational graphthatdependsonGandaddstoitanextrasetofnodes.These
formasubgraph BwithonenodepernodeofG.Computation inBproceedsin
exactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes
thederivative∂ u() n
∂ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone
2 0 8
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Algorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs
u( 1 )to u( n i )toanoutput u( ) n.Thisdeﬁnesacomputational graphwhereeachnode
computesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments
A( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a∈ ( u( ) i).The
inputtothecomputational graphisthevectorx,andissetintotheﬁrst n inodes
u( 1 )to u( n i ).Theoutputofthecomputational graphisreadoﬀthelast(output)
node u( ) n , n = 1 ido
u( ) i← x i
endfor
for i n= i+1 ,

============================================================

=== CHUNK 052 ===
Palavras: 358
Caracteres: 3788
--------------------------------------------------
, ndo
A( ) i←{ u( ) j|∈ j P a u(( ) i)}
u( ) i← f( ) i( A( ) i)
endfor
return u( ) n
usingthechainrulewithrespecttoscalaroutput u( ) n:
∂ u( ) n
∂ u( ) j=
i j P a u :∈ (() i )∂ u( ) n
∂ u( ) i∂ u( ) i
∂ u( ) j(6.49)
asspeciﬁedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach
edgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith
thecomputationof∂ u() i
∂ u() j.Inaddition,adotproductisperformedforeachnode,
betweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren
of u( ) jandthevectorcontainingthepartialderivatives∂ u() i
∂ u() jforthesamechildren
nodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming
theback-propagationscaleslinearlywiththenumberofedgesinG,wherethe
computationforeachedgecorrespondstocomputingapartialderivative(ofone
nodewithrespecttooneofitsparents)aswellasperformingonemultiplication
andoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which
isjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore
eﬃcientimplementations Theback-propagationalgorithmisdesignedtoreducethenumberofcommon
subexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorder
ofoneJacobianproductpernodeinthegraph Thiscanbeseenfromthefact
thatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof
thegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂ u() i
∂ u() j 2 0 9
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Algorithm6.2Simpliﬁedversionoftheback-propagation algorithmforcomputing
thederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis
intendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariables
arescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph The
computational costofthisalgorithmisproportional tothenumberofedgesin
thegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires
aconstanttime.Thisisofthesameorderasthenumberofcomputations for
theforwardpropagation Each∂ u() i
∂ u() jisafunctionoftheparents u( ) jof u( ) i,thus
linkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation
graph Runforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1
tionsofthenetwork
Initialize grad_table,adatastructurethatwillstorethederivativesthathave
beencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof
∂ u() n
∂ u() i g r a d t a b l e_ [ u( ) n] 1←
for do j n= −1downto1
Thenextlinecomputes∂ u() n
∂ u() j=
i j P a u :∈ (() i )∂ u() n
∂ u() i∂ u() i
∂ u() jusingstoredvalues:
g r a d t a b l e_ [ u( ) j] ←
i j P a u :∈ (() i ) g r a d t a b l e_ [ u( ) i]∂ u() i
∂ u() j
endfor
return{ g r a d t a b l e_ [ u( ) i] = 1 | i , , n i}
Back-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming
simpliﬁcationsonthecomputational graph,ormaybeabletoconservememoryby
recomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas
afterdescribingtheback-propagation algorithmitself 6.5.4Back-PropagationComputationinFully-ConnectedMLP
Toclarifytheabovedeﬁnitionoftheback-propagation computation,letusconsider
thespeciﬁcgraphassociatedwithafully-connected multi-layerMLP Algorithmﬁrstshowstheforwardpropagation, whichmapsparametersto 6.3
thesupervisedloss L(ˆyy ,)associatedwithasingle(input,target) trainingexample
( )xy ,,with ˆytheoutputoftheneuralnetworkwhenisprovidedininput x
Algorithm  then shows thecorresponding computation to be donefor 6.4
2 1 0
CHAPTER6.DEEPFEEDFORWARDNETWORKS
z z
xxyy
w wfff
Figure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing
thegradient.Let w∈ Rbetheinputtothegraph.Weusethesamefunction f: R R→
astheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y)

============================================================

=== CHUNK 053 ===
Palavras: 350
Caracteres: 4715
--------------------------------------------------
Tocompute∂ z
∂ w,weapplyequationandobtain: 6.44
∂ z
∂ w(6.50)
=∂ z
∂ y∂ y
∂ x∂ x
∂ w(6.51)
= f() y f() x f() w (6.52)
= f((())) f f w f(()) f w f() w (6.53)
Equationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only
onceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation
algorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53
f( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime
itisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the
back-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52
runtime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53
usefulwhenmemoryislimited 2 1 1
CHAPTER6.DEEPFEEDFORWARDNETWORKS
applyingtheback-propagation algorithmtothisgraph Algorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4
straightforwardtounderstand.However, theyarespecializedtoonespeciﬁc
problem Modernsoftwareimplementations arebasedonthegeneralizedformofback-
propagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6
tationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic
computation Algorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand
thecomputationofthecostfunction.Theloss L(ˆyy ,)dependsontheoutput
ˆyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1
obtainthetotalcost J,thelossmaybeaddedtoaregularizer Ω( θ),where θ
containsalltheparameters(weightsandbiases).Algorithm showshowto 6.4
computegradientsof JwithrespecttoparametersWandb.Forsimplicity,this
demonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould
useaminibatch.Seesectionforamorerealisticdemonstration 6.5.7
Require:Networkdepth, l
Require:W( ) i, i , , l , ∈{1 }theweightmatricesofthemodel
Require:b( ) i, i , , l , ∈{1 }thebiasparametersofthemodel
Require:x,theinputtoprocess
Require:y,thetargetoutput
h( 0 )= x
fordo k , , l = 1
a( ) k= b( ) k+W( ) kh( 1 ) k−
h( ) k= ( fa( ) k)
endfor
ˆyh= ( ) l
J L= (ˆyy ,)+Ω() λ θ
6.5.5Symbol-to-SymbolDerivatives
Algebraicexpressionsandcomputational graphsbothoperateonsymbols,or
variables thatdo not havespeciﬁc values.Thesealgebraic and graph-based
representationsarecalledsymbolicrepresentations.Whenweactuallyuseor
trainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.We
replaceasymbolicinputtothenetworkxwithaspeciﬁcnumericvalue,suchas
[123765 18] 2 1 2
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Algorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-
rithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation
yieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe
outputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,
whichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchange
toreduceerror,onecanobtainthegradientontheparametersofeachlayer.The
gradientsonweightsandbiasescanbeimmediately usedaspartofastochas-
ticgradientupdate(performingtheupdaterightafterthegradientshavebeen
computed)orusedwithothergradient-basedoptimization methods Aftertheforwardcomputation,computethegradientontheoutputlayer:
g←∇ ˆ y J= ∇ ˆ y L(ˆyy ,)
for do k l , l , , = −1 1
Convert thegradienton thelayer’s output into a gradient into thepre-
nonlinearityactivation(element-wisemultiplicationifiselement-wise): f
g←∇a() k J f = g(a( ) k)
Computegradientsonweightsandbiases(includingtheregularizationterm,
whereneeded):
∇b() k J λ = +g ∇b() kΩ() θ
∇W() k J= gh( 1 ) k−+ λ∇W() kΩ() θ
Propagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:
g←∇h(1) k − J= W( ) kg
endfor
2 1 3
CHAPTER6.DEEPFEEDFORWARDNETWORKS
z z
xxyy
w wfffz z
xxyy
w wfff
d z
d yd z
d yf
d y
d xd y
d xf
d z
d xd z
d x×
d x
d wd x
d wf
d z
d wd z
d w×
Figure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In
thisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual
speciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow
tocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe
derivativesforanyspeciﬁcnumericvalues ( L e f t )Inthisexample,webeginwithagraph
representing z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )
ittoconstructthegraphfortheexpressioncorrespondingtod z
d w.Inthisexample,wedo
notexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate
whatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe
derivative Someapproachestoback-propagationtakeacomputational graphandaset
ofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical
valuesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-
to-number”diﬀerentiation ThisistheapproachusedbylibrariessuchasTorch
( ,)andCaﬀe(,)

============================================================

=== CHUNK 054 ===
Palavras: 416
Caracteres: 7711
--------------------------------------------------
Collobert e t a l .2011b Jia2013
Anotherapproachistotakeacomputational graphandaddadditionalnodes
tothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This
istheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012
andTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015
isillustratedinﬁgure.Theprimaryadvantageofthisapproachisthat 6.10
thederivativesaredescribedinthesamelanguageastheoriginalexpression Becausethederivativesarejustanothercomputational graph,itispossibletorun
back-propagationagain,diﬀerentiating thederivativesinordertoobtainhigher
derivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10
Wewillusethelatterapproachanddescribetheback-propagationalgorithmin
2 1 4
CHAPTER6.DEEPFEEDFORWARDNETWORKS
termsofconstructingacomputational graphforthederivatives.Anysubsetofthe
graphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.This
allowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits
parents’valuesareavailable Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-
to-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas
performingexactlythesamecomputations asaredoneinthegraphbuiltbythe
symbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number
approachdoesnotexposethegraph 6.5.6GeneralBack-Propagation
Theback-propagationalgorithmisverysimple.Tocomputethegradientofsome
scalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving
thatthegradientwithrespectto zisgivenbyd z
d z=1.Wecanthencompute
thegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe
currentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue
multiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil
wereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough
twoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsat
thatnode Moreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve
maximumgenerality,wedescribethisvariableasbeingatensor V Tensorcan
ingeneralhaveanynumberofdimensions Theysubsumescalars,vectors,and
matrices Weassumethateachvariableisassociatedwiththefollowingsubroutines: V
• g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-
sentedbytheedgescominginto Vinthecomputational graph.Forexample,
theremaybeaPythonorC++classrepresentingthematrixmultiplication
operation,andtheget_operationfunction.Supposewehaveavariablethat
iscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)
returnsapointertoaninstanceofthecorrespondingC++class • g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof
Vinthecomputational graph.G
• G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V
inthecomputational graph.G
2 1 5
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Eachoperationopisalsoassociatedwithabpropoperation.Thisbprop
operationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47
Thisishowtheback-propagationalgorithmisabletoachievegreatgenerality Eachoperationisresponsibleforknowinghowtoback-propagate throughthe
edgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix
multiplicationoperationtocreateavariableC=AB.Supposethatthegradient
ofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation
isresponsiblefordeﬁningtwoback-propagation rules,oneforeachofitsinput
arguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto
AgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe
matrixmultiplicationoperationmuststatethatthegradientwithrespecttoA
isgivenbyGB.Likewise,ifwecallthe b p r o pmethodtorequestthegradient
withrespecttoB,thenthematrixoperationisresponsibleforimplementing the
b p r o pmethodandspecifyingthatthedesiredgradientisgivenbyAG.The
back-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiation rules.It
onlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,
o p b p r o p i n p u t s ( , , X G)mustreturn

i(∇ X o p f i n p u t s .( ) i) G i , (6.54)
whichisjustanimplementation ofthechainruleasexpressedinequation.6.47
Here, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe
mathematical functionthattheoperationimplements, Xistheinputwhosegradient
wewishtocompute,andisthegradientontheoutputoftheoperation G
Theop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct
fromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed
twocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe
derivativewithrespecttobothinputs.Theback-propagation algorithmwilllater
addbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal
derivativeon x
Softwareimplementationsofback-propagation usuallyprovideboththeopera-
tionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare
abletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix
multiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda
newimplementationofback-propagationoradvanceduserswhoneedtoaddtheir
ownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor
anynewoperationsmanually Theback-propagationalgorithmisformallydescribedinalgorithm .6.5
2 1 6
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Algorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This
portiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens
inthe subroutineofalgorithm build_grad 6.6 Require: T,thetargetsetofvariableswhosegradientsmustbecomputed Require:G,thecomputational graph
Require: z,thevariabletobediﬀerentiated
LetGbeGprunedtocontainonlynodesthatareancestorsof zanddescendents
ofnodesin T
Initialize ,adatastructureassociatingtensorstotheirgradients grad_table
g r a d t a b l e_ [] 1 z←
fordo Vin T
b u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )
endfor
Return restrictedto grad_table T
Insection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2
avoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive
algorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions Nowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstandits
computational cost.Ifweassumethateachoperationevaluationhasroughlythe
samecost,thenwemayanalyzethecomputational costintermsofthenumber
ofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe
fundamentalunitofourcomputational graph,whichmightactuallyconsistofvery
manyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix
multiplicationasasingleoperation).Computingagradientinagraphwith nnodes
willneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan
O( n2) operations.Herewearecountingoperationsinthecomputational graph,not
individualoperationsexecutedbytheunderlyinghardware,soitisimportantto
rememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,
multiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto
asingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas
most O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute
all nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,
wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm
addsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per
edgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic
graphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused
inpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare
2 1 7
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Algorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )of
theback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁned
inalgorithm .6.5
Require: V,thevariablewhosegradientshouldbeaddedtoand

============================================================

=== CHUNK 055 ===
Palavras: 414
Caracteres: 4029
--------------------------------------------------
Ggrad_table
Require:G,thegraphtomodify Require:G,therestrictionoftonodesthatparticipateinthegradient G
Require:grad_table,adatastructuremappingnodestotheirgradients
if then Visingrad_table
Return_ g r a d t a b l e[] V
endif
i←1
for C V in_ g e t c o n s u m e r s( ,G)do
o p g e t o p e r a t i o n ←_ () C
D C ← b u i l d g r a d_ ( , ,GG, g r a d t a b l e_ )
G( ) i← G o p b p r o p g e t i n p u t s (_ ( C ,) ) , , V D
i i←+1
endfor
G←
i G( ) i
g r a d t a b l e_ [] = V G
Insertandtheoperationscreatingitinto G G
Return G
roughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar
betterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany
nodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting
therecursivechainrule(equation)non-recursively: 6.49
∂ u( ) n
∂ u( ) j=
pa t h ( u( π1), u( π2), , u( π t)) ,
f r o m π1 = t o j π t = nt
k = 2∂ u( π k )
∂ u( π k −1 ) (6.55)
Sincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe
lengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber
ofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation
graph.Thislargecostwouldbeincurredbecausethesamecomputationfor
∂ u() i
∂ u() jwouldberedonemanytimes Toavoidsuchrecomputation, wecanthink
ofback-propagation asatable-ﬁllingalgorithmthattakesadvantageofstoring
intermediateresults∂ u() n
∂ u() i.Eachnodeinthegraphhasacorrespondingslotina
tabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,
2 1 8
CHAPTER6.DEEPFEEDFORWARDNETWORKS
back-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁlling
strategyissometimescalled dynamicprogramming
6.5.7Example:Back-PropagationforMLPTraining
Asanexample,wewalkthroughtheback-propagation algorithmasitisusedto
trainamultilayerperceptron Herewedevelopaverysimplemultilayerperceptionwithasinglehidden
layer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent Theback-propagationalgorithmisusedtocomputethegradientofthecostona
singleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetraining
setformattedasadesignmatrixXandavectorofassociatedclasslabelsy ThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To
simplifythepresentationwedonotusebiasesinthismodel.Weassumethatour
graphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-
wise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen
givenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy
operationthatcomputesthecross-entropybetweenthetargetsyandtheprobability
distributiondeﬁnedbytheseunnormalized logprobabilities Theresultingcross-
entropydeﬁnesthecost J M LE.Minimizingthiscross-entropyperformsmaximum
likelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,
wealsoincludearegularizationterm.Thetotalcost
J J= M LE+ λ

i , j
W( 1 )
i , j2
+
i , j
W( 2 )
i , j2
 (6.56)
consistsofthecross-entropyandaweightdecaytermwithcoeﬃcient λ.The
computational graphisillustratedinﬁgure.6.11
Thecomputational graphforthegradientofthisexampleislargeenoughthat
itwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁts
oftheback-propagation algorithm,whichisthatitcanautomatically generate
gradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto
derivemanually Wecanroughlytraceoutthebehavioroftheback-propagation algorithm
bylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish 6.11
tocomputeboth∇W(1) Jand ∇W(2) J.Therearetwodiﬀerentpathsleading
backwardfrom Jtotheweights:onethroughthecross-entropycost,andone
throughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill
alwayscontribute 2 λW( ) itothegradientonW( ) i 2 1 9
CHAPTER6.DEEPFEEDFORWARDNETWORKS
XXW( 1 )W( 1 )U( 1 )U( 1 )
m a t m u lHH
r e l u
U( 3 )U( 3 )
s q ru( 4 )u( 4 )
s u mλ λ u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )
m a t m u ly yJ M L E J M L E
c r o s s _ e n t r o p y
U( 5 )U( 5 )
s q ru( 6 )u( 6 )
s u mu( 8 )u( 8 )J J
+
×
+
Figure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample
ofasingle-layerMLPusingthecross-entropylossandweightdecay

============================================================

=== CHUNK 056 ===
Palavras: 357
Caracteres: 10058
--------------------------------------------------
Theotherpaththroughthecross-entropycostisslightlymorecomplicated LetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby
thecross_entropyoperation.Theback-propagation algorithmnowneedsto
exploretwodiﬀerentbranches.Ontheshorterbranch,itaddsHGtothe
gradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto
thematrixmultiplication operation.Theotherbranchcorrespondstothelonger
chaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm
computes ∇ H J=GW( 2 )usingtheback-propagationrulefortheﬁrstargument
tothematrixmultiplication operation.Next,thereluoperationusesitsback-
propagationruletozerooutcomponentsofthegradientcorrespondingtoentries
ofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G.Thelaststepofthe
back-propagationalgorithmistousetheback-propagation ruleforthesecond
argumentoftheoperationtoadd matmul XGtothegradientonW( 1 ) Afterthesegradientshavebeencomputed,itistheresponsibilityofthegradient
descentalgorithm,oranotheroptimization algorithm,tousethesegradientsto
updatetheparameters FortheMLP,thecomputational costisdominatedbythecostofmatrix
multiplication Duringtheforwardpropagationstage,wemultiplybyeachweight
2 2 0
CHAPTER6.DEEPFEEDFORWARDNETWORKS
matrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During
thebackwardpropagationstage,wemultiplybythetransposeofeachweight
matrix,whichhasthesamecomputational cost.Themainmemorycostofthe
algorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas
returnedtothesamepoint.Thememorycostisthus O( m n h),where misthe
numberofexamplesintheminibatchand n histhenumberofhiddenunits 6.5.8Complications
Ourdescriptionoftheback-propagation algorithmhereissimplerthantheimple-
mentationsactuallyusedinpractice Asnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobea
functionthatreturnsasingletensor.Mostsoftwareimplementations needto
supportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish
tocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis
besttocomputebothinasinglepassthroughmemory,soitismosteﬃcientto
implementthisprocedureasasingleoperationwithtwooutputs We havenot described how tocontrolthememoryconsumption ofback-
propagation Back-propagati onofteninvolvessummationofmanytensorstogether Inthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then
allofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly
highmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerand
addingeachvaluetothatbuﬀerasitiscomputed Real-worldimplementationsofback-propagation alsoneedtohandlevarious
datatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign Someoperationshaveundeﬁnedgradients,anditisimportanttotrackthese
casesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned Variousothertechnicalitiesmakereal-worlddiﬀerentiation morecomplicated Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey
intellectualtoolsneededtocomputederivatives,butitisimportanttobeaware
thatmanymoresubtletiesexist 6.5.9DiﬀerentiationoutsidetheDeepLearningCommunity
The deeplearning comm unityhas beensomewhat isolatedfrom the broader
computersciencecommunityandhaslargelydevelopeditsownculturalattitudes
2 2 1
CHAPTER6.DEEPFEEDFORWARDNETWORKS
concerninghowtoperformdiﬀerentiation Moregenerally,theﬁeldofautomatic
diﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic
diﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse
modeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain
ruleindiﬀerentorders.Ingeneral, determining theorderofevaluationthat
resultsinthelowestcomputational costisadiﬃcultproblem.Findingtheoptimal
sequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008
inthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast
expensiveform Forexample,supposewehavevariables p 1 , p 2 , , p nrepresentingprobabilities
andvariables z 1 , z 2 , , z nrepresentingunnormalized logprobabilities Suppose
wedeﬁne
q i=exp( z i)
iexp( z i), (6.57)
wherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision
operations, and construct a cross-entropyloss J=−
i p ilog q i.Ahuman
mathematician canobservethatthederivativeof Jwithrespectto z itakesavery
simpleform: q i− p i.Theback-propagation algorithmisnotcapableofsimplifying
thegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof
thelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware
librariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012
performsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed
bythepureback-propagation algorithm Whentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative
∂ u() i
∂ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation
guaranteesthatthenumberofcomputations forthegradientcomputationisof
thesameorderasthenumberofcomputations fortheforwardcomputation: this
canbeseeninalgorithm becauseeachlocalpartialderivative 6.2∂ u() i
∂ u() jneedsto
becomputedonlyoncealongwithanassociatedmultiplication andadditionfor
therecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49
therefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe
computational graphconstructedbyback-propagation,andthisisanNP-complete
task ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon
matchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplify
thegraph.Wedeﬁnedback-propagation onlyforthecomputationofagradientofa
scalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either
of kdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontaining k
values).Anaiveimplementation maythenneed ktimesmorecomputation: for
2 2 2
CHAPTER6.DEEPFEEDFORWARDNETWORKS
eachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation
computes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof
thegraphislargerthanthenumberofinputs,itissometimespreferabletouse
anotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation Forwardmodecomputationhasbeenproposedforobtainingreal-timecomputation
ofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989
alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading
oﬀcomputational eﬃciencyformemory.Therelationshipbetweenforwardmode
andbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus
right-multiplyingasequenceofmatrices,suchas
ABCD , (6.58)
wherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD
isacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha
singleoutputandmanyinputs,andstartingthemultiplications fromtheend
andgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto
thebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea
seriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore
expensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun
themultiplications left-to-right,correspondingtotheforwardmode Inmanycommunitiesoutsideofmachinelearning,itismorecommontoim-
plementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramming
languagecode,suchasPythonorCcode,andautomatically generatesprograms
thatdiﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-
munity,computational graphsareusuallyrepresentedbyexplicitdatastructures
createdbyspecializedlibraries.Thespecializedapproachhasthedrawbackof
requiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperation
andlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned However,thespecializedapproachalsohasthebeneﬁtofallowingcustomized
back-propagationrulestobedevelopedforeachoperation,allowingthedeveloper
toimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure
wouldpresumablybeunabletoreplicate Back-propagationisthereforenottheonlywayortheoptimalwayofcomputing
thegradient,butitisaverypracticalmethodthatcontinuestoservethedeep
learningcommunityverywell.Inthefuture,diﬀerentiation technologyfordeep
networksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances
inthebroaderﬁeldofautomaticdiﬀerentiation 2 2 3
CHAPTER6.DEEPFEEDFORWARDNETWORKS
6.5.10Higher-OrderDerivatives
Somesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe
deeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor
derivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.This
meansthatthesymbolicdiﬀerentiation machinerycanbeappliedtoderivatives Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivative
ofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian
matrix.Ifwehaveafunction f: Rn→ R,thentheHessianmatrixisofsize n n× Intypicaldeeplearningapplications, nwillbethenumberofparametersinthe
model,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis
thusinfeasibletoevenrepresent InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach
istouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor
performingvariousoperationslikeapproximately invertingamatrixorﬁnding
approximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation
otherthanmatrix-vector products InordertouseKrylovmethodsontheHessian,weonlyneedtobeableto
computetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A
straightforwardtechnique( ,)fordoingsoistocompute Christianson1992
Hv= ∇ x
(∇ x f x())v (6.59)
Bothofthegradientcomputations inthisexpressionmaybecomputedautomati-
callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression
takesthegradientofafunctionoftheinnergradientexpression Ifvisitselfavectorproducedbyacomputational graph,itisimportantto
specifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethrough
thegraphthatproduced.v
WhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith
Hessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 ,

============================================================

=== CHUNK 057 ===
Palavras: 356
Caracteres: 11824
--------------------------------------------------
, n ,where
e( ) iistheone-hotvectorwith e( ) i
i= 1andallotherentriesequalto0 6 Hi s t or i c a l Not es
Feedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximators
basedonusinggradientdescenttominimizetheerrorinafunctionapproximation 2 2 4
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Fromthispointofview,themodernfeedforwardnetworkistheculminationof
centuriesofprogressonthegeneralfunctionapproximationtask Thechainrulethatunderliestheback-propagation algorithmwasinvented
inthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L’Hôpital1696
longbeenusedtosolveoptimization problemsinclosedform,butgradientdescent
wasnotintroducedasatechniqueforiterativelyapproximating thesolutionto
optimization problemsuntilthe19thcentury(Cauchy1847,) Beginninginthe1940s,thesefunctionapproximation techniqueswereusedto
motivatemachinelearningmodelssuchastheperceptron.However,theearliest
modelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout
severaloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearnthe
XORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-
ceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcient
applicationsofthechainrulebasedondynamicprogramming begantoappear
inthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand
Denham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor
sensitivityanalysis(,) Linnainmaa1976Werbos1981()proposedapplyingthese
techniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydeveloped
inpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985
Parker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-
cessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswith
back-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b
tothepopularization ofback-propagation andinitiatedaveryactiveperiodof
researchinmulti-layerneuralnetworks However,theideasputforwardbythe
authorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond
back-propagation Theyincludecrucialideasaboutthepossiblecomputational
implementationofseveralcentralaspectsofcognitionandlearning,whichcame
underthenameof“connectionism” becauseoftheimportancethisschoolofthought
placesontheconnectionsbetweenneuronsasthelocusoflearningandmemory Inparticular,theseideasincludethenotionofdistributedrepresentation(Hinton
e t a l .,).1986
Followingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-
ularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning
techniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat
beganin2006 Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-
stantiallysincethe1980s Thesameback-propagationalgorithmandthesame
2 2 5
CHAPTER6.DEEPFEEDFORWARDNETWORKS
approachestogradientdescentarestillinuse.Mostoftheimprovementinneural
networkperformancefrom1986to2015canbeattributedtotwofactors.First,
largerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa
challengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,
duetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a
smallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural
networksnoticeably Oneofthesealgorithmicchangeswasthereplacementofmeansquarederror
withthecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin
the1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe
principleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity
andthemachinelearningcommunity.Theuseofcross-entropylossesgreatly
improvedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which
hadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemean
squarederrorloss Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformance
offeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise
linearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0 , z}
functionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast
asfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly
modelsdid notuserectiﬁed linearunits, but insteadappliedrectiﬁcation to
nonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwas
largelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter
whenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunits
wereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith
non-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009 Jarrett2009 e t a l .()observedthat“usingarectifyingnonlinearityisthesinglemost
importantfactorinimprovingtheperformanceofarecognitionsystem”among
severaldiﬀerentfactorsofneuralnetworkarchitecturedesign Forsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009
linearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers Randomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁed
linearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerent
featurevectorstoclassidentities Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledge
toexceedtheperformanceofrandomlychosenparameters () Glorot e t a l .2011a
showedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeep
networksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions 2 2 6
CHAPTER6.DEEPFEEDFORWARDNETWORKS
Rectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat
neurosciencehascontinuedtohave aninﬂuenceonthe developmentofdeep
learningalgorithms ()motivaterectiﬁedlinearunitsfrom Glorot e t a l .2011a
biologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture
thesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare
completelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportional
toitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere
theyareinactive(i.e.,theyshouldhavesparseactivations) Whenthemodernresurgenceofdeeplearningbeganin2006,feedforward
networkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely
believedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted
byothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe
rightresourcesandengineeringpractices,feedforwardnetworksperformverywell Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop
probabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial
networks,describedinchapter.Ratherthanbeingviewedasanunreliable 20
technologythatmustbesupportedbyothertechniques,gradient-basedlearningin
feedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat
maybeappliedtomanyothermachinelearningtasks.In2006,thecommunity
usedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it
ismorecommontousesupervisedlearningtosupportunsupervisedlearning Feedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,we
expecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization
algorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This
chapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe
subsequentchapters,weturntohowtousethesemodels—howtoregularizeand
trainthem 2 2 7
C h a p t e r 7
Regularization f or D e e p L e ar n i n g
Acentralprobleminmachinelearningishowtomakeanalgorithmthatwill
performwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies
usedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly
attheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively
asregularization As wewillseethereareagreatmanyformsofregularization
availabletothedeeplearningpractitioner Infact, developingmoreeﬀective
regularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld Chapterintroducedthebasicconceptsofgeneralization, underﬁtting,overﬁt- 5
ting,bias,varianceandregularization Ifyouarenotalreadyfamiliarwiththese
notions,pleaserefertothatchapterbeforecontinuingwiththisone Inthischapter,wedescriberegularizationinmoredetail,focusingonregular-
izationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks
toformdeepmodels Somesectionsofthischapterdealwithstandardconceptsinmachinelearning Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevant
sections.However,mostofthischapterisconcernedwiththeextensionofthese
basicconceptstotheparticularcaseofneuralnetworks Insection,wedeﬁnedregularizationas“anymodiﬁcationwemaketo 5.2.2
alearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot
itstrainingerror.”Therearemanyregularizationstrategies.Someputextra
constraints ona machine learning model, such asadding restrictionson the
parametervalues.Someaddextratermsintheobjectivefunctionthatcanbe
thoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen
carefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance
228
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
onthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode
speciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties
aredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto
promotegeneralization Sometimespenaltiesandconstraintsarenecessarytomake
anunderdetermined problemdetermined.Otherformsofregularization,knownas
ensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon
regularizingestimators.Regularizationofanestimatorworksbytradingincreased
biasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtable
trade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwe
discussedgeneralization andoverﬁttinginchapter,wefocusedonthreesituations, 5
wherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating
process—correspondingtounderﬁttingandinducingbias,or(2)matchedthetrue
datageneratingprocess,or(3)includedthegeneratingprocessbutalsomany
otherpossiblegeneratingprocesses—theoverﬁttingregimewherevariancerather
thanbiasdominatestheestimationerror.Thegoalofregularizationistotakea
modelfromthethirdregimeintothesecondregime Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe
targetfunctionorthetruedatageneratingprocess,orevenacloseapproximation
ofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso
wecanneverknowforsureifthemodelfamilybeingestimatedincludesthe
generatingprocessornot.However,mostapplicationsofdeeplearningalgorithms
aretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside
themodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely
complicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue
generationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome
extent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)into
aroundhole(ourmodelfamily) Whatthismeansisthatcontrollingthecomplexityofthemodelisnota
simplematterofﬁndingthemodeloftherightsize,withtherightnumberof
parameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,
wealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizing
generalization error)isalargemodelthathasbeenregularizedappropriately Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized
model 2 2 9
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
7.1ParameterNormPenalties
Regularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear
modelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,
andeﬀectiveregularizationstrategies Manyregularizationapproachesarebasedonlimitingthecapacityofmodels,
suchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-
rameternormpenalty Ω(θ)totheobjectivefunction J.Wedenotetheregularized
objectivefunctionby˜ J:
˜ J , J , α (;θXy) = (;θXy)+Ω()θ (7.1)
where α∈[0 ,∞)isahyperparameter thatweightstherelativecontributionofthe
normpenaltyterm,,relativetothestandardobjectivefunction Ω J.Setting αto0
resultsinnoregularization

============================================================

=== CHUNK 058 ===
Palavras: 384
Caracteres: 3510
--------------------------------------------------
Largervaluesof αcorrespondtomoreregularization Whenourtrainingalgorithmminimizestheregularizedobjectivefunction ˜ Jit
willdecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure
ofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerent
choicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred Ω
Inthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties
onthemodelparameters Beforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethat
forneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩ
penalizes oftheaﬃnetransformationateachlayerandleaves onlytheweights
thebiasesunregularized Thebiasestypicallyrequirelessdatatoﬁtaccurately
thantheweights Eachweightspeciﬁeshowtwovariablesinteract Fittingthe
weightwellrequiresobservingbothvariablesinavarietyofconditions.Each
biascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch
variancebyleavingthebiasesunregularized Also,regularizingthebiasparameters
canintroduceasigniﬁcantamountofunderﬁtting Wethereforeusethevectorw
toindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethe
vectorθdenotesalloftheparameters,includingbothwandtheunregularized
parameters Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparate
penaltywithadiﬀerent αcoeﬃcientforeachlayerofthenetwork.Becauseitcan
beexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill
reasonabletousethesameweightdecayatalllayersjusttoreducethesizeof
searchspace 2 3 0
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
7 1 L2P a ra m et e r Regu l a ri z a t i o n
Wehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2
ofparameternormpenalty:the L2parameternormpenaltycommonlyknownas
weightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1
byaddingaregularizationtermΩ(θ) =1
2w2
2totheobjectivefunction.Inother
academiccommunities, L2regularizationisalsoknownasridgeregressionor
Tikhonovregularization Wecangainsomeinsightintothebehaviorofweightdecayregularization
bystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe
presentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthe
followingtotalobjectivefunction:
˜ J , (;wXy) =α
2wwwXy +( J; ,) , (7.2)
withthecorrespondingparametergradient
∇ w˜ J , α (;wXy) = w+∇ w J , (;wXy) (7.3)
Totakeasinglegradientsteptoupdatetheweights,weperformthisupdate:
www ← −  α( +∇ w J , (;wXy)) (7.4)
Writtenanotherway,theupdateis:
ww ← −(1  α)−∇  w J , (;wXy) (7.5)
Wecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearning
ruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,
justbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin
asinglestep.Butwhathappensovertheentirecourseoftraining Wewillfurthersimplifytheanalysisbymakingaquadraticapproximation
totheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat
obtainsminimalunregularized trainingcost,w∗=argminw J(w).Iftheobjective
functionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith
1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i ﬁ c p o i n t i n s p a c e
a n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e ﬀ e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e
c l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f
t h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e

============================================================

=== CHUNK 059 ===
Palavras: 430
Caracteres: 5054
--------------------------------------------------
S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e
m o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n 2 3 1
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
meansquarederror,thentheapproximationisperfect.Theapproximation ˆ Jis
givenby
ˆ J J () = θ (w∗)+1
2(ww−∗)Hww (−∗) , (7.6)
whereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Thereis
noﬁrst-orderterminthisquadraticapproximation, becausew∗isdeﬁnedtobea
minimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofa
minimumof,wecanconcludethatispositivesemideﬁnite J H
Theminimumofˆ Joccurswhereitsgradient
∇ wˆ J() = (wHww−∗) (7.7)
isequalto 0
Tostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe 7.7
weightdecaygradient.Wecannowsolvefortheminimumoftheregularized
versionofˆ J.Weusethevariable ˜wtorepresentthelocationoftheminimum α˜wH+ (˜ww−∗) = 0 (7.8)
(+ )H αI˜wHw = ∗(7.9)
˜wHI = (+ α)− 1Hw∗ (7.10)
As αapproaches0,theregularizedsolution ˜wapproachesw∗.Butwhat
happensas αgrows?BecauseHisrealandsymmetric,wecandecomposeit
intoadiagonalmatrix Λandanorthonormal basisofeigenvectors,Q,suchthat
HQQ = Λ.Applyingthedecompositiontoequation,weobtain:7.10
˜wQQ = ( Λ+ ) αI− 1QQ Λw∗(7.11)
=
QIQ (+ Λ α)− 1
QQ Λw∗(7.12)
= (+ )Q Λ αI− 1ΛQw∗ (7.13)
Weseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedby
theeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththe
i-theigenvectorofHisrescaledbyafactorofλ i
λ i + α.(Youmaywishtoreview
howthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3
AlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,
where λ i α,theeﬀectofregularizationisrelativelysmall.However,components
with λ i αwillbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustrated
inﬁgure.7.1
2 3 2
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
w 1w 2w∗
˜ w
Figure7.1:Anillustrationoftheeﬀectof L2(orweightdecay)regularizationonthevalue
oftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized
objective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At
thepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,the
eigenvalueoftheHessianof Jissmall Theobjectivefunctiondoesnotincreasemuch
whenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpress
astrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis Theregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction
isverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,
indicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionof w2relatively
little Onlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducing
theobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot
contributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian
tellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient Componentsoftheweightvectorcorrespondingtosuchunimportant directions
aredecayedawaythroughtheuseoftheregularizationthroughouttraining Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimization
ofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelateto
machinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,a
modelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe
samekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill
beabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow
phrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis
2 3 3
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
thesumofsquarederrors:
( )Xwy−( )Xwy− (7.14)
Whenweadd L2regularization, theobjectivefunctionchangesto
( )Xwy−( )+Xwy−1
2αww (7.15)
Thischangesthenormalequationsforthesolutionfrom
wX= (X)− 1Xy (7.16)
to
wX= (XI+ α)− 1Xy (7.17)
ThematrixXXinequationisproportionaltothecovariancematrix 7.161
mXX Using L2regularizationreplacesthismatrixwith
XXI+ α− 1inequation.7.17
Thenewmatrixisthesameastheoriginalone,butwiththeadditionof αtothe
diagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach
inputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm
to“perceive”theinputXashavinghighervariance,whichmakesitshrinkthe
weightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto
thisaddedvariance 2 L1Regu l a ri z a t i o n
While L2weightdecayisthemostcommonformofweightdecay,thereareother
waystopenalizethesizeofthemodelparameters Anotheroptionistouse L1
regularization Formally, L1regularizationonthemodelparameter isdeﬁnedas:w
Ω() = θ ||||w 1=
i| w i| , (7.18)
thatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill
nowdiscusstheeﬀectof L1regularizationonthesimplelinearregressionmodel,
withnobiasparameter,thatwestudiedinouranalysisof L2regularization In
particular,weareinterestedindelineatingthediﬀerencesbetween L1and L2forms
2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t
z e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o

============================================================

=== CHUNK 060 ===
Palavras: 351
Caracteres: 4457
--------------------------------------------------
In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d
i n t ro d u c e t h e t e rmΩ() = θ ||− w w( ) o|| 1=
i| w i− w( ) o
i| 2 3 4
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
ofregularization Aswith L2weightdecay, L1weightdecaycontrolsthestrength
oftheregularizationbyscalingthepenaltyusingapositivehyperparameter Ω α Thus,theregularizedobjectivefunction ˜ J , (;wXy)isgivenby
˜ J , α (;wXy) = ||||w 1+(; ) JwXy , , (7.19)
withthecorrespondinggradient(actually,sub-gradient):
∇ w˜ J , α (;wXy) = sign( )+w ∇ w J ,(Xyw;) (7.20)
where issimplythesignofappliedelement-wise sign( )w w
Byinspectingequation,wecanseeimmediately thattheeﬀectof 7.20 L1
regularizationisquitediﬀerentfromthatof L2regularization Speciﬁcally,wecan
seethattheregularizationcontributiontothegradientnolongerscaleslinearly
witheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One
consequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean
algebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2
regularization Oursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent
viaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor
seriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient
inthissettingisgivenby
∇ wˆ J() = (wHww−∗) , (7.21)
where,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww∗ Becausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase
ofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption
thattheHessianisdiagonal,H=diag([ H 1 1 , , , H n , n]),whereeach H i , i >0 Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen
preprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe
accomplishedusingPCA Ourquadraticapproximationofthe L1regularizedobjectivefunctiondecom-
posesintoasumovertheparameters:
ˆ J , J (;wXy) = (w∗; )+Xy ,
i1
2H i , i(w i−w∗
i)2+ α w| i|
.(7.22)
Theproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution
(foreachdimension),withthefollowingform: i
w i= sign( w∗
i)max
| w∗
i|−α
H i , i,0 (7.23)
2 3 5
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Considerthesituationwhere w∗
i > i 0forall.Therearetwopossibleoutcomes:
1.Thecasewhere w∗
i≤α
H i , i.Heretheoptimalvalueof w iundertheregularized
objectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)
totheregularizedobjective˜ J(w;Xy ,)isoverwhelmed—indirection i—by
the L1regularizationwhichpushesthevalueof w itozero 2.Thecasewhere w∗
i >α
H i , i.Inthiscase,theregularizationdoesnotmovethe
optimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya
distanceequaltoα
H i , i Asimilarprocesshappenswhen w∗
i <0,butwiththe L1penaltymaking w iless
negativebyα
H i , i,or0 Incomparisonto L2regularization, L1regularizationresultsinasolutionthat
ismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters
haveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively
diﬀerentbehaviorthanariseswith L2regularization Equationgavethe7.13
solution ˜ wfor L2regularization Ifwerevisitthatequationusingtheassumption
ofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisof
L1regularization,weﬁndthat˜ w i=H i , i
H i , i + αw∗
i.If w∗
iwasnonzero,then ˜ w iremains
nonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters
tobecomesparse,while L1regularizationmaydosoforlargeenough α
Thesparsitypropertyinducedby L1regularizationhasbeenusedextensively
asafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearning
problembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In
particular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995
selectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast
squarescostfunction.The L1penaltycausesasubsetoftheweightstobecome
zero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded Insection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1
asMAPBayesianinference,andthatinparticular, L2regularizationisequivalent
toMAPBayesianinferencewithaGaussianpriorontheweights For L1regu-
larization,thepenalty αΩ(w)= α
i| w i|usedtoregularizeacostfunctionis
equivalenttothelog-priortermthatismaximizedbyMAPBayesianinference
whenthepriorisanisotropicLaplacedistribution(equation)over3.26w∈ Rn:
log() = pw
ilogLaplace( w i;0 ,1
α) = −|||| αw 1+log log2 n α n− .(7.24)
2 3 6
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Fromthepointofviewoflearningviamaximization withrespecttow,wecan
ignorethe termsbecausetheydonotdependon

============================================================

=== CHUNK 061 ===
Palavras: 366
Caracteres: 12382
--------------------------------------------------
log log2 α− w
7.2NormPenaltiesasConstrainedOptimization
Considerthecostfunctionregularizedbyaparameternormpenalty:
˜ J , J , α (;θXy) = (;θXy)+Ω()θ (7.25)
Recallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4
byconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective
functionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,
calledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresenting
whethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthan
someconstant,wecouldconstructageneralizedLagrangefunction k
L − (; ) = (; )+(Ω() θ , αXy , JθXy , αθ k .) (7.26)
Thesolutiontotheconstrainedproblemisgivenby
θ∗= argmin
θmax
α , α≥ 0L()θ , α (7.27)
Asdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 θ
and α.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2
constraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,
whileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinall
procedures αmustincreasewheneverΩ(θ) > kanddecreasewheneverΩ(θ) < k Allpositive αencourage Ω(θ)toshrink.Theoptimalvalue α∗willencourage Ω(θ)
toshrink,butnotsostronglytomakebecomelessthan Ω()θ k
Togainsomeinsightintotheeﬀectoftheconstraint,wecanﬁx α∗andview
theproblemasjustafunctionof:θ
θ∗= argmin
θL(θ , α∗) = argmin
θJ , α (;θXy)+∗Ω()θ .(7.28)
Thisisexactlythesameastheregularizedtrainingproblemofminimizing ˜ J Wecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe
weights.Ifisthe Ω L2norm,thentheweightsareconstrainedtolieinan L2
ball Ifisthe Ω L1norm,thentheweightsareconstrainedtolieinaregionof
2 3 7
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
limited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe
imposebyusingweightdecaywithcoeﬃcient α∗becausethevalueof α∗doesnot
directlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship
between kand α∗dependsontheformof J.Whilewedonotknowtheexactsize
oftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing α
inordertogroworshrinktheconstraintregion.Larger αwillresultinasmaller
constraintregion.Smallerwillresultinalargerconstraintregion α
Sometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As
describedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4
descenttotakeastepdownhillon J(θ)andthenprojectθbacktothenearest
pointthatsatisﬁesΩ(θ) < k.Thiscanbeusefulifwehaveanideaofwhatvalue
of kisappropriateanddonotwanttospendtimesearchingforthevalueof αthat
correspondstothis k
Anotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing
constraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization
procedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentraining
neuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral
“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthe
functionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare
allverysmall Whentrainingwithapenaltyonthenormoftheweights,these
conﬁgurations canbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduce
Jbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection
canworkmuchbetterinthesecasesbecausetheydonotencouragetheweights
toapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly
haveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraint
region Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose
somestabilityontheoptimization procedure.Whenusinghighlearningrates,it
ispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce
largegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates
consistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfrom
theoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojection
preventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights
withoutbound ()recommendusingconstraintscombinedwith Hintonetal.2012c
ahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining
somestability Inparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro
andShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix
2 3 8
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
ofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire
weightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone
hiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa
penaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha
separateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT
multiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit
obeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas
anexplicitconstraintwithreprojection 7.3RegularizationandUnder-ConstrainedProblems
Insomecases,regularizationisnecessaryformachinelearningproblemstobeprop-
erlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregression
andPCA,dependoninvertingthematrixXX.Thisisnotpossiblewhenever
XXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-
butiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin
somedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures
(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting
XXI+ αinstead.Thisregularizedmatrixisguaranteedtobeinvertible Theselinearproblemshaveclosedformsolutionswhentherelevantmatrix
isinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe
underdetermined Anexampleislogisticregressionappliedtoaproblemwhere
theclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect
classiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood Aniterativeoptimization procedurelikestochasticgradientdescentwillcontinually
increasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical
implementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweights
tocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowthe
programmerhasdecidedtohandlevaluesthatarenotrealnumbers Mostformsofregularizationareabletoguaranteetheconvergenceofiterative
methodsappliedtounderdetermined problems Forexample,weightdecaywill
causegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe
slopeofthelikelihoodisequaltotheweightdecaycoeﬃcient Theideaofusingregularizationtosolveunderdetermined problemsextends
beyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra
problems Aswesawinsection,wecansolveunderdetermined linearequationsusing 2.9
2 3 9
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
theMoore-Penrosepseudoinverse.Recallthatonedeﬁnitionofthepseudoinverse
X+ofamatrixisX
X+=lim
α 0(XXI+ α)− 1X (7.29)
Wecannowrecognizeequationasperforminglinearregressionwithweight 7.29
decay.Speciﬁcally,equationisthelimitofequationastheregularization 7.29 7.17
coeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing
underdetermined problemsusingregularization 7.4DatasetAugmentation
Thebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton
moredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway
togetaroundthisproblemistocreatefakedataandaddittothetrainingset Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew
fakedata Thisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-
cated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevariety
oftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming
theinputsinourtrainingset x
Thisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it
isdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehave
alreadysolvedthedensityestimationproblem Datasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁc
classiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandinclude
anenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan
oftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto
bepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques
describedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9
theimagehavealsoprovenquiteeﬀective Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrect
class.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe
diﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontal
ﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthese
tasks 2 4 0
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Therearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariant
to,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot
beimplementedasasimplegeometricoperationontheinputpixels Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly
andHinton2013,) Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)
canalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationand
evensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall
randomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust
tonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness
ofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir
inputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch
asthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks
whenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset
augmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed
thatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthe
noiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe
describedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12
multiplyingbynoise Whencomparingmachinelearningbenchmarkresults,itisimportanttotake
theeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddataset
augmentationschemescandramaticallyreducethegeneralization errorofamachine
learningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm
toanother,itisnecessarytoperformcontrolledexperiments.Whencomparing
machinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary
tomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed
datasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith
nodatasetaugmentationandalgorithmBperformswellwhencombinedwith
numeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe
synthetictransformationscausedtheimprovedperformance,ratherthantheuse
ofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment
hasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine
learningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset
augmentation.Usually,operationsthataregenerallyapplicable(suchasadding
Gaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,
whileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomly
croppinganimage)areconsideredtobeseparatepre-processingsteps 2 4 1
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
7.5NoiseRobustness
Sectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4
augmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimal
varianceattheinputofthemodelisequivalenttoimposingapenaltyonthe
normoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab
rememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking
theparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise
appliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate
discussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12
ofthatapproach Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodels
isbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe
contextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,) Thiscan
beinterpretedasa stochasticimplementation of Bayesianinference overthe
weights TheBayesiantreatmentoflearningwouldconsiderthemodelweights
tobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthis
uncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂect
thisuncertainty Noiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome
assumptions)toamoretraditionalformofregularization, encouragingstabilityof
thefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain
afunction ˆ y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares
costfunctionbetweenthemodelpredictions ˆ y()xandthetruevalues: y
J= E p x , y ( )(ˆ y y ()x−)2

============================================================

=== CHUNK 062 ===
Palavras: 353
Caracteres: 6710
--------------------------------------------------
(7.30)
Thetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , Wenowassumethatwitheachinputpresentationwealsoincludearandom
perturbation  W∼N(; 0 , ηI)ofthenetworkweights.Letusimaginethatwe
haveastandard l-layerMLP.Wedenotetheperturbedmodelasˆ y  W(x).Despite
theinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe
outputofthenetwork.Theobjectivefunctionthusbecomes:
˜ J W= E p , y , ( x  W )
(ˆ y  W() )x− y2
(7.31)
= E p , y , ( x  W )
ˆ y2
 W()2ˆx− y y  W()+x y2
.(7.32)
Forsmall η,theminimization of Jwithaddedweightnoise(withcovariance
ηI)isequivalenttominimization of Jwithanadditionalregularizationterm:
2 4 2
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
η E p , y ( x )∇ Wˆ y()x2
.Thisformofregularizationencouragestheparametersto
gotoregionsofparameterspacewheresmallperturbationsoftheweightshave
arelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodel
intoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe
weights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedby
ﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinear
regression(where,forinstance, ˆ y(x) =wx+ b),thisregularizationtermcollapses
into η E p ( ) x
x2
,whichisnotafunctionofparametersandthereforedoesnot
contributetothegradientof˜ J Wwithrespecttothemodelparameters 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s
Mostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto
maximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly
modelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall
constant ,thetrainingsetlabel yiscorrectwithprobability 1− ,andotherwise
anyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto
incorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing
noisesamples.Forexample,labelsmoothingregularizesamodelbasedona
softmaxwith koutputvaluesbyreplacingthehardandclassiﬁcationtargets 0 1
withtargetsof
k− 1and1− ,respectively.Thestandardcross-entropylossmay
thenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax
classiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcannever
predictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1
andlargerweights,makingmoreextremepredictionsforever.Itispossibleto
preventthisscenariousingotherregularizationstrategieslikeweightdecay.Label
smoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout
discouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980s
andcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy
etal.,).2015
7.6Semi-SupervisedLearning
Intheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)
andlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom
x Inthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto
learningarepresentationh= f(x) .Thegoalistolearnarepresentationso
2 4 3
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
thatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised
learningcanprovideusefulcuesforhowtogroupexamplesinrepresentation
space.Examplesthatclustertightlyintheinputspaceshouldbemappedto
similarrepresentations.Alinearclassiﬁerinthenewspacemayachievebetter
generalization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A
long-standingvariantofthisapproachistheapplicationofprincipalcomponents
analysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojected
data) Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthe
model,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or
P( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan
thentrade-oﬀthesupervisedcriterion −log P( y x|)withtheunsupervisedor
generativeone(suchas−log P( x)or−log P( x y ,)).Thegenerativecriterionthen
expressesaparticularformofpriorbeliefaboutthesolutiontothesupervised
learningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is
connectedtothestructureof P( y x|)inawaythatiscapturedbytheshared
parametrization Bycontrollinghowmuchofthegenerativecriterionisincluded
inthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerative
orapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand
Bengio2008,) SalakhutdinovandHinton2008()describeamethodforlearningthekernel
functionofakernelmachineusedforregression,inwhichtheusageofunlabeled
examplesformodeling improvesquitesigniﬁcantly P() x P( ) y x|
See ()formoreinformationaboutsemi-supervisedlearning Chapelle etal.2006
7.7Multi-TaskLearning
Multi-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993
theexamples(whichcanbeseenassoftconstraintsimposedontheparameters)
arisingoutofseveraltasks Inthesamewaythatadditionaltrainingexamples
putmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize
well,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore
constrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyielding
bettergeneralization Figureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2
diﬀerentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell
assomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof
2 4 4
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
factors.Themodelcangenerallybedividedintotwokindsofpartsandassociated
parameters:
1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtask
toachievegoodgeneralization) Thesearetheupperlayersoftheneural
networkinﬁgure.7.2
2.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthe
pooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork
inﬁgure.7.2
h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )
h( s h a r e d )h( s h a r e d )
xx
Figure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks
andthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbut
involvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit
issupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)
canbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectively
withtheweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga
sharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon
pooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated
withasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level
hiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and
y(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In
theunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe
associatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof
theinputvariationsbutarenotrelevantforpredicting y(1)or y(2)

============================================================

=== CHUNK 063 ===
Palavras: 351
Caracteres: 7947
--------------------------------------------------
Improvedgeneralization andgeneralization errorbounds(,)canbe Baxter1995
achievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe
2 4 5
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
0 50 100 150 200 250
Time(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s
V a l i d a t i o n s e t l o s s
Figure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover
time(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis
example,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective
decreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto
increaseagain,forminganasymmetricU-shapedcurve greatlyimproved(inproportionwiththeincreasednumberofexamplesforthe
sharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis
willhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween
thediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssome
ofthetasks Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe
following:amongthefactorsthat explainthevariations observed inthedata
associatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks 7.8EarlyStopping
Whentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁt
thetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but
validationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis 7.3
behavior.Thisbehavioroccursveryreliably Thismeanswecanobtainamodelwithbettervalidationseterror(andthus,
hopefullybettertestseterror)byreturningtotheparametersettingatthepointin
timewiththelowestvalidationseterror.Everytimetheerroronthevalidationset
improves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm
terminates,wereturntheseparameters,ratherthanthelatestparameters.The
2 4 6
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
algorithmterminateswhennoparametershaveimprovedoverthebestrecorded
validationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureis
speciﬁedmoreformallyinalgorithm .7.1
Algorithm 7.1Theearlystopping meta-algorithmfor determiningthe best
amountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks
wellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe
validationset Letbethenumberofstepsbetweenevaluations n
Let pbethe“patience,”thenumberoftimestoobserveworseningvalidationset
errorbeforegivingup Letθ obetheinitialparameters θθ← o
i←0
j←0
v←∞
θ∗←θ
i∗← i
whiledo j < p
Updatebyrunningthetrainingalgorithmforsteps θ n
i i n ←+
v←ValidationSetError ()θ
if v< vthen
j←0
θ∗←θ
i∗← i
v v←
else
j j←+1
endif
endwhile
Bestparametersareθ∗,bestnumberoftrainingstepsis i∗
Thisstrategyisknownasearlystopping.Itisprobablythemostcommonly
usedformofregularizationindeeplearning.Itspopularityisduebothtoits
eﬀectivenessanditssimplicity Onewaytothinkofearlystoppingisasaveryeﬃcienthyperparameter selection
algorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter Wecanseeinﬁgurethatthishyperparameter hasaU-shapedvalidationset 7.3
2 4 7
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
performancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha
U-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof 5.3
earlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermining
howmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbe
chosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter
atthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The
“trainingtime” hyperparam eterisuniqueinthatbydeﬁnitionasinglerunof
trainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcost
tochoosingthishyperparameter automatically viaearlystoppingisrunningthe
validationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein
paralleltothetrainingprocessonaseparatemachine,separateCPU,orseparate
GPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe
costoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis
smallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless
frequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthe
bestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore
theseparametersinaslowerandlargerformofmemory(forexample,trainingin
GPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk
drive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring
training,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime Earlystoppingisaveryunobtrusiveformofregularization, inthatitrequires
almostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,
orthesetofallowableparametervalues.Thismeansthatitiseasytouseearly
stoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight
decay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe
networkinabadlocalminimumcorrespondingtoasolutionwithpathologically
smallweights Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-
tionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective
functiontoencouragebettergeneralization, itisrareforthebestgeneralization to
occuratalocalminimumofthetrainingobjective Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot
fedtothemodel.Tobestexploitthisextradata,onecanperformextratraining
aftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra
trainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies
onecanuseforthissecondtrainingprocedure Onestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2
2 4 8
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
ofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas
theearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Thereare
somesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood
wayofknowingwhethertoretrainforthesamenumberofparameterupdatesor
thesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,
eachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe
trainingsetisbigger Algorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong
totrain,thenretrainingonallthedata LetX( ) t r a i nandy( ) t r a i nbethetrainingset SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))
respectively Runearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand
y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This
returns i∗,theoptimalnumberofsteps Settorandomvaluesagain θ
TrainonX( ) t r a i nandy( ) t r a i nfor i∗steps Anotherstrategyforusingallofthedataistokeeptheparametersobtained
fromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallof
thedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms
ofanumberofsteps Instead,wecanmonitortheaveragelossfunctiononthe
validationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining
setobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids
thehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For
example,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill
everreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate Thisprocedureispresentedmoreformallyinalgorithm .7.3
Earlystoppingisalsousefulbecauseitreducesthecomputational costofthe
trainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber
oftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithout
requiringtheadditionofpenaltytermstothecostfunctionorthecomputationof
thegradientsofsuchadditionalterms Howearlystoppingactsasaregularizer:Sofarwehavestatedthatearly
stoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is
showinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What
2 4 9
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Algorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-
tivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached

============================================================

=== CHUNK 064 ===
Palavras: 405
Caracteres: 3265
--------------------------------------------------
LetX( ) t r a i nandy( ) t r a i nbethetrainingset SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))
respectively Runearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand
y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This
updates.θ
 J , ←(θX( ) s ubtr a i n,y( ) s ubtr a i n)
while J ,(θX( v a l i d ),y( v a l i d )) > do
TrainonX( ) t r a i nandy( ) t r a i nforsteps n
endwhile
istheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop
()and ()arguedthatearlystoppinghastheeﬀectof 1995aSjöbergandLjung1995
restrictingtheoptimization proceduretoarelativelysmallvolumeofparameter
spaceintheneighborhoodoftheinitialparametervalueθ o,asillustratedin
ﬁgure.Morespeciﬁcally,imaginetaking 7.4 τoptimization steps(corresponding
to τtrainingiterations)andwithlearningrate .Wecanviewtheproduct  τ
asameasureofeﬀectivecapacity.Assumingthegradientisbounded,restricting
boththenumberofiterationsandthelearningratelimitsthevolumeofparameter
spacereachablefromθ o.Inthissense,  τbehavesasifitwerethereciprocalof
thecoeﬃcientusedforweightdecay Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic
errorfunctionandsimplegradientdescent—earlystoppingisequivalentto L2
regularization Inordertocomparewithclassical L2regularization, weexamineasimple
settingwheretheonlyparametersarelinearweights(θ=w).Wecanmodel
thecostfunction Jwithaquadraticapproximationintheneighborhoodofthe
empiricallyoptimalvalueoftheweightsw∗:
ˆ J J () = θ (w∗)+1
2(ww−∗)Hww (−∗) , (7.33)
whereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Giventhe
assumptionthatw∗isaminimumof J(w),weknowthatHispositivesemideﬁnite UnderalocalTaylorseriesapproximation,thegradientisgivenby:
∇ wˆ J() = (wHww−∗) (7.34)
2 5 0
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
w 1w 2w∗
˜ w
w 1w 2w∗
˜ w
Figure7.4:Anillustrationoftheeﬀectofearlystopping ( L e f t )Thesolidcontourlines
indicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory
takenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗that
minimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w ( R i g h t )Anillustrationoftheeﬀectof L2regularizationforcomparison.Thedashedcircles
indicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie
nearertheoriginthantheminimumoftheunregularizedcost Wearegoingtostudythetrajectoryfollowedbytheparametervectorduring
training.Forsimplicity,letussettheinitialparametervectortotheorigin,3that
isw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby
analyzinggradientdescentonˆ J:
w( ) τ= w( 1 ) τ−−∇  wˆ J(w( 1 ) τ−) (7.35)
= w( 1 ) τ−− Hw(( 1 ) τ−−w∗) (7.36)
w( ) τ−w∗= ( )(IH− w( 1 ) τ−−w∗) (7.37)
LetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting
theeigendecompositionofH:H=QQ Λ,where ΛisadiagonalmatrixandQ
isanorthonormalbasisofeigenvectors w( ) τ−w∗= (IQQ −  Λ)(w( 1 ) τ−−w∗)(7.38)
Q(w( ) τ−w∗) = ( )I−  ΛQ(w( 1 ) τ−−w∗) (7.39)
3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e
a l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n

============================================================

=== CHUNK 065 ===
Palavras: 392
Caracteres: 6527
--------------------------------------------------
Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 2
i n i t i a l v a l u e w( 0 ) 2 5 1
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Assumingthatw( 0 )=0andthat ischosentobesmallenoughtoguarantee
|1−  λ i| <1,theparametertrajectoryduringtrainingafter τparameterupdates
isasfollows:
Qw( ) τ= [ ( )I−I−  Λτ]Qw∗ (7.40)
Now,theexpressionforQ˜winequationfor7.13 L2regularizationcanberear-
rangedas:
Q˜wI = (+ Λ α)− 1ΛQw∗(7.41)
Q˜wII = [−(+ Λ α)− 1α]Qw∗(7.42)
Comparingequationandequation,weseethatifthehyperparameters 7.40 7.42 ,
α τ,andarechosensuchthat
( )I−  Λτ= (+ ) Λ αI− 1α , (7.43)
then L2regularizationandearlystoppingcanbeseentobeequivalent(atleast
underthequadraticapproximation oftheobjectivefunction).Goingevenfurther,
bytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude
thatifall λ iaresmall(thatis,  λ i1and λ i /α1)then
τ≈1
 α, (7.44)
α≈1
τ  (7.45)
Thatis,undertheseassumptions,thenumberoftrainingiterations τplaysarole
inverselyproportionaltothe L2regularizationparameter,andtheinverseof τ 
playstheroleoftheweightdecaycoeﬃcient Parametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(ofthe
objectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,
inthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond
todirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameters
correspondingtodirectionsoflesscurvature Thederivationsinthissectionhaveshownthatatrajectoryoflength τends
atapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early
stoppingisofcoursemorethanthemererestrictionofthetrajectorylength;
instead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin
ordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping
thereforehastheadvantageoverweightdecaythatearlystoppingautomatically
determinesthecorrectamountofregularizationwhileweightdecayrequiresmany
trainingexperimentswithdiﬀerentvaluesofitshyperparameter 2 5 2
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
7.9ParameterTyingandParameterSharing
Thusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties
totheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint Forexample, L2regularization(orweightdecay)penalizesmodelparametersfor
deviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedother
waystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake
butweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere
shouldbesomedependencies betweenthemodelparameters Acommontypeofdependencythatweoftenwanttoexpressisthatcertain
parametersshouldbeclosetooneanother.Considerthefollowingscenario:we
havetwomodelsperformingthesameclassiﬁcationtask(withthesamesetof
classes)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodel
Awithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels
maptheinput totwo diﬀerent, but related outputs:ˆ y( ) A= f(w( ) A,x)and
ˆ y( ) B= ( gw( ) B,x) Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinput
andoutputdistributions)thatwebelievethemodelparametersshouldbeclose
toeachother: ∀ i, w( ) A
ishouldbecloseto w( ) B
i.Wecanleveragethisinformation
throughregularization Speciﬁcally,wecanuseaparameternormpenaltyofthe
form: Ω(w( ) A,w( ) B)=w( ) A−w( ) B2
2 Hereweusedan L2penalty,butother
choicesarealsopossible Thiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006
theparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,to
beclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm
(tocapturethedistributionoftheobservedinputdata).Thearchitectures were
constructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbe
pairedtocorrespondingparametersintheunsupervisedmodel Whileaparameternormpenaltyisonewaytoregularizeparameterstobe
closetooneanother,themorepopularwayistouseconstraints:toforcesets
ofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas
parametersharing,becauseweinterpretthevariousmodelsormodelcomponents
assharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharing
overregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya
subsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain
models—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcant
reductioninthememoryfootprintofthemodel 2 5 3
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
ConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse
ofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied
tocomputervision Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel
totheright.CNNstakethispropertyintoaccountbysharingparametersacross
multipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)
iscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁnda
catwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn
i+1intheimage ParametersharinghasallowedCNNstodramaticallylowerthenumberofunique
modelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringa
correspondingincreaseintrainingdata Itremainsoneofthebestexamplesof
howtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture CNNswillbediscussedinmoredetailinchapter.9
7.10SparseRepresentations
Weightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another
strategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,
encouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated
penaltyonthemodelparameters Wehave alreadydiscussed (insection)how7.1.2 L1penalizationinduces
asparseparametrization—meaning thatmanyoftheparametersbecomezero
(orcloseto zero).Representationalsparsity, on theother hand, des cribesa
representationwheremanyoftheelementsoftherepresentationarezero(orclose
tozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextof
linearregression:

18
5
15
−9
−3
=
400 20 0 −
00 10 3 0 −
050 0 0 0
100 10 4 − −
100 0 50 −

2
3
−2
−5
1
4

y∈ RmA∈ Rm n×x∈ Rn(7.46)
2 5 4
CHAPTER7.REGULARIZATIONFORDEEPLEARNING

−14
1
19
2
23
=
3 12 54 1 − −
4 2 3 11 3 − −
− − − 15 4 2 3 2
3 1 2 30 3 − −
− − − − 54 22 5 1

0
2
0
0
−3
0

y∈ RmB∈ Rm n×h∈ Rn(7.47)
Intheﬁrstexpression,wehaveanexampleofasparselyparametrized linear
regressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-
tionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents
theinformationpresentin,butdoessowithasparsevector

============================================================

=== CHUNK 066 ===
Palavras: 356
Caracteres: 8866
--------------------------------------------------
x
Representationalregularizationisaccomplishedbythesamesortsofmechanisms
thatwehaveusedinparameterregularization Normpenaltyregularizationofrepresentationsisperformedbyaddingtothe
lossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted
Ω()h.Asbefore,wedenotetheregularizedlossfunctionby˜ J:
˜ J , J , α (;θXy) = (;θXy)+Ω()h (7.48)
where α∈[0 ,∞)weightstherelativecontributionofthenormpenaltyterm,with
largervaluesofcorrespondingtomoreregularization α
Justasan L1penaltyontheparametersinducesparametersparsity,an L1
penaltyontheelementsoftherepresentationinducesrepresentationalsparsity:
Ω(h) =||||h 1=
i| h i| Ofcourse,the L1penaltyisonlyonechoiceofpenalty
thatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom
aStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011
andKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008
usefulforrepresentationswithelementsconstrainedtolieontheunitinterval Lee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies
basedonregularizingtheaverageactivationacrossseveralexamples,1
m
ih( ) i,to
benearsometargetvalue,suchasavectorwith.01foreachentry Otherapproachesobtainrepresentationalsparsitywithahardconstrainton
theactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,
1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained
optimization problem
argmin
h h , 0 < k− xWh2, (7.49)
where h 0isthenumberofnon-zeroentriesofh Thisproblemcanbesolved
eﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled
2 5 5
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
OMP- kwiththevalueof kspeciﬁedtoindicatethenumberofnon-zerofeatures
allowed ()demonstratedthatOMP-canbeaveryeﬀective CoatesandNg2011 1
featureextractorfordeeparchitectures Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughout
thisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof
contexts 7.11BaggingandOtherEnsembleMethods
Bagging(shortforbootstrapaggregating)isatechniqueforreducinggen-
eralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994
trainseveraldiﬀerentmodelsseparately,thenhaveallofthemodelsvoteonthe
outputfortestexamples.Thisisanexampleofageneralstrategyinmachine
learningcalledmodelaveraging.Techniquesemployingthisstrategyareknown
asensemblemethods Thereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusually
notmakeallthesameerrorsonthetestset Considerforexampleasetof kregressionmodels.Supposethateachmodel
makesanerror  ioneachexample, withtheerrorsdrawnfromazero-mean
multivariatenormaldistributionwithvariances E[ 2
i] = vandcovariances E[  i  j] =
c Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis
1
k
i  i.Theexpectedsquarederroroftheensemblepredictoris
E

1
k
i i2
=1
k2E

i
 2
i+
j i= i  j

(7.50)
=1
kv+k−1
kc (7.51)
Inthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared
errorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere
theerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe
ensembleisonly1
kv.Thismeansthattheexpectedsquarederroroftheensemble
decreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble
willperformatleastaswellasanyofitsmembers,andifthemembersmake
independenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely
2 5 6
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
8
8F i r s t   e nse m b l e   m e m b e r
Se c ond e nse m b l e   m e m b e rO r i gi nal   data s e t
F i r s t   r e s am pl e d   d a t a s e t
Se c ond re s am p l e d   d a t a s e t
Figure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron
thedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerent
resampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets
bysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthis
dataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On
theseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns
thatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual
classiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,
achievingmaximalconﬁdenceonlywhenbothloopsofthe8arepresent diﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging
isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective
functiontobereusedseveraltimes Speciﬁcally,bagginginvolvesconstructing kdiﬀerentdatasets.Eachdataset
hasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis
constructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans
that,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe
originaldatasetandalsocontainsseveralduplicateexamples(onaveragearound
2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining
set,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset
i.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultin
diﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample.7.5
Neuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan
oftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesame
dataset.Diﬀerencesinrandominitialization, randomselectionofminibatches,
diﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-determinis ticimple-
mentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersofthe
2 5 7
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
ensembletomakepartiallyindependenterrors Modelaveragingisanextremelypowerfulandreliablemethodforreducing
generalization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms
forscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-
tiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel Machinelearningcontestsareusuallywonbymethodsusingmodelaverag-
ingoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrand
Prize(Koren2009,) Notalltechniquesforconstructingensemblesaredesignedtomaketheensemble
moreregularizedthantheindividualmodels.Forexample,atechniquecalled
boosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher
capacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles
ofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural
networkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual
neuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a
unitstotheneuralnetwork 7.12Dropout
Dropout(Srivastava2014etal.,)providesacomputationally inexpensivebut
powerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,
dropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles
ofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,
andevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical
wheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch
networksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles
ofﬁvetotenneuralnetworks— ()usedsixtowintheILSVRC— Szegedy etal.2014a
butmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive
approximationtotrainingandevaluatingabaggedensembleofexponentiallymany
neuralnetworks Speciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthat
canbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,
asillustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof 7.6
aﬃnetransformationsandnonlinearities, wecaneﬀectivelyremoveaunitfroma
networkbymultiplyingitsoutputvaluebyzero Thisprocedurerequiressome
slightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake
2 5 8
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
thediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresent
thedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan
betriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthe
network Recallthattolearnwithbagging,wedeﬁne kdiﬀerentmodels,construct k
diﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen
trainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan
exponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,
weuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas
stochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we
randomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhidden
unitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof
theothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe
included)isahyperparameter ﬁxedbeforetrainingbegins Itisnotafunction
ofthecurrentvalueofthemodelparametersortheinputexample.Typically,
aninputunitisincludedwithprobability0.8andahiddenunitisincludedwith
probability0.5.Wethenrunforwardpropagation, back-propagation,andthe
learningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7
withdropout

============================================================

=== CHUNK 067 ===
Palavras: 354
Caracteres: 3930
--------------------------------------------------
Moreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,
and J(θµ ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ Thendropouttrainingconsistsinminimizing E µ J(θµ ,).Theexpectationcontains
exponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient
bysamplingvaluesof.µ
Dropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof
bagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare
parameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromthe
parentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan
exponentialnumberofmodelswithatractableamountofmemory.Inthecaseof
bagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe
caseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,
themodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-
networkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible
sub-networksareeachtrainedforasinglestep,andtheparametersharingcauses
theremainingsub-networkstoarriveatgoodsettingsoftheparameters.These
aretheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.For
example,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof
theoriginaltrainingsetsampledwithreplacement 2 5 9
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
yy
h 1 h 1 h 2 h 2
x 1 x 1 x 2 x 2yy
h 1 h 1 h 2 h 2
x 1 x 1 x 2 x 2yy
h 1 h 1 h 2 h 2
x 2 x 2yy
h 1 h 1 h 2 h 2
x 1 x 1yy
h 2 h 2
x 1 x 1 x 2 x 2
yy
h 1 h 1
x 1 x 1 x 2 x 2yy
h 1 h 1 h 2 h 2yy
x 1 x 1 x 2 x 2yy
h 2 h 2
x 2 x 2
yy
h 1 h 1
x 1 x 1yy
h 1 h 1
x 2 x 2yy
h 2 h 2
x 1 x 1yy
x 1 x 1
yy
x 2 x 2yy
h 2 h 2yy
h 1 h 1yyB ase   ne t w or k
E nse m bl e   of   s u b n e t w or k s
Figure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbe
constructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we
beginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen
possiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed
bydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,
alargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting
theinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwider
layers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes
smaller 2 6 0
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
ˆ x 1ˆ x 1
µ x 1 µ x 1 x 1 x 1ˆ x 2ˆ x 2
x 2 x 2 µ x 2 µ x 2h 1 h 1 h 2 h 2µ h 1 µ h 1 µ h 2 µ h 2ˆ h 1ˆ h 1ˆ h 2ˆ h 2yyyy
h 1 h 1 h 2 h 2
x 1 x 1 x 2 x 2
Figure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing
dropout ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one
hiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )
propagationwithdropout,werandomlysampleavectorµwithoneentryforeachinput
orhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependently
fromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5
forthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby
thecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe
networkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom
ﬁgureandrunningforwardpropagationthroughit 7.6
2 6 1
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Tomakeaprediction,abaggedensemblemustaccumulatevotesfromallof
itsmembers.Werefertothisprocessasinferenceinthiscontext Sofar,our
descriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly
probabilistic.Now,weassumethatthemodel’sroleistooutputaprobability
distribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution
p( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall
ofthesedistributions,
1
kk
i = 1p( ) i( ) y|x (7.52)
Inthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-
abilitydistribution p( y ,|xµ).Thearithmeticmeanoverallmasksisgiven
by
µp p y , ()µ(|xµ) (7.53)
where p(µ)istheprobabilitydistributionthatwasusedtosampleµattraining
time

============================================================

=== CHUNK 068 ===
Palavras: 358
Caracteres: 4633
--------------------------------------------------
Becausethissumincludesanexponentialnumberofterms,itisintractable
toevaluateexceptincaseswherethestructureofthemodelpermitssomeform
ofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractable
simpliﬁcation.Instead, wecan approximatetheinferencewithsampling, by
averagingtogethertheoutputfrommanymasks.Even10-20masksareoften
suﬃcienttoobtaingoodperformance However,thereisanevenbetterapproach,thatallowsustoobtainagood
approximationtothepredictionsoftheentireensemble,atthecostofonlyone
forwardpropagation Todoso,wechangetousingthegeometricmeanratherthan
thearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-
Farley2014etal.()presentargumentsandempiricalevidencethatthegeometric
meanperformscomparablytothearithmeticmeaninthiscontext Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe
aprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,
weimposetherequirementthatnoneofthesub-modelsassignsprobability0toany
event,andwerenormalizetheresultingdistribution.Theunnormalized probability
distributiondeﬁneddirectlybythegeometricmeanisgivenby
˜ p e nse m bl e( ) = y|x 2d
µp y , (|xµ) (7.54)
where disthenumberofunitsthatmaybedropped.Hereweuseauniform
distributionoverµtosimplifythepresentation,butnon-uniformdistributionsare
2 6 2
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
alsopossible.Tomakepredictionswemustre-normalizetheensemble:
p e nse m bl e( ) = y|x˜ p e nse m bl e( ) y|x
y˜ p e nse m bl e( y|x) (7.55)
Akeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c
mate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but
withtheweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit
i.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueofthe
outputfromthatunit.Wecallthisapproachtheweightscalinginferencerule Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate
inferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell Becauseweusuallyuseaninclusionprobabilityof1
2,theweightscalingrule
usuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 
themodelasusual.Anotherwaytoachievethesameresultistomultiplythe
statesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2
theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected
totalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare
missingonaverage Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight
scalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression
classiﬁerwithinputvariablesrepresentedbythevector: n v
P y (= y | v) = softmax
Wv+b
y (7.56)
Wecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe
inputwithabinaryvector: d
P y (= y | v;) = dsoftmax
W( )+d vb
y.(7.57)
Theensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverall
ensemblemembers’predictions:
P e nse m bl e(= ) =y y| v˜ P e nse m bl e(= )y y| v
y˜ P e nse m bl e(= y y| v)(7.58)
where
˜ P e nse m bl e(= ) =y y| v2n
d∈{} 0 1 ,nP y (= y | v;)d (7.59)
2 6 3
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Toseethattheweightscalingruleisexact,wecansimplify ˜ P e nse m bl e:
˜ P e nse m bl e(= ) =y y| v2n
d∈{} 0 1 ,nP y (= y | v;)d(7.60)
= 2n
d∈{} 0 1 ,nsoftmax (W( )+)d vby (7.61)
= 2n
d∈{} 0 1 ,nexp
Wy , :( )+d v b y

yexp
W
y , :( )+d v b y (7.62)
=2n
d∈{} 0 1 ,nexp
Wy , :( )+d v b y
2n
d∈{} 0 1 ,n
yexp
W
y , :( )+d v b y(7.63)
Because˜ Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat
areconstantwithrespectto: y
˜ P e nse m bl e(= ) y y| v∝2n
d∈{} 0 1 ,nexp
Wy , :( )+d v b y
(7.64)
= exp
1
2n
d∈{} 0 1 ,nW
y , :( )+d v b y
 (7.65)
= exp1
2W
y , : v+ b y (7.66)
Substitutingthisbackintoequationweobtainasoftmaxclassiﬁerwithweights 7.58
1
2W Theweightscalingruleisalsoexactinothersettings,includingregression
networkswithconditionallynormaloutputs,anddeepnetworksthathavehidden
layerswithoutnonlinearities However,theweightscalingruleisonlyanapproxi-
mationfordeepmodelsthathavenonlinearities Thoughtheapproximationhas
notbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow
etal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a
better(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximations tothe
ensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas
allowedtosampleupto1,000sub-networks ()found GalandGhahramani2015
thatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand
2 6 4
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
theMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference
approximationisproblem-dependent

============================================================

=== CHUNK 069 ===
Palavras: 372
Caracteres: 13363
--------------------------------------------------
Srivastava2014etal.()showedthatdropoutismoreeﬀectivethanother
standardcomputationally inexpensiveregularizers,suchasweightdecay,ﬁlter
normconstraintsandsparseactivityregularization Dropoutmayalsobecombined
withotherformsofregularizationtoyieldafurtherimprovement Oneadvantageofdropoutisthatitisverycomputationally cheap.Using
dropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,
togenerate nrandombinarynumbersandmultiplythembythestate.Depending
ontheimplementation,itmayalsorequire O( n)memorytostorethesebinary
numbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel
hasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay
thecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon
examples Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimit
thetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly
anymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic
gradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels
suchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent
neuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother
regularizationstrategiesofcomparablepowerimposemoresevererestrictionson
thearchitectureofthemodel Thoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,
thecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropout
isaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀset
thiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation
seterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch
largermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge
datasets,regularizationconferslittlereductioningeneralization error Inthese
cases,thecomputational costofusingdropoutandlargermodelsmayoutweigh
thebeneﬁtofregularization Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless
eﬀective.Bayesian neuralnetworks(, )outperform dropout onthe Neal1996
AlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011
areavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,
unsupervisedfeaturelearningcangainanadvantageoverdropout Wager2013etal.()showedthat,whenappliedtolinearregression,dropout
isequivalentto L2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor
2 6 5
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
eachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientis
determinedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep
models,dropoutisnotequivalenttoweightdecay Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe
approach’ssuccess.Itisjustameansofapproximating thesumoverallsub-
models.WangandManning2013()derivedanalyticalapproximationstothis
marginalization Theirapproximation,knownasfastdropoutresultedinfaster
convergencetimeduetothereducedstochasticityinthecomputationofthe
gradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled
(butalsomorecomputationally expensive)approximation totheaverageoverall
sub-networksthantheweightscalingapproximation.Fastdropouthasbeenused
tonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork
problems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoa
largeproblem Justasstochasticityisnotnecessarytoachievetheregularizing eﬀect of
dropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()
designedcontrolexperimentsusingamethodcalleddropoutboostingthatthey
designedtouseexactlythesamemasknoiseastraditionaldropoutbutlack
itsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointly
maximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional
dropoutisanalogoustobagging, this approachisanalogoustoboosting.As
intended,experimentswithdropoutboostingshowalmostnoregularizationeﬀect
comparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat
theinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof
dropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleis
onlyachievedwhenthestochasticallysampledensemblemembersaretrainedto
performwellindependently ofeachother Dropouthasinspiredotherstochasticapproachestotrainingexponentially
largeensemblesofmodelsthatshareweights DropConnectisaspecialcaseof
dropoutwhereeachproductbetweenasinglescalarweightandasinglehidden
unitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic
poolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3
ofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerent
spatiallocationsofeachfeaturemap Sofar,dropoutremainsthemostwidely
usedimplicitensemblemethod Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic
behaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions
implementsaformofbaggingwithparametersharing.Earlier, wedescribed
2 6 6
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
dropoutas bagginganensembleofmodelsformedbyincludingor excluding
units.However,thereisnoneedforthismodelaveragingstrategytobebasedon
inclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible Inpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareable
tolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast
approximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrized
byavectorµastraininganensembleconsistingof p( y ,|xµ)forallpossible
valuesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.For
example,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe
weightsbyµ∼N( 1 , I)canoutperformdropoutbasedonbinarymasks.Because
E[µ] = 1thestandardnetworkautomatically implementsapproximate inference
intheensemble,withoutneedinganyweightscaling Sofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,
approximatebagging.However,thereisanotherviewofdropoutthatgoesfurther
thanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble
ofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto
performwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits
mustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal ()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c
swappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressurefor
genestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerent
organisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir
environmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures
ofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe
notmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts Warde-
Farley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand
concludedthatdropoutoﬀersadditionalimprovementstogeneralization error
beyondthoseobtainedbyensemblesofindependentmodels Itisimportanttounderstandthatalargeportionofthepowerofdropout
arisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This
canbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation
contentoftheinputratherthandestructionoftherawvaluesoftheinput.For
example,ifthemodellearnsahiddenunit h ithatdetectsafacebyﬁndingthenose,
thendropping h icorrespondstoerasingtheinformationthatthereisanosein
theimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe
presenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare
notabletorandomlyerasetheinformationaboutanosefromanimageofaface
unlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin
2 6 7
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
theimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues
allowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput
distributionthatthemodelhasacquiredsofar Anotherimportantaspectofdropoutisthatthenoiseismultiplicative Ifthe
noisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunit h iwith
addednoise couldsimplylearntohave h ibecomeverylargeinordertomake
theaddednoise insigniﬁcantbycomparison.Multiplicativenoisedoesnotallow
suchapathologicalsolutiontothenoiserobustnessproblem Anotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel
inawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden
unitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove
optimization, butthenoisecanhavearegularizingeﬀect,andsometimesmakes
dropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1
7.13AdversarialTraining
Inmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen
evaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese
modelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder
toprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan
searchforexamplesthatthemodelmisclassiﬁes ()foundthat Szegedy etal.2014b
evenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%
errorrateonexamplesthatareintentionallyconstructedbyusinganoptimization
proceduretosearchforaninputxnearadatapointxsuchthatthemodel
outputisverydiﬀerentatx.Inmanycases,xcanbesosimilartoxthata
humanobservercannottellthediﬀerencebetweentheoriginalexampleandthe
adversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.See
ﬁgureforanexample.7.8
Adversarialexampleshavemanyimplications,forexample,incomputersecurity,
thatarebeyondthescopeofthischapter However,theyareinterestinginthe
contextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d testsetviaadversarialtraining—trainingonadversariallyperturbedexamples
fromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,) Goodfellow2014betal.()showedthatoneoftheprimarycausesofthese
adversarial examplesis excessive linearity.Neural networks arebuilt out of
primarilylinearbuildingblocks Insomeexperimentstheoverallfunctionthey
implementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy
2 6 8
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
+ .007× =
x sign(∇ x J(θx , , y))x+
sign(∇ x J(θx , , y))
y=“panda” “nematode”“gibbon”
w/57.7%
conﬁdencew/8.2%
conﬁdencew/99.3%
conﬁdence
Figure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet
( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a
elementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith
respecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproduced
withpermissionfrom () Goodfellow e t a l .2014b
tooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly
ifithasnumerousinputs.Ifwechangeeachinputby ,thenalinearfunction
withweightswcanchangebyasmuchas ||||w 1,whichcanbeaverylarge
amountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly
sensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant
intheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly
introducingalocalconstancypriorintosupervisedneuralnets Adversarialtraininghelpstoillustratethepowerofusingalargefunction
familyincombinationwithaggressiveregularization Purelylinearmodels,like
logisticregression,arenotabletoresistadversarialexamplesbecausetheyare
forcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange
fromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapture
lineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised
learning.Atapointxthatisnotassociatedwithalabelinthedataset,the
modelitselfassignssomelabel ˆ y.Themodel’slabel ˆ ymaynotbethetruelabel,
butifthemodelishighquality,thenˆ yhasahighprobabilityofprovidingthe
truelabel.Wecanseekanadversarialexamplexthatcausestheclassiﬁerto
outputalabel ywith y=ˆ y.Adversarialexamplesgeneratedusingnotthetrue
labelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial
examples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthe
samelabeltoxandx.Thisencouragestheclassiﬁertolearnafunctionthatis
2 6 9
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
robusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata
lies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylie
ondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump
fromoneclassmanifoldtoanotherclassmanifold 7.14Tangent Distance, TangentProp,and Manifold
TangentClassiﬁer
Manymachinelearningalgorithmsaimtoovercomethecurseofdimensionality
byassumingthatthedataliesnearalow-dimensional manifold,asdescribedin
section.5.11.3
Oneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe
tangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998
nearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean
distancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich
probabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand
thatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁer
shouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement
onthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween
pointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey
respectivelybelong.Althoughthatmaybecomputationally diﬃcult(itwould
requiresolvinganoptimization problem,toﬁndthenearestpairofpointson M 1
and M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits
tangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween
atangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional
linearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires
onetospecifythetangentvectors

============================================================

=== CHUNK 070 ===
Palavras: 371
Caracteres: 9456
--------------------------------------------------
Inarelatedspirit,thetangentpropalgorithm( ,)(ﬁgure) Simardetal.19927.9
trainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutput f(x)of
theneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof
variationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe
sameclassconcentrate.Localinvarianceisachievedbyrequiring ∇ x f(x)tobe
orthogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat
thedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga
regularizationpenalty:Ω
Ω() = f
i
(∇ x f())xv( ) i2 (7.67)
2 7 0
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
Thisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for
mostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone
output f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,
thetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof
theeﬀectoftransformationssuchastranslation,rotation,andscalinginimages Tangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992
butalsointhecontextofreinforcementlearning(,) Thrun1995
Tangentpropagation is closelyrelated todataset augmentation.In both
cases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask
byspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe
network.Thediﬀerenceisthatinthecaseofdatasetaugmentation, thenetworkis
explicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying
morethananinﬁnitesimalamountofthesetransformations.Tangentpropagation
doesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically
regularizesthemodeltoresistperturbationinthedirectionscorrespondingto
the speciﬁed transformation.While thisanalytical approac h isintellectually
elegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist
inﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto
largerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodels
basedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivatives
byturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheir
derivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh
unitscan.Datasetaugmentation workswellwithrectiﬁedlinearunitsbecause
diﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsof
eachoriginalinput Tangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,
1992)andadversarialtraining( ,; ,) Szegedy etal.2014bGoodfellowetal.2014b
DoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining
ﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame
outputontheseasontheoriginalinputs.Tangentpropagation anddataset
augmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthe
modelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput Doublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe
invarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all
asdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,
adversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop Themanifoldtangentclassiﬁer(,),eliminatestheneedto Rifaietal.2011c
knowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14
2 7 1
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
x 1x 2N o r m a lT a ng e nt
Figure7.9: Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l 1992 Rifai2011c )andmanifoldtangentclassiﬁer( e t a l .,),whichbothregularizethe
classiﬁeroutputfunction f(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,
illustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe
classmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe
classmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany
tangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctionto
changerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas
itmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent
classiﬁerregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent
propagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent
directions(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass
manifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirections
bytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimate
manifoldswillbedescribedinchapter.14
estimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuse
ofthistechniquetoavoidneedinguser-speciﬁedtangentvectors Asillustrated
inﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10
thatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)
andincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchas
movingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁer
isthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby
unsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁer
asintangentprop(equation).7.67
Thischapterhasdescribedmostofthegeneralstrategiesusedtoregularize
neuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch
2 7 2
CHAPTER7.REGULARIZATIONFORDEEPLEARNING
willberevisitedperiodicallybymostoftheremainingchapters.Anothercentral
themeofmachinelearningisoptimization, describednext 2 7 3
C h a p t e r 8
OptimizationforTrainingDeep
Models
Deeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,
performinginferenceinmodelssuchasPCAinvolvessolvinganoptimization
problem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms Ofallofthemanyoptimization problemsinvolvedindeeplearning,themost
diﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof
timeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural
networktrainingproblem.Becausethisproblemissoimportantandsoexpensive,
aspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit Thischapterpresentstheseoptimization techniquesforneuralnetworktraining Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,
wesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4
optimization ingeneral Thischapterfocusesononeparticularcaseofoptimization: ﬁndingtheparam-
etersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunction J(θ),which
typicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas
wellasadditionalregularizationterms Webeginwithadescriptionofhowoptimization usedasatrainingalgorithm
foramachinelearningtaskdiﬀersfrompureoptimization Next,wepresentseveral
oftheconcretechallengesthatmakeoptimization ofneuralnetworksdiﬃcult.We
thendeﬁneseveralpracticalalgorithms,includingbothoptimization algorithms
themselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms
adapttheirlearningratesduringtrainingorleverageinformationcontainedin
274
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
thesecondderivativesofthecostfunction.Finally,weconcludewithareviewof
severaloptimization strategiesthatareformedbycombiningsimpleoptimization
algorithmsintohigher-levelprocedures 8.1HowLearningDiﬀersfromPureOptimization
Optimization algorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional
optimization algorithmsinseveralways.Machinelearningusuallyactsindirectly Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasure
P,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable.We
thereforeoptimize Ponlyindirectly.Wereduceadiﬀerentcostfunction J(θ)in
thehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,
whereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining
deepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureof
machinelearningobjectivefunctions Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,
suchas
J() = θ E ( ) ˆ x ,y ∼ pdataL f , y , ((;)xθ) (8.1)
where Listheper-examplelossfunction, f(x;θ)isthepredictedoutputwhen
theinputisx,ˆ p da t aistheempiricaldistribution.Inthesupervisedlearningcase,
yisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized
supervisedcase,wheretheargumentsto Lare f(x;θ)and y.However,itistrivial
toextendthisdevelopment,forexample,toincludeθorxasarguments,orto
exclude yasarguments,inordertodevelopvariousformsofregularizationor
unsupervisedlearning Equationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We 8.1
wouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe
expectationistakenacrossthedatageneratingdistribution p da t aratherthanjust
overtheﬁnitetrainingset:
J∗() = θ E ( ) x ,y ∼ pdataL f , y ((;)xθ) (8.2)
8.1.1EmpiricalRiskMinimization
Thegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization
errorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere
thattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew
thetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task
2 7 5
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
solvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)
butonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem Thesimplestwaytoconvertamachinelearningproblembackintoanop-
timizationproblemistominimizetheexpectedlossonthetrainingset.This
meansreplacingthetruedistribution p(x , y) withtheempiricaldistributionˆ p(x , y)
deﬁnedbythetrainingset.Wenowminimizetheempiricalrisk
E x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1
mm 
i = 1L f((x( ) i;)θ , y( ) i)(8.3)
whereisthenumberoftrainingexamples

============================================================

=== CHUNK 071 ===
Palavras: 359
Caracteres: 11994
--------------------------------------------------
m
Thetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown
asempiricalriskminimization.Inthissetting,machinelearningisstillvery
similartostraightforwardoptimization Ratherthanoptimizingtheriskdirectly,
weoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyas
well.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk
canbeexpectedtodecreasebyvariousamounts However,empiricalriskminimization ispronetooverﬁtting.Modelswith
highcapacitycansimplymemorizethetrainingset.Inmanycases,empirical
riskminimization isnotreallyfeasible.Themosteﬀectivemodernoptimization
algorithmsarebasedongradientdescent,butmanyusefullossfunctions,such
as0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁned
everywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we
rarelyuseempiricalriskminimization Instead,wemustuseaslightlydiﬀerent
approach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent
fromthequantitythatwetrulywanttooptimize 8.1.2SurrogateLossFunctionsandEarlyStopping
Sometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnot
onethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1
lossistypicallyintractable(exponentialintheinputdimension),evenforalinear
classiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes
asurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa
surrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate
theconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan
dothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorin
expectation 2 7 6
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Insomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn
more.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong
timeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe
log-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,
onecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapart
fromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextracting
moreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply
minimizingtheaverage0-1lossonthetrainingset Averyimportantdiﬀerencebetweenoptimization ingeneralandoptimization
asweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt
atalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes
asurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly
stopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased 7.8
onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,
andisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,
whichisverydiﬀerentfromthepureoptimization setting,whereanoptimization
algorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall 8.1.3BatchandMinibatchAlgorithms
Oneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral
optimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum
overthetrainingexamples.Optimization algorithmsformachinelearningtypically
computeeachupdatetotheparametersbasedonanexpectedvalueofthecost
functionestimatedusingonlyasubsetofthetermsofthefullcostfunction Forexample,maximumlikelihoodestimationproblems,whenviewedinlog
space,decomposeintoasumovereachexample:
θ M L= argmax
θm 
i = 1log p m o de l(x( ) i, y( ) i;)θ (8.4)
Maximizingthissumisequivalenttomaximizingtheexpectationoverthe
empiricaldistributiondeﬁnedbythetrainingset:
J() = θ E x ,y ∼ ˆ pdatalog p m o de l(;)x , yθ (8.5)
Mostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-
mizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the
2 7 7
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
mostcommonlyusedpropertyisthegradient:
∇ θ J() = θ E x ,y ∼ ˆ pdata∇ θlog p m o de l(;)x , yθ (8.6)
Computing this expectation exactly isvery expensive because it requires
evaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan
computetheseexpectationsbyrandomlysamplingasmallnumberofexamples
fromthedataset,thentakingtheaverageoveronlythoseexamples Recallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n
samplesisgivenby σ /√n ,where σisthetruestandarddeviationofthevalueof
thesamples.Thedenominator of√nshowsthattherearelessthanlinearreturns
tousingmoreexamplestoestimatethegradient.Comparetwohypothetical
estimatesofthegradient,onebasedon100examplesandanotherbasedon10,000
examples.Thelatterrequires100timesmorecomputationthantheformer,but
reducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization
algorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof
numberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates
ofthegradientratherthanslowlycomputingtheexactgradient Anotherconsiderationmotivatingstatisticalestimationofthegradientfroma
smallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all
msamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-
basedestimateofthegradientcouldcomputethecorrectgradientwithasingle
sample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we
areunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlarge
numbersofexamplesthatallmakeverysimilarcontributionstothegradient Optimization algorithmsthatusetheentiretrainingsetarecalledbatchor
deterministicgradientmethods,becausetheyprocessallofthetrainingexamples
simultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing
becausetheword“batch”isalsooftenusedtodescribetheminibatchusedby
minibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”
impliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribe
agroupofexamplesdoesnot Forexample,itisverycommontousetheterm
“batchsize”todescribethesizeofaminibatch Optimization algorithmsthatuseonlyasingleexampleatatimearesometimes
calledstochasticorsometimesonlinemethods.Thetermonlineisusually
reservedforthecasewheretheexamplesaredrawnfromastreamofcontinually
createdexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveral
passesaremade Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore
2 7 8
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
thanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled
minibatchorminibatchstochasticmethodsanditisnowcommontosimply
callthemstochasticmethods Thecanonicalexampleofastochasticmethodisstochasticgradientdescent,
presentedindetailinsection.8.3.1
Minibatchsizesaregenerallydrivenbythefollowingfactors:
•Largerbatchesprovideamoreaccurateestimateofthegradient,butwith
lessthanlinearreturns •Multicorearchitectures areusuallyunderutilized byextremelysmallbatches Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere
isnoreductioninthetimetoprocessaminibatch •Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically
thecase),thentheamountofmemoryscaleswiththebatchsize.Formany
hardwaresetupsthisisthelimitingfactorinbatchsize •Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀer
betterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16
sometimesbeingattemptedforlargemodels •Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003
perhapsduetothenoisetheyaddtothelearningprocess.Generalization
errorisoftenbestforabatchsizeof1 Trainingwithsuchasmallbatch
sizemightrequireasmalllearningratetomaintainstabilityduetothehigh
varianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh
duetotheneedtomakemoresteps,bothbecauseofthereducedlearning
rateandbecauseittakesmorestepstoobservetheentiretrainingset Diﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-
batchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthan
others,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccurately
withfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling
errorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare
usuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order
methods,whichusealsotheHessianmatrixHandcomputeupdatessuchas
H− 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch
sizesarerequiredtominimizeﬂuctuationsintheestimatesofH− 1g.Suppose
thatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by
2 7 9
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Horitsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing Verysmallchangesintheestimateofgcanthuscauselargechangesintheupdate
H− 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly
approximately,sotheupdateH− 1gwillcontainevenmoreerrorthanwewould
predictfromapplyingapoorlyconditionedoperationtotheestimateof.g
Itisalsocrucialthattheminibatchesbeselectedrandomly.Computingan
unbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose
samplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe
independentfromeachother,sotwosubsequentminibatchesofexamplesshould
alsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged
inawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight
haveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This
listmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerent
timesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthe
secondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe
weretodrawexamplesinorderfromthislist,theneachofourminibatcheswould
beextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe
manypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset
holdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselecting
minibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof
examplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly
atrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice
itisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitin
shuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutive
examplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel
willbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining
data.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea
signiﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycan
seriouslyreducetheeﬀectivenessofthealgorithm Manyoptimization problemsinmachinelearningdecomposeoverexamples
wellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamples
inparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for
oneminibatchofexamplesXatthesametimethatwecomputetheupdatefor
severalotherminibatches.Suchasynchronousparalleldistributedapproachesare
discussedfurtherinsection.12.1.3
Aninterestingmotivationforminibatchstochasticgradientdescentisthatit
followsthegradientofthetruegeneralizationerror(equation)solongasno 8.2
examplesarerepeated.Mostimplementations ofminibatchstochasticgradient
2 8 0
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
descentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Onthe
ﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue
generalization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis
formedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining
newfairsamplesfromthedatageneratingdistribution Thefactthatstochasticgradientdescentminimizesgeneralization erroris
easiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn
fromastreamofdata.Inotherwords,insteadofreceivingaﬁxed-sizetraining
set,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,
witheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y) Inthisscenario,examplesareneverrepeated;everyexperienceisafairsample
from p da t a Theequivalenceiseasiesttoderivewhenbothxand yarediscrete Inthis
case,thegeneralization error(equation)canbewrittenasasum 8.2
J∗() =θ
x
yp da t a()((;)) x , y L fxθ , y , (8.7)
withtheexactgradient
g= ∇ θ J∗() =θ
x
yp da t a()x , y∇ θ L f , y

============================================================

=== CHUNK 072 ===
Palavras: 351
Caracteres: 11318
--------------------------------------------------
((;)xθ)(8.8)
Wehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-
tionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L
besidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,
undermildassumptionsregarding p da t aand L
Hence, wecanobtainanunbiasedestimatoroftheexactgradientof the
generalization errorbysamplingaminibatchofexamples {x( 1 ), .x( ) m}withcor-
respondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing
thegradientofthelosswithrespecttotheparametersforthatminibatch:
ˆg=1
m∇ θ
iL f((x( ) i;)θ , y( ) i) (8.9)
Updatinginthedirectionof θ ˆgperformsSGDonthegeneralization error Ofcourse, thisinterpretation only applies whenexamplesarenotreused Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,
unlessthetrainingsetisextremelylarge When multiplesuchepochsareused,
onlytheﬁrstepochfollowstheunbiasedgradientofthegeneralization error,but
2 8 1
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
ofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreased
trainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentraining
errorandtesterror Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it
isbecomingmorecommonformachinelearningapplicationstouseeachtraining
exampleonlyonceoreventomakeanincompletepassthroughthetraining
set.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,so
underﬁttingandcomputational eﬃciencybecomethepredominant concerns.See
also ()foradiscussionoftheeﬀectofcomputational BottouandBousquet2008
bottlenecksongeneralization error,asthenumberoftrainingexamplesgrows 8.2ChallengesinNeuralNetworkOptimization
Optimization ingeneralisanextremelydiﬃculttask.Traditionally,machine
learninghasavoidedthediﬃcultyofgeneraloptimization bycarefullydesigning
theobjectivefunctionandconstraintstoensurethattheoptimization problemis
convex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex
case.Evenconvexoptimization isnotwithoutitscomplications Inthissection,
wesummarizeseveralofthemostprominentchallengesinvolvedinoptimization
fortrainingdeepmodels 8.2.1Ill-Conditioning
Somechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost
prominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral
probleminmostnumericaloptimization, convexorotherwise,andisdescribedin
moredetailinsection.4.3.1
Theill-conditioning problemisgenerallybelievedtobepresentinneural
networktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget
“stuck”inthesensethatevenverysmallstepsincreasethecostfunction Recallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9
costfunctionpredictsthatagradientdescentstepofwilladd − g
1
22gHgg− g (8.10)
tothecost.Ill-conditioningofthegradientbecomesaproblemwhen1
22gHg
exceeds gg Todeterminewhetherill-conditioning isdetrimentaltoaneural
network training task, one canmonitorthe squaredgradientnormggand
2 8 2
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
−50050100150200250
Trainingtime(epochs)−20246810121416Gradient norm
0 50100150200250
Trainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classiﬁcationerrorrate
Figure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis
example,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused
forobjectdetection ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient
evaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm
isplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid
curve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould
expectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )
gradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcation
errordecreasestoalowlevel thegHgterm.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantly
throughoutlearning,butthegHgtermgrowsbymorethananorderofmagnitude Theresultisthatlearningbecomesveryslowdespitethepresenceofastrong
gradientbecausethelearningratemustbeshrunktocompensateforevenstronger
curvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly 8.1
duringthesuccessfultrainingofaneuralnetwork Thoughill-conditioning ispresentinothersettingsbesidesneuralnetwork
training,someofthetechniquesusedtocombatitinothercontextsareless
applicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttool
forminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin
thesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcant
modiﬁcationbeforeitcanbeappliedtoneuralnetworks 8.2.2LocalMinima
Oneofthemostprominentfeaturesofaconvexoptimization problemisthatit
canbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis
2 8 3
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
guaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionat
thebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch
aﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we
knowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind Withnon-convexfunctions,suchasneuralnets,itispossibletohavemany
localminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave
anextremelylargenumberoflocalminima.However,aswewillsee,thisisnot
necessarilyamajorproblem Neuralnetworksandanymodelswithmultipleequivalentlyparametrized latent
variablesallhavemultiplelocalminimabecauseofthemodelidentiﬁability
problem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcan
ruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariables
areoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanging
latentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand
modifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming
weightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe
have mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden
units.Thiskindofnon-identiﬁabilit yisknownasweightspacesymmetry Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshave
additionalcausesofnon-identiﬁabilit y.Forexample,inanyrectiﬁedlinearor
maxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby
αifwealsoscaleallofitsoutgoingweightsby1
α.Thismeansthat—ifthecost
functiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe
weightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinear
ormaxoutnetworkliesonan( m n×)-dimensionalhyperbolaofequivalentlocal
minima Thesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylarge
orevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcost
function.However,alloftheselocalminimaarisingfromnon-identiﬁabilit yare
equivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare
notaproblematicformofnon-convexity Localminimacanbeproblematiciftheyhavehighcostincomparisontothe
globalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden
units,thathavelocalminimawithhighercostthantheglobalminimum(Sontag
andSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima
withhighcostarecommon,thiscouldposeaseriousproblemforgradient-based
optimization algorithms Itremainsanopenquestionwhethertherearemanylocalminimaofhighcost
2 8 4
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
fornetworksofpracticalinterestandwhetheroptimization algorithmsencounter
them.Formanyyears,mostpractitioners believedthatlocalminimawerea
commonproblemplaguingneuralnetworkoptimization Today,thatdoesnot
appeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts
nowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavea
lowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimum
ratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost
(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska
etal.,).2014
Manypractitioners attributenearlyalldiﬃcultywithneuralnetworkoptimiza-
tiontolocalminima.Weencouragepractitioners tocarefullytestforspeciﬁc
problems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe
normofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto
insigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical
point.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional
spaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaarethe
problem.Manystructuresotherthanlocalminimaalsohavesmallgradients 8.2.3Plateaus,SaddlePointsandOtherFlatRegions
Formanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)
areinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle
point.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,
whileothershavealowercost Atasaddlepoint,theHessianmatrixhasboth
positiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith
positiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying
alongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas
beingalocalminimumalongonecross-sectionofthecostfunctionandalocal
maximumalonganothercross-section.Seeﬁgureforanillustration 4.5
Manyclasses ofrandomfunctionsexhibitthefollowingbehavior:inlow-
dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local
minimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn→ Rof
thistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows
exponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe
thattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues The
Hessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues Imaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingle
dimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads
once.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill
2 8 5
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
beheads.See ()forareviewoftherelevanttheoreticalwork Dauphinetal.2014
Anamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe
Hessianbecomemorelikelytobepositiveaswereachregionsoflowercost In
ourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup
heads ntimesifweareatacriticalpointwithlowcost Thismeansthatlocal
minimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith
highcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely
highcostaremorelikelytobelocalmaxima Thishappensformanyclassesofrandomfunctions.Doesithappenforneural
networks ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989
(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin
chapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14
localminimawithhighercostthantheglobalminimum.Theyobservedwithout
proofthattheseresultsextendtodeepernetworkswithoutnonlinearities The
outputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful
tostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis
anon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust
multiplematricescomposedtogether ()providedexactsolutions Saxeetal.2013
tothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin
thesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof
deepmodelswithnonlinearactivationfunctions ()showed Dauphinetal.2014
experimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery
manyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional
theoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom
functionsrelatedtoneuralnetworksdoessoaswell

============================================================

=== CHUNK 073 ===
Palavras: 353
Caracteres: 12470
--------------------------------------------------
Whataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-
rithms?Forﬁrst-orderoptimization algorithmsthatuseonlygradientinformation,
thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle
point.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape
saddlepointsinmanycases ()providedvisualizationsof Goodfellowetal.2015
severallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample
giveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear 8.2
aprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe
gradientdescenttrajectoryrapidlyescapingthisregion () Goodfellowetal.2015
alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe
repelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation
maybediﬀerentformorerealisticusesofgradientdescent ForNewton’smethod, itisclearthatsaddlepointsconstituteaproblem 2 8 6
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
P r o j e c t i o n2o f θ
P r o j e c t i o n 1 o f θJ(
)θ
Figure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted
withpermissionfromGoodfellow2015 e t a l .() Thesevisualizationsappearsimilarfor
feedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied
torealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these
visualizationsusuallydonotshowmanyconspicuousobstacles Priortothesuccessof
stochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,
neuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex
structurethanisrevealedbytheseprojections Theprimaryobstaclerevealedbythis
projectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as
indicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily Mostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,
whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix
inthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleinthe
ﬁgureviaanindirectarcingpath 2 8 7
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Gradientdescentisdesignedtomove“downhill”andisnotexplicitlydesigned
toseekacriticalpoint.Newton’smethod,however,isdesignedtosolvefora
pointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjump
toasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces
presumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing
gradientdescentforneuralnetworktraining ()introduceda Dauphinetal.2014
saddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit
improvessigniﬁcantlyoverthetraditionalversion.Second-order methodsremain
diﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds
promiseifitcouldbescaled Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddle
points.Therearealsomaxima, whic haremuchlikesaddlepointsfromthe
perspectiveofoptimization—many algorithmsarenotattractedtothem, but
unmodiﬁedNewton’smethodis.Maximaofmanyclassesofrandomfunctions
becomeexponentiallyrareinhighdimensionalspace,justlikeminimado Theremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,the
gradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor
problemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,
ﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimization
problem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction 8.2.4CliﬀsandExplodingGradients
Neuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling
cliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral 8.3
largeweightstogether.Onthefaceofanextremelysteepcliﬀstructure,the
gradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀ
ofthecliﬀstructurealtogether 2 8 8
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS

 

Figure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor
recurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting
fromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery
highderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,a
gradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe
optimizationworkthathadbeendone FigureadaptedwithpermissionfromPascanu
e t a l .().2013
Thecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,
butfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient
clippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1
thegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection
withinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithm
proposestomakeaverylargestep,thegradientclippingheuristicintervenesto
reducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion
wherethegradientindicatesthedirectionofapproximately steepestdescent.Cliﬀ
structuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,
becausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor
foreachtimestep.Longtemporalsequencesthusincuranextremeamountof
multiplication 8.2.5Long-TermDependencies
Anotherdiﬃcultythatneuralnetworkoptimization algorithmsmustovercome
arises when thecomputational gra ph becomes extremely deep.Feedforward
networkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent
networks,describedinchapter,whichconstructverydeepcomputational graphs 10
2 8 9
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
byrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal
sequence.Repeatedapplicationofthesameparametersgivesrisetoespecially
pronounceddiﬃculties Forexample,supposethatacomputational graphcontainsapaththatconsists
ofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-
tiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V− 1 Inthissimplecase,itisstraightforwardtoseethat
Wt=
VλVdiag()− 1t= ()VdiagλtV− 1 (8.11)
Anyeigenvalues λ ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1
aregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1
vanishingandexplodinggradientproblemreferstothefactthatgradients
throughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradients
makeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprove
thecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀ
structuresdescribedearlierthatmotivategradientclippingareanexampleofthe
explodinggradientphenomenon Therepeatedmultiplication byWateachtimestepdescribedhereisvery
similartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueof
amatrixWandthecorrespondingeigenvector.Fromthispointofviewitis
notsurprisingthatxWtwilleventuallydiscardallcomponentsofxthatare
orthogonaltotheprincipaleigenvectorof.W
RecurrentnetworksusethesamematrixWateachtimestep,butfeedforward
networksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe
vanishingandexplodinggradientproblem(,) Sussillo2014
Wedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks
untilsection,afterrecurrentnetworkshavebeendescribedinmoredetail 10.7
8.2.6InexactGradients
Mostoptimization algorithmsaredesignedwiththeassumptionthatwehave
accesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave
anoisyorevenbiasedestimateofthesequantities Nearlyeverydeeplearning
algorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch
oftrainingexamplestocomputethegradient Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableas
well.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise
2 9 0
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
withthemoreadvancedmodelsinpart.Forexample,contrastivedivergence III
givesatechniqueforapproximatingthegradientoftheintractablelog-likelihood
ofaBoltzmannmachine Variousneuralnetworkoptimization algorithmsaredesignedtoaccountfor
imperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing
asurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss 8.2.7PoorCorrespondencebetweenLocalandGlobalStructure
Manyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe
lossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepif J(θ)is
poorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddle
pointhidingtheopportunitytomakeprogressdownhillfromthegradient Itispossibletoovercomealloftheseproblemsatasinglepointandstill
performpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes
notpointtowarddistantregionsofmuchlowercost Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto
thelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2
thelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda
mountain-shapedstructure Muchofresearchintothediﬃcultiesofoptimization hasfocusedonwhether
trainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin
practiceneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1
showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,
suchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction
−log p( y|x;θ)canlackaglobalminimumpointandinsteadasymptotically
approachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwith
discrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan
becomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery
exampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof
zero.Likewise,amodelofrealvalues p( y|x) =N( y; f(θ) , β− 1)canhavenegative
log-likelihoodthatasymptotestonegativeinﬁnity—if f(θ)isabletocorrectly
predictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease
βwithoutbound.Seeﬁgureforanexampleofafailureoflocaloptimization to 8.4
ﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle
points Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat
inﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome
2 9 1
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
θJ ( ) θ
Figure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes
notpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,
eveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction
containsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyin
thiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingableto
traverseit Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate
suchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin
excessivetrainingtime,asillustratedinﬁgure.8.2
oftheprocess Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor
problemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithms
thatusenon-localmoves Gradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefor
trainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious
sectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves
canbediﬃculttocompute.Wemaybeabletocomputesomepropertiesofthe
objectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance
inourestimateofthecorrectdirection.Inthesecases,localdescentmayormay
notdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactually
abletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues
suchaspoorconditioningordiscontinuousgradients,causingtheregionwhere
thegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In
thesecases,localdescentwithstepsofsize maydeﬁneareasonablyshortpath
tothesolution,butweareonlyabletocomputethelocaldescentdirectionwith
stepsofsize δ .Inthesecases,localdescentmayormaynotdeﬁneapath
tothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa
2 9 2
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
highcomputational cost.Sometimeslocalinformationprovidesusnoguide,when
thefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacritical
point(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly
forcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoes
notdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy
andleadusalongapaththatmovesdownhillbutawayfromanysolution,asin
ﬁgure,oralonganunnecessarilylongtrajectorytothesolution,asinﬁgure 8.4 8.2
Currently,wedonotunderstandwhichoftheseproblemsaremostrelevantto
makingneuralnetworkoptimization diﬃcult,andthisisanactiveareaofresearch Regardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbe
avoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution
byapaththatlocaldescentcanfollow,andifweareabletoinitializelearning
withinthatwell-behavedregion

============================================================

=== CHUNK 074 ===
Palavras: 352
Caracteres: 9066
--------------------------------------------------
Thislastviewsuggestsresearchintochoosing
goodinitialpointsfortraditionaloptimization algorithmstouse 8.2.8TheoreticalLimitsofOptimization
Severaltheoreticalresultsshowthattherearelimitsontheperformanceofany
optimization algorithmwemightdesignforneuralnetworks(BlumandRivest,
1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave
littlebearingontheuseofneuralnetworksinpractice Sometheoreticalresultsapplyonlytothecasewheretheunitsofaneural
networkoutput discretevalues.However, most neuralnetworkunitsoutput
smoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some
theoreticalresultsshowthatthereexistproblemclassesthatareintractable,but
itcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Other
resultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,but
inpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmany
moreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe
contextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexact
minimumofafunction,butseekonlytoreduceitsvaluesuﬃcientlytoobtaingood
generalization error Theoretical analysisofwhetheranoptimization algorithm
canaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticbounds
ontheperformanceofoptimization algorithmsthereforeremainsanimportant
goalformachinelearningresearch 2 9 3
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
8.3BasicAlgorithms
Wehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3
followsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated
considerablybyusingstochasticgradientdescenttofollowthegradientofrandomly
selectedminibatchesdownhill,asdiscussedinsectionandsection 5.9 8.1.3
8.3.1StochasticGradientDescent
Stochasticgradientdescent(SGD)anditsvariantsareprobablythemostused
optimization algorithmsformachinelearningingeneralandfordeeplearning
inparticular As discussedinsection,itispossibletoobtainanunbiased 8.1.3
estimateofthegradientbytakingtheaveragegradientonaminibatchof m
examplesdrawni.i.dfromthedatageneratingdistribution Algorithmshowshowtofollowthisestimateofthegradientdownhill 8.1
Algorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k
Require:Learningrate  k Require:Initialparameterθ
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondingtargetsy( ) i Computegradientestimate: ˆg←+1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
Applyupdate:θθ← − ˆg
endwhile
AcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we
havedescribedSGDasusingaﬁxedlearningrate .Inpractice,itisnecessaryto
graduallydecreasethelearningrateovertime,sowenowdenotethelearningrate
atiterationas k  k ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the
randomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive
ataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes
smallandthen 0whenweapproachandreachaminimumusingbatchgradient
descent,sobatchgradientdescentcanuseaﬁxedlearningrate.Asuﬃcient
conditiontoguaranteeconvergenceofSGDisthat
∞
k = 1 k= and ∞ , (8.12)
2 9 4
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
∞
k = 12
k < .∞ (8.13)
Inpractice,itiscommontodecaythelearningratelinearlyuntiliteration: τ
 k= (1 )− α  0+ α  τ (8.14)
with α=k
τ.Afteriteration,itiscommontoleaveconstant τ 
Thelearningratemaybechosenbytrialanderror,butitisusuallybest
tochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa
functionoftime.Thisismoreofanartthanascience,andmostguidanceonthis
subjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,
theparameterstochooseare  0,  τ,and τ.Usually τmaybesettothenumberof
iterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually
 τshouldbesettoroughlythevalueof 1%  0.Themainquestionishowtoset  0 Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost
functionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyif
trainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe
useofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe
initiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe
ﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance
aftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrst
severaliterationsandusealearningratethatishigherthanthebest-performing
learningrateatthistime,butnotsohighthatitcausessevereinstability ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-
basedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe
numberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber
oftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay
convergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithas
processedtheentiretrainingset Tostudytheconvergencerateofanoptimization algorithmitiscommonto
measuretheexcesserror J(θ)−min θ J(θ),whichistheamountthatthecurrent
costfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex
problem,theexcesserroris O(1√
k)after kiterations,whileinthestronglyconvex
caseitis O(1
k).Theseboundscannotbeimprovedunlessextraconditionsare
assumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic
gradientdescentintheory.However,theCramér-Raobound(,;, Cramér1946Rao
1945)statesthatgeneralization errorcannotdecreasefasterthan O(1
k).Bottou
2 9 5
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
andBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue
anoptimization algorithmthatconvergesfasterthan O(1
k)formachinelearning
tasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,the
asymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent
hasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake
rapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples
outweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin
theremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelost
intheconstantfactorsobscuredbythe O(1
k)asymptoticanalysis.Onecanalso
tradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygradually
increasingtheminibatchsizeduringthecourseoflearning FormoreinformationonSGD,see() Bottou1998
8.3.2Momentum
Whilestochasticgradientdescentremainsaverypopularoptimization strategy,
learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)
isdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut
consistentgradients,ornoisygradients.Themomentumalgorithmaccumulates
anexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove
intheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5
Formally,themomentumalgorithmintroducesavariablevthatplaystherole
ofvelocity—itisthedirectionandspeedatwhichtheparametersmovethrough
parameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe
negativegradient.Thenamemomentumderivesfromaphysicalanalogy,in
whichthenegativegradientisaforcemovingaparticlethroughparameterspace,
accordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv
mayalsoberegardedasthemomentumoftheparticle.Ahyperparameter α∈[0 ,1)
determineshowquicklythecontributionsofpreviousgradientsexponentiallydecay Theupdateruleisgivenby:
vv← α−∇  θ
1
mm 
i = 1L((fx( ) i;)θ ,y( ) i)
, (8.15)
θθv ← + (8.16)
Thevelocityvaccumulatesthegradientelements∇ θ1
mm
i = 1 L((fx( ) i;)θ ,y( ) i) Thelarger αisrelativeto ,themorepreviousgradientsaﬀectthecurrentdirection TheSGDalgorithmwithmomentumisgiveninalgorithm .8.2
2 9 6
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
− − − 3 0 2 0 1 0 0 1 0 2 0− 3 0− 2 0− 1 001 02 0
Figure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe
Hessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum
overcomestheﬁrstofthesetwoproblems.Thecontourlinesdepictaquadraticloss
functionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe
contoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis
function.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient
descentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective
lookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses
thecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe
narrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient 4.6
descentwithoutmomentum 2 9 7
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Previously,thesizeofthestepwassimplythenormofthegradientmultiplied
bythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow
alignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive
gradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways
observesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachinga
terminalvelocitywherethesizeofeachstepis
||||g
1− α (8.17)
Itisthushelpfultothinkofthemomentumhyperparameterintermsof1
1 − α.For
example, α= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10
thegradientdescentalgorithm

============================================================

=== CHUNK 075 ===
Palavras: 369
Caracteres: 10837
--------------------------------------------------
Commonvaluesof αusedinpracticeinclude .5, .9,and .99.Likethelearning
rate, αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand
islaterraised.Itislessimportanttoadapt αovertimethantoshrink overtime Algorithm8.2Stochasticgradientdescent(SGD)withmomentum
Require:Learningrate,momentumparameter  α
Require:Initialparameter,initialvelocity θ v
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondingtargetsy( ) i Computegradientestimate:g←1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
Computevelocityupdate:vvg ← α− 
Applyupdate:θθv ← +
endwhile
Wecanviewthemomentumalgorithmassimulatingaparticlesubjectto
continuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild
intuitionforhowthemomentumandgradientdescentalgorithmsbehave Thepositionoftheparticleatanypointintimeisgivenbyθ( t).Theparticle
experiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t
f() = t∂2
∂ t2θ() t (8.18)
Ratherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,
wecanintroducethevariablev( t)representingthevelocityoftheparticleattime
tandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:
v() = t∂
∂ tθ() t , (8.19)
2 9 8
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
f() = t∂
∂ tv() t (8.20)
Themomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvia
numericalsimulation.Asimplenumericalmethodforsolvingdiﬀerentialequations
isEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedby
theequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient Thisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyare
theforces?Oneforceisproportionaltothenegativegradientofthecostfunction:
−∇ θ J(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach
gradient,buttheNewtonianscenariousedbythemomentumalgorithminstead
usesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle
asbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa
steeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection
untilitbeginstogouphillagain Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,
thentheparticlemightnevercometorest.Imagineahockeypuckslidingdown
onesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,
assumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone
otherforce,proportionalto−v( t).Inphysicsterminology,thisforcecorresponds
toviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas
syrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually
convergetoalocalminimum Whydoweuse−v( t)andviscousdraginparticular Partofthereasonto
use−v( t)ismathematical convenience—anintegerpowerofthevelocityiseasy
toworkwith.However,otherphysicalsystemshaveotherkindsofdragbased
onotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough
theairexperiencesturbulentdrag,withforceproportionaltothesquareofthe
velocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha
forceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,
proportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis
small.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle
withanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag
willmoveawayfromitsinitialpositionforever,withthedistancefromthestarting
pointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the
constantforceduetofrictioncancausetheparticletocometorestbeforereaching
alocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough
2 9 9
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
thatthegradientcancontinuetocausemotionuntilaminimumisreached,but
strongenoughtopreventmotionifthegradientdoesnotjustifymoving 8.3.3NesterovMomentum
Sutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas
inspiredbyNesterov’sacceleratedgradientmethod(,,).The Nesterov19832004
updaterulesinthiscasearegivenby:
vv← α−∇  θ
1
mm 
i = 1L
fx(( ) i;+ )θ αv ,y( ) i
,(8.21)
θθv ← + , (8.22)
wheretheparameters αand playasimilarroleasinthestandardmomentum
method.ThediﬀerencebetweenNesterovmomentumandstandardmomentumis
wherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated
afterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum
asattemptingtoaddacorrectionfactortothestandardmethodofmomentum ThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3
Intheconvexbatchgradientcase,Nesterovmomentumbringstherateof
convergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown
byNesterov1983().Unfortunately, inthestochasticgradientcase,Nesterov
momentumdoesnotimprovetherateofconvergence Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum
Require:Learningrate,momentumparameter  α
Require:Initialparameter,initialvelocity θ v
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondinglabelsy( ) i Applyinterimupdate: ˜θθv ← + α
Computegradient(atinterimpoint):g←1
m∇ ˜ θ
i L f((x( ) i;˜θy) ,( ) i)
Computevelocityupdate:vvg ← α− 
Applyupdate:θθv ← +
endwhile
3 0 0
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
8.4ParameterInitializationStrategies
Someoptimization algorithmsarenotiterativebynatureandsimplysolvefora
solutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when
appliedtotherightclassofoptimization problems,convergetoacceptablesolutions
inanacceptableamountoftimeregardlessofinitialization Deeplearningtraining
algorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep
learningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify
someinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep
modelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedby
thechoiceofinitialization Theinitialpointcandeterminewhetherthealgorithm
convergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm
encountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,
theinitialpointcandeterminehowquicklylearningconvergesandwhetherit
convergestoapointwithhigh orlowcost.Also, pointsofcomparablecost
canhavewildlyvaryinggeneralization error,andtheinitialpointcanaﬀectthe
generalization aswell Moderninitialization strategiesaresimpleandheuristic.Designingimproved
initialization strategiesisadiﬃculttaskbecauseneuralnetworkoptimization is
notyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome
nicepropertieswhenthenetworkisinitialized.However,wedonothaveagood
understandingofwhichofthesepropertiesarepreservedunderwhichcircumstances
afterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpoints
maybebeneﬁcialfromtheviewpointofoptimization butdetrimentalfromthe
viewpointofgeneralization Ourunderstandingofhowtheinitialpointaﬀects
generalization isespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselect
theinitialpoint Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitial
parametersneedto“breaksymmetry” betweendiﬀerentunits.Iftwohidden
unitswiththesameactivationfunctionareconnectedtothesameinputs,then
theseunitsmusthavediﬀerentinitialparameters Iftheyhavethesameinitial
parameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost
andmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe
modelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerent
updatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusually
besttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheother
units.Thismayhelptomakesurethatnoinputpatternsarelostinthenull
spaceofforwardpropagationandnogradientpatternsarelostinthenullspace
ofback-propagation Thegoalofhavingeachunitcomputeadiﬀerentfunction
3 0 1
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
motivatesrandominitialization oftheparameters.Wecouldexplicitlysearch
foralargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,
butthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat
mostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization
onaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery
diﬀerentfunctionfromeachotherunit.Randominitialization fromahigh-entropy
distributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely
toassignanyunitstocomputethesamefunctionaseachother Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,and
initializeonlytheweightsrandomly.Extraparameters,forexample,parameters
encodingtheconditionalvarianceofaprediction,areusuallysettoheuristically
chosenconstantsmuchlikethebiasesare Wealmostalwaysinitializealltheweightsin themodel tovalues drawn
randomly froma Gaussian oruniform distribution.Thechoiceof Gaussian
oruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen
exhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea
largeeﬀectonboththeoutcomeoftheoptimization procedureandontheability
ofthenetworktogeneralize Largerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helping
toavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor
back-propagationthroughthelinearcomponentofeachlayer—largervaluesinthe
matrixresultinlargeroutputsofmatrixmultiplication Initialweightsthatare
toolargemay,however,resultinexplodingvaluesduringforwardpropagationor
back-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos
(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior
ofthedeterministicforwardpropagationprocedureappearsrandom) Tosome
extent,theexplodinggradientproblemcanbemitigatedbygradientclipping
(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep) Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction
tosaturate,causingcompletelossofgradientthroughsaturatedunits.These
competingfactorsdeterminetheidealinitialscaleoftheweights Theperspectivesofregularizationandoptimization cangiveverydiﬀerent
insightsintohowweshouldinitializeanetwork.Theoptimization perspective
suggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-
fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse
ofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall
incrementalchangestotheweightsandtendstohaltinareasthatarenearerto
theinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or
3 0 2
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
duetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesa
priorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recall
fromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8
decayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis
notthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout
theeﬀectofinitialization

============================================================

=== CHUNK 076 ===
Palavras: 372
Caracteres: 11983
--------------------------------------------------
Wecanthinkofinitializingtheparametersθtoθ 0as
beingsimilartoimposingaGaussianprior p(θ)withmeanθ 0.Fromthispoint
ofview,itmakessensetochooseθ 0tobenear0.Thispriorsaysthatitismore
likelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units
interactonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong
preferenceforthemtointeract.Ontheotherhand,ifweinitializeθ 0tolarge
values,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,and
howtheyshouldinteract Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.One
heuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand
noutputsbysamplingeachweightfrom U(−1√m,1√m),whileGlorotandBengio
()suggestusingthe 2010 normalizedinitialization
W i , j∼ U
−
6
m n+,
6
m n+ (8.23)
Thislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing
alllayerstohavethesameactivationvarianceandthegoalofinitializingall
layerstohavethesamegradientvariance.Theformulaisderivedusingthe
assumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,
withnononlinearities Realneuralnetworksobviouslyviolatethisassumption,
butmanystrategiesdesignedforthelinearmodelperformreasonablywellonits
nonlinearcounterparts Saxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with
acarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied
ateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesof
nonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya
modelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities Undersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof
trainingiterationsrequiredtoreachconvergenceisindependentofdepth Increasingthescalingfactor gpushesthenetworktowardtheregimewhere
activationsincreaseinnormastheypropagateforwardthroughthenetworkand
gradientsincreaseinnormastheypropagatebackward ()showed Sussillo2014
thatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas
3 0 3
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
1,000layers,withoutneedingtouseorthogonalinitializations A keyinsightof
thisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow
orshrinkoneachstepofforwardorback-propagation, followingarandomwalk
behavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixat
eachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward
networkscanmostlyavoidthevanishingandexplodinggradientsproblemthat
ariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5
Unfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto
optimalperformance.Thismaybeforthreediﬀerentreasons.First,wemay
beusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethe
normofasignalthroughouttheentirenetwork.Second,thepropertiesimposed
atinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the
criteriamightsucceedatimprovingthespeedofoptimization butinadvertently
increasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe
weightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut
notexactlyequaltothetheoreticalpredictions Onedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe
samestandarddeviation,suchas1√m,isthateveryindividualweightbecomes
extremelysmallwhenthelayersbecomelarge ()introducedan Martens2010
alternativeinitialization schemecalledsparseinitializationinwhicheachunitis
initializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount
ofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe
magnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps
toachievemorediversityamongtheunitsatinitialization time.However,italso
imposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian
values.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”large
values,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits
thathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother Whencomputational resourcesallowit,itisusuallyagoodideatotreatthe
initialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese
scalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2
asrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization
canalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor
thebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto
lookattherangeorstandarddeviationofactivationsorgradientsonasingle
minibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe
minibatchwillshrinkastheactivationspropagateforwardthroughthenetwork Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptably smallactivationsand
3 0 4
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
increasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable
initialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe
usefultolookattherangeorstandarddeviationofthegradientsaswellasthe
activations Thisprocedurecaninprinciplebeautomatedandisgenerallyless
computationally costlythanhyperparameter optimization basedonvalidationset
errorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona
singlebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation
set.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmore
formallyandstudiedby () MishkinandMatas2015
So far we have focused on the initialization ofthe weights.Fortunately,
initialization ofotherparametersistypicallyeasier Theapproachforsettingthebiasesmustbecoordinatedwiththeapproach
forsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight
initialization schemes.Thereareafewsituationswherewemaysetsomebiasesto
non-zerovalues:
•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiasto
obtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat
theinitialweightsaresmallenoughthattheoutputoftheunitisdetermined
onlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivation
functionappliedtothemarginalstatisticsoftheoutputinthetrainingset Forexample,iftheoutputisadistributionoverclassesandthisdistribution
isahighlyskeweddistributionwiththemarginalprobabilityofclass igiven
byelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving
theequationsoftmax (b) =c.Thisappliesnotonlytoclassiﬁersbutalsoto
modelswewillencounterinPart,suchasautoencodersandBoltzmann III
machines.Thesemodelshavelayerswhoseoutputshouldresembletheinput
datax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto
matchthemarginaldistributionover.x
•Sometimeswemay wanttochoosethebiastoavoidcausingtoo much
saturationatinitialization Forexample,wemaysetthebiasofaReLU
hiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization Thisapproachisnotcompatiblewithweightinitialization schemesthatdo
notexpectstronginputfromthebiasesthough.Forexample, itisnot
recommendedforusewithrandomwalkinitialization (,) Sussillo2014
•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina
function.Insuchsituations,wehaveaunitwithoutput uandanotherunit
h∈[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h We
3 0 5
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
canview hasagatethatdetermineswhether u h u≈or u h≈0 Inthese
situations,wewanttosetthebiasfor hsothat h≈1mostofthetimeat
initialization Otherwise udoesnothaveachancetolearn.Forexample,
Jozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1
theLSTMmodel,describedinsection.10.10
Anothercommontypeofparameterisavarianceorprecisionparameter.For
example,wecanperformlinearregressionwithaconditionalvarianceestimate
usingthemodel
p y y (| Nx) = (|wTx+1) b , /β (8.24)
where βisaprecisionparameter.Wecanusuallyinitializevarianceorprecision
parametersto1safely.Anotherapproachistoassumetheinitialweightsareclose
enoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,
thensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset
thevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame-
ters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon
strategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III
theparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming
supervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that
oﬀersfasterconvergencethanarandominitialization Someoftheseinitialization
strategiesmayyieldfasterconvergenceandbettergeneralization becausethey
encodeinformationaboutthedistributionintheinitialparametersofthemodel Othersapparentlyperformwellprimarilybecausetheysettheparameterstohave
therightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother 8.5AlgorithmswithAdaptiveLearningRates
Neuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone
ofthehyperparameters thatisthemostdiﬃculttosetbecauseithasasigniﬁcant
impactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2
costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive
toothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but
doessoattheexpenseofintroducinganotherhyperparameter Inthefaceofthis,
itisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof
sensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning
3 0 6
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
rateforeachparameter,andautomatically adapttheselearningratesthroughout
thecourseoflearning The algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988
toadaptingindividuallearningratesformodelparametersduringtraining.The
approachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect
toagivenmodelparameter,remainsthesamesign,thenthelearningrateshould
increase.Ifthepartialderivativewithrespecttothatparameterchangessign,
thenthelearningrateshoulddecrease Ofcourse,thiskindofrulecanonlybe
appliedtofullbatchoptimization Morerecently,anumberofincremental(ormini-batch-bas ed)methodshave
beenintroducedthatadaptthelearningratesofmodelparameters.Thissection
willbrieﬂyreviewafewofthesealgorithms 8.5.1AdaGrad
TheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4
ratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare
rootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011
parameterswiththelargestpartialderivativeofthelosshaveacorrespondingly
rapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives
havearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreater
progressinthemoregentlyslopeddirectionsofparameterspace Inthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome
desirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—for
trainingdeepneuralnetworkmodels—theaccumulation ofsquaredgradientsfrom
thebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe
eﬀectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning
models 8.5.2RMSProp
TheRMSPropalgorithm(,)modiﬁesAdaGradtoperformbetterin Hinton2012
thenon-convexsettingbychangingthegradientaccumulation intoanexponentially
weightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied
toaconvexfunction When appliedtoanon-convexfunctiontotrainaneural
network,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresand
eventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe
learningrateaccordingtotheentirehistoryofthesquaredgradientandmay
3 0 7
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Algorithm8.4TheAdaGradalgorithm
Require:Globallearningrate 
Require:Initialparameterθ
Require:Smallconstant,perhaps δ 10− 7,fornumericalstability
Initializegradientaccumulationvariabler= 0
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondingtargetsy( ) i Computegradient:g←1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
Accumulatesquaredgradient:rrgg ←+
Computeupdate: ∆θ←−
δ +√rg.(Divisionandsquarerootapplied
element-wise)
Applyupdate:θθθ ← +∆
endwhile
havemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure

============================================================

=== CHUNK 077 ===
Palavras: 353
Caracteres: 5972
--------------------------------------------------
RMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe
extremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifit
wereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl RMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5
Nesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6
movingaverageintroducesanewhyperparameter, ρ,thatcontrolsthelengthscale
ofthemovingaverage Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-
timizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to
optimization methodsbeingemployedroutinelybydeeplearningpractitioners 8.5.3Adam
Adam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014
algorithmandispresentedinalgorithm .Thename“Adam” derivesfrom 8.7
thephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itis
perhapsbestseenasavariantonthecombinationofRMSPropandmomentum
withafewimportantdistinctions.First,inAdam,momentumisincorporated
directlyasanestimateoftheﬁrstordermoment(withexponentialweighting)of
thegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto
applymomentumtotherescaledgradients.Theuseofmomentumincombination
withrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes
3 0 8
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Algorithm8.5TheRMSPropalgorithm
Require:Globallearningrate,decayrate  ρ
Require:Initialparameterθ
Require:Smallconstant δ, usually 10− 6, usedtostabilizedivision bysmall
numbers Initializeaccumulation variablesr= 0
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondingtargetsy( ) i Computegradient:g←1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
Accumulatesquaredgradient:rrgg ← ρ+(1 )− ρ
Computeparameterupdate: ∆θ=−√
δ + rg.(1√
δ + rappliedelement-wise)
Applyupdate:θθθ ← +∆
endwhile
biascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentum
term)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization
attheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7
(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,
unlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias
earlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice
ofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom
thesuggesteddefault 8.5.4ChoosingtheRightOptimizationAlgorithm
Inthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress
thechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach
modelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone
choose Unfortunately,thereiscurrentlynoconsensusonthispoint () Schauletal.2014
presentedavaluablecomparisonofalargenumberofoptimization algorithms
acrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof
algorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)
performedfairlyrobustly,nosinglebestalgorithmhasemerged Currently,themostpopularoptimization algorithmsactivelyinuseinclude
SGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta
andAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend
3 0 9
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Algorithm8.6RMSPropalgorithmwithNesterovmomentum
Require:Globallearningrate,decayrate,momentumcoeﬃcient  ρ α
Require:Initialparameter,initialvelocity θ v
Initializeaccumulation variabler= 0
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondingtargetsy( ) i Computeinterimupdate: ˜θθv ← + α
Computegradient:g←1
m∇ ˜ θ
i L f((x( ) i;˜θy) ,( ) i)
Accumulategradient:rrgg ← ρ+(1 )− ρ
Computevelocityupdate:vv← α−√rg.(1√rappliedelement-wise)
Applyupdate:θθv ← +
endwhile
largelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparameter
tuning) 8.6ApproximateSecond-OrderMethods
Inthissectionwediscusstheapplicationofsecond-ordermethodstothetraining
ofdeepnetworks.See ()foranearliertreatmentofthissubject LeCunetal.1998a
Forsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical
risk:
J() = θ E x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1
mm 
i = 1L f((x( ) i;)θ , y( ) i) .(8.25)
Howeverthemethodswediscusshereextendreadilytomoregeneralobjective
functionsthat,forinstance,includeparameterregularizationtermssuchasthose
discussedinchapter.7
8.6.1Newton’sMethod
Insection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst- 4.3
ordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove
optimization Themostwidelyusedsecond-ordermethodisNewton’smethod.We
nowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationto
neuralnetworktraining 3 1 0
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Algorithm8.7TheAdamalgorithm
Require:Stepsize(Suggesteddefault: )  0001 Require:Exponentialdecayratesformomentestimates, ρ 1and ρ 2in[0 ,1) (Suggesteddefaults:andrespectively) 09 Require:Smallconstant δusedfornumericalstabilization.(Suggesteddefault:
10− 8)
Require:Initialparametersθ
Initialize1stand2ndmomentvariables ,s= 0r= 0
Initializetimestep t= 0
while do stoppingcriterionnotmet
Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), ,x( ) m}with
correspondingtargetsy( ) i Computegradient:g←1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
t t←+1
Updatebiasedﬁrstmomentestimate:s← ρ 1s+(1− ρ 1)g
Updatebiasedsecondmomentestimate:r← ρ 2r+(1− ρ 2)gg
Correctbiasinﬁrstmoment:ˆs←s
1 − ρt
1
Correctbiasinsecondmoment:ˆr←r
1 − ρt
2
Computeupdate: ∆= θ− ˆs√
ˆ r + δ(operationsappliedelement-wise)
Applyupdate:θθθ ← +∆
endwhile
Newton’smethodisanoptimization schemebasedonusingasecond-orderTay-
lorseriesexpansiontoapproximate J(θ)nearsomepointθ 0,ignoringderivatives
ofhigherorder:
J J () θ≈(θ 0)+(θθ− 0)∇ θ J(θ 0)+1
2(θθ− 0)Hθθ (− 0) ,(8.26)
whereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Ifwethensolvefor
thecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:
θ∗= θ 0−H− 1∇ θ J(θ 0) (8.27)
Thusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescaling
thegradientbyH− 1,Newton’smethodjumpsdirectlytotheminimum

============================================================

=== CHUNK 078 ===
Palavras: 430
Caracteres: 7253
--------------------------------------------------
If the
objectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this
updatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’s
method,giveninalgorithm .8.8
3 1 1
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
Algorithm8.8Newton’smethodwithobjective J(θ) =
1
mm
i = 1 L f((x( ) i;)θ , y( ) i) Require:Initialparameterθ 0
Require:Trainingsetofexamples m
while do stoppingcriterionnotmet
Computegradient:g←1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
ComputeHessian:H←1
m∇2
θ
i L f((x( ) i;)θ ,y( ) i)
ComputeHessianinverse:H− 1
Computeupdate: ∆= θ−H− 1g
Applyupdate:θθθ = +∆
endwhile
Forsurfacesthatarenotquadratic,aslongastheHessianremainspositive
deﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step
iterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-
ingthequadraticapproximation) Second, updatetheparametersaccordingto
equation.8.27
Insection,wediscussedhowNewton’smethodisappropriateonlywhen 8.2.3
theHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjective
functionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that
areproblematicforNewton’smethod IftheeigenvaluesoftheHessianarenot
allpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactually
causeupdatestomoveinthewrongdirection.Thissituationcanbeavoided
byregularizingtheHessian.Commonregularizationstrategiesincludeaddinga
constant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes α
θ∗= θ 0−[(( H fθ 0))+ ] αI− 1∇ θ f(θ 0) (8.28)
Thisregularizationstrategyisusedinapproximations toNewton’smethod,such
astheLevenberg–Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and
worksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively
closetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the
valueof αwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues However,as αincreasesinsize,theHessianbecomesdominatedbythe αIdiagonal
andthedirectionchosenbyNewton’smethodconvergestothestandardgradient
dividedby α Whenstrongnegativecurvatureispresent, αmayneedtobeso
largethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith
aproperlychosenlearningrate Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,
3 1 2
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
suchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneural
networksislimitedbythesigniﬁcantcomputational burdenitimposes.The
numberofelementsintheHessianissquaredinthenumberofparameters,sowith
kparameters(andforevenverysmallneuralnetworksthenumberofparameters
kcanbeinthemillions),Newton’smethodwouldrequiretheinversionofa k k×
matrix—with computational complexityof O( k3).Also,sincetheparameterswill
changewitheveryupdate,theinverseHessianhastobecomputed ateverytraining
iteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters
canbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,
wewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’s
methodwhileside-steppingthecomputational hurdles 8.6.2ConjugateGradients
Conjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverse
Hessianbyiterativelydescendingconjugatedirections.Theinspirationforthis
approachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest
descent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3
thedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6
steepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀective
back-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,
whengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline
searchdirection Lettheprevioussearchdirectionbed t − 1.Attheminimum,wheretheline
searchterminates,thedirectionalderivativeiszeroindirectiond t − 1:∇ θ J(θ)·
d t − 1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,
d t=∇ θ J(θ) willhavenocontributioninthedirectiond t − 1.Thusd tisorthogonal
tod t − 1.Thisrelationshipbetweend t − 1andd tisillustratedinﬁgurefor8.6
multipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceof
orthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious
searchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby
descendingtotheminimuminthecurrentgradientdirection,wemustre-minimize
theobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat
theendofeachlinesearchweare,inasense,undoingprogresswehavealready
madeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients
seekstoaddressthisproblem Inthemethodofconjugategradients,weseektoﬁndasearchdirectionthat
isconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress
madeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes
3 1 3
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
                     
Figure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The
methodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline
deﬁnedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems
seenwithusingaﬁxedlearningrateinﬁgure,butevenwiththeoptimalstepsize 4.6
thealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,at
theminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointis
orthogonaltothatdirection theform:
d t= ∇ θ J β ()+θ td t − 1 (8.29)
where β tisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,d t − 1,
weshouldaddbacktothecurrentsearchdirection Twodirections,d tandd t − 1,aredeﬁnedasconjugateifd
tHd t − 1= 0,where
HistheHessianmatrix Thestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe
eigenvectorsofHtochoose β t,whichwouldnotsatisfyourgoalofdeveloping
amethodthatismorecomputationally viablethanNewton’smethodforlarge
problems Canwecalculatetheconjugatedirectionswithoutresortingtothese
calculations?Fortunatelytheanswertothatisyes Twopopularmethodsforcomputingthe β tare:
1 Fletcher-Reeves:
β t=∇ θ J(θ t)∇ θ J(θ t)
∇ θ J(θ t − 1)∇ θ J(θ t − 1)(8.30)
3 1 4
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
2 Polak-Ribière:
β t=(∇ θ J(θ t)−∇ θ J(θ t − 1))∇ θ J(θ t)
∇ θ J(θ t − 1)∇ θ J(θ t − 1)(8.31)
Foraquadraticsurface,theconjugatedirectionsensurethatthegradientalong
thepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe
minimumalongthepreviousdirections.Asaconsequence,ina k-dimensional
parameterspace,theconjugategradientmethodrequiresatmost klinesearchesto
achievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9
Algorithm8.9Theconjugategradientmethod
Require:Initialparametersθ 0
Require:Trainingsetofexamples m
Initializeρ 0= 0
Initialize g 0= 0
Initialize t= 1
while do stoppingcriterionnotmet
Initializethegradientg t= 0
Computegradient:g t←1
m∇ θ
i L f((x( ) i;)θ ,y( ) i)
Compute β t=( g t − g t −1 )g t
g
t −1g t −1(Polak-Ribière)
(Nonlinearconjugategradient:optionallyreset β ttozero,forexampleif tis
amultipleofsomeconstant,suchas) k k= 5
Computesearchdirection:ρ t= −g t+ β tρ t − 1
Performlinesearchtoﬁnd: ∗= argmin 1
mm
i = 1 L f((x( ) i;θ t+ ρ t) ,y( ) i)
(Onatrulyquadraticcostfunction,analyticallysolvefor ∗ratherthan
explicitlysearchingforit)
Applyupdate:θ t + 1= θ t+ ∗ρ t
t t←+1
endwhile
NonlinearConjugateGradients:Sofarwehavediscussedthemethodof
conjugategradientsasitisappliedtoquadraticobjectivefunctions

============================================================

=== CHUNK 079 ===
Palavras: 386
Caracteres: 9084
--------------------------------------------------
Ofcourse,
ourprimaryinterestinthischapteristoexploreoptimization methodsfortraining
neuralnetworksandotherrelateddeeplearningmodelswherethecorresponding
objectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof
conjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirections
3 1 5
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
arenolongerassuredtoremainattheminimumoftheobjectiveforprevious
directions.Asaresult,thenonlinearconjugategradientsalgorithmincludes
occasionalresetswherethemethodofconjugategradientsisrestartedwithline
searchalongtheunalteredgradient Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugate
gradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialto
initializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore
commencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate
gradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch
versionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshave
beenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller
1993) 8.6.3BFGS
TheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptsto
bringsomeoftheadvantagesofNewton’smethodwithoutthecomputational
burden.In thatrespect, BFGSissimilartotheconjugategradientmethod However,BFGStakesamoredirectapproachtotheapproximation ofNewton’s
update.RecallthatNewton’supdateisgivenby
θ∗= θ 0−H− 1∇ θ J(θ 0) , (8.32)
whereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Theprimary
computational diﬃcultyinapplyingNewton’supdateisthecalculationofthe
inverseHessianH− 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich
theBFGSalgorithmisthemostprominent)istoapproximate theinversewith
amatrixM tthatisiterativelyreﬁnedbylowrankupdatestobecomeabetter
approximationofH− 1 ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany
textbooksonoptimization, includingLuenberger1984() OncetheinverseHessianapproximationM tisupdated,thedirectionofdescent
ρ tisdeterminedbyρ t=M tg t.Alinesearchisperformedinthisdirectionto
determinethesizeofthestep, ∗,takeninthisdirection.Theﬁnalupdatetothe
parametersisgivenby:
θ t + 1= θ t+ ∗ρ t (8.33)
Likethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof
linesearcheswiththedirectionincorporatingsecond-orderinformation However
3 1 6
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
unlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent
onthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend
lesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust
storetheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS
impracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof
parameters Limited Memory BFGS (or L-BFGS)The memory costs ofthe BFGS
algorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverse
HessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM
usingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption
thatM( 1 ) t −istheidentitymatrix,ratherthanstoringtheapproximation fromone
steptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGS
aremutuallyconjugate.However,unlikethemethodofconjugategradients,this
procedureremainswellbehavedwhentheminimumofthelinesearchisreached
onlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe
generalizedtoincludemoreinformationabouttheHessianbystoringsomeofthe
vectorsusedtoupdateateachtimestep,whichcostsonlyperstep M O n()
8.7OptimizationStrategiesandMeta-Algorithms
Manyoptimization techniquesarenotexactlyalgorithms, butrathergeneral
templatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe
incorporatedintomanydiﬀerentalgorithms 8.7.1BatchNormalization
Batchnormalization ( ,)isoneofthemostexcitingrecent IoﬀeandSzegedy2015
innovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization
algorithmatall.Instead,itisamethodofadaptivereparametrization, motivated
bythediﬃcultyoftrainingverydeepmodels Verydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The
gradienttellshowtoupdateeachparameter,undertheassumptionthattheother
layersdonotchange.Inpractice,weupdateallofthelayerssimultaneously Whenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions
composedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed
undertheassumptionthattheotherfunctionsremainconstant.Asasimple
3 1 7
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
example,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer
anddoesnotuseanactivationfunctionateachhiddenlayer:ˆ y= x w 1 w 2 w 3 Here, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i − 1 w i Theoutput ˆ yisalinearfunctionoftheinput x,butanonlinearfunctionofthe
weights w i.Supposeourcostfunctionhasputagradientofon1 ˆ y,sowewishto
decreaseˆ yslightly.Theback-propagationalgorithmcanthencomputeagradient
g=∇ wˆ y.Considerwhathappenswhenwemakeanupdatewwg ← − .The
ﬁrst-orderTaylorseriesapproximation ofˆ ypredictsthatthevalueofˆ ywilldecrease
by gg.Ifwewantedtodecreaseˆ yby .1,thisﬁrst-orderinformationavailablein
thegradientsuggestswecouldsetthelearningrate to 1
gg.However,theactual
updatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforder l Thenewvalueofˆ yisgivenby
x w( 1−  g 1)( w 2−  g 2)( (8.34)
Anexampleofonesecond-ordertermarisingfromthisupdateis 2g 1 g 2l
i = 3 w i Thistermmightbenegligibleifl
i = 3 w iissmall,ormightbeexponentiallylarge
iftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1
tochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetothe
parametersforonelayerdependssostronglyonalloftheotherlayers.Second-order
optimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese
second-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,
evenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimization
algorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent
themfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions Building
an n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe
doinstead Batchnormalization providesanelegantwayofreparametrizing almostanydeep
network.Thereparametrization signiﬁcantlyreducestheproblemofcoordinating
updatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput
orhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer
tonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample
appearinginarowofthematrix.Tonormalize,wereplaceitwith H
H=Hµ−
σ, (8.35)
whereµisavectorcontainingthemeanofeachunitandσisavectorcontaining
thestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting
thevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Within
eachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting µ j
3 1 8
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
anddividingby σ j.TherestofthenetworkthenoperatesonHinexactlythe
samewaythattheoriginalnetworkoperatedon.H
Attrainingtime,
µ=1
m
iH i , : (8.36)
and
σ=
δ+1
m
i( )Hµ−2
i , (8.37)
where δisasmallpositivevaluesuchas10− 8imposedtoavoidencountering
theundeﬁnedgradientof√zat z=0.Crucially, weback-propagatethrough
theseoperationsforcomputingthemeanandthestandarddeviation,andfor
applyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose
anoperation that actssimplytoincreasethestandard deviationormeanof
h i;thenormalization operationsremovetheeﬀectofsuchanactionandzero
outitscomponentinthegradient.Thiswasamajorinnovationofthebatch
normalization approach Previous approacheshadinvolvedaddingpenaltiesto
thecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor
involvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep Theformerapproachusuallyresultedinimperfectnormalization andthelatter
usuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedly
proposedchangingthemeanandvarianceandthenormalization steprepeatedly
undidthischange.Batchnormalization reparametrizes themodeltomakesome
unitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems Attesttime,µandσmaybereplacedbyrunningaveragesthatwerecollected
duringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,
withoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch Revisitingtheˆ y= x w 1 w 2 w lexample,weseethatwecanmostlyresolvethe
diﬃcultiesinlearningthismodelbynormalizing h l − 1.Supposethat xisdrawn
fromaunitGaussian.Then h l − 1willalsocomefromaGaussian,becausethe
transformationfrom xto h lislinear.However, h l − 1willnolongerhavezeromean
andunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized
ˆh l − 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany
updatetothelowerlayers,ˆh l − 1willremainaunitGaussian.Theoutput ˆ ymay
thenbelearnedasasimplelinearfunction ˆ y= w lˆ h l − 1.Learninginthismodelis
nowverysimplebecausetheparametersatthelowerlayerssimplydonothavean
eﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian

============================================================

=== CHUNK 080 ===
Palavras: 427
Caracteres: 9713
--------------------------------------------------
In
somecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelower
layerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0
3 1 9
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
ofoneofthelowerweightscanﬂiptherelationshipbetweenˆ h l − 1and y These
situationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave
anextremeeﬀectonthestatisticsof h l − 1.Batchnormalization hasthusmade
thismodelsigniﬁcantlyeasiertolearn Inthisexample,theeaseoflearningof
coursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,
thelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhave
anybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecond
orderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneural
networkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear
transformationsofthedata,sotheyremainuseful.Batchnormalization actsto
standardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,
butallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle
unittochange Becausetheﬁnallayerofthenetworkisabletolearnalineartransformation,
wemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina
layer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015
theinspirationforbatchnormalization Unfortunately, eliminating alllinear
interactionsismuchmoreexpensivethanstandardizingthemeanandstandard
deviationofeachindividualunit,andsofarbatchnormalization remainsthemost
practicalapproach Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressive
powerofthe neuralnetworkcontainingthatunit.Inordertomaintainthe
expressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit
activationsHwithγH+βratherthansimplythenormalizedH.Thevariables
γandβarelearnedparametersthatallowthenewvariabletohaveanymean
andstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidweset
themeanto 0,andthenintroduceaparameterthatallowsittobesetbackto
anyarbitraryvalueβ?Theansweristhatthenewparametrization canrepresent
thesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew
parametrization hasdiﬀerentlearningdynamics.Intheoldparametrization, the
meanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters
inthelayersbelowH.Inthenewparametrization, themeanofγH+βis
determinedsolelybyβ.Thenewparametrization ismucheasiertolearnwith
gradientdescent Mostneuralnetworklayerstaketheformof φ(XW+b)where φissome
ﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.It
isnaturaltowonderwhetherweshouldapplybatchnormalization totheinput
X,ortothetransformedvalueXW+b ()recommend IoﬀeandSzegedy2015
3 2 0
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
thelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversion
ofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith
the βparameterappliedbythebatchnormalization reparametrization Theinput
toalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe
rectiﬁedlinearfunctioninapreviouslayer Thestatisticsoftheinputarethus
morenon-Gaussianandlessamenabletostandardizationbylinearoperations Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9
samenormalizing µand σateveryspatiallocationwithinafeaturemap,sothat
thestatisticsofthefeaturemapremainthesameregardlessofspatiallocation 8.7.2CoordinateDescent
Insomecases,itmaybepossibletosolveanoptimization problemquicklyby
breakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle
variable x i, then minimize it with respect to another variable x jand soon,
repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)
minimum.Thispracticeisknownascoordinatedescent,becauseweoptimize
onecoordinateatatime Moregenerally,blockcoordinatedescentrefersto
minimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm
“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellas
thestrictlyindividualcoordinatedescent Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesinthe
optimization problemcanbeclearlyseparatedintogroupsthatplayrelatively
isolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis
signiﬁcantlymoreeﬃcientthanoptimization withrespecttoallofthevariables Forexample,considerthecostfunction
J ,(HW) =
i , j| H i , j|+
i , j
XW−H2
i , j.(8.38)
Thisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis
toﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues
HtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso
involveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder
topreventthepathologicalsolutionwithextremelysmallandlarge.HW
Thefunction Jisnotconvex.However, wecandividetheinputstothe
trainingalgorithmintotwosets:thedictionaryparametersWandthecode
representationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof
thesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives
3 2 1
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
usanoptimization strategythatallowsustouseeﬃcientconvexoptimization
algorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizing
HWwithﬁxed Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariable
stronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunction f(x) =
( x 1− x 2)2+ α
x2
1+ x2
2
where αisapositiveconstant.Theﬁrsttermencourages
thetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem
tobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolve
theprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem However,forsmall α,coordinatedescentwillmakeveryslowprogressbecausethe
ﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀers
signiﬁcantlyfromthecurrentvalueoftheothervariable 8.7.3PolyakAveraging
Polyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral
points inthe trajectory through parameter spacevisited by anoptimization
algorithm If titerationsofgradientdescentvisitpointsθ( 1 ), ,θ( ) t,thenthe
outputofthePolyakaveragingalgorithmisˆθ( ) t=1
t
iθ( ) i Onsomeproblem
classes,suchasgradientdescentappliedtoconvexproblems,thisapproachhas
strongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcation
ismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe
optimization algorithmmayleapbackandforthacrossavalleyseveraltimes
withoutevervisitingapointnearthebottomofthevalley.Theaverageofallof
thelocationsoneithersideshouldbeclosetothebottomofthevalleythough Innon-convexproblems,thepathtakenbytheoptimization trajectorycanbe
verycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameter
spacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge
barriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,
whenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean
exponentiallydecayingrunningaverage:
ˆθ( ) t= αˆθ( 1 ) t −+(1 )− αθ( ) t (8.39)
Therunningaverageapproachisusedinnumerousapplications.SeeSzegedy
etal.()forarecentexample 2015
3 2 2
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
8.7.4SupervisedPretraining
Sometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitious
ifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itis
sometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmake
themodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolve
asimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthat
involvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof
trainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas
pretraining Greedyalgorithmsbreakaproblemintomanycomponents,thensolvefor
theoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe
individuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete
solution.However,greedyalgorithmscanbecomputationally muchcheaperthan
algorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution
isoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya
ﬁne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal
solutiontothefullproblem.Initializingthejointoptimization algorithmwitha
greedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit
ﬁnds Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousin
deeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithms
thatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning
problems.Thisapproachisknownas greedysupervisedpretraining
Intheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007
eachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof
thelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretraining
isillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7
ofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained
hiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman
()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015
theﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeeper
networks(withuptonineteenlayersofweights).Themiddlelayersofthenew,
verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained Anotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs
trainedMLPs,aswellastherawinput,asinputsforeachaddedstage Why would greedy sup ervised pretraining help?The hypothesis  initially
discussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007
3 2 3
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
y y
h( 1 )h( 1 )
x x
( a )U( 1 )U( 1 )
W( 1 )W( 1 ) y yh( 1 )h( 1 )
x x
( b )U( 1 )U( 1 )W( 1 )W( 1 )
y yh( 1 )h( 1 )
x x
( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )
y y U( 2 )U( 2 ) W( 2 )W( 2 )
y yh( 1 )h( 1 )
x x
( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y
U( 2 )U( 2 )
W( 2 )W( 2 )
Figure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,)

============================================================

=== CHUNK 081 ===
Palavras: 390
Caracteres: 12624
--------------------------------------------------
Bengio e t a l .2007
( a )Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe ( b )
samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )
discardthehidden-to-outputlayer.Wesendtheoutputoftheﬁrsthiddenlayerasinput
toanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective
astheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas
manylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork ( d )
Tofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyat
theendorateachstageofthisprocess 3 2 4
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
intermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin
termsofoptimization andintermsofgeneralization Anapproachrelatedtosupervisedpretrainingextendstheideatothecontext
oftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8
layersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)
andtheninitializeasame-sizenetworkwiththeﬁrst klayersoftheﬁrstnet.All
thelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are
thenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000
ImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetof
tasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin
section.15.2
AnotherrelatedlineofworkistheFitNets( ,)approach Romeroetal.2015
Thisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat
enoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen
becomesateacherforasecondnetwork,designatedthestudent.Thestudent
networkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe
diﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthe
studentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict
theoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer
oftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe
hiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional
parametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork
fromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting
theﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayer
oftheteachernetwork Thelowerlayersofthestudentnetworksthushavetwo
objectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as
wellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin
anddeepnetworkappearstobemorediﬃculttotrainthanawideandshallow
network,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower
computational costifitisthinenoughtohavefarfewerparameters.Without
thehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe
experiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus
beoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃcultto
train,butotheroptimization techniquesorchangesinthearchitecturemayalso
solvetheproblem 3 2 5
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
8.7.5DesigningModelstoAidOptimization
Toimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization
algorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave
comefromdesigningthemodelstobeeasiertooptimize Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein
jaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely
diﬃcult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto
optimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin
neuralnetworklearningoverthepast30yearshavebeenobtainedbychanging
themodelfamilyratherthanchangingtheoptimization procedure.Stochastic
gradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe
1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications Speciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-
formationsbetweenlayersandactivationfunctionsthatarediﬀerentiable almost
everywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain Inpar-
ticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunits
haveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep
networksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake
optimization easier.Thegradientﬂowsthroughmanylayersprovidedthatthe
Jacobianofthelineartransformationhasreasonablesingularvalues Moreover,
linearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’s
outputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient
whichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,
modernneuralnetshavebeendesignedsothattheirlocalgradientinformation
correspondsreasonablywelltomovingtowardadistantsolution Othermodeldesignstrategiescanhelptomakeoptimization easier.For
example,linearpathsorskipconnectionsbetweenlayersreducethelengthof
theshortestpathfromthelower layer’sparameters totheoutput, and thus
mitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea
toskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe
intermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a
anddeeply-supervisednets(,).These“auxiliaryheads”aretrained Leeetal.2014
toperformthesametaskastheprimaryoutputatthetopofthenetworkinorder
toensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete
theauxiliaryheadsmaybediscarded Thisisanalternativetothepretraining
strategies,whichwereintroducedintheprevioussection.Inthisway,onecan
trainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat
intermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey
3 2 6
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
shoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers 8.7.6ContinuationMethodsandCurriculumLearning
Asarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7
globalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter
estimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis
problemistoattempttoinitializetheparametersinaregionthatisconnected
tothesolutionbyashortpaththroughparameterspacethatlocaldescentcan
discover Continuationmethodsareafamilyofstrategiesthatcanmakeoptimization
easierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof
itstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis
toconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto
minimizeacostfunction J(θ),wewillconstructnewcostfunctions { J( 0 ), Thesecostfunctionsaredesignedtobeincreasinglydiﬃcult,with J( 0 )beingfairly
easytominimize,and J( ) n,themostdiﬃcult,being J(θ),thetruecostfunction
motivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we
meanthatitiswellbehavedovermoreofθspace.Arandominitialization ismore
likelytolandintheregionwherelocaldescentcanminimizethecostfunction
successfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned
sothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby
solvinganeasyproblemthenreﬁnethesolutiontosolveincrementally harder
problemsuntilwearriveatasolutiontothetrueunderlyingproblem Traditionalcontinuationmethods(predatingtheuseofcontinuationmethods
forneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction SeeWu1997()foranexampleofsuchamethodandareviewofsomerelated
methods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,
whichaddsnoisetotheparameters(Kirkpatrick 1983etal.,).Continuation
methodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher
()foranoverviewofrecentliterature,especiallyforAIapplications 2015
Continuationmethodstraditionallyweremostlydesignedwiththegoalof
overcomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedto
reachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,
thesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”the
originalcostfunction.Thisblurringoperationcanbedonebyapproximating
J( ) i() = θ Eθ∼ N ( θ; θ , σ()2 i) J(θ) (8.40)
viasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions
3 2 7
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
becomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves
enoughinformationaboutthelocationofaglobalminimumthatwecanﬁndthe
globalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This
approachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁne
aseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfrom
onefunctiontothenextarrivingattheglobalminimum,butitmightrequireso
manyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh NP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods
areapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond
tothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,
nomatterhowmuchitisblurred.Considerforexamplethefunction J(θ) =−θθ Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum
ofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe
originalcostfunction Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe
problemoflocalminima,localminimaarenolongerbelievedtobetheprimary
problemforneuralnetworkoptimization Fortunately,continuationmethodscan
stillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan
eliminateﬂatregions,decreasevarianceingradientestimates,improveconditioning
oftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates
easiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections
andprogresstowardaglobalsolution Bengio2009etal.()observedthatanapproachcalledcurriculumlearning
orshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis
basedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts
andprogresstolearningmorecomplexconceptsthatdependonthesesimpler
concepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal
training(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine
learning(,;,;,) () Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009
justiﬁedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby
increasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributions
tothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),and
experimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga
curriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning
hasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;
Collobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer
vision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013
tasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayin
whichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011
3 2 8
CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS
moreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurface
withthelessobviouscases.Curriculum-based strategiesaremoreeﬀectivefor
teachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan
alsoincreasetheeﬀectivenessofotherteachingstrategies( , BasuandChristensen
2013) Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthe
contextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:
ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha
stochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalways
presentedtothelearner,butwheretheaverageproportionofthemorediﬃcult
examples(here,thosewithlonger-termdependencies)isgraduallyincreased.With
adeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining
fromthefulltrainingset)wasobserved Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto
regularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof
theneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand
processinputdatathathasspecialstructure.Theoptimization methodsdiscussed
inthischapterareoftendirectlyapplicabletothesespecializedarchitectures with
littleornomodiﬁcation 3 2 9
C h a p t e r 9
C on v ol u t i on al N e t w orks
Con v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al
net w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata
thathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan
bethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,
whichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen
tremendouslysuccessfulinpracticalapplications.Thename“convolutionalneural
network”indicatesthatthenetworkemploysamathematical operationcalled
c o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional
networksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix
multiplicationinatleastoneoftheirlayers

============================================================

=== CHUNK 082 ===
Palavras: 354
Caracteres: 6996
--------------------------------------------------
Inthis chapter, wewillﬁrst describewhatconvolutionis.Next, wewill
explainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen
describeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks
employ.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot
correspondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuch
asengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe
convolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We
willalso show how convolutionmaybeappliedtomanykindsofdata, with
diﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolution
moreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁc
principlesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,
thenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed
inthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto
choosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris
todescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11
330
CHAPTER9.CONVOLUTIONALNETWORKS
describesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances Researchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew
bestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,
renderingitimpracticaltodescribethebestarchitectureinprint.However,the
bestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed
here 9.1TheConvolutionOperation
Initsmostgeneralform,convolutionisanoperationontwofunctionsofareal-
valuedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamples
oftwofunctionswemightuse Supposewearetrackingthelocationofaspaceshipwithalasersensor.Our
lasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime
t.Both xand tarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelaser
sensoratanyinstantintime Nowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy
estimateofthespaceship’sposition,wewouldliketoaveragetogetherseveral
measurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill
wantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements Wecandothiswithaweightingfunction w( a),where aistheageofameasurement Ifweapplysuchaweightedaverageoperationateverymoment,weobtainanew
functionprovidingasmoothedestimateofthepositionofthespaceship: s
s t() =
x a w t a d a ()( −) (9.1)
Thisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically
denotedwithanasterisk:
s t x w t () = ( ∗)() (9.2)
Inourexample, wneedstobeavalidprobabilitydensityfunction,orthe
outputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0
oritwilllookintothefuture,whichispresumablybeyondourcapabilities.These
limitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁned
foranyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforother
purposesbesidestakingweightedaverages Inconvolutionalnetworkterminology,theﬁrstargument(inthisexample,the
function x)totheconvolutionisoftenreferredtoasthe i nputandthesecond
3 3 1
CHAPTER9.CONVOLUTIONALNETWORKS
argument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes
referredtoasthe f e at ur e m ap
Inourexample,theideaofalasersensorthatcanprovidemeasurements
ateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona
computer,timewillbediscretized,andoursensorwillprovidedataatregular
intervals.Inourexample,itmightbemorerealistictoassumethatourlaser
providesameasurementoncepersecond.Thetimeindex tcanthentakeononly
integervalues.Ifwenowassumethat xand waredeﬁnedonlyoninteger t,we
candeﬁnethediscreteconvolution:
s t x w t () = ( ∗)() =∞
a = − ∞x a w t a ()( −) (9.3)
Inmachinelearningapplications,theinputisusuallyamultidimensional array
ofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare
adaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays
astensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored
separately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe
ﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe
canimplementtheinﬁnitesummationasasummationoveraﬁnitenumberof
arrayelements Finally,weoftenuseconvolutionsovermorethanoneaxisatatime.For
example,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant
touseatwo-dimensionalkernel: K
S i , j I K i , j () = ( ∗)() =
m
nI m , n K i m , j n ( )( − −)(9.4)
Convolutioniscommutative,meaningwecanequivalentlywrite:
S i , j K I i , j () = ( ∗)() =
m
nI i m , j n K m , n ( − −)( )(9.5)
Usuallythelatterformulaismorestraightforwardtoimplementinamachine
learninglibrary,becausethereislessvariationintherangeofvalidvaluesof m
and n
Thecommutativepropertyofconvolutionarisesbecausewehave ﬂi pp e dthe
kernelrelativetotheinput,inthesensethatas mincreases,theindexintothe
inputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂip
thekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty
3 3 2
CHAPTER9.CONVOLUTIONALNETWORKS
isusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural
networkimplementation.Instead,manyneuralnetworklibrariesimplementa
relatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution
butwithoutﬂippingthekernel:
S i , j I K i , j () = ( ∗)() =
m
nI i m , j n K m , n (+ +)( )(9.6)
Manymachinelearninglibrariesimplementcross-correlationbutcallitconvolution Inthistextwewillfollowthisconventionofcallingbothoperationsconvolution,
andspecifywhetherwemeantoﬂipthekernelornotincontextswherekernel
ﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill
learntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm
basedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelative
tothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorarefor
convolutiontobeusedaloneinmachinelearning;insteadconvolutionisused
simultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes
notcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelor
not Seeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1
toa2-Dtensor Discreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the
matrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,
forunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe
equaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z
m at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto
convolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto
eachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix
whoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch
smallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith
matrixmultiplication anddoesnotdependonspeciﬁcpropertiesofthematrix
structureshouldworkwithconvolution,withoutrequiringanyfurtherchanges
totheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof
furtherspecializationsinordertodealwithlargeinputseﬃciently,buttheseare
notstrictlynecessaryfromatheoreticalperspective

============================================================

=== CHUNK 083 ===
Palavras: 422
Caracteres: 3681
--------------------------------------------------
3 3 3
CHAPTER9.CONVOLUTIONALNETWORKS
a b c d
e f g h
i j k lw x
y z
a w + b x +
e y + f za w + b x +
e y + f zb w + c x +
f y + g zb w + c x +
f y + g zc w + d x +
g y + h zc w + d x +
g y + h z
e w + f x +
i y + j ze w + f x +
i y + j zf w + g x +
j y + k zf w + g x +
j y + k zg w + h x +
k y + l zg w + h x +
k y + l zI nput
K e r ne l
O ut put
Figure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestrict
theoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”
convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left
elementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding
upper-leftregionoftheinputtensor 3 3 4
CHAPTER9.CONVOLUTIONALNETWORKS
9.2Motivation
Convolutionleveragesthreeimportantideasthatcanhelpimproveamachine
learningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t
r e pr e se n t at i o ns.Moreover, convolutionprovidesameansforworkingwith
inputsofvariablesize.Wenowdescribeeachoftheseideasinturn Traditionalneuralnetworklayersusematrixmultiplicationbyamatrixof
parameterswithaseparateparameterdescribingtheinteractionbetweeneachinput
unitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput
unit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also
referredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby
makingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,
theinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,
meaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof
pixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe
memoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italso
meansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements
ineﬃciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then
matrixmultiplication requires m n ×parametersandthealgorithmsusedinpractice
have O( m n ×)runtime(perexample).Ifwelimitthenumberofconnections
eachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly
k n ×parametersand O( k n ×)runtime.Formanypracticalapplications,itis
possibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping
kseveralordersofmagnitudesmallerthan m Forgraphicaldemonstrationsof
sparseconnectivity,seeﬁgureandﬁgure.Inadeepconvolutionalnetwork, 9.2 9.3
unitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,
asshowninﬁgure.Thisallowsthenetworktoeﬃcientlydescribecomplicated 9.4
interactionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple
buildingblocksthateachdescribeonlysparseinteractions P ar amet e r shar i ngreferstousingthesameparameterformorethanone
functioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix
isusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby
oneelementoftheinputandthenneverrevisited.Asasynonymforparameter
sharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe
weightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In
aconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition
oftheinput(exceptperhapssomeoftheboundarypixels, dependingonthe
designdecisionsregardingtheboundary).Theparametersharingusedbythe
convolutionoperationmeansthatratherthanlearningaseparatesetofparameters
3 3 5
CHAPTER9.CONVOLUTIONALNETWORKS
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
Figure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,
andalsohighlighttheoutputunitsin sthatareaﬀectedbythisunit

============================================================

=== CHUNK 084 ===
Palavras: 357
Caracteres: 2635
--------------------------------------------------
( T o p )When sis
formedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby 3 x ( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s
alloftheoutputsareaﬀectedby x 3 3 3 6
CHAPTER9.CONVOLUTIONALNETWORKS
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
Figure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e :  Wehighlightoneoutputunit, s 3,
andalsohighlighttheinputunitsin xthataﬀectthisunit.Theseunitsareknown
asthereceptiveﬁeldof s 3 ( T o p )When sisformedbyconvolutionwithakernelof
width,onlythreeinputsaﬀect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,
connectivityisnolongersparse,soalloftheinputsaﬀect s 3 x 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3
x 4 x 4h 4 h 4
x 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5
Figure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetwork
islargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesif
thenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling 9.12
(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare
verysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe
inputimage 3 3 7
CHAPTER9.CONVOLUTIONALNETWORKS
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5
Figure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular
parameterintwodiﬀerentmodels ( T o p )Theblackarrowsindicateusesofthecentral
elementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this
singleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )
theuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel
hasnoparametersharingsotheparameterisusedonlyonce foreverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeof
forwardpropagation—it isstill O( k n ×)—butitdoesfurtherreducethestorage
requirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders
ofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis
practicallyinsigniﬁcantcomparedto m n ×.Convolutionisthusdramatically more
eﬃcientthandensematrixmultiplication intermsofthememoryrequirements
andstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,
seeﬁgure.9.5
Asanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6
howsparseconnectivityandparametersharingcandramatically improvethe
eﬃciencyofalinearfunctionfordetectingedgesinanimage

============================================================

=== CHUNK 085 ===
Palavras: 452
Caracteres: 8942
--------------------------------------------------
Inthecaseofconvolution,theparticularformofparametersharingcausesthe
layertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis
equivariantmeansthatiftheinputchanges,theoutputchangesinthesameway Speciﬁcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)) Inthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,
i.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I
beafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction
3 3 8
CHAPTER9.CONVOLUTIONALNETWORKS
mappingoneimagefunctiontoanotherimagefunction,suchthat I= g( I)is
theimagefunctionwith I( x , y)= I( x −1 , y).Thisshiftseverypixelof Ione
unittotheright.Ifweapplythistransformationto I,thenapplyconvolution,
theresultwillbethesameasifweappliedconvolutionto I,thenappliedthe
transformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans
thatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeatures
appearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact
samerepresentationofitwillappearintheoutput,justlaterintime.Similarly
withimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin
theinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe
sameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction
ofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput
locations.Forexample,whenprocessingimages,itisusefultodetectedgesin
theﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless
everywhereintheimage,soitispracticaltoshareparametersacrosstheentire
image.Insomecases,wemaynotwishtoshareparametersacrosstheentire
image.Forexample,ifweareprocessingimagesthatarecroppedtobecentered
onanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerent
locations—thepartofthenetworkprocessingthetopofthefaceneedstolookfor
eyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto
lookforachin Convolutionisnotnaturallyequivarianttosomeothertransformations,such
aschangesinthescaleorrotationofanimage.Othermechanismsarenecessary
forhandlingthesekindsoftransformations Finally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedby
matrixmultiplication withaﬁxed-shapematrix.Convolutionenablesprocessing
ofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7
9.3Pooling
Atypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7
Intheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproducea
setoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough
anonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction Thisstageissometimescalledthe det e c t o rstage Inthethirdstage,weusea
p o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther Apoolingfunctionreplacestheoutputofthenetatacertainlocationwitha
summarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou
3 3 9
CHAPTER9.CONVOLUTIONALNETWORKS
Figure9.6: E ﬃ c i e n c y o f e d g e d e t e c t i o n Theimageontherightwasformedbytaking
eachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe
left Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,
whichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall Theinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This
transformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and
requires319 ×280 ×3=267 ,960ﬂoatingpointoperations(twomultiplicationsand
oneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame
transformationwithamatrixmultiplicationwouldtake320 ×280 ×319 ×280,orover
eightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientfor
representingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm
performsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000
timesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe
zero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication
andconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute Thematrixwouldstillneedtocontain2 ×319 ×280=178 ,640entries.Convolution
isanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelinear
transformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula
Goodfellow)
3 4 0
CHAPTER9.CONVOLUTIONALNETWORKS
Convolutional Layer
Input to layerConvolution stage:
Ane transform ﬃDetector stage:
Nonlinearity
e.g., rectiﬁed linearPooling stageNext layer
Input to layersConvolution layer:
Ane transform  ﬃDetector layer: Nonlinearity
e.g., rectiﬁed linearPooling layerNext layerComplex layer terminology Simple layer terminology
Figure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo
commonlyusedsetsofterminologyfordescribingtheselayers ( L e f t )Inthisterminology,
theconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with
eachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemapping
betweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology ( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple
layers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat
notevery“layer”hasparameters 3 4 1
CHAPTER9.CONVOLUTIONALNETWORKS
andChellappa1988,)operationreportsthemaximumoutputwithinarectangular
neighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular
neighborhood,the L2normofarectangularneighborhood,oraweightedaverage
basedonthedistancefromthecentralpixel Inallcases,poolinghelpstomaketherepresentationbecomeapproximately
i n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat
ifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled
outputsdonotchange.Seeﬁgureforanexampleofhowthisworks 9.8 Invariance
tolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether
somefeatureispresentthanexactlywhereitis.Forexample,whendetermining
whetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith
pixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside
ofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore
importanttopreservethelocationofafeature.Forexample,ifwewanttoﬁnda
cornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreserve
thelocationoftheedgeswellenoughtotestwhethertheymeet Theuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthat
thefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis
assumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork Poolingoverspatialregionsproducesinvariancetotranslation,butifwepool
overtheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn
whichtransformationstobecomeinvariantto(seeﬁgure).9.9
Becausepoolingsummarizestheresponsesoverawholeneighborhood,itis
possibletousefewerpoolingunitsthandetectorunits,byreportingsummary
statisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See
ﬁgureforanexample.Thisimprovesthecomputational eﬃciencyofthe 9.10
networkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When
thenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas
whenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this
reductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyand
reducedmemoryrequirementsforstoringtheparameters Formanytasks,poolingisessentialforhandlinginputsofvaryingsize For
example,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcation
layermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan
oﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthe
samenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the
ﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummary
statistics,oneforeachquadrantofanimage,regardlessoftheimagesize 3 4 2
CHAPTER9.CONVOLUTIONALNETWORKS
0 .D E T E C T O R   S T A GEP O O L I N G  ST A GE
P O O L I N G  ST A GE
D E T E C T O R   S T A GE
Figure9.8:Maxpoolingintroducesinvariance ( T o p )Aviewofthemiddleoftheoutput
ofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop
rowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions
andapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )
theinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas
changed,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling
unitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation 3 4 3
CHAPTER9.CONVOLUTIONALNETWORKS
L ar ge   r e s pon s e
i n  po ol i ng uni tL ar ge   r e s pon s e
i n  po ol i ng uni t
L ar ge
r e s ponse
i n  de t e c t or
uni t   1L ar ge
r e s ponse
i n  de t e c t or
uni t   3
Figure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures
thatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof
theinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearn
tobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5

============================================================

=== CHUNK 086 ===
Palavras: 352
Caracteres: 8652
--------------------------------------------------
Eachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsin
theinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetector
unit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit
wasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resulting
intwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughly
thesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,
2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally
invarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother
transformations 1
Figure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof
threeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor
oftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note
thattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot
wanttoignoresomeofthedetectorunits 3 4 4
CHAPTER9.CONVOLUTIONALNETWORKS
Sometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould
useinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010
poolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe
locationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011
diﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearna
singlepoolingstructurethatisthenappliedtoallimages(,) Jiaetal.2012
Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse
top-downinformation, suchasBoltzmannmachinesandautoencoders.These
issueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III
PoolinginconvolutionalBoltzmannmachinesispresentedinsection The20.6
inverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswill
becoveredinsection.20.10.6
Someexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcation
usingconvolutionandpoolingareshowninﬁgure.9.11
9.4Convolutionand Pooling asan InﬁnitelyStrong
Prior
Recalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2
aprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs
aboutwhatmodelsarereasonable,beforewehaveseenanydata Priorscanbeconsideredweakorstrongdependingonhowconcentratedthe
probabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh
entropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows
thedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow
entropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa
moreactiveroleindeterminingwheretheparametersendup Aninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays
thattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch
supportthedatagivestothosevalues Wecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,
butwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongprior
saysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits
neighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,
exceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathidden
unit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitely
strongpriorprobabilitydistributionovertheparametersofalayer.Thisprior
3 4 5
CHAPTER9.CONVOLUTIONALNETWORKS
Input image: 
256x256x3Output of 
convolution + 
ReLU: 256x256x64Output of pooling 
with stride 4: 
64x64x64Output of 
convolution + 
ReLU: 64x64x64Output of pooling 
with stride 4: 
16x16x64Output of reshape to 
vector:
16,384 unitsOutput of matrix 
multiply: 1,000 unitsOutput of softmax: 
1,000 class 
probabilities
Input image: 
256x256x3Output of 
convolution + 
ReLU: 256x256x64Output of pooling 
with stride 4: 
64x64x64Output of 
convolution + 
ReLU: 64x64x64Output of pooling to 
3x3 grid: 3x3x64Output of reshape to 
vector:
576 unitsOutput of matrix 
multiply: 1,000 unitsOutput of softmax: 
1,000 class 
probabilities
Input image: 
256x256x3Output of 
convolution + 
ReLU: 256x256x64Output of pooling 
with stride 4: 
64x64x64Output of 
convolution + 
ReLU: 64x64x64Output of 
convolution:
16x16x1,000Output of average 
pooling: 1x1x1,000Output of softmax: 
1,000 class 
probabilities
Output of pooling 
with stride 4: 
16x16x64
Figure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.The
speciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyare
designedtobeveryshallowinordertoﬁtontothepage Realconvolutionalnetworks
alsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresused
hereforsimplicity ( L e f t )Aconvolutionalnetworkthatprocessesaﬁxedimagesize Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe
convolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therest
ofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinchapter.6
( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains
afullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools
butaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothe
fullyconnectedportionofthenetwork Aconvolutionalnetworkthatdoesnot ( R i g h t )
haveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone
featuremapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto
occurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides
theargumenttothesoftmaxclassiﬁeratthetop 3 4 6
CHAPTER9.CONVOLUTIONALNETWORKS
saysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis
equivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongprior
thateachunitshouldbeinvarianttosmalltranslations Ofcourse,implementing aconvolutionalnetasafullyconnectednetwithan
inﬁnitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking
ofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcan
giveussomeinsightsintohowconvolutionalnetswork Onekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting Like
anyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade
bythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial
information, thenusingpoolingonallfeaturescanincreasethetrainingerror Someconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a
usepoolingonsomechannelsbutnotonotherchannels,inordertogetboth
highlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslation
invariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom
verydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe
inappropriate Anotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-
tionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning
performance.Modelsthatdonotuseconvolutionwouldbeabletolearneven
ifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there
areseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust
discovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge
ofspatialrelationshipshard-codedintothembytheirdesigner 9.5VariantsoftheBasicConvolutionFunction
Whendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo
notreferexactlytothestandarddiscreteconvolutionoperationasitisusually
understoodinthemathematical literature.Thefunctionsusedinpracticediﬀer
slightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeuseful
propertiesofthefunctionsusedinneuralnetworks First,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually
actuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin
parallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind
offeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour
networktoextractmanykindsoffeatures,atmanylocations 3 4 7
CHAPTER9.CONVOLUTIONALNETWORKS
Additionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa
gridofvector-valuedobservations Forexample,acolorimagehasared,green
andblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput
tothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutput
ofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,we
usuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with
oneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinates
ofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey
willactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesin
thebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity Becauseconvolutionalnetworksusuallyusemulti-channelconvolution,the
linearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif
kernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeif
eachoperationhasthesamenumberofoutputchannelsasinputchannels

============================================================

=== CHUNK 087 ===
Palavras: 376
Caracteres: 4741
--------------------------------------------------
Assumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection
strengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe
input,withanoﬀsetof krowsand lcolumnsbetweentheoutputunitandthe
inputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving
thevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour
outputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K
acrosswithoutﬂipping,then V K
Z i , j , k=
l , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n (9.7)
wherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing
operationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto
arraysusingafortheﬁrstentry.Thisnecessitatesthe 1 −1intheaboveformula ProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0
theaboveexpressionevensimpler Wemaywanttoskipoversomepositionsofthekernelinordertoreducethe
computational cost(attheexpenseofnotextractingourfeaturesasﬁnely).We
canthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If
wewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan
deﬁneadownsampledconvolutionfunctionsuchthat c
Z i , j , k= ( ) c K V , , s i , j , k=
l , m , n
Vl , j s m , k s n ( − × 1 ) + ( − × 1 ) + K i , l , m , n
.(9.8)
Wereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible
3 4 8
CHAPTER9.CONVOLUTIONALNETWORKS
todeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan9.12
illustration Oneessentialfeatureofanyconvolutionalnetworkimplementationistheability
toimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,
thewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth
ateachlayer Zeropaddingtheinputallowsustocontrolthekernelwidthand
thesizeoftheoutputindependently.Withoutzeropadding,weareforcedto
choosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall
kernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork 9.13
Threespecialcasesofthezero-paddingsettingareworthmentioning.Oneis
theextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution
kernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely
withintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In
thiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin
theinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,
thesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand
thekernelhaswidth k,theoutputwillbeofwidth m k −+1 Therateofthis
shrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis
greaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded
inthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill
eventuallydropto1 ×1,atwhichpointadditionallayerscannotmeaningfully
beconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis
whenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto
thesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the
networkcancontainasmanyconvolutionallayersastheavailablehardwarecan
support,sincetheoperationofconvolutiondoesnotmodifythearchitectural
possibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder
inﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake
theborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe
otherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough
zeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting
inanoutputimageofwidth m+ k −1.Inthiscase,theoutputpixelsnearthe
borderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This
canmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsin
theconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in
termsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”
convolution 3 4 9
CHAPTER9.CONVOLUTIONALNETWORKS
x 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2
x 4 x 4 x 5 x 5s 3 s 3
x 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3
x 4 x 4z 4 z 4
x 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d
c onv ol ut i on
D ow nsampl i n g
C onv ol ut i on
Figure 9.12:Convolution witha stride.Inthisexample,we use astride oftwo ( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation ( Bot-
t o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto
convolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach
involvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues
thatarethendiscarded 3 5 0
CHAPTER9.CONVOLUTIONALNETWORKS Figure9.13: T h e e ﬀ e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork
withakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so
onlytheconvolutionoperationitselfshrinksthenetworksize

============================================================

=== CHUNK 088 ===
Palavras: 350
Caracteres: 4522
--------------------------------------------------
( T o p )Inthisconvolutional
network,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto
shrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly
abletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,
soarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan
bemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome
shrinkingisinevitableinthiskindofarchitecture Byaddingﬁveimplicitzeroes ( Bottom )
toeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto
makeanarbitrarilydeepconvolutionalnetwork 3 5 1
CHAPTER9.CONVOLUTIONALNETWORKS
Insomecases,wedonotactuallywanttouseconvolution,butratherlocally
connectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989
graphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁed
bya6-Dtensor W Theindicesinto Warerespectively: i,theoutputchannel,
j,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoﬀset
withintheinput,and n,thecolumnoﬀsetwithintheinput.Thelinearpartofa
locallyconnectedlayeristhengivenby
Z i , j , k=
l , m , n[ V l , j m , k n + − 1 + − 1 w i , j , k, l , m , n] (9.9)
Thisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-
ationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters
acrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14
connections Locallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe
afunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame
featureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage
isapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe
image Itcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers
inwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput
channel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon
waytodothisistomaketheﬁrst moutputchannelsconnecttoonlytheﬁrst
ninputchannels,thesecond moutputchannelsconnecttoonlythesecond n
inputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions 9.15
betweenfewchannelsallowsthenetworktohavefewerparametersinorderto
reducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreduces
theamountofcomputationneededtoperformforwardandback-propagation It
accomplishesthesegoalswithoutreducingthenumberofhiddenunits T i l e d c o n v o l ut i o n( ,;,)oﬀersacom- GregorandLeCun2010aLeetal.2010
promisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan
learningaseparatesetofweightsatspatiallocation,welearnasetofkernels every
thatwerotatethroughaswemovethroughspace.Thismeansthatimmediately
neighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,
butthememoryrequirementsforstoringtheparameterswillincreaseonlybya
factorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput
featuremap.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiled 9.16
convolution,andstandardconvolution 3 5 2
CHAPTER9.CONVOLUTIONALNETWORKS
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
x 1 x 1 x 2 x 2s 1 s 1 s 3 s 3
x 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
a                b a                b a                b a                b a               a                b c             d e           f g            h   i    
x 4 x 4 x 3 x 3s 4 s 4 s 2 s 2
Figure9.14:Comparisonoflocalconnections,convolution,andfullconnections ( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith
auniquelettertoshowthateachedgeisassociatedwithitsownweightparameter ( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly
thesameconnectivityasthelocallyconnectedlayer.Thediﬀerenceliesnotinwhichunits
interactwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer
hasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly
acrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge ( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach
edgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis
diagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected
layer 3 5 3
CHAPTER9.CONVOLUTIONALNETWORKS
I nput T e nsorO ut put T e nsor
S p a t i a l   c o o r d i n a t e sC h a n n e l   c o o r d i n a t e s
Figure9.15: Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedto
onlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly
thesecondtwoinputchannels

============================================================

=== CHUNK 089 ===
Palavras: 412
Caracteres: 5852
--------------------------------------------------
3 5 4
CHAPTER9.CONVOLUTIONALNETWORKS
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
a                b a                b a                b a                b a             a                b c             d e           f g            h   i    
x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3
x 4 x 4s 4 s 4
x 5 x 5s 5 s 5
a                b c                 d a                b c                 d a               
Figure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard
convolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame
sizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide Thediﬀerencesbetweenthemethodsliesinhowtheyshareparameters ( T o p )Alocally
connectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight
bylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r )
tdiﬀerentkernels.Hereweillustratethecaseof t= 2 Oneofthesekernelshasedges
labeled“a”and“b,”whiletheotherhasedgeslabeled“c”and“d.” Eachtimewemoveone
pixeltotherightintheoutput,wemoveontousingadiﬀerentkernel.Thismeansthat,
likethelocallyconnectedlayer,neighboringunitsintheoutputhavediﬀerentparameters Unlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels,
wecyclebacktotheﬁrstkernel.Iftwooutputunitsareseparatedbyamultipleof t
steps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom )
convolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated
inthediagrambyusingthekernelwithweightslabeled“a”and“b”everywhere 3 5 5
CHAPTER9.CONVOLUTIONALNETWORKS
Todeﬁnetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof
thedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthan
havingaseparateindexforeachlocationintheoutputmap,outputlocationscycle
throughasetof tdiﬀerentchoicesofkernelstackineachdirection.If tisequalto
theoutputwidth,thisisthesameasalocallyconnectedlayer Z i , j , k=
l , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)
whereis themodulooperation,with % t% t=0(, t+1)% t=1,etc.It is
straightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeach
dimension Bothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting
interactionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby
diﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsof
thesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe
learnedtransformation(seeﬁgure).Convolutionallayersarehard-codedtobe 9.9
invariantspeciﬁcallytotranslation Otheroperationsbesidesconvolutionareusuallynecessarytoimplementa
convolutionalnetwork.Toperformlearning,onemustbeabletocomputethe
gradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs Insomesimplecases, thisoperationcanbeperformedusingtheconvolution
operation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,
donothavethisproperty Recallthatconvolutionisalinearoperationandcanthusbedescribedasa
matrixmultiplication (ifweﬁrstreshapetheinputtensorintoaﬂatvector).The
matrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand
eachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview
helpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional
network Multiplication bythetransposeofthematrixdeﬁnedbyconvolutionisone
suchoperation.Thisistheoperationneededtoback-propagate errorderivatives
throughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks
thathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe
wishtoreconstructthevisibleunitsfromthehiddenunits( ,) Simard etal.1992
Reconstructingthevisibleunitsisanoperationcommonlyusedinthemodels
describedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding III
Transposeconvolutionisnecessarytoconstructconvolutionalversionsofthose
models.Likethekernelgradientoperation,thisinputgradientoperationcanbe
3 5 6
CHAPTER9.CONVOLUTIONALNETWORKS
implementedusingaconvolutioninsomecases,butinthegeneralcaserequires
athirdoperationtobeimplemented.Caremustbetakentocoordinatethis
transposeoperationwiththeforwardpropagation Thesizeoftheoutputthatthe
transposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof
theforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’s
outputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan
resultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly
toldwhatthesizeoftheoriginalinputwas Thesethreeoperations—convolution,backpropfromoutputtoweights,and
backpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradients
neededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain
convolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof
convolution See ()forafullderivationoftheequationsinthe Goodfellow2010
fullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese
equationswork,wepresentthetwodimensional,singleexampleversionhere Supposewewanttotrainaconvolutionalnetworkthatincorporatesstrided
convolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas
deﬁnedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8
function J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto
output Z,whichisthenpropagatedthroughtherestofthenetworkandusedto
computethecostfunction J.Duringback-propagation, wewillreceiveatensor G
suchthat G i , j , k=∂
∂ Z i , j , kJ , ( V K)
Totrainthenetwork,weneedtocomputethederivativeswithrespecttothe
weightsinthekernel.Todoso,wecanuseafunction
g , , s ( G V) i , j , k, l=∂
∂ K i , j , k, lJ ,( V K) =
m , nG i , m , n V j , m s k, n s l ( − × 1 ) + ( − × 1 ) + .(9.11)
Ifthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute
thegradientwithrespectto Vinordertoback-propagate theerrorfartherdown

============================================================

=== CHUNK 090 ===
Palavras: 364
Caracteres: 8212
--------------------------------------------------
Todoso,wecanuseafunction
h , , s ( K G) i , j , k=∂
∂ V i , j , kJ ,( V K) (9.12)
=
l , m
s ( 1 ) + = l − × s m j
n , p
s ( 1 ) + = n − × s p k
qK q , i , m , p G q , l , n .(9.13)
Autoencodernetworks, describedinchapter, arefeedforwardnetworks 14
trainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,
3 5 7
CHAPTER9.CONVOLUTIONALNETWORKS
thatcopiesitsinput xtoanapproximatereconstruction rusingthefunction
WW x.Itiscommonformore general autoencoders tousemultiplication
bythetransposeoftheweightmatrixjustasPCAdoes Tomakesuchmodels
convolutional,wecanusethefunction htoperformthetransposeoftheconvolution
operation.Supposewehavehiddenunits Hinthesameformatas Zandwedeﬁne
areconstruction
R K H = ( h , , s .) (9.14)
Inordertotraintheautoencoder,wewillreceivethegradientwithrespect
to Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith
respectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain
thegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto
diﬀerentiatethrough gusing cand h,buttheseoperationsarenotneededforthe
back-propagationalgorithmonanystandardnetworkarchitectures Generally,wedonotuseonlyalinearoperationinordertotransformfrom
theinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome
biastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion
ofhowtoshareparametersamongthebiases Forlocallyconnectedlayersitis
naturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto
sharethebiaseswiththesametilingpatternasthekernels.Forconvolutional
layers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross
alllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxed
size,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap Separatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,but
alsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerent
locations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe
edgeoftheimagereceivelesstotalinputandmayneedlargerbiases 9.6StructuredOutputs
Convolutionalnetworkscanbeusedtooutputahigh-dimensional,structured
object,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorareal
valueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya
standardconvolutionallayer.Forexample,themodelmightemitatensor S,where
S i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass
i.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks
thatfollowtheoutlinesofindividualobjects Oneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe
3 5 8
CHAPTER9.CONVOLUTIONALNETWORKS
ˆ Y( 1 )ˆ Y( 1 )ˆ Y( 2 )ˆ Y( 2 )ˆ Y( 3 )ˆ Y( 3 )
H( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )
XXU U UV V V W W
Figure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The
inputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X
channels(red,green,blue).Thegoalistooutputatensoroflabelsˆ Y,withaprobability
distributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,
imagecolumns,andthediﬀerentclasses.Ratherthanoutputtingˆ Yinasingleshot,the
recurrentnetworkiterativelyreﬁnesitsestimateˆ Ybyusingapreviousestimateofˆ Y
asinputforcreatinganewestimate Thesameparametersareusedforeachupdated
estimate,andtheestimatecanbereﬁnedasmanytimesaswewish.Thetensorof
convolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe
inputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe
hiddenvalues.Onallbuttheﬁrststep,thekernels Wareconvolvedoverˆ Ytoprovide
inputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Because
thesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as
describedinchapter.10
inputplane,asshowninﬁgure.Inthekindsofarchitectures typicallyusedfor 9.13
classiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatial
dimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In
ordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling
altogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007
gridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015
useapoolingoperatorwithunitstride Onestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess
oftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetween
neighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondsto
usingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof
thedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007
bythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular
kindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17
thearchitectureofsucharecurrentconvolutionalnetwork 3 5 9
CHAPTER9.CONVOLUTIONALNETWORKS
Onceapredictionforeachpixelismade,variousmethodscanbeusedto
furtherprocessthesepredictionsinordertoobtainasegmentationoftheimage
intoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,) Thegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe
associatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic
relationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork
canbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining
objective(,; ,) Ningetal.2005Thompsonetal.2014
9.7DataTypes
Thedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,
eachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspace
ortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities 9.1
andnumberofchannels Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal ().2010
Sofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest
datahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks
isthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof
inputsimplycannotberepresentedbytraditional,matrixmultiplication-based
neuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks
evenwhencomputational costandoverﬁttingarenotsigniﬁcantissues Forexample,consideracollectionofimages,whereeachimagehasadiﬀerent
widthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof
ﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda
diﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe
convolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix
multiplication; thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblock
circulantmatrixforeachsizeofinput Sometimes theoutputofthenetworkis
allowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign
aclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis
necessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,for
exampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase
wemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose
poolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto
maintainaﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategy
areshowninﬁgure.9.11
3 6 0
CHAPTER9.CONVOLUTIONALNETWORKS
Singlechannel Multi-channel
1-DAudio waveform:The axis we
convolveovercorrespondsto
time.Wediscretizetimeand
measuretheamplitudeofthe
waveformoncepertimestep.Skeletonanimationdata:Anima-
tionsof3-Dcomputer-rendered
charactersaregeneratedbyalter-
ingtheposeofa“skeleton”over
time.Ateachpointintime,the
poseofthecharacterisdescribed
byaspeciﬁcationoftheanglesof
eachofthejointsinthecharac-
ter’sskeleton.Eachchannelin
thedatawefeedtotheconvolu-
tionalmodelrepresentstheangle
aboutoneaxisofonejoint 2-DAudiodatathathasbeenprepro-
cessedwithaFouriertransform:
Wecantransformtheaudiowave-
formintoa2Dtensorwithdif-
ferentrowscorrespondingtodif-
ferentfrequencies anddiﬀerent
columnscorrespondingtodiﬀer-
entpointsintime.Usingconvolu-
tioninthetimemakesthemodel
equivarianttoshiftsintime.Us-
ingconvolutionacrossthefre-
quencyaxismakesthemodel
equivarianttofrequency,sothat
thesamemelodyplayedinadif-
ferentoctaveproducesthesame
representationbutatadiﬀerent
heightinthenetwork’soutput.Colorimagedata:Onechannel
containstheredpixels,onethe
green pixels, and one theblue
pixels.Theconvolutionkernel
movesoverboththehorizontal
andverticalaxesofthe image,
conferringtranslationequivari-
anceinbothdirections

============================================================

=== CHUNK 091 ===
Palavras: 351
Caracteres: 12805
--------------------------------------------------
3-DVolumetricdata:Acommon
sourceofthiskindofdataismed-
icalimagingtechnology,suchas
CTscans.Colorvideodata:Oneaxiscorre-
spondstotime,onetotheheight
ofthevideoframe,andoneto
thewidthofthevideoframe Table9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional
networks 3 6 1
CHAPTER9.CONVOLUTIONALNETWORKS
Notethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes
senseforinputsthathavevariablesizebecausetheycontainvaryingamounts
ofobservationofthesamekindofthing—diﬀeren tlengthsofrecordingsover
time,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake
senseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerent
kindsofobservations.Forexample,ifweareprocessingcollegeapplications,and
ourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery
applicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe
sameweightsoverboththefeaturescorrespondingtothegradesandthefeatures
correspondingtothetestscores 9.8EﬃcientConvolutionAlgorithms
Modernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore
thanonemillionunits.Powerfulimplementations exploitingparallelcomputation
resources,asdiscussedinsection,areessential However,inmanycasesit 12.1
isalsopossibletospeedupconvolutionbyselectinganappropriateconvolution
algorithm Convolutionisequivalenttoconvertingboththeinputandthekerneltothe
frequencydomainusingaFouriertransform,performingpoint-wisemultiplication
ofthetwosignals, andconvertingbacktothetimedomainusinganinverse
Fouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive
implementationofdiscreteconvolution Whena d-dimensionalkernelcanbeexpressedas theouterproductof d
vectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe
kernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocompose d
one-dimensional convolutionswitheachofthesevectors.Thecomposedapproach
issigniﬁcantlyfasterthanperformingone d-dimensionalconvolutionwiththeir
outerproduct.Thekernelalsotakesfewerparameterstorepresentasvectors Ifthekernelis welementswideineachdimension,thennaivemultidimensional
convolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable
convolutionrequires O( w d ×)runtimeandparameterstoragespace.Ofcourse,
noteveryconvolutioncanberepresentedinthisway Devisingfasterwaysofperformingconvolutionorapproximateconvolution
withoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-
niquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecause
inthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof
anetworkthantoitstraining 3 6 2
CHAPTER9.CONVOLUTIONALNETWORKS
9.9RandomorUnsupervisedFeatures
Typically,themostexpensivepartofconvolutionalnetworktrainingislearningthe
features.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber
offeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof
pooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient
steprequiresacompleterunofforwardpropagationandbackwardpropagation
throughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork
trainingistousefeaturesthatarenottrainedinasupervisedfashion Therearethreebasicstrategiesforobtaining con volutionkernelswithout
supervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo
designthembyhand,forexamplebysettingeachkerneltodetectedgesata
certainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised
criterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall
imagepatches,thenuseeachlearnedcentroidasaconvolutionkernel PartIII
describesmanymoreunsupervisedlearningapproaches.Learningthefeatures
withanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe
classiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor
theentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe
lastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,
assumingthelastlayerissomethinglikelogisticregressionoranSVM Randomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett
etal ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal ()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011
becomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights Theyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof
aconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutional
networkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese
architecturesandtraintheentirearchitectureusingamoreexpensiveapproach Anintermediate approachistolearnthefeatures,butusingmethodsthatdo
notrequirefullforwardandback-propagationateverygradientstep.Aswith
multilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayer
inisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthe
secondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8
howtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII
togreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The
canonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe
convolutionaldeepbeliefnetwork(,).Convolutionalnetworksoﬀer Leeetal.2009
3 6 3
CHAPTER9.CONVOLUTIONALNETWORKS
ustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible
withmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata
time,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means Wecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernels
ofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning
totrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining
process.Usingthisapproach,wecantrainverylargemodelsandincurahigh
computational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal 2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular
fromroughly2007–2013,whenlabeleddatasetsweresmallandcomputational
powerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina
purelysupervisedfashion,usingfullforwardandback-propagation throughthe
entirenetworkoneachtrainingiteration Aswithotherapproachestounsupervisedpretraining,itremainsdiﬃcultto
teaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervised
pretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmay
simplyallowustotrainmuchlargerarchitectures duetothereducedcomputational
costofthelearningrule 9.10TheNeuroscientiﬁcBasisforConvolutionalNet-
works
Convolutional networksare perhaps the greatest successstory ofbiologically
inspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguided
bymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworkswere
drawnfromneuroscience Thehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperiments
longbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists
DavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany
ofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland
Wiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith
aNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporary
deeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin
cats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojected
inpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas
thatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁc
patternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto
otherpatterns 3 6 4
CHAPTER9.CONVOLUTIONALNETWORKS
Theirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare
beyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan
focusonasimpliﬁed,cartoonviewofbrainfunction Inthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknown
asthe pr i m ar y v i sual c o r t e x V1istheﬁrstareaofthebrainthatbeginsto
performsigniﬁcantlyadvancedprocessingofvisualinput Inthiscartoonview,
imagesareformedbylightarrivingintheeyeandstimulatingtheretina,the
light-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform
somesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis
represented.Theimagethenpassesthroughtheopticnerveandabrainregion
calledthelateralgeniculatenucleus Themainrole,asfarasweareconcerned
here,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom
theeyetoV1,whichislocatedatthebackofthehead AconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:
1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure
mirroring the structure of theimage in the retina.For example, light
arrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfof
V1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures
deﬁnedintermsoftwodimensionalmaps 2.V1containsmany si m pl e c e l l s.Asimplecell’sactivitycantosomeextent
becharacterizedbyalinear function oftheimagein asmall, spatially
localizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkare
designedtoemulatethesepropertiesofsimplecells 3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat
aresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant
tosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits
ofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges
inlightingthatcannotbecapturedsimplybypoolingoverspatiallocations Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies
inconvolutionalnetworks,suchasmaxoutunits( ,) Goodfellow etal.2013a
ThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame
basicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof
thevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly
appliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical
layersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconcept
andareinvarianttomanytransformationsoftheinput.Thesecellshavebeen
3 6 5
CHAPTER9.CONVOLUTIONALNETWORKS
nicknamed“grandmother cells”—theideaisthatapersoncouldhaveaneuronthat
activateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe
appearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof
herfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin
shadow,etc Thesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,
inaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005
testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals Theyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividual
neuronthatisactivatedbytheconceptofHalleBerry.Thisneuronﬁreswhena
personseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining
thewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;
otherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc Thesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern
convolutionalnetworks,whichwouldnotautomatically generalizetoidentifying
apersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional
network’slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex
(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughthe
LGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst
100msofglimpsinganobject Ifapersonisallowedtocontinuelookingatthe
objectformoretime,theninformationwillbegintoﬂowbackwardsasthebrain
usestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas However,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthat
resultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobe
verysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT
ﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobject
recognitiontasks(,) DiCarlo2013
Thatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworks
andthemammalianvisionsystem.Someofthesediﬀerencesarewellknown
tocomputational neuroscientists,butoutsidethescopeofthisbook.Someof
thesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthe
mammalianvisionsystemworksremainunanswered.Asabrieflist:
•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe
f o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat
armslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,
thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches
togetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually
receivelargefullresolutionphotographsasinput.Thehumanbrainmakes
3 6 6
CHAPTER9.CONVOLUTIONALNETWORKS
severaleyemovementscalled sac c adestoglimpsethemostvisuallysalient
ortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms
intodeeplearningmodelsisanactiveresearchdirection.Inthecontextof
deeplearning,attentionmechanismshavebeenmostsuccessfulfornatural
languageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1
withfoveationmechanismshavebeendevelopedbutsofarhavenotbecome
thedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,)

============================================================

=== CHUNK 092 ===
Palavras: 389
Caracteres: 6685
--------------------------------------------------
•Thehumanvisualsystemisintegratedwithmanyothersenses,suchas
hearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks
sofararepurelyvisual •Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis
abletounderstandentirescenesincludingmanyobjectsandrelationships
betweenobjects,andprocessesrich3-Dgeometricinformationneededfor
ourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen
appliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy •EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher
levels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut
hasnotyetbeenshowntooﬀeracompellingimprovement •WhilefeedforwardITﬁringratescapturemuchofthesameinformationas
convolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate
computations are.Thebrainprobablyusesverydiﬀerentactivationand
poolingfunctions.Anindividualneuron’sactivationprobablyisnotwell-
characterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involves
multiplequadraticﬁltersforeachneuron(,).Indeedour Rustetal.2005
cartoonpictureof“simplecells” and“complexcells” mightcreateanon-
existentdistinction;simplecellsandcomplexcellsmightbothbethesame
kindofcellbutwiththeir“parameters”enablingacontinuumofbehaviors
rangingfromwhatwecall“simple”towhatwecall“complex.”
Itis alsoworthmentioningthatneuroscience hastold usrelativelylittle
abouthowtotrainconvolutionalnetworks.Modelstructureswithparameter
sharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels
ofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976
back-propagationalgorithmandgradientdescent.Forexample,theNeocognitron
(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof
themodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering
algorithm 3 6 7
CHAPTER9.CONVOLUTIONALNETWORKS
Lang andHinton 1988()introducedthe use ofback-propagation totrain
t i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,
TDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-
propagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-
tionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess
ofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989
themodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D
convolutionappliedtoimages Sofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor
certainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome
transformationsofthesesimplecellfeatures,andstacksoflayersthatalternate
betweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciﬁc
phenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect Inadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionof
individualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheir
responsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecan
justdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding
channelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we
donothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe
neuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’s
retina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan
thenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximation of
theneuron’sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach
andShapley2004,) ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed
by G ab o r f unc t i o ns TheGaborfunctiondescribestheweightata2-Dpoint
intheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,
I( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof
locations,deﬁnedbyasetof xcoordinates Xandasetof ycoordinates, Y,and
applyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint
ofview,theresponseofasimplecelltoanimageisgivenby
s I() =
x ∈ X
y ∈ Yw x , y I x , y ()() (9.15)
Speciﬁcally,takestheformofaGaborfunction: w x , y()
w x , y α , β (; x , β y , f , φ , x 0 , y 0 , τ α) = exp− β x x 2− β y y 2
cos( f x+) φ ,(9.16)
where
x= ( x x − 0)cos()+( τ y y − 0)sin() τ (9.17)
3 6 8
CHAPTER9.CONVOLUTIONALNETWORKS
and
y= ( − x x − 0)sin()+( τ y y − 0)cos() τ (9.18)
Here, α, β x, β y, f, φ, x 0, y 0,and τareparametersthatcontroltheproperties
oftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18
diﬀerentsettingsoftheseparameters Theparameters x 0, y 0,and τdeﬁneacoordinatesystem Wetranslateand
rotate xand ytoform xand y.Speciﬁcally,thesimplecellwillrespondtoimage
featurescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness
aswemovealongalinerotatedradiansfromthehorizontal τ
Viewedasafunctionof xand y,thefunction wthenrespondstochangesin
brightnessaswemovealongthe xaxis Ithastwoimportantfactors:oneisa
Gaussianfunctionandtheotherisacosinefunction TheGaussianfactor αexp
− β x x 2− β y y 2
canbeseenasagatingtermthat
ensuresthesimplecellwillonlyrespondtovaluesnearwhere xand yareboth
zero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactor
αadjuststhetotalmagnitudeofthesimplecell’sresponse,while β xand β ycontrol
howquicklyitsreceptiveﬁeldfallsoﬀ Thecosinefactor cos( f x+ φ) controlshowthesimplecellrespondstochanging
brightnessalongthe xaxis.Theparameter fcontrolsthefrequencyofthecosine
andcontrolsitsphaseoﬀset φ
Altogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds
toaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁc
location.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage
hasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe
weightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost
inhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—when
theimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare
negative Thecartoonviewofacomplexcellisthatitcomputesthe L2normofthe
2-Dvectorcontainingtwosimplecells’responses: c( I)=
s 0() I2+ s 1() I2 An
importantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except
for φ,and φissetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis
case, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldeﬁnedinthisway
respondswhentheGaussianreweightedimage I( x , y)exp( − β x x 2− β y y 2) contains
ahighamplitudesinusoidalwavewithfrequency findirection τnear ( x 0 , y 0),
regardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellis
invarianttosmalltranslationsoftheimageindirection τ,ortonegatingtheimage
3 6 9
CHAPTER9.CONVOLUTIONALNETWORKS
Figure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates
largepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray
correspondstozeroweight

============================================================

=== CHUNK 093 ===
Palavras: 355
Caracteres: 10142
--------------------------------------------------
( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters
thatcontrolthecoordinatesystem: x 0, y 0,and τ EachGaborfunctioninthisgridis
assignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and τischosenso
thateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid Fortheothertwoplots, x 0, y 0,and τareﬁxedtozero Gaborfunctionswith ( C e n t e r )
diﬀerentGaussianscaleparameters β xand β y.Gaborfunctionsarearrangedinincreasing
width(decreasing β x)aswemovelefttorightthroughthegrid,andincreasingheight
(decreasing β y)aswemovetoptobottom.Fortheothertwoplots,the βvaluesareﬁxed
to1.5 ×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters ( R i g h t ) f
and φ.Aswemovetoptobottom, fincreases,andaswemovelefttoright, φincreases Fortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth φ f ×
(replacingblackwithwhiteandviceversa) Someofthemoststrikingcorrespondencesbetweenneuroscienceandmachine
learningcomefromvisuallycomparingthefeatureslearnedbymachinelearning
modelswiththoseemployedbyV1 ()showedthat OlshausenandField1996
asimpleunsupervisedlearningalgorithm, sparse coding,learnsfeatureswith
receptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat
anextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith
Gabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep
learningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19
showssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedge
detectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe
“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan
certainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not
whenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe
statisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerent
approachestostatisticalmodeling.SeeHyvärinen 2009etal.()forareviewofthe
ﬁeldofnaturalimagestatistics 3 7 0
CHAPTER9.CONVOLUTIONALNETWORKS
Figure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁc
colorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof
theGaborfunctionsknowntobepresentinprimaryvisualcortex ( L e f t )Weightslearned
byanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall
imagepatches ( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised
convolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit 9.11ConvolutionalNetworksandtheHistoryofDeep
Learning
Convolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep
learning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained
bystudyingthebraintomachinelearningapplications.Theywerealsosomeof
theﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere
consideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneural
networkstosolveimportantcommercialapplicationsandremainattheforefront
ofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the
neuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor
readingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b
byNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR
andhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby
Microsoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12
andmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010
foramorein-depthhistoryofconvolutionalnetworksupto2010 Convolutionalnetworkswerealsousedtowinmanycontests.Thecurrent
intensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal ()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012
3 7 1
CHAPTER9.CONVOLUTIONALNETWORKS
hadbeenusedtowinothermachinelearningandcomputervisioncontestswith
lessimpactforyearsearlier Convolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwith
back-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded
whengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay
simplybethatconvolutionalnetworksweremorecomputationally eﬃcientthan
fullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem
andtunetheirimplementation andhyperparameters.Largernetworksalsoseem
tobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks
appeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere
availableandactivationfunctionsthatwerepopularduringthetimeswhenfully
connectednetworkswerebelievednottoworkwell.Itmaybethattheprimary
barrierstothesuccessofneuralnetworkswerepsychological(practitioners did
notexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouse
neuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks
performedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof
deeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral Convolutionalnetworksprovideawaytospecializeneuralnetworkstowork
withdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto
verylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,
imagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto
anotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural
networks 3 7 2
C h a p t e r 1 0
S e qu e n ce Mo d e l i n g: Recurren t
an d Recursiv e N e t s
RecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a
neuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork
isaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas
animage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor
processingasequenceofvaluesx( 1 ), ,x( ) τ.Justasconvolutionalnetworks
canreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional
networkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch
longersequencesthanwouldbepracticalfornetworkswithoutsequence-based
specialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable
length Togofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-
tageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof
the1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharing
makesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms
(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters
foreachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot
seenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengths
andacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhen
aspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence Forexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,
IwenttoNepal.”Ifweaskamachinelearningmodeltoreadeachsentenceand
extracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize
theyear2009astherelevantpieceofinformation,whetheritappearsinthesixth
373
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
wordorthesecondwordofthesentence.Supposethatwetrainedafeedforward
networkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnected
feedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit
wouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin
thesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights
acrossseveraltimesteps Arelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This
convolutionalapproachisthebasisfortime-delayneuralnetworks(Langand
Hinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation
allowsanetworktoshareparametersacrosstime,butisshallow.Theoutput
ofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof
asmallnumberofneighboringmembersoftheinput.Theideaofparameter
sharingmanifestsintheapplicationofthesameconvolutionkernelateachtime
step.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberofthe
outputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe
outputisproducedusingthesameupdateruleappliedtothepreviousoutputs Thisrecurrentformulationresultsinthesharingofparametersthroughavery
deepcomputational graph Forthesimplicityofexposition,werefertoRNNsasoperatingonasequence
thatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 τ.In
practice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,
withadiﬀerentsequencelength τforeachmemberoftheminibatch.Wehave
omittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex
neednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers
onlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions
acrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,
thenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe
entiresequenceisobservedbeforeitisprovidedtothenetwork Thischapterextendstheideaofacomputational graphtoincludecycles.These
cyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalue
atafuturetimestep.Suchcomputational graphsallowustodeﬁnerecurrent
neuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,and
userecurrentneuralnetworks Formoreinformationonrecurrentneuralnetworksthanisavailableinthis
chapter,wereferthereadertothetextbookofGraves2012() 3 7 4
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
10.1UnfoldingComputationalGraphs
Acomputational graphisawaytoformalizethestructureofasetofcomputations,
suchasthoseinvolvedinmappinginputsandparameterstooutputsandloss Pleaserefertosectionforageneralintroduction Inthissectionweexplain 6.5.1
theideaofunfoldingarecursiveorrecurrentcomputationintoacomputational
graphthathasarepetitivestructure,typicallycorrespondingtoachainofevents Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork
structure Forexample,considertheclassicalformofadynamicalsystem:
s( ) t= ( fs( 1 ) t −;)θ , (10.1)
wheres( ) tiscalledthestateofthesystem Equationisrecurrentbecausethedeﬁnitionof 10.1 sattime trefersbackto
thesamedeﬁnitionattime t−1
Foraﬁnitenumberoftimesteps τ,thegraphcanbeunfoldedbyapplying
thedeﬁnition τ−1times.Forexample,ifweunfoldequationfor10.1 τ= 3time
steps,weobtain
s( 3 )=( fs( 2 );)θ (10.2)
=(( f fs( 1 ););)θθ (10.3)
Unfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhas
yieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan
nowberepresentedbyatraditionaldirectedacycliccomputational graph

============================================================

=== CHUNK 094 ===
Palavras: 446
Caracteres: 5647
--------------------------------------------------
The
unfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3
ﬁgure.10.1
s( t − 1 )s( t − 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t
f fs( ) f f f f f f
Figure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1
unfoldedcomputationalgraph Eachnoderepresentsthestateatsometime tandthe
function fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue
ofusedtoparametrize)areusedforalltimesteps θ f
Asanotherexample,letusconsideradynamicalsystemdrivenbyanexternal
signalx( ) t,
s( ) t= ( fs( 1 ) t −,x( ) t;)θ , (10.4)
3 7 5
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
whereweseethatthestatenowcontainsinformationaboutthewholepastsequence Recurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchas
almostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially
anyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork Manyrecurrentneuralnetworksuseequationorasimilarequationto 10.5
deﬁnethevaluesoftheirhiddenunits Toindicatethatthestateisthehidden
unitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent
thestate:
h( ) t= ( fh( 1 ) t −,x( ) t;)θ , (10.5)
illustratedinﬁgure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2
asoutputlayersthatreadinformationoutofthestatetomakepredictions.h
Whentherecurrentnetworkistrainedtoperformataskthatrequirespredicting
thefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy
summaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This
summaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence
(x( ) t,x( 1 ) t −,x( 2 ) t −, ,x( 2 ),x( 1 ))toaﬁxedlengthvectorh( ) t.Dependingonthe
trainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast
sequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused
instatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious
words,itmaynotbenecessarytostorealloftheinformationintheinputsequence
uptotime t,butratheronlyenoughinformationtopredicttherestofthesentence Themostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow
onetoapproximately recovertheinputsequence,asinautoencoderframeworks
(chapter).14
f fhh
x xh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t
x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) f f
U nf ol df f f f f
Figure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses
informationfromtheinputxbyincorporatingitintothestatehthatispassedforward
throughtime ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime
step.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )
nodeisnowassociatedwithoneparticulartimeinstance Equationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN 10.5
iswithadiagramcontainingonenodeforeverycomponentthatmightexistina
3 7 6
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
physicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis
view,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalparts
whosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2
Throughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate
thataninteractiontakesplacewithadelayofasingletimestep,fromthestate
attime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan
unfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany
diﬀerentvariables,withonevariablepertimestep,representingthestateofthe
componentatthatpointintime.Eachvariableforeachtimestepisdrawnasa
separatenodeofthecomputational graph,asintherightofﬁgure.Whatwe10.2
callunfoldingistheoperationthatmapsacircuitasintheleftsideoftheﬁgure
toacomputational graphwithrepeatedpiecesasintherightside.Theunfolded
graphnowhasasizethatdependsonthesequencelength Wecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:
h( ) t= g( ) t(x( ) t,x( 1 ) t −,x( 2 ) t −, ,x( 2 ),x( 1 )) (10.6)
=( fh( 1 ) t −,x( ) t;)θ (10.7)
Thefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t −,x( 2 ) t −, ,x( 2 ),x( 1 ))
asinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure
allowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding
processthusintroducestwomajoradvantages:
1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame
inputsize,becauseitisspeciﬁedintermsoftransitionfromonestateto
anotherstate,ratherthanspeciﬁedintermsofavariable-length historyof
states 2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters
ateverytimestep Thesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson
alltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate
model g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows
generalization tosequencelengthsthatdidnotappearinthetrainingset,and
allowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe
requiredwithoutparametersharing Boththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent
graphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich
computations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof
3 7 7
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
informationﬂowforwardintime(computingoutputsandlosses)andbackward
intime(computinggradients)byexplicitlyshowingthepathalongwhichthis
informationﬂows 10.2RecurrentNeuralNetworks
Armedwiththegraphunrollingandparametersharingideasofsection,we10.1
candesignawidevarietyofrecurrentneuralnetworks UUV V
WWo( t − 1 )o( t − 1 )
hhooy y
LL
x xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t
h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t
x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW
h( )

============================================================

=== CHUNK 095 ===
Palavras: 354
Caracteres: 5420
--------------------------------------------------
.V V V V V V
UU UU UUU nf ol d
Figure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork
thatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues Aloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing
softmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally
computesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden
connectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections
parametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby
aweightmatrixV.Equationdeﬁnesforwardpropagationinthismodel 10.8 ( L e f t )The
RNNanditslossdrawnwithrecurrentconnections ( R i g h t )Thesameseenasantime-
unfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular
timeinstance Someexamplesofimportantdesignpatternsforrecurrentneuralnetworks
includethefollowing:
3 7 8
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
•Recurrentnetworksthatproduceanoutputateachtimestepandhave
recurrentconnectionsbetweenhiddenunits,illustratedinﬁgure.10.3
•Recurrentnetworksthatproduceanoutputateachtimestepandhave
recurrentconnectionsonlyfromtheoutputatonetimesteptothehidden
unitsatthenexttimestep,illustratedinﬁgure10.4
•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that
readanentiresequenceandthenproduceasingleoutput,illustratedin
ﬁgure.10.5
ﬁgureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3
mostofthechapter Therecurrentneuralnetworkofﬁgureandequationisuniversalinthe 10.3 10.8
sensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch
arecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafter
anumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps
usedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput
(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;
Hyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,
sotheseresultsregardexactimplementation ofthefunction,notapproximations TheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits
outputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall
functionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(Siegelmannand
Sontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcation
ofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring
machineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproof
cansimulateanunboundedstackbyrepresentingitsactivationsandweightswith
rationalnumbersofunboundedprecision WenowdeveloptheforwardpropagationequationsfortheRNNdepictedin
ﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe 10.3
hiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,
theﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake Hereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords
orcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput
oasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete
variable.Wecanthenapplythesoftmaxoperationasapost-processingstepto
obtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagation
beginswithaspeciﬁcationoftheinitialstateh( 0 ).Then,foreachtimestepfrom
3 7 9
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
UV
Wo( t − 1 )o( t − 1 )
hhooy y
LL
x xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t
h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t
x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) .V V V
U U UU nf ol d
Figure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput
tothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare
h( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t ( L e f t )Circuitdiagram ( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa
smallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.TheRNN 10.3
inﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3
representationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedto
putaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosend
tothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush
isconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce Unlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation
fromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasierto
trainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater
parallelizationduringtraining,asdescribedinsection.10.2.1
3 8 0
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
t t τ = 1to= ,weapplythefollowingupdateequations:
a( ) t= +bWh( 1 ) t −+Ux( ) t(10.8)
h( ) t=tanh(a( ) t) (10.9)
o( ) t= +cVh( ) t(10.10)
ˆy( ) t=softmax(o( ) t) (10.11)
wheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices
U,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-
hiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan
inputsequencetoanoutputsequenceofthesamelength.Thetotallossfora
givensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y
thesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative
log-likelihoodof y( ) tgivenx( 1 ), ,x( ) t,then
L
{x( 1 ), ,y( ) τ}
(10.12)
=
tL( ) t(10.13)
=−
tlog p m o de l
y( ) t|{x( 1 ), ,x( ) t}
, (10.14)
where p m o de l
y( ) t|{x( 1 ),

============================================================

=== CHUNK 096 ===
Palavras: 356
Caracteres: 4219
--------------------------------------------------
,x( ) t}
isgivenbyreadingtheentryfor y( ) tfromthe
model’soutputvectorˆy( ) t.Computingthegradientofthislossfunctionwithrespect
totheparametersisanexpensiveoperation.Thegradientcomputationinvolves
performingaforwardpropagationpassmovinglefttorightthroughourillustration
oftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass 10.3
movingrighttoleftthroughthegraph.Theruntimeis O( τ) andcannotbereduced
byparallelization becausetheforwardpropagationgraphisinherentlysequential;
eachtimestepmayonlybecomputedafterthepreviousone Statescomputed
intheforwardpassmustbestoreduntiltheyarereusedduringthebackward
pass,sothememorycostisalso O( τ).Theback-propagation algorithmapplied
totheunrolledgraphwith O( τ)costiscalledback-propagationthroughtime
orBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2
betweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean
alternative 10.2.1TeacherForcingandNetworkswithOutputRecurrence
Thenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto
thehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful 10.4
3 8 1
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
becauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot
simulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden
recurrence,itrequiresthattheoutputunitscapturealloftheinformationabout
thepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits
areexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture
thenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser
knowshowtodescribethefullstateofthesystemandprovidesitaspartofthe
trainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence
isthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe
trainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe
parallelized,withthegradientforeachstep tcomputedinisolation.Thereisno
needtocomputetheoutputfortheprevioustimestepﬁrst,becausethetraining
setprovidestheidealvalueofthatoutput h( t − 1 )h( t − 1 )
Wh( ) th( ) t x( t − 1 )x( t − 1 )x( ) tx( ) tx( ) .W W
U U Uh( ) τh( ) τ
x( ) τx( ) τW
Uo( ) τo( ) τy( ) τy( ) τL( ) τL( ) τ
V Figure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend
ofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea
ﬁxed-sizerepresentationusedasinputforfurtherprocessing Theremightbeatarget
rightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby
back-propagatingfromfurtherdownstreammodules Modelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto
themodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure
thatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe
modelreceivesthegroundtruthoutput y( ) tasinputattime t+1 Wecansee
thisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum
3 8 2
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
o( t − 1 )o( t − 1 )o( ) to( ) t
h( t − 1 )h( t − 1 )h( ) th( ) t
x( t − 1 )x( t − 1 )x( ) tx( ) tW
V V
U Uo( t − 1 )o( t − 1 )o( ) to( ) tL( t − 1 )L( t − 1 )L( ) tL( ) ty( t − 1 )y( t − 1 )y( ) ty( ) t
h( t − 1 )h( t − 1 )h( ) th( ) t
x( t − 1 )x( t − 1 )x( ) tx( ) tW
V V
U U
T r ai n  t i m e T e s t   t i m e
Figure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis
applicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe
nexttimestep ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain
setasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )
notknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel’soutput
o( ) t,andfeedtheoutputbackintothemodel 3 8 3
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
likelihoodcriterionis
log p
y( 1 ),y( 2 )|x( 1 ),x( 2 )
(10.15)
=log p
y( 2 )|y( 1 ),x( 1 ),x( 2 )
+log p
y( 1 )|x( 1 ),x( 2 )
(10.16)
Inthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe
conditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy
valuefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,
ratherthanfeedingthemodel’sownoutputbackintoitself,theseconnections
shouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe

============================================================

=== CHUNK 097 ===
Palavras: 395
Caracteres: 4847
--------------------------------------------------
Thisisillustratedinﬁgure.10.6
Weoriginallymotivatedteacherforcingasallowingustoavoidback-propagation
throughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing
maystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas
theyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe
nexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier
timesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained
withbothteacherforcingandBPTT Thedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe
laterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom
theoutputdistribution)fedbackasinput Inthiscase,thekindofinputsthat
thenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs
thatitwillseeattesttime Onewaytomitigatethisproblemistotrainwith
bothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting
thecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent
output-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount
inputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not
seenduringtrainingandhowtomapthestatebacktowardsonethatwillmake
thenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio
e t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b
inputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata
valuesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually
usemoreofthegeneratedvaluesasinput 10.2.2ComputingtheGradientinaRecurrentNeuralNetwork
Computingthegradientthrougharecurrentneuralnetworkisstraightforward Onesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6
3 8 4
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
totheunrolledcomputational graph.Nospecializedalgorithmsarenecessary Gradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose
gradient-basedtechniquestotrainanRNN TogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean
exampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove
(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12
theparametersU,V,W,bandcaswellasthesequenceofnodesindexedby
tforx( ) t,h( ) t,o( ) tand L( ) t Foreachnode Nweneedtocomputethegradient
∇ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe
graph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss
∂ L
∂ L( ) t= 1 (10.17)
Inthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe
softmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealso
assumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe
inputsofar.Thegradient∇o( ) t Lontheoutputsattimestep t,forall i , t,isas
follows:
(∇o( ) t L)i=∂ L
∂ o( ) t
i=∂ L
∂ L( ) t∂ L( ) t
∂ o( ) t
i=ˆ y( ) t
i− 1i , y( ) t .(10.18)
Weworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnal
timestep, τh( ) τonlyhaso( ) τasadescendent,soitsgradientissimple:
∇h( ) τ L= V∇o( ) τ L (10.19)
Wecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,
from t= τ−1downto t= 1,notingthath( ) t(for t < τ)hasasdescendentsboth
o( ) tandh( + 1 ) t.Itsgradientisthusgivenby
∇h( ) t L=
∂h( + 1 ) t
∂h( ) t
(∇h( +1) t L)+
∂o( ) t
∂h( ) t
(∇o( ) t L) (10.20)
= W(∇h( +1) t L)diag
1−
h( + 1 ) t2
+V(∇o( ) t L)(10.21)
where diag
1−
h( + 1 ) t2
indicatesthediagonalmatrixcontainingtheelements
1−( h( + 1 ) t
i)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe
hiddenunitattime i t+1
3 8 5
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
Oncethegradientsonthe internalnodesofthe computational graphare
obtained, wecanobtainthegradientsontheparameternodes.Becausethe
parametersaresharedacrossmanytimesteps,wemusttakesomecarewhen
denotingcalculusoperationsinvolvingthesevariables.Theequationswewishto
implementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6
ofasingleedgeinthecomputational graphtothegradient.However,the∇ W f
operatorusedincalculustakesintoaccountthecontributionofWtothevalue
of fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l
introducedummyvariablesW( ) tthataredeﬁnedtobecopiesofWbutwitheach
W( ) tusedonlyattimestep t.Wemaythenuse∇W( ) ttodenotethecontribution
oftheweightsattimesteptothegradient t
Usingthisnotation,thegradientontheremainingparametersisgivenby:
∇ c L=
t
∂o( ) t
∂c
∇o( ) t L=
t∇o( ) t L (10.22)
∇ b L=
t
∂h( ) t
∂b( ) t
∇h( ) t L=
tdiag
1−
h( ) t2
∇h( ) t L(10.23)
∇ V L=
t
i
∂ L
∂ o( ) t
i
∇ V o( ) t
i=
t(∇o( ) t L)h( ) t(10.24)
∇ W L=
t
i
∂ L
∂ h( ) t
i
∇W( ) t h( ) t
i (10.25)
=
tdiag
1−
h( ) t2
(∇h( ) t L)h( 1 ) t −(10.26)
∇ U L=
t
i
∂ L
∂ h( ) t
i
∇U( ) t h( ) t
i (10.27)
=
tdiag
1−
h( ) t2
(∇h( ) t L)x( ) t(10.28)
Wedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause
itdoesnothaveanyparametersasancestorsinthecomputational graphdeﬁning
theloss

============================================================

=== CHUNK 098 ===
Palavras: 422
Caracteres: 6148
--------------------------------------------------
3 8 6
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
10.2.3RecurrentNetworksasDirectedGraphicalModels
Intheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere
cross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward
network,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork Thelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we
usuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and
weusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss Meansquarederroristhecross-entropylossassociatedwithanoutputdistribution
thatisaunitGaussian,forexample,justaswithafeedforwardnetwork When we use apredictivelog-likelihood trainingobjective,such asequa-
tion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12
sequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize
thelog-likelihood
log( py( ) t|x( 1 ), ,x( ) t) , (10.29)
or,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext
timestep,
log( py( ) t|x( 1 ), (10.30)
Decomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof
one-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution
acrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat
conditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges
fromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare
conditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe
actualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)
backintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i
valuesinthepasttothecurrent y( ) tvalue Asasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya
sequenceofscalarrandomvariables Y={y( 1 ), ,y( ) τ},withnoadditionalinputs
x.Theinputattimestep tissimplytheoutputattimestep t−1.TheRNNthen
deﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint
distributionoftheseobservationsusingthechainrule(equation)forconditional3.6
probabilities:
P P () = Y ( y( 1 ), , y( ) τ) =τ
t = 1P( y( ) t| y( 1 ) t −, y( 2 ) t −, , y( 1 ))(10.31)
wheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe
negativelog-likelihoodofasetofvalues { y( 1 ), , y( ) τ}accordingtosuchamodel
3 8 7
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
y( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) Figure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), .:every
pastobservation y( ) imayinﬂuencetheconditionaldistributionofsome y( ) t(for t > i),
giventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis
graph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof 10.6
inputsandparametersforeachelementofthesequence.RNNsobtainthesamefull
connectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8
is
L=
tL( ) t(10.32)
where
L( ) t= log( − Py( ) t= y( ) t| y( 1 ) t −, y( 2 ) t −, , y( 1 )) .(10.33)
y( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) Figure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even
thoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery
eﬃcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t
andy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan
sharethesameparameterswiththeotherstages Theedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother
variables.Manygraphicalmodelsaimtoachievestatisticalandcomputational
eﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.For
3 8 8
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
example,itiscommontomaketheMarkovassumptionthatthegraphicalmodel
shouldonlycontainedgesfrom{y( ) t k −, ,y( 1 ) t −}toy( ) t,ratherthancontaining
edgesfromtheentirepasthistory.However,insomecases,webelievethatallpast
inputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsare
usefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i
fromthedistantpastinawaythatisnotcapturedbytheeﬀectofy( ) iony( 1 ) t − OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas
deﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent
directdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey
valueswiththecompletegraphstructureisshowninﬁgure.Thecomplete10.7
graphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby
marginalizing themoutofthemodel ItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat
resultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe
hiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcient
parametrization ofthejointdistributionovertheobservations.Supposethatwe
representedanarbitraryjointdistributionoverdiscretevalueswithatabular
representation—anarraycontainingaseparateentryforeachpossibleassignment
ofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment
occurring If ycantakeon kdiﬀerentvalues,thetabularrepresentationwould
have O( kτ)parameters.Bycomparison,duetoparametersharing,thenumberof
parametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof
parametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced
toscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5
long-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplications
ofthesamefunction fandsameparametersθateachtimestep.Figure10.8
illustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin
thegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate
quantitybetweenthem.Avariable y( ) iinthedistantpastmayinﬂuenceavariable
y( ) tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbe
eﬃcientlyparametrized byusingthesameconditionalprobabilitydistributionsat
eachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe
jointassignmentofallvariablescanbeevaluatedeﬃciently Evenwiththeeﬃcientparametrization ofthegraphicalmodel,someoperations
remaincomputationally challenging.Forexample,itisdiﬃculttopredictmissing
1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c

============================================================

=== CHUNK 099 ===
Palavras: 357
Caracteres: 5213
--------------------------------------------------
Th i s i s
p e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c
h i d d e n u n i t s 3 8 9
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
valuesinthemiddleofthesequence Thepricerecurrentnetworkspayfortheirreducednumberofparametersis
that theparametersmaybediﬃcult o p t i m i z i ng
Theparametersharingusedinrecurrentnetworksreliesontheassumption
thatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,the
assumptionisthattheconditionalprobabilitydistributionoverthevariablesat
time t+1 giventhevariablesattime tisstationary,meaningthattherelationship
betweentheprevioustimestepandthenexttimestepdoesnotdependon t.In
principle,itwouldbepossibletouse tasanextrainputateachtimestepandlet
thelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween
diﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerent
conditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto
extrapolatewhenfacedwithnewvaluesof t
TocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow
todrawsamplesfromthemodel.Themainoperationthatweneedtoperformis
simplytosamplefromtheconditionaldistributionateachtimestep However,
thereisoneadditionalcomplication The RNNmusthavesomemechanismfor
determiningthelengthofthesequence.Thiscanbeachievedinvariousways Inthecasewhentheoutputisasymboltakenfromavocabulary,onecan
addaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,) Whenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,
weinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) τ
ineachtrainingexample AnotheroptionistointroduceanextraBernoullioutputtothemodelthat
representsthedecisiontoeithercontinuegenerationorhaltgenerationateach
timestep.Thisapproachismoregeneralthantheapproachofaddinganextra
symboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan
onlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto
anRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya
sigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis
trainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe
sequenceendsorcontinuesateachtimestep Anotherwaytodeterminethesequencelength τistoaddanextraoutputto
themodelthatpredictstheinteger τitself.Themodelcansampleavalueof τ
andthensample τstepsworthofdata.Thisapproachrequiresaddinganextra
inputtotherecurrentupdateateachtimestepsothattherecurrentupdateis
awareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput
caneitherconsistofthevalueof τorcanconsistof τ t−,thenumberofremaining
3 9 0
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
timesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat
endabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis
basedonthedecomposition
P(x( 1 ), ,x( ) τ) = ()( P τ Px( 1 ), ,x( ) τ| τ .)(10.34)
Thestrategyofpredicting τdirectlyisusedforexamplebyGoodfellow e t a l ().2014d
10.2.4ModelingSequencesConditionedonContextwithRNNs
IntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected
graphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of
course,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8
inputsx( 1 ),x( 2 ), ,x( ) τ.Ingeneral,RNNsallowtheextensionofthegraphical
modelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut
alsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof
feedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;θ)
canbereinterpretedasamodelrepresentingaconditionaldistribution P(yω|)
withω=θ.Wecanextendsuchamodeltorepresentadistribution P(yx|)by
usingthesame P(yω|)asbefore,butmakingωafunctionofx.Inthecaseof
anRNN,thiscanbeachievedindiﬀerentways.Wereviewherethemostcommon
andobviouschoices Previously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor
t=1 , Anotheroptionistotakeonlyasinglevectorxasinput Whenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN
thatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto
anRNNare:
1 asanextrainputateachtimestep,or
2 astheinitialstateh( 0 ),or
3 Theﬁrstandmostcommonapproachisillustratedinﬁgure.Theinteraction10.9
betweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly
introducedweightmatrixRthatwasabsentfromthemodelofonlythesequence
of yvalues ThesameproductxRisaddedasadditionalinputtothehidden
unitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue
3 9 1
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
ofxRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits Theweightsremainindependentoftheinput.Wecanthinkofthismodelastaking
theparametersθofthenon-conditional modelandturningthemintoω,where
thebiasparameterswithinarenowafunctionoftheinput ω
o( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t
h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W
s( ) R R R R R
Figure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequences
Y.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis
usedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage

============================================================

=== CHUNK 100 ===
Palavras: 351
Caracteres: 3429
--------------------------------------------------
Eachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent
timestep)and,duringtraining,astarget(fortheprevioustimestep) Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive
asequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8
spondstoaconditionaldistribution P(y( 1 ), ,x( ) τ)thatmakesa
conditionalindependence assumptionthatthisdistributionfactorizesas

tP(y( ) t|x( 1 ), (10.35)
Toremovetheconditionalindependenceassumption,wecanaddconnectionsfrom
theoutputattime ttothehiddenunitattime t+1,asshowninﬁgure.The10.10
modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence Thiskindofmodelrepresentingadistributionoverasequencegivenanother
3 9 2
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
o( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t
h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W
h( ) .V V V
U U U
x( t − 1 )x( t − 1 )R
x( ) tx( ) tx( + 1 ) tx( + 1 ) tR R
Figure10.10: Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence
ofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto
ﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate 10.3
TheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy
givensequencesofxofthesamelength.TheRNNofﬁgureisonlyabletorepresent 10.3
distributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven
thevalues.x
3 9 3
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
sequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust
bethesame.Wedescribehowtoremovethisrestrictioninsection.10.4
o( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t
h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t
x( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t − 1 )g( t − 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t
Figure10.11: Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant
tolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t Thehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe
grecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach
point t,theoutputunitso( ) tcanbeneﬁtfromarelevantsummaryofthepastinitsh( ) t
inputandfromarelevantsummaryofthefutureinitsg( ) tinput 10.3BidirectionalRNNs
Alloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-
ture,meaningthatthestateattime tonlycapturesinformationfromthepast,
x( 1 ), ,x( 1 ) t −,andthepresentinputx( ) t.Someofthemodelswehavediscussed
alsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhenthey
valuesareavailable However,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay
3 9 4
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
dependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect
interpretationofthecurrentsoundasaphonememaydependonthenextfew
phonemesbecauseofco-articulationandpotentiallymayevendependonthenext
fewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere
aretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we
mayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis
alsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning
tasks,describedinthenextsection

============================================================

=== CHUNK 101 ===
Palavras: 353
Caracteres: 6591
--------------------------------------------------
Bidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented
toaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-
cessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting
recognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-
tion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (
e t a l .,).1999
Asthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward
throughtimebeginningfromthestartofthesequencewithanotherRNNthat
movesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11
illustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe
sub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe
sub-RNNthatmovesbackwardthroughtime Thisallowstheoutputunitso( ) t
tocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut
ismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya
ﬁxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,
aconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer) Thisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by
havingRNNs,eachonegoinginoneofthefourdirections: up, down,left, f o u r
right.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea
representationthatwouldcapturemostlylocalinformationbutcouldalsodepend
on long-range inputs,ifthe RNN isable tolearn tocarry that information Comparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore
expensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe
samefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the
forwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows
theyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior
totherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral
interactions 3 9 5
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
10.4Encoder-DecoderSequence-to-SequenceArchitec-
tures
WehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size 10.5
vector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa 10.9
sequence Wehaveseeninﬁgures,,andhowanRNNcan 10.310.410.1010.11
mapaninputsequencetoanoutputsequenceofthesamelength E nc ode r
…
x( 1 )x( 1 )x( 2 )x( 2 )x( ) .x( n x )x( n x )
D e c ode r
…
y( 1 )y( 1 )y( 2 )y( 2 )y( ) .y( n y )y( n y )CC
Figure10.12: Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,
forlearningtogenerateanoutputsequence( y( 1 ), , y( n y ))givenaninputsequence
( x( 1 ), x( 2 ), , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence
andadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa
givenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputea
generallyﬁxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput
sequenceandisgivenasinputtothedecoderRNN HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan
outputsequencewhichisnotnecessarilyofthesamelength This comesupin
manyapplications,suchasspeechrecognition,machinetranslationorquestion
3 9 6
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
answering,wheretheinputandoutputsequencesinthetrainingsetaregenerally
notofthesamelength(althoughtheirlengthsmightberelated) WeoftencalltheinputtotheRNNthe“context.”Wewanttoproducea
representationofthiscontext, C.Thecontext Cmightbeavectororsequenceof
vectorsthatsummarizetheinputsequenceXx= (( 1 ), ThesimplestRNNarchitectureformappingavariable-length sequenceto
anothervariable-length sequencewasﬁrstproposedby ()and Cho e t a l .2014a
shortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-
tectureandweretheﬁrsttoobtainstate-of-the-art translationusingthisapproach Theformersystemisbasedonscoringproposalsgeneratedbyanothermachine
translationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate
thetranslations Theseauthorsrespectivelycalledthisarchitecture, illustrated
inﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12
ideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput
sequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits
ﬁnalhiddenstate (2)adecoderorwriteroroutputRNNisconditionedon
thatﬁxed-lengthvector(justlikeinﬁgure)togeneratetheoutputsequence 10.9
Y=(y( 1 ), ,y( n y )).Theinnovationofthiskindofarchitectureoverthose
presentedinearliersectionsofthischapteristhatthelengths n xand n ycan
varyfromeachother,whilepreviousarchitectures constrained n x= n y= τ.Ina
sequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize
theaverageoflog P(y( 1 ), ,x( n x ))overallthepairsofxandy
sequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically
usedasarepresentation Coftheinputsequencethatisprovidedasinputtothe
decoderRNN Ifthecontext Cisavector,thenthedecoderRNNissimplyavector-to-
sequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4
twowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided
astheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits
ateachtimestep.Thesetwowayscanalsobecombined Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer
asthedecoder Oneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe
encoderRNNhasadimensionthatistoosmalltoproperlysummarizealong
sequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015
ofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather
thanaﬁxed-sizevector.Additionally,theyintroducedanattentionmechanism
thatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput
3 9 7
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
sequence.Seesectionformoredetails 12.4.5.1
10.5DeepRecurrentNetworks
ThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters
andassociatedtransformations:
1 fromtheinputtothehiddenstate,
2 fromtheprevioushiddenstatetothenexthiddenstate,and
3 fromthehiddenstatetotheoutput WiththeRNNarchitectureofﬁgure,eachofthesethreeblocksisassociated 10.3
withasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each
ofthesecorrespondstoashallowtransformation Byashallowtransformation,
wemeanatransformationthatwouldberepresentedbyasinglelayerwithin
adeepMLP.Typicallythisisatransformationrepresentedbyalearnedaﬃne
transformationfollowedbyaﬁxednonlinearity Woulditbeadvantageoustointroducedepthineachoftheseoperations Experimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests
so.Theexperimentalevidenceisinagreementwiththeideathatweneedenough
depthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),
ElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs

============================================================

=== CHUNK 102 ===
Palavras: 395
Caracteres: 5636
--------------------------------------------------
Graves2013 e t a l .()weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing
thestateofanRNNintomultiplelayersasinﬁgure(left).Wecanthink 10.13
ofthelowerlayersinthehierarchydepictedinﬁgureaasplayingarole 10.13
intransformingtherawinputintoarepresentationthatismoreappropriate,at
thehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther
andproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks
enumeratedabove,asillustratedinﬁgureb.Considerationsofrepresentational 10.13
capacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing
sobyaddingdepthmayhurtlearningbymakingoptimization diﬃcult.Ingeneral,
itiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof
ﬁgurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable
intimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden
layerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe
shortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththe
ordinaryRNNofﬁgure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this
3 9 8
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
hy
xz
( a) ( b) ( c )xhy
xhy
Figure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu
e t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )
hierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )
hidden,hidden-to-hiddenandhidden-to-outputparts Thismaylengthentheshortest
pathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby ( c )
introducingskipconnections 3 9 9
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
canbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as
illustratedinﬁgurec.10.13
10.6RecursiveNeuralNetworks
x( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L
x( 4 )x( 4 )Voo
U W U WUW
Figure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe
recurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), ,x( ) tcan
bemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters
(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhich
sometargetisprovidedwhichisassociatedwiththewholesequence y
Recursiveneuralnetworks2representyetanothergeneralization ofrecurrent
networks,withadiﬀerentkindofcomputational graph,whichisstructuredasa
deeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational
graphforarecursivenetworkisillustratedinﬁgure.Recursiveneural 10.14
2W e s u g g e s t t o n o t a b b re v i a t e “ re c u rs i v e n e u ra l n e t w o rk ” a s “ R NN” t o a v o i d c o n f u s i o n with
“ re c u rre n t n e u ra l n e t w o rk ”
4 0 0
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
networkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto
reasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011
appliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,
1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin
computervision( ,) Socher e t a l .2011b
Oneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence
ofthesamelength τ,thedepth(measuredasthenumberofcompositionsof
nonlinearoperations)canbedrasticallyreducedfrom τto O(log τ),whichmight
helpdealwithlong-termdependencies.Anopenquestionishowtobeststructure
thetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,
suchasabalancedbinarytree.Insomeapplicationdomains,externalmethods
cansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural
languagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedto
thestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage
parser( ,,) Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a
discoverandinferthetreestructurethatisappropriateforanygiveninput,as
suggestedby() Bottou2011
Manyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi
e t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,
andassociatethe inputsandtargetswith individualnodesofthe tree.The
computationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcial
neuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotone
nonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a
andbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships
betweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare
representedbycontinuousvectors(embeddings) 10.7TheChallengeofLong-TermDependencies
Themathematical challengeoflearninglong-termdependenciesinrecurrentnet-
workswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5
agatedovermanystagestendtoeithervanish(mostofthetime)orexplode
(rarely,butwithmuchdamagetotheoptimization) Evenifweassumethatthe
parametersaresuchthattherecurrentnetworkisstable(canstorememories,
withgradientsnotexploding),thediﬃcultywithlong-termdependenciesarises
fromtheexponentiallysmallerweightsgiventolong-terminteractions(involving
themultiplicationofmanyJacobians)comparedtoshort-termones.Manyother
sourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,
4 0 1
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
− − − 6 0 4 0 2 0 0 2 0 4 0 6 0
I nput c o o r di na t e− 4− 3− 2− 101234P r o j e c t i o n o f o utput0
1
2
3
4
5
Figure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown
here),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny
derivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing
anddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate
downtoasingledimension,plottedonthe y-axis

============================================================

=== CHUNK 103 ===
Palavras: 352
Caracteres: 8025
--------------------------------------------------
The x-axisisthecoordinateofthe
initialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis
plotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction
aftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction
hasbeencomposed 1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore
detail.Theremainingsectionsdescribeapproachestoovercomingtheproblem Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple
times,oncepertimestep.Thesecompositionscanresultinextremelynonlinear
behavior,asillustratedinﬁgure.10.15
Inparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks
somewhatresemblesmatrixmultiplication Wecanthinkoftherecurrencerelation
h( ) t= Wh( 1 ) t −(10.36)
asaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,
andlackinginputsx.As described insection , thisrecurrencerelation 8.2.5
essentiallydescribesthepowermethod.Itmaybesimpliﬁedto
h( ) t=
Wth( 0 ), (10.37)
andifadmitsaneigendecompositionoftheform W
WQQ = Λ, (10.38)
4 0 2
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
withorthogonal ,therecurrencemaybesimpliﬁedfurtherto Q
h( ) t= QΛtQh( 0 ) (10.39)
Theeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude
lessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto
explode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector
willeventuallybediscarded Thisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine
multiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor
explodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent
networkthathasadiﬀerentweight w( ) tateachtimestep,thesituationisdiﬀerent Iftheinitialstateisgivenby,thenthestateattime 1 tisgivenby
t w( ) t.Suppose
thatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with
zeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome
desiredvariance v∗wemaychoosetheindividualweightswithvariance v=n√
v∗ Verydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe
vanishingandexplodinggradientproblem,asarguedby() Sussillo2014
ThevanishingandexplodinggradientproblemforRNNswasindependently
discoveredbyseparateresearchers(,; ,,) Hochreiter1991Bengio e t a l .19931994
Onemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof
parameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in
ordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN
mustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993
1994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,
thegradientofalongterminteractionhasexponentiallysmallermagnitudethan
thegradientofashortterminteraction It doesnotmeanthatitisimpossible
tolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,
becausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest
ﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments
in ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994
needtobecaptured,gradient-basedoptimization becomesincreasinglydiﬃcult,
withtheprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly
reaching0forsequencesofonlylength10or20 Foradeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya
(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995
inPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious
approachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-
termdependencies(insomecasesallowinganRNNtolearndependenciesacross
4 0 3
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
hundredsofsteps),buttheproblemoflearninglong-termdependenciesremains
oneofthemainchallengesindeeplearning 10.8EchoStateNetworks
Therecurrentweightsmappingfromh( 1 ) t −toh( ) tandtheinputweightsmapping
fromx( ) ttoh( ) taresomeofthemostdiﬃcultparameterstolearninarecurrent
network.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004
Jaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweights
suchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast
inputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently
proposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b
andliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002
thatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued
hiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed
reservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthat
thehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerent
aspectsofthehistoryofinputs Onewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat
theyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the
historyofinputsuptotime t)intoaﬁxed-lengthvector(therecurrentstateh( ) t),
onwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve
theproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe
convexasafunctionoftheoutputweights.Forexample,iftheoutputconsists
oflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining
criterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith
simplelearningalgorithms(,) Jaeger2003
Theimportantquestionistherefore:howdowesettheinputandrecurrent
weightssothatarichsetofhistoriescanberepresentedintherecurrentneural
networkstate Theanswerproposedinthereservoircomputingliteratureisto
viewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent
weightssuchthatthedynamicalsystemisneartheedgeofstability TheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-
statetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5
characteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians
J( ) t=∂ s( ) t
∂ s( 1 ) t −.OfparticularimportanceisthespectralradiusofJ( ) t,deﬁnedto
bethemaximumoftheabsolutevaluesofitseigenvalues 4 0 4
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
Tounderstandtheeﬀectofthespectralradius,considerthesimplecaseof
back-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This
casehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas
aneigenvectorvwithcorrespondingeigenvalue λ.Considerwhathappensaswe
propagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient
vectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n
stepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate
aperturbedversionofg.Ifwebeginwithg+ δv,thenafteronestep,wewill
haveJ(g+ δv).After nsteps,wewillhaveJn(g+ δv).Fromthiswecansee
thatback-propagationstartingfromgandback-propagationstartingfromg+ δv
divergeby δJnvafter nstepsofback-propagation.Ifvischosentobeaunit
eigenvectorofJwitheigenvalue λ,thenmultiplicationbytheJacobiansimply
scalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationare
separatedbyadistanceof δ λ||n.Whenvcorrespondstothelargestvalueof|| λ,
thisperturbationachievesthewidestpossibleseparationofaninitialperturbation
ofsize δ
When || λ >1,thedeviationsize δ λ||ngrowsexponentiallylarge.When || λ <1,
thedeviationsizebecomesexponentiallysmall Ofcourse,thisexampleassumedthattheJacobianwasthesameatevery
timestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena
nonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon
manytimesteps,andhelptopreventtheexplosionresultingfromalargespectral
radius Indeed,themostrecentworkonechostatenetworksadvocatesusinga
spectralradiusmuchlargerthanunity(,;,) Yildiz e t a l .2012Jaeger2012
Everythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-
cationappliesequallytoforwardpropagationinanetworkwithnononlinearity,
wherethestateh( + 1 ) t= h( ) t W WhenalinearmapWalwaysshrinkshasmeasuredbythe L2norm,then
wesaythatthemapiscontractive.Whenthespectralradiusislessthanone,
themappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller
aftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout
thepastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostore
thestatevector

============================================================

=== CHUNK 104 ===
Palavras: 362
Caracteres: 8359
--------------------------------------------------
TheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep
forward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,
duringback-propagation NotethatneitherWnorJneedtobesymmetric(al-
thoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand
eigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory
4 0 5
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
behavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora
smallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan
beexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto
themagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis
coeﬃcients, whenwemultiplythematrixbythevector.Aneigenvaluewith
magnitudegreaterthanonecorrespondstomagniﬁcation (exponentialgrowth,if
appliediteratively)orshrinking(exponentialdecay,ifappliediteratively) Withanonlinearmap, theJacobianisfreetochangeateachstep.The
dynamicsthereforebecomemorecomplicated.However,itremainstruethata
smallinitialvariationcanturnintoalargevariationafterseveralsteps.One
diﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof
asquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome
bounded.Notethat itispossible forback-propagation to retainunbounded
dynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,
whenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare
connectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1
rareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint tanh
Thestrategyofechostatenetworksissimplytoﬁxtheweightstohavesome
spectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3
doesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearities liketanh Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights
inESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e
work(withthehidden-to-hidden recurrentweightstrainedusingback-propagation
throughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;
e t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013
withthesparseinitialization schemedescribedinsection.8.4
10.9LeakyUnitsandOtherStrategiesforMultiple
TimeScales
Onewaytodealwithlong-termdependencies istodesignamodelthatoperates
atmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grained
timescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime
scalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently Variousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.These
includetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegrate
signalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections
4 0 6
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
usedtomodelﬁne-grainedtimescales 10.9.1AddingSkipConnectionsthroughTime
Onewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin
thedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections
datesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996
feedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988
network,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1 Itispossibletoconstructrecurrentnetworkswithlongerdelays(,) Bengio1991
Aswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5
w i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996
connectionswithatime-delayof dtomitigatethisproblem.Gradientsnow
diminishexponentiallyasafunctionofτ
dratherthan τ.Sincethereareboth
delayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin τ Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall
long-termdependencies mayberepresentedwellinthisway 10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales
Anotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto
haveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections Whenweaccumulatearunningaverage µ( ) tofsomevalue v( ) tbyapplyingthe
update µ( ) t← α µ( 1 ) t −+(1− α) v( ) tthe αparameterisanexampleofalinearself-
connectionfrom µ( 1 ) t −to µ( ) t.When αisnearone,therunningaverageremembers
informationaboutthepastforalongtime,andwhen αisnearzero,information
aboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan
behavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky
units Skipconnectionsthrough dtimestepsareawayofensuringthataunitcan
alwayslearntobeinﬂuencedbyavaluefrom dtimestepsearlier.Theuseofa
linearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthatthe
unitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows
thiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valued
αratherthanbyadjustingtheinteger-valuedskiplength Theseideaswereproposedby()andby () Mozer1992 ElHihiandBengio1996
Leakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks
(,) Jaeger e t a l .2007
4 0 7
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
Therearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky
units Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,for
examplebysamplingtheirvaluesfromsomedistributiononceatinitialization time Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem Havingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term
dependencies(,;Mozer1992Pascanu2013 e t a l .,) 10.9.3RemovingConnections
Anotherapproachtohandlelong-termdependenciesistheideaoforganizing
thestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996
informationﬂowingmoreeasilythroughlongdistancesattheslowertimescales Thisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlier
becauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem
withlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateona
longtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d
newconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto
focusontheirothershort-termconnections Therearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto
operateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,
buttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales Thiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu
e t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013
atdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisis
theapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked
wellonanumberofbenchmarkdatasets 10.10TheLongShort-TermMemoryandOtherGated
RNNs
Asofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications
arecalledgatedRNNs.Theseincludethelongshort-termmemoryand
networksbasedonthe gatedrecurrentunit
Likeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough
timethathavederivativesthatneithervanishnorexplode.Leakyunits did
thiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere
parameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange
4 0 8
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
ateachtimestep ×
i nput i nput gate f or ge t   gate output gateoutput
s t at es e l f - l oop×
+ ×
Figure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnected
recurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks Aninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbe
accumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa
linearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan
beshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe
inputunitcanhaveanysquashingnonlinearity Thestateunitcanalsobeusedasan
extrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep Leakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence
foraparticularfeatureorcategory)overalongduration.However,oncethat
informationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe
oldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky
unittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto
forgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento
clearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This
4 0 9
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
iswhatgatedRNNsdo

============================================================

=== CHUNK 105 ===
Palavras: 468
Caracteres: 4816
--------------------------------------------------
10.10.1LSTM
Thecleverideaofintroducingself-loopstoproducepathswherethegradient
canﬂowforlongdurationsisacorecontributionoftheinitiallongshort-term
memory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition
hasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan
ﬁxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000
byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically Inthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescale
ofintegrationcanchangebasedontheinputsequence,becausethetimeconstants
areoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful
inmanyapplications, suchasunconstrainedhandwriting recognition(Graves
e t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),
handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),
imagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and
parsing(Vinyals2014a e t a l .,) TheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding 10.16
forwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent
networkarchitecture Deeperarchitectures havealsobeensuccessfullyused(Graves
e t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-
wisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTM
recurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),
inadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs
andoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda
systemofgatingunitsthatcontrolstheﬂowofinformation Themostimportant
componentisthestateunit s( ) t
ithathasalinearself-loopsimilartotheleaky
unitsdescribedintheprevioussection.However,here,theself-loopweight(orthe
associatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t
i(fortimestep t
andcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i
f( ) t
i= σ
 bf
i+
jUf
i , j x( ) t
j+
jWf
i , j h( 1 ) t −
j
 ,(10.40)
wherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,
containingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively
biases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell
4 1 0
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
internalstateisthusupdatedasfollows,butwithaconditionalself-loopweight
f( ) t
i:
s( ) t
i= f( ) t
i s( 1 ) t −
i + g( ) t
i σ
 b i+
jU i , j x( ) t
j+
jW i , j h( 1 ) t −
j
 ,(10.41)
whereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent
weightsintotheLSTMcell.Theexternalinputgateunit g( ) t
iiscomputed
similarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween
0and1),butwithitsownparameters:
g( ) t
i= σ
 bg
i+
jUg
i , j x( ) t
j+
jWg
i , j h( 1 ) t −
j
 .(10.42)
Theoutput h( ) t
ioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgate q( ) t
i,
whichalsousesasigmoidunitforgating:
h( ) t
i= tanh
s( ) t
i
q( ) t
i (10.43)
q( ) t
i= σ
 bo
i+
jUo
i , j x( ) t
j+
jWo
i , j h( 1 ) t −
j
 (10.44)
whichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent
weights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t
i
asanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown
inﬁgure.Thiswouldrequirethreeadditionalparameters 10.16
LSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily
thanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfor
testingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter
andSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence
processingtaskswherestate-of-the-art performance wasobtained(Graves2012,;
Graves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM
havebeenstudiedandusedandarediscussednext 10.10.2OtherGatedRNNs
Whichpieces ofthe LSTMarchitecture are actually necessary?Whatother
successfularchitecturescouldbedesignedthatallowthenetworktodynamically
controlthetimescaleandforgettingbehaviorofdiﬀerentunits 4 1 1
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
SomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,
whoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b
Chung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain
diﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe
forgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations
arethefollowing:
h( ) t
i= u( 1 ) t −
i h( 1 ) t −
i+(1− u( 1 ) t −
i) σ
 b i+
jU i , j x( 1 ) t −
j +
jW i , j r( 1 ) t −
j h( 1 ) t −
j
 ,
(10.45)
whereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedas
usual:
u( ) t
i= σ
 bu
i+
jUu
i , j x( ) t
j+
jWu
i , j h( ) t
j
 (10.46)
and
r( ) t
i= σ
 br
i+
jUr
i , j x( ) t
j+
jWr
i , j h( ) t
j
 .(10.47)
Theresetandupdatesgatescanindividually“ignore”partsofthestatevector

============================================================

=== CHUNK 106 ===
Palavras: 354
Caracteres: 8498
--------------------------------------------------
Theupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany
dimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely
ignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value
(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol
whichpartsofthestategetusedtocomputethenexttargetstate,introducingan
additionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate Manymorevariantsaroundthisthemecanbedesigned.Forexamplethe
resetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchas
anentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol
andlocalcontrol.However,severalinvestigationsoverarchitectural variations
oftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese
acrossawiderangeoftasks(,; Greﬀ e t a l .2015Jozefowicz2015Greﬀ e t a l .,) e t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz
e t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015
advocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000
exploredarchitecturalvariants 4 1 2
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
10.11OptimizationforLong-TermDependencies
Section andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7
problemsthatoccurwhenoptimizingRNNsovermanytimesteps AninterestingideaproposedbyMartensandSutskever2011()isthatsecond
derivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-order
optimization algorithmsmayroughlybeunderstoodasdividingtheﬁrstderivative
bythesecondderivative(inhigherdimension,multiplyingthegradientbythe
inverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrst
derivative,thentheratioofﬁrstandsecondderivativesmayremainrelatively
constant.Unfortunately,second-ordermethodshavemanydrawbacks,including
highcomputational cost,theneedforalargeminibatch,andatendencytobe
attractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults
usingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler
methodssuchasNesterovmomentumwithcarefulinitialization couldachieve
similarresults.SeeSutskever2012()formoredetail Bothoftheseapproaches
havelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied
toLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften
mucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore
powerfuloptimization algorithm 10.11.1ClippingGradients
Asdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4
byarecurrentnetovermanytimestepstendtohavederivativesthatcanbe
eitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3
ﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17
parameters)hasa“landscape” inwhichoneﬁnds“cliﬀs”:wideandratherﬂat
regionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,
formingakindofcliﬀ Thediﬃcultythatarisesisthatwhentheparametergradientisverylarge,a
gradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa
regionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad
beendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat
correspondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthe
currentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmay
begintocurvebackupwards.Theupdatemustbechosentobesmallenoughto
avoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat
4 1 3
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
decayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning
rate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis
ofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe
landscapeonthenextstep 
 
               

 
            
Figure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwith
twoparameterswandb.Gradientclippingcanmakegradientdescentperformmore
reasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccur
inrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly Thecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix
ismultipliedbyitselfonceforeachtimestep ( L e f t )Gradientdescentwithoutgradient
clippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient
fromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe
axesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )
reactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothat
itcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith
permissionfromPascanu2013 e t a l .() Asimpletypeofsolutionhasbeeninusebypractitioners formanyyears:
clippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;
Pascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch
e l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p
t h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter
update:
if||||g > v (10.48)
g←g v
||||g(10.49)
4 1 4
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
where visthenormthresholdandgisusedtoupdateparameters.Becausethe
gradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchas
weightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter
methodhastheadvantagethatitguaranteesthateachstepisstillinthegradient
direction,butexperimentssuggestthatbothformsworksimilarly.Although
theparameterupdatehasthesamedirectionasthetruegradient,withgradient
normclipping,theparameterupdatevectornormisnowbounded.Thisbounded
gradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In
fact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove
athresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe
gradientisnumerically InforNan(consideredinﬁniteornot-a-number),then
arandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe
numericallyunstableconﬁguration Clippingthegradientnormper-minibatchwill
notchangethedirectionofthegradientforanindividualminibatch.However,
takingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot
equivalenttoclippingthenormofthetruegradient(thegradientformedfrom
usingallexamples).Examplesthathavelargegradientnorm,aswellasexamples
thatappearinthesameminibatchassuchexamples,willhavetheircontribution
totheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch
gradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall
minibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses
anunbiasedestimateofthegradient,whilegradientdescentwithnormclipping
introducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-
wiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient
ortheminibatchgradient,butitisstilladescentdirection.Ithasalsobeen
proposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto
hiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we
conjecturethatallthesemethodsbehavesimilarly 10.11.2RegularizingtoEncourageInformationFlow
Gradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith
vanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term
dependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof
theunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated
witharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-
loopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10
toregularizeorconstraintheparameterssoastoencourage“informationﬂow.”
Inparticular,wewouldlikethegradientvector∇h( ) t Lbeingback-propagatedto
4 1 5
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
maintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe
endofthesequence.Formally,wewant
(∇h( ) t L)∂h( ) t
∂h( 1 ) t −(10.50)
tobeaslargeas
∇h( ) t L (10.51)
Withthisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:
Ω =
t
|∇(h( ) t L)∂ h( ) t
∂ h( 1 ) t −|
||∇h( ) t L||−1
2 (10.52)
Computingthegradientofthisregularizermayappeardiﬃcult,butPascanu
e t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013
vectors∇h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so
thatthereisnoneedtoback-propagatethroughthem).Theexperimentswith
thisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which
handlesgradientexplosion),theregularizercanconsiderablyincreasethespanof
thedependenciesthatanRNNcanlearn

============================================================

=== CHUNK 107 ===
Palavras: 353
Caracteres: 11371
--------------------------------------------------
BecauseitkeepstheRNNdynamics
ontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding AkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor
taskswheredataisabundant,suchaslanguagemodeling 10.12ExplicitMemory
Intelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,
whichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,
therearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-
conscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooks
diﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively
straightforwardtoputintowords—everydaycommonsense knowledge,like“acat
isakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplish
yourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom
141.”
Neuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto
memorizefacts Stochasticgradientdescentrequiresmanypresentationsofthe
4 1 6
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
T ask   ne t w or k ,
c ontrol l i ng th e   m e m o r yMe m or y   c e l l s
W r i t i ng
m e c hani s mR e adi ng
m e c hani s m
Figure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing
someofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe
distinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrent
netinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcan
storefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfrom
andwheretowritetowithinthememory(throughthereadingandwritingmechanisms,
indicatedbyboldarrowspointingatthereadingandwritingaddresses) 4 1 7
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
sameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,
thatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized
thatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory
systemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof
informationthat arerelevantto achieving some goal.Suchexplicit memory
componentswouldallowoursystemsnotonlytorapidlyand“intentionally”store
andretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneed
forneuralnetworksthatcanprocessinformationinasequenceofsteps,changing
thewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized
asimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive
responsestotheinput(,) Hinton1990
Toresolvethisdiﬃculty,Weston2014 e t a l .()introducedmemorynetworks
thatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-
nism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem
howtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural
Turingmachine,whichisabletolearntoreadfromandwritearbitrarycontent
tomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,
andallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof
acontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015
tion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1
relatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows
gradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;
Kumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,) Eachmemorycellcanbethoughtofasanextensionofthememorycellsin
LSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstate
thatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina
digitalcomputerreadfromorwritetoaspeciﬁcaddress Itisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.To
alleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells
simultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they
modifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperations
arechosentobefocusedonasmallnumberofcells,forexample,byproducing
themviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows
thefunctionscontrollingaccesstothememorytobeoptimizedusinggradient
descent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshould
beincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose
memoryaddressesreceivingalargecoeﬃcient Thesememorycellsaretypicallyaugmentedtocontainavector,ratherthan
4 1 8
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
thesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons
toincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe
costofaccessingamemorycell Wepaythecomputational costofproducinga
coeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmall
numberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan
oﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat
theyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor
writefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea
completevector-valuedmemoryifweareabletoproduceapatternthatmatches
somebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan
recallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based
readinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘We
allliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwe
maketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredina
separatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,
location-basedaddressingisnotallowedtorefertothecontentofthememory Wecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsof
thesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensible
mechanismevenwhenthememorycellsaresmall Ifthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then
theinformationitcontainscanbepropagatedforwardintimeandthegradients
propagatedbackwardintimewithouteithervanishingorexploding Theexplicitmemoryapproachisillustratedinﬁgure,whereweseethat 10.18
a“taskneuralnetwork” iscoupledwithamemory.Althoughthattaskneural
networkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork Thetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses ExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM
RNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand
gradientscanbepropagated(forwardintimeorbackwardsintime,respectively)
forverylongdurations Asanalternativetoback-propagationthroughweightedaveragesofmemory
cells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilities and
stochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels
thatmakediscretedecisionsrequiresspecializedoptimization algorithms,described
insection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1
decisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft
decisions Whetheritissoft(allowingback-propagation) orstochasticandhard,the
4 1 9
CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS
mechanism forchoosing anaddress isin itsform identical totheattention
mechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine
translation( ,)anddiscussedinsection Theidea Bahdanau e t a l .2015 12.4.5.1
ofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe
contextofhandwritinggeneration(Graves2013,),withanattentionmechanism
thatwasconstrainedtomoveonlyforwardintimethroughthesequence.In
thecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof
attentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep Recurrentneuralnetworksprovideawaytoextenddeeplearningtosequential
data.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow
movestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world
tasks 4 2 0
C h a p t e r 1 1
Practical Methodology
Successfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood
knowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey
work.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean
algorithmforaparticularapplicationandhowtomonitorandrespondtofeedback
obtainedfromexperimentsinordertoimproveamachinelearningsystem.During
daytodaydevelopmentofmachinelearningsystems,practitioners needtodecide
whethertogathermoredata,increaseordecreasemodelcapacity,addorremove
regularizingfeatures,improvetheoptimization ofamodel,improveapproximate
inferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof
theseoperationsareattheveryleasttime-consuming totryout,soitisimportant
tobeabletodeterminetherightcourseofactionratherthanblindlyguessing Mostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-
rithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost
importantingredienttobeingamachinelearningexpertisknowingawidevariety
ofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-
tice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace
algorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof
analgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe
recommendations inthischapterareadaptedfrom().Ng2015
Werecommendthefollowingpracticaldesignprocess:
•Determineyourgoals—whaterrormetrictouse,andyourtargetvaluefor
thiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe
problemthattheapplicationisintendedtosolve •Establishaworkingend-to-endpipelineassoonaspossible,includingthe
421
CHAPTER11.PRACTICALMETHODOLOGY
estimationoftheappropriateperformancemetrics •Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-
nosewhichcomponentsareperformingworsethanexpectedandwhetherit
isduetooverﬁtting,underﬁtting, oradefectinthedataorsoftware •Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting
hyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfrom
yourinstrumentation Asarunningexample,wewilluseStreetViewaddressnumbertranscription
system( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d
buildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord
theGPScoordinatesassociatedwitheachphotograph Aconvolutionalnetwork
recognizestheaddressnumberineachphotograph, allowingtheGoogleMaps
databasetoaddthataddressinthecorrectlocation.Thestoryofhowthis
commercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign
methodologyweadvocate Wenowdescribeeachofthestepsinthisprocess 11.1PerformanceMetrics
Determiningyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrst
stepbecauseyourerrormetricwillguideallofyourfutureactions Youshould
alsohaveanideaofwhatlevelofperformanceyoudesire Keepinmindthatformostapplications,itisimpossibletoachieveabsolute
zeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopeto
achieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobability
distribution.This isbecause your inputfeatures maynot contain complete
informationabouttheoutputvariable,orbecausethesystemmightbeintrinsically
stochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata Theamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour
goalistobuildthebestpossiblereal-worldproductorservice,youcantypically
collectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh
thisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,
money,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolves
performinginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestion
aboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark
4 2 2
CHAPTER11.PRACTICALMETHODOLOGY
speciﬁcationusuallydeterminesthetrainingsetandyouarenotallowedtocollect
moredata

============================================================

=== CHUNK 108 ===
Palavras: 350
Caracteres: 14090
--------------------------------------------------
Howcanonedetermineareasonablelevelofperformancetoexpect?Typically,
intheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable
basedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we
havesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,
cost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic
desirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate Anotherimportantconsiderationbesidesthetargetvalueoftheperformance
metricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetrics
maybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludes
machinelearningcomponents.Theseperformancemetricsareusuallydiﬀerent
fromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2
commontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem However,manyapplicationsrequiremoreadvancedmetrics Sometimesitismuchmorecostlytomakeonekindofamistakethananother Forexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:
incorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga
spammessagetoappearintheinbox.Itismuchworsetoblockalegitimate
messagethantoallowaquestionablemessagetopassthrough.Ratherthan
measuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeform
oftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost
ofallowingspammessages Sometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsome
rareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose
thatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve
99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiﬁer
toalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto
characterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis
toinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections
reportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents
thatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve
perfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease
wouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho
havethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina
millionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,
withprecisiononthe y-axisandrecallonthe x-axis.Theclassiﬁergeneratesascore
thatishigheriftheeventtobedetectedoccurred Forexample,afeedforward
4 2 3
CHAPTER11.PRACTICALMETHODOLOGY
networkdesignedtodetectadiseaseoutputs ˆ y= P( y=1| x),estimatingthe
probabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas
thedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome
threshold Byvaryingthethreshold,wecantradeprecisionforrecall Inmany
cases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumber
ratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan
F-scor egivenby
F=2 pr
p r+ (11.1)
AnotheroptionistoreportthetotalarealyingbeneaththePRcurve Insomeapplications,itispossibleforthemachinelearningsystemtorefuseto
makeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate
howconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncan
beharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet
Viewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto
transcribetheaddressnumberfromaphotographinordertoassociatethelocation
wherethephotowastakenwiththecorrectaddressinamap.Becausethevalue
ofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd
anaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem
thinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,
thenthebestcourseofactionistoallowahumantotranscribethephotoinstead Ofcourse,themachinelearningsystemisonlyusefulifitisabletodramatically
reducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural
performancemetrictouseinthissituationis c o v e r age.Coverageisthefraction
ofexamplesforwhichthemachinelearningsystemisabletoproducearesponse Itispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy
byrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe
StreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription
accuracywhilemaintaining95%coverage.Human-levelperformanceonthistask
is98%accuracy Manyothermetricsarepossible.Wecanforexample,measureclick-through
rates,collectusersatisfactionsurveys,andsoon Manyspecializedapplication
areashaveapplication-speciﬁccriteriaaswell Whatisimportantistodeterminewhichperformancemetrictoimproveahead
oftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,
itcanbediﬃculttotellwhetherchangestoamachinelearningsystemmake
progressornot 4 2 4
CHAPTER11.PRACTICALMETHODOLOGY
11.2DefaultBaselineModels
Afterchoosingperformancemetricsandgoals, thenextstepinanypractical
applicationistoestablishareasonableend-to-endsystemassoonaspossible.In
thissection,weproviderecommendations forwhichalgorithmstouseastheﬁrst
baselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch
progressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon
afterthiswriting Dependingonthecomplexityofyourproblem,youmayevenwanttobegin
withoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby
justchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple
statisticalmodellikelogisticregression Ifyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobject
recognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely
todowellbybeginningwithanappropriatedeeplearningmodel First,choosethegeneralcategoryofmodelbasedonthestructureofyour
data.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,
useafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown
topologicalstructure(forexample,iftheinputisanimage),useaconvolutional
network.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear
unit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If
yourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU) Areasonablechoiceofoptimization algorithmisSGDwithmomentumwitha
decayinglearningrate(populardecayschemesthatperformbetterorworseon
diﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimumlearning
rate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10
eachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam Batchnormalization canhaveadramaticeﬀectonoptimization performance,
especiallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities Whileitisreasonabletoomitbatchnormalization fromtheveryﬁrstbaseline,it
shouldbeintroducedquicklyifoptimization appearstobeproblematic Unlessyourtrainingsetcontainstensofmillionsofexamplesormore,you
shouldincludesomemildformsofregularizationfromthestart.Earlystopping
shouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy
toimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch
normalization alsosometimesreducesgeneralization errorandallowsdropoutto
beomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize
eachvariable 4 2 5
CHAPTER11.PRACTICALMETHODOLOGY
Ifyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you
willprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalready
knowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy
atrainedmodelfromthattask.Forexample,itiscommontousethefeatures
fromaconvolutionalnetworktrainedonImageNettosolveothercomputervision
tasks( ,) Girshicketal.2015
Acommonquestioniswhethertobeginbyusingunsupervisedlearning,de-
scribedfurtherinpart.Thisissomewhatdomainspeciﬁc.Somedomains,such III
asnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-
visedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother
domains,suchascomputervision,currentunsupervisedlearningtechniquesdo
notbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberof
labeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour
applicationisinacontextwhereunsupervisedlearningisknowntobeimportant,
thenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervised
learninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.You
canalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial
baselineoverﬁts 11.3DeterminingWhethertoGatherMoreData
Aftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-
manceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning
novicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms However,itisoftenmuchbettertogathermoredatathantoimprovethelearning
algorithm Howdoesonedecidewhethertogathermoredata?First,determinewhether
theperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining
setispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready
available,sothereisnoreasontogathermoredata.Instead,tryincreasingthe
sizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearning
ratehyperparameter Iflargemodelsandcarefullytunedoptimization algorithms
donotworkwell,thentheproblemmightbetheofthetrainingdata.The quality
datamaybetoonoisyormaynotincludetherightinputsneededtopredictthe
desiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga
richersetoffeatures Iftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-
4 2 6
CHAPTER11.PRACTICALMETHODOLOGY
formanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,
thenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan
trainingsetperformance,thengatheringmoredataisoneofthemosteﬀective
solutions Thekeyconsiderationsarethecostandfeasibilityofgatheringmore
data,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe
amountofdatathatisexpectedtobenecessarytoimprovetestsetperformance
signiﬁcantly Atlargeinternetcompanieswithmillionsorbillionsofusers,itis
feasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably
lessthantheotheralternatives,sotheanswerisalmostalwaystogathermore
trainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof
themostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas
medicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple
alternativetogatheringmoredataistoreducethesizeofthemodelorimprove
regularization, byadjustinghyperparameters suchasweightdecaycoeﬃcients,
orbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegap
betweentrainandtestperformanceisstillunacceptable evenaftertuningthe
regularizationhyperparameters ,thengatheringmoredataisadvisable Whendecidingwhethertogathermoredata,itisalsonecessarytodecide
howmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween
trainingsetsizeandgeneralization error,likeinﬁgure.Byextrapolatingsuch 5.4
curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto
achieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal
numberofexampleswillnothaveanoticeableimpactongeneralization error.Itis
thereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,
forexampledoublingthenumberofexamplesbetweenconsecutiveexperiments Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove
generalization erroristoimprovethelearningalgorithmitself.Thisbecomesthe
domainofresearchandnotthedomainofadviceforappliedpractitioners 11.4SelectingHyperparameters
Mostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany
aspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthetime
andmemorycostofrunningthealgorithm.Someofthesehyperparameters aﬀect
thequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer
correctresultswhendeployedonnewinputs Therearetwobasicapproachestochoosingthesehyperparameters :choosing
themmanuallyandchoosingthemautomatically .Choosingthehyperparameters
4 2 7
CHAPTER11.PRACTICALMETHODOLOGY
manuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine
learningmodelsachievegoodgeneralization Automatichyperparameterselection
algorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften
muchmorecomputationally costly 1 Ma n u a l Hyp erp a ra m et er T u n i n g
Tosethyperparameters manually,onemustunderstandtherelationshipbetween
hyperparameters,trainingerror,generalization errorandcomputational resources
(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-
damentalideasconcerningtheeﬀectivecapacityofalearningalgorithmfrom
chapter.5
Thegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-
izationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow
todeterminetheruntimeandmemoryimpactofvarioushyperparametershere
becausethisishighlyplatform-dependent Theprimarygoalofmanualhyperparametersearchistoadjusttheeﬀective
capacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacity
isconstrainedbythreefactors: therepresentationalcapacityofthemodel,the
abilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto
trainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure
regularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas
higherrepresentationalcapacity—itiscapableofrepresentingmorecomplicated
functions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if
thetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof
minimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid
someofthesefunctions Thegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas
afunctionofoneofthehyperparameters ,asinﬁgure Atoneextreme,the 5.3
hyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh
becausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,
thehyperparameter valuecorrespondstohighcapacity,andthegeneralization
errorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere
inthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible
generalization error,byaddingamediumgeneralization gaptoamediumamount
oftrainingerror Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-
parameterislarge Thenumberofhiddenunitsinalayerisonesuchexample,
4 2 8
CHAPTER11.PRACTICALMETHODOLOGY
becauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel

============================================================

=== CHUNK 109 ===
Palavras: 355
Caracteres: 8859
--------------------------------------------------
Forsomehyperparameters ,overﬁttingoccurswhenthevalueofthehyperparame-
terissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzero
correspondstothegreatesteﬀectivecapacityofthelearningalgorithm NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve Manyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe
numberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints
alongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters
areswitchesthat specify whetherornotto usesomeoptionalcomponentof
thelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput
featuresbysubtractingtheirmeananddividingbytheirstandarddeviation.These
hyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters
havesomeminimumormaximumvaluethatpreventsthemfromexploringsome
partofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.This
meansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotenter
theoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,
somehyperparameters canonlysubtractcapacity Thelearningrateisperhapsthemostimportanthyperparameter Ifyou
have timeto tuneonly onehyperparameter, tune thelearning rate It con-
trolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanother
hyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearning
rateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially
largeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,
illustratedinﬁgure.Whenthelearningrateistoolarge,gradientdescent 11.1
caninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized
quadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits
optimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a
isnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror Thiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction) Tuningtheparametersotherthanthelearningraterequiresmonitoringboth
trainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,
thenadjustingitscapacityappropriately Ifyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave
nochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare
conﬁdentthatyouroptimization algorithmisperformingcorrectly,thenyoumust
addmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this
increasesthecomputational costsassociatedwiththemodel Ifyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan
4 2 9
CHAPTER11.PRACTICALMETHODOLOGY
1 0− 21 0− 11 00
L e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r
Figure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice
thesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxed
trainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya
factorproportionaltothelearningratereduction Generalizationerrorcanfollowthis
curveorbecomplicatedbyregularizationeﬀectsarisingoutofhavingatoolargeor
toosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent
overﬁtting,andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralization
error nowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand
thegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading
oﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetraining
errorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily
drivenbythegapbetweentrainandtesterror Yourgoalistoreducethisgap
withoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,
changeregularizationhyperparameters toreduceeﬀectivemodelcapacity,suchas
byaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma
largemodelthatisregularizedwell,forexamplebyusingdropout Mosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor
decreasemodelcapacity.SomeexamplesareincludedinTable.11.1
Whilemanuallytuninghyperparameters,donotlosesightofyourendgoal:
goodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve
thisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-
izationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically
guaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize
untilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational
costoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In
4 3 0
CHAPTER11.PRACTICALMETHODOLOGY
HyperparameterIncreases
capacity
when...Reason Caveats
Numberofhid-
denunitsincreasedIncreasingthenumberof
hiddenunitsincreasesthe
representationalcapacity
ofthemodel.Increasingthenumber
ofhiddenunits increases
boththetimeandmemory
costofessentiallyeveryop-
erationonthemodel Learningratetunedop-
timallyAnimproperlearningrate,
whether toohigh ortoo
low,resultsinamodel
withloweﬀectivecapacity
duetooptimizationfailure
Convolutionker-
nelwidthincreasedIncreasingthekernelwidth
increasesthenumberofpa-
rametersinthemodelAwiderkernelresultsin
anarroweroutputdimen-
sion,reducingmodelca-
pacityunlessyouuseim-
plicitzeropaddingtore-
ducethiseﬀect.Wider
kernelsrequiremoremem-
oryforparameterstorage
andincreaseruntime,but
anarroweroutputreduces
memorycost Implicitzero
paddingincreasedAddingimplicitzerosbe-
foreconvolutionkeepsthe
representationsizelargeIncreasedtimeandmem-
orycostofmostopera-
tions Weightdecayco-
eﬃcientdecreasedDecreasingtheweightde-
caycoeﬃcientfreesthe
modelparameterstobe-
comelarger
DropoutratedecreasedDroppingunitslessoften
givestheunitsmoreoppor-
tunitiesto“conspire”with
eachothertoﬁtthetrain-
ingset
Table11.1:Theeﬀectofvarioushyperparametersonmodelcapacity 4 3 1
CHAPTER11.PRACTICALMETHODOLOGY
principle,thisapproachcouldfailduetooptimization diﬃculties,butformany
problemsoptimization doesnotseemtobeasigniﬁcantbarrier,providedthatthe
modelischosenappropriately 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s
Theideallearningalgorithmjusttakesadatasetandoutputsafunction,without
requiringhand-tuning ofhyperparameters .Thepopularityofseverallearning
algorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto
performwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan
sometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but
oftenbeneﬁtsigniﬁcantlyfromtuningoffortyormorehyperparameters .Manual
hyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,
suchasonedeterminedbyothershavingworkedonthesametypeofapplication
andarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring
hyperparametervaluesforneuralnetworksappliedtosimilartasks.However,
formanyapplications,thesestartingpointsarenotavailable.Inthesecases,
automatedalgorithmscanﬁndusefulvaluesofthehyperparameters Ifwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor
goodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:
wearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjective
function,suchasvalidationerror,sometimesunderconstraints(suchasabudget
fortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,
to develop h y p e r par am e t e r   o p t i m i z a t i o nalgorithms thatwrap a learnin g
algorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe
learningalgorithmfromtheuser.Unfortunately,hyperparameter optimization
algorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat
shouldbeexploredforeachofthelearningalgorithm’shyperparameters .However,
thesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat
acceptableperformancemaybeachievedonawiderangeoftasksusingthesame
secondaryhyperparameters foralltasks 3 G ri d S ea rch
Whentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform
g r i d se ar c h.Foreachhyperparameter, the userselectsasmallﬁnitesetof
valuestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint
speciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvalues
foreachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation
4 3 2
CHAPTER11.PRACTICALMETHODOLOGY
Grid Random
Figure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe
displaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore ( L e f t )To
performgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch
algorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese
sets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )
hyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependent
fromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude
uniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa
samplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint
hyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearch
andrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration

============================================================

=== CHUNK 110 ===
Palavras: 354
Caracteres: 9284
--------------------------------------------------
Theﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcant
inﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis
hasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponential
inthenumberofnon-inﬂuentialhyperparameters,whilerandomsearchtestsaunique
valueofeveryinﬂuentialhyperparameteronnearlyeverytrial.Figurereproducedwith
permissionfrom () BergstraandBengio2012
4 3 3
CHAPTER11.PRACTICALMETHODOLOGY
seterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof
ﬁgureforanillustrationofagridofhyperparameter values 11.2
Howshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical
(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen
conservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure
thattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid
searchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning
ratetakenwithintheset{ .1 , .01 ,10−3,10−4,10−5},oranumberofhiddenunits
takenwiththeset { } 501002005001000 2000 , , , , ,
Gridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,
supposethatweranagridsearchoverahyperparameter αusingvaluesof{−1 ,0 ,1} Ifthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 α
liesandweshouldshiftthegridandrunanothersearchwith αin,forexample,
{1 ,2 ,3}.Ifweﬁndthatthebestvalueof αis,thenwemaywishtoreﬁneour 0
estimatebyzoominginandrunningagridsearchover , , .101
Theobviousproblemwithgridsearchisthatitscomputational costgrows
exponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,
eachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials
requiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose
parallelism(withalmostnoneedforcommunication betweendiﬀerentmachines
carryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,
evenparallelization maynotprovideasatisfactorysizeofsearch 4 Ra n d o m S ea rch
Fortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more
convenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :
randomsearch( ,) BergstraandBengio2012
Arandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistribution
foreachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete
hyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued
hyperparameters.Forexample,
l o g l e a r n i n g r a t e __ ∼−− u(1 ,5) (11.2)
l e a r n i n g r a t e_ = 10loglearningrate _ _ (11.3)
where u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b) Similarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,
log(2000) ) 4 3 4
CHAPTER11.PRACTICALMETHODOLOGY
Unlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues
ofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes
notincuradditionalcomputational cost Infact,asillustratedinﬁgure,a11.2
randomsearchcanbeexponentiallymoreeﬃcientthanagridsearch,whenthere
areseveralhyperparametersthatdonotstronglyaﬀecttheperformancemeasure Thisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012
searchreducesthevalidationseterrormuchfasterthangridsearch,intermsof
thenumberoftrialsrunbyeachmethod Aswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom
search,toreﬁnethesearchbasedontheresultsoftheﬁrstrun Themainreasonwhyrandomsearchﬁndsgoodsolutionsfasterthangridsearch
isthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,
whentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )
wouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters
wouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they
wouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovalues
doesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,grid
searchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch
willstillgivetwoindependentexplorationsoftheotherhyperparameters 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n
Thesearchforgoodhyperparameters canbecastasanoptimization problem Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe
validationseterrorthatresultsfromtrainingusingthesehyperparameters .In
simpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiable
errormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan
simplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal 2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either
duetoitshighcomputationandmemorycost,orduetohyperparametershaving
intrinsicallynon-diﬀerentiable interactionswiththevalidationseterror,asinthe
caseofdiscrete-valuedhyperparameters Tocompensateforthislackofagradient,wecanbuildamodelofthevalidation
seterror,thenproposenewhyperparameterguessesbyperformingoptimization
withinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea
Bayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset
errorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-
mizationthusinvolvesatradeoﬀbetweenexploration(proposinghyperparameters
4 3 5
CHAPTER11.PRACTICALMETHODOLOGY
forwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay
alsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel
isconﬁdentwillperformaswellasanyhyperparameters ithasseensofar—usually
hyperparametersthatareverysimilartoonesithasseenbefore).Contemporary
approachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012
TPE( ,)andSMAC( ,) Bergstraetal.2011 Hutteretal.2011
Currently,wecannotunambiguously recommendBayesianhyperparameter
optimization asanestablishedtoolforachievingbetterdeeplearningresultsor
forobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimization
sometimesperformscomparablytohumanexperts,sometimesbetter,butfails
catastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson
aparticularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeing
said,hyperparameter optimization isanimportantﬁeldofresearchthat,while
oftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁt
notonlytheentireﬁeldofmachinelearningbutthedisciplineofengineeringin
general Onedrawbackcommontomosthyperparameter optimization algorithmswith
moresophisticationthanrandomsearchisthattheyrequireforatrainingex-
perimenttoruntocompletionbeforetheyareabletoextractanyinformation
fromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-
mationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman
practitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is
completelypathological ()haveintroducedanearlyversion Swerskyetal.2014
ofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime
points,thehyperparameter optimization algorithmcanchoosetobeginanew
experiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”
andresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven
moreinformation 11.5DebuggingStrategies
Whenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotell
whetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere
isabugintheimplementation ofthealgorithm Machine learningsystemsare
diﬃculttodebugforavarietyofreasons Inmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe
algorithmis.Infact,theentirepointofusingmachinelearningisthatitwill
discoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina
4 3 6
CHAPTER11.PRACTICALMETHODOLOGY
neuralnetworkonaclassiﬁcationtaskanditachieves5%testerror,wehave new
nostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal
behavior Afurtherdiﬃcultyisthatmostmachinelearningmodelshavemultipleparts
thatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill
achieveroughlyacceptableperformance.Forexample,supposethatwearetraining
aneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose
furtherthatwehavemanuallyimplemented thegradientdescentruleforeach
parameterseparately,andwemadeanerrorintheupdateforthebiases:
b b←− α (11.4)
where αisthelearningrate.Thiserroneousupdatedoesnotusethegradientat
all.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which
isclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The
bugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough Dependingonthedistributionoftheinput,theweightsmaybeabletoadaptto
compensateforthenegativebiases Mostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor
bothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthe
correctbehavioractuallycanbepredicted,orwedesignatestthatexercisesone
partoftheneuralnetimplementationinisolation Someimportantdebuggingtestsinclude:
Visualizethemodelinaction:Whentrainingamodeltodetectobjectsin
images,viewsomeimageswiththedetectionsproposedbythemodeldisplayed
superimposedontheimage.Whentrainingagenerativemodelofspeech,listento
someofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto
fallintothepracticeofonlylookingatquantitativeperformancemeasurements
likeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel
performingitstaskwillhelptodeterminewhetherthequantitativeperformance
numbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost
devastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis
performingwellwhenitisnot

============================================================

=== CHUNK 111 ===
Palavras: 375
Caracteres: 11680
--------------------------------------------------
Visualizetheworstmistakes: Mostmodelsareabletooutputsomesortof
conﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedona
softmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned
tothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasin
itsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthese
valuesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,
4 3 7
CHAPTER11.PRACTICALMETHODOLOGY
buttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless
likelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By
viewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan
oftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled Forexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere
theaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit
someofthedigits.Thetranscriptionnetworkthenassignedverylowprobability
tothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost
conﬁdentmistakesshowedthattherewasasystematicproblemwiththecropping Modifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter
performanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded
tobeabletoprocessgreatervariationinthepositionandscaleoftheaddress
numbers Reasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃcultto
determinewhethertheunderlyingsoftwareiscorrectlyimplemented Someclues
canbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror
ishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe
modelisoverﬁttingforfundamentalalgorithmicreasons.Analternativepossibility
isthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe
modelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata
wasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainandtesterrorare
high,thenitisdiﬃculttodeterminewhetherthereisasoftwaredefectorwhether
themodelisunderﬁttingduetofundamentalalgorithmicreasons.Thisscenario
requiresfurthertests,describednext Fitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether
itisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmall
modelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,
aclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiases
oftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectly
labelasingleexample,anautoencodertosuccessfullyreproduceasingleexample
withhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga
singleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe
trainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples Compareback-propagatedderivativestonumericalderivatives:Ifyouareusing
asoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-
putations,orifyouareaddinganewoperationtoadiﬀerentiation libraryand
mustdeﬁneitsbpropmethod,thenacommonsourceoferrorisimplementingthis
gradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect
4 3 8
CHAPTER11.PRACTICALMETHODOLOGY
istocomparethederivativescomputedbyyourimplementation ofautomatic
diﬀerentiationtothederivativescomputedbya .Because ﬁni t e di ﬀ e r e nc e s
f() =lim x
 →0f x  f x (+)−()
, (11.5)
wecanapproximate thederivativebyusingasmall,ﬁnite: 
f() x≈f x  f x (+)−()
 (11.6)
Wecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di ﬀ e r -
e nc e:
f() x≈f x(+1
2 f x )−(−1
2 )
 (11.7)
Theperturbationsize mustchosentobelargeenoughtoensurethatthepertur-
bationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations Usually,wewillwanttotestthegradientorJacobianofavector-valuedfunction
g: Rm→ Rn.Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasingle
derivativeatatime.Wecaneitherrunﬁnitediﬀerencing m ntimestoevaluateall
ofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses
randomprojectionsatboththeinputandoutputof g.Forexample,wecanapply
ourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),
where uand varerandomlychosenvectors.Computing f( x)correctlyrequires
beingabletoback-propagatethrough gcorrectly,yetiseﬃcienttodowithﬁnite
diﬀerencesbecause fhasonlyasingleinputandasingleoutput.Itisusually
agoodideatorepeatthistestformorethanonevalueof uand vtoreduce
thechancethatthetestoverlooksmistakesthatareorthogonaltotherandom
projection Ifonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis
averyeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbers
asinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe
observationthat
f x i  f x i  f (+) = ()+()+( x O 2) (11.8)
real((+)) = ()+( f x i  f x O 2)imag( ,f x i  (+)
) = f()+( x O 2) ,(11.9)
where i=√
−1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeﬀect
duetotakingthediﬀerencebetweenthevalueof fatdiﬀerentpoints.Thisallows
theuseoftinyvaluesof like = 10−150,whichmakethe O( 2)errorinsigniﬁcant
forallpracticalpurposes 4 3 9
CHAPTER11.PRACTICALMETHODOLOGY
Monitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize
statisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount
oftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits
cantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,
howoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,
theaverageoftheabsolutevalueofthepre-activationstellsushowsaturated
theunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor
quicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe
magnitudeofparametergradientstothemagnitudeoftheparametersthemselves Assuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015
overaminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,
not50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay
bethatsomegroupsofparametersaremovingatagoodpacewhileothersare
stalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay
beveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir
evolution Finally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout
theresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III
imateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization
problems Typicallythesecanbedebuggedbytestingeachoftheirguarantees Someguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjective
functionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith
respecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,
andthatthegradientwithrespecttoallvariableswillbezeroatconvergence Usuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital
computer,sothedebuggingtestshouldincludesometoleranceparameter 11.6Example:Multi-DigitNumberRecognition
Toprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology
inpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,
fromthepointofviewofdesigningthedeeplearningcomponents.Obviously,
manyothercomponentsofthecompletesystem,suchastheStreetViewcars,the
databaseinfrastructure,andsoon,wereofparamountimportance Fromthepointofviewofthemachinelearningtask,theprocessbeganwith
datacollection The carscollectedtherawdataandhumanoperatorsprovided
labels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdataset
curation,includingusingothermachinelearningtechniquestodetectthehouse
4 4 0
CHAPTER11.PRACTICALMETHODOLOGY
numberspriortotranscribingthem Thetranscriptionprojectbeganwithachoiceofperformancemetricsand
desiredvaluesforthesemetrics Animportantgeneralprincipleistotailorthe
choiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful
iftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement
forthisproject Speciﬁcally,thegoalwastoobtainhuman-level,98%accuracy Thislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach
thislevelofaccuracy,theStreetViewtranscriptionsystemsacriﬁcescoverage Coveragethusbecamethemainperformancemetricoptimizedduringtheproject,
withaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame
possibletoreducetheconﬁdencethresholdbelowwhichthenetworkrefusesto
transcribetheinput,eventuallyexceedingthegoalof95%coverage Afterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-
ogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa
convolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbegan
withsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork
tooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible
baseline,theﬁrstimplementation oftheoutputlayerofthemodelconsistedof n
diﬀerentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits
weretrainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmax
unittrainedindependently Ourrecommendedmethodologyistoiterativelyreﬁnethebaselineandtest
whethereachchangemakesanimprovement.TheﬁrstchangetotheStreetView
transcriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage
metricandthestructureofthedata.Speciﬁcally,thenetworkrefusestoclassify
aninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor
somethreshold t.Initially,thedeﬁnitionof p( y x|)wasad-hoc,basedonsimply
multiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment
ofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled
log-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction
muchmoreeﬀectively Atthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical
problemswiththeapproach.Ourmethodologythereforesuggeststoinstrument
thetrainandtestsetperformanceinordertodeterminewhethertheproblem
isunderﬁttingoroverﬁtting.Inthiscase,trainandtestseterrorwerenearly
identical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe
availabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain
andtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue
4 4 1
CHAPTER11.PRACTICALMETHODOLOGY
tounderﬁttingorduetoaproblemwiththetrainingdata.Oneofthedebugging
strategieswerecommendistovisualizethemodel’sworsterrors.Inthiscase,that
meantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe
highestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinput
imagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing
removedbythecroppingoperation.Forexample,aphotoofanaddress“1849”
mightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblem
couldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress
numberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,
theteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe
cropregiontobesystematicallywiderthantheaddressnumberdetectionsystem
predicted.Thissinglechangeaddedtenpercentagepointstothetranscription
system’scoverage Finally,thelastfewpercentagepointsofperformancecamefromadjusting
hyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-
tainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror
remainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitsweredue
tounderﬁtting, aswellasduetoafewremainingproblemswiththedatasetitself Overall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof
millionsofaddressestobetranscribedbothfasterandatlowercostthanwould
havebeenpossibleviahumaneﬀort Wehopethatthedesignprinciplesdescribedinthischapterwillleadtomany
othersimilarsuccesses 4 4 2
C h a p t e r 1 2
A p p l i cat i on s
Inthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-
putervision,speechrecognition,naturallanguageprocessing,andotherapplication
areasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork
implementationsrequiredformostseriousAIapplications.Next,wereviewseveral
speciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve

============================================================

=== CHUNK 112 ===
Palavras: 359
Caracteres: 11969
--------------------------------------------------
Whileone
goalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad
varietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision
tasksrequireprocessingalargenumberofinputfeatures(pixels)perexample Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe
vocabulary)perinputfeature 1 L arge- S c a l e D eep L earni n g
Deeplearningisbasedonthephilosophyofconnectionism: whileanindividual
biologicalneuronoranindividualfeatureinamachinelearningmodelisnot
intelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan
exhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe
numberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe
improvementinneuralnetwork’saccuracyandtheimprovementofthecomplexity
oftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein
thesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3
grownexponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksare
onlyaslargeasthenervoussystemsofinsects Becausethesizeofneuralnetworksisofparamountimportance,deeplearning
443
CHAPTER12.APPLICATIONS
requireshighperformancehardwareandsoftwareinfrastructure 12.1.1FastCPUImplementations
Traditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine Today,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPU
computingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto
theseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould
notmanagethehighcomputational workloadrequiredbyneuralnetworks AdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyond
thescopeofthisbook,butweemphasizeherethatcarefulimplementation for
speciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest
CPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-point
arithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-
pointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover
astrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformance
characteristics,sosometimesﬂoating-pointimplementations canbefastertoo Theimportantprincipleisthatcarefulspecializationofnumericalcomputation
routinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouse
ﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemisses
andusingvectorinstructions.Manymachinelearningresearchersneglectthese
implementationdetails,butwhentheperformanceofanimplementation restricts
thesizeofthemodel,theaccuracyofthemodelsuﬀers 12.1.2GPUImplementations
Mostmodernneuralnetworkimplementationsarebasedongraphicsprocessing
units.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents
thatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor
videogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The
performancecharacteristicsneededforgoodvideogamingsystemsturnouttobe
beneﬁcialforneuralnetworksaswell Videogamerenderingrequiresperformingmanyoperationsinparallelquickly Modelsof characters and environments arespeciﬁed intermsof listsof 3-D
coordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and
divisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D
on-screencoordinates.Thegraphicscardmustthenperformmanycomputations
ateachpixelinparalleltodeterminethecolorofeachpixel Inbothcases,the
4 4 4
CHAPTER12.APPLICATIONS
computations arefairlysimpleanddonotinvolvemuchbranchingcomparedto
thecomputational workloadthataCPUusuallyencounters.Forexample,each
vertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno
needtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply
by.Thecomputations arealsoentirelyindependentofeachother,andthusmay
beparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuﬀersof
memory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject
toberendered.Together,thisresultsingraphicscardshavingbeendesignedto
haveahighdegreeofparallelismandhighmemorybandwidth,atthecostof
havingalowerclockspeedandlessbranchingcapabilityrelativetotraditional
CPUs Neuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe
real-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve
largeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,
eachofwhichmustbecompletelyupdatedduringeverystepoftraining.These
buﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer
sothememorybandwidthofthesystemoftenbecomestheratelimitingfactor GPUsoﬀeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth Neuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor
sophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural
networkscanbedividedintomultipleindividual“neurons”thatcanbeprocessed
independentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily
beneﬁtfromtheparallelismofGPUcomputing GPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor
graphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustom
subroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto
pixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe
basedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputingby
writingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrau e t a l ()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005
reportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,
Chellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto
acceleratesupervisedconvolutionalnetworks Thepopularityofgraphicscardsforneuralnetworktrainingexplodedafter
theadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary
code,notjustrenderingsubroutines NVIDIA’sCUDAprogramming language
providedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir
relativelyconvenientprogramming model,massiveparallelism,andhighmemory
4 4 5
CHAPTER12.APPLICATIONS
bandwidth,GP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming Thisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame
available(,; ,) Raina e t a l .2009Ciresan e t a l .2010
WritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospe-
cialists ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery
diﬀerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually
designedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most
writablememorylocationsarenotcached,soitcanactuallybefastertocompute
thesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory GPUcodeisalsoinherentlymulti-threaded andthediﬀerentthreadsmustbe
coordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif
theycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan
eachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory
transaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofread
orwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n
threads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower
of2 TheexactspeciﬁcationsdiﬀerbetweenmodelsofGPU.Anothercommon
considerationforGPUsismakingsurethateachthreadinagroupexecutesthe
sameinstructionsimultaneously.Thismeansthatbranchingcanbediﬃculton
GPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp
executesthesameinstructionduringeachcycle,soifdiﬀerentthreadswithinthe
samewarpneedtoexecutediﬀerentcodepaths,thesediﬀerentcodepathsmust
betraversedsequentiallyratherthaninparallel DuetothediﬃcultyofwritinghighperformanceGPUcode,researchersshould
structuretheirworkﬂowtoavoidneedingtowritenewGPUcodeinordertotest
newmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary
ofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then
specifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the
machinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciﬁesallofits
machinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010
Bastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010
high-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor
multiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon
eitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l 2011b)providesimilarfeatures 4 4 6
CHAPTER12.APPLICATIONS
12.1.3Large-ScaleDistributedImplementations
Inmanycases,thecomputational resourcesavailableonasinglemachineare
insuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinference
acrossmanymachines Distributinginferenceissimple,becauseeachinputexamplewewanttoprocess
canberunbyaseparatemachine.Thisisknownas .dataparallelism
Itisalsopossibletogetmodelparallelism,wheremultiplemachineswork
togetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthe
model.Thisisfeasibleforbothinferenceandtraining Dataparallelismduringtrainingissomewhatharder.Wecanincreasethesize
oftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear
returnsintermsofoptimization performance.Itwouldbebettertoallowmultiple
machinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,
thestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:
thegradientatstepisafunctionoftheparametersproducedbystep t t−1
Thiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-
gio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare
thememoryrepresentingtheparameters.Eachcorereadsparameterswithouta
lock,thencomputesagradient,thenincrementstheparameterswithoutalock Thisreducestheaverageamountofimprovementthateachgradientdescentstep
yields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreased
rateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean
e t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012
togradientdescent,wheretheparametersaremanagedbyaparameterserver
ratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent
remainstheprimarystrategyfortraininglargedeepnetworksandisusedby
mostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,
2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescale
ofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild
distributednetworkswithrelativelylow-costhardwareavailableintheuniversity
setting( ,) Coates e t a l .2013
12.1.4ModelCompression
Inmanycommercialapplications,itismuchmoreimportantthatthetimeand
memorycostofrunninginferenceinamachinelearningmodelbelowthanthat
thetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire
4 4 7
CHAPTER12.APPLICATIONS
personalization,itispossibletotrainamodelonce,thendeployittobeusedby
billionsofusers.Inmanycases,theenduserismoreresource-constrainedthan
thedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha
powerfulcomputercluster,thendeployitonmobilephones Akeystrategyforreducingthecostofinferenceismodelcompression(Bu-
ciluˇa2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,
expensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto
storeandevaluate Modelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven
primarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththe
lowestgeneralization errorisanensembleofseveralindependentlytrainedmodels Evaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel
generalizesbetterifitislarge(forexample,ifitisregularizedwithdropout) Theselargemodelslearnsomefunction f(x),butdosousingmanymore
parametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto
thelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunction
f(x),wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simply
byapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,
modeltomatch f(x)onthesepoints.Inordertomosteﬃcientlyusethecapacity
ofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution
resemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan
bedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative
modeltrainedontheoriginaltrainingset

============================================================

=== CHUNK 113 ===
Palavras: 352
Caracteres: 10809
--------------------------------------------------
Alternatively,onecantrainthesmallermodelonlyontheoriginaltraining
points,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior
distributionovertheincorrectclasses(Hinton20142015 e t a l .,,) 12.1.5DynamicStructure
Onestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems
thathavedynamicstructureinthegraphdescribingthecomputationneeded
toprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich
subsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural
networkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset
offeatures(hiddenunits)tocomputegiveninformationfromtheinput.This
formofdynamicstructureinsideneuralnetworksissometimescalledconditional
computation(,; ,) Sincemanycomponentsof Bengio2013Bengio e t a l .2013b
thearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the
4 4 8
CHAPTER12.APPLICATIONS
systemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded Dynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied
generallythroughoutthesoftwareengineeringdiscipline Thesimplestversions
ofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich
subsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should
beappliedtoaparticularinput Avenerablestrategyforacceleratinginferenceinaclassiﬁeristouseacascade
ofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe
presenceofarareobject(orevent).Toknowforsurethattheobjectispresent,
wemustuseasophisticatedclassiﬁerwithhighcapacity,thatisexpensivetorun However,becausetheobjectisrare,wecanusuallyusemuchlesscomputation
torejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain
asequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacity,
andaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure
wedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁer
istrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe
classiﬁersinasequence,abandoninganyexampleassoonasanyoneelementin
thecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith
highconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecost
offullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascade
canachievehighcapacity.Onewayistomakethelatermembersofthecascade
individuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas
highcapacity,becausesomeofitsindividualmembersdo Itisalsopossibleto
makeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem
asawholehashighcapacityduetothecombinationofmanysmallmodels.Viola
andJones2001()usedacascadeofboosteddecisiontreestoimplementafastand
robustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiﬁer
localizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows
areexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades
usestheearliermodelstoimplementasortofhardattentionmechanism:the
earlymembersofthecascadelocalizeanobjectandlatermembersofthecascade
performfurtherprocessinggiventhelocationoftheobject.Forexample,Google
transcribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade
thatﬁrstlocatestheaddressnumberwithonemachinelearningmodelandthen
transcribesitwithanother(Goodfellow2014d e t a l .,) Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach
nodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput Asimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure
4 4 9
CHAPTER12.APPLICATIONS
istotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe
splittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992
donewiththeprimarygoalofacceleratinginferencecomputations Inthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect
whichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,
giventhecurrentinput.Theﬁrstversionofthisideaiscalledthemixtureof
experts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset
ofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,
andtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputof
theexperts.Inthatcase, theuseofthegaterdoesnotoﬀerareductionin
computational cost,butifasingleexpertischosenbythegaterforeachexample,
weobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002
canconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell
whenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial But
whenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossible
tousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)
allthegaterconﬁgurations Todealwiththisproblem,severalapproacheshave
beenexploredtotraincombinatorialgaters ()experimentwith Bengio e t a l .2013b
severalestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l ()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a
gradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget
anactualreductionincomputational costwithoutimpactingnegativelyonthe
qualityoftheapproximation Another kindof dynamicstructure isa switch, where ahidden unit can
receiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicrouting
approachcanbeinterpretedasanattentionmechanism( ,) Olshausen e t a l .1993
Sofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,
andthusdonotachieveallofthepossiblecomputational beneﬁtsofdynamic
structure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1
Onemajorobstacletousingdynamicallystructuredsystemsisthedecreased
degreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranches
fordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed
asmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We
canwritemorespecializedsub-routinesthatconvolveeachexamplewithdiﬀerent
kernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumns
ofweights.Unfortunately, thesemorespecializedsubroutinesarediﬃcultto
implementeﬃciently.CPUimplementations willbeslowduetothelackofcache
4 5 0
CHAPTER12.APPLICATIONS
coherenceandGPUimplementations willbeslowduetothelackofcoalesced
memorytransactionsandtheneedtoserializewarpswhenmembersofawarptake
diﬀerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe
examplesintogroupsthatalltakethesamebranch,andprocessingthesegroups
ofexamplessimultaneously Thiscanbeanacceptablestrategyforminimizing
thetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄinesetting.In
areal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning
theworkloadcanresultinload-balancing issues.Forexample,ifweassignone
machinetoprocesstheﬁrststepinacascadeandanothermachinetoprocess
thelaststepinacascade,thentheﬁrstwilltendtobeoverloadedandthelast
willtendtobeunderloaded Similarissuesariseifeachmachineisassignedto
implementdiﬀerentnodesofaneuraldecisiontree 12.1.6SpecializedHardwareImplementationsofDeepNetworks
Sincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked
onspecializedhardwareimplementations thatcouldspeeduptrainingand/or
inferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof
specializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l 2003MisraandSaha2010 ; ,) Diﬀerentformsofspecializedhardware(GrafandJackel1989Meadand ,;
Ismail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have
beendevelopedoverthelastdecades,eitherwithASICs(application-speciﬁcinte-
gratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),
analog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-
mentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations
(combiningdigitalandanalogcomponents).InrecentyearsmoreﬂexibleFPGA
(ﬁeldprogrammable gatedarray)implementations(wheretheparticularsofthe
circuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs
andGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoatingpoint
numbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at
leastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;
andHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,
2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning
hasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster
hardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent
researchonspecializedhardwarefordeepnetworksisthattherateofprogressof
asingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin
4 5 1
CHAPTER12.APPLICATIONS
computingspeedhavecomefromparallelization acrosscores(eitherinCPUsor
GPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneural
networkera)wherethehardwareimplementations ofneuralnetworks(whichmight
taketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith
therapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized
hardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware
designsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor
general-public applicationsofdeeplearning(e.g.,withspeech,computervisionor
naturallanguage) Recentworkonlow-precisionimplementationsofbackprop-based neuralnets
(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests
thatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeep
neuralnetworkswithback-propagation Whatisclearisthatmoreprecisionis
requiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic
ﬁxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare
requiredpernumber.Traditionalﬁxedpointnumbersarerestrictedtoaﬁxed
range(whichcorrespondstoagivenexponentinaﬂoatingpointrepresentation) Dynamicﬁxedpointrepresentationssharethatrangeamongasetofnumbers
(suchasalltheweightsinonelayer).Usingﬁxedpointratherthanﬂoatingpoint
representationsandusinglessbitspernumberreducesthehardwaresurfacearea,
powerrequirementsandcomputingtimeneededforperformingmultiplications,
andmultiplications arethemostdemandingoftheoperationsneededtouseor
trainamoderndeepnetworkwithbackprop 2 C om p u t er V i s i on
Computervisionhastraditionallybeenoneofthemostactiveresearchareasfor
deeplearningapplications,becausevisionisataskthatiseﬀortlessforhumans
andmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983
themostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms
ofobjectrecognitionoropticalcharacterrecognition Computervisionisaverybroadﬁeldencompassingawidevarietyofways
ofprocessingimages,andanamazingdiversityofapplications Applicationsof
computervisionrangefromreproducinghumanvisualabilities,suchasrecognizing
faces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof
thelattercategory,onerecentcomputervisionapplicationistorecognizesound
wavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l

============================================================

=== CHUNK 114 ===
Palavras: 353
Caracteres: 7270
--------------------------------------------------
2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch
4 5 2
CHAPTER12.APPLICATIONS
exoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut
ratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep
learningforcomputervisionisusedforobjectrecognitionordetectionofsome
form,whetherthismeansreportingwhichobjectispresentinanimage,annotating
animagewithboundingboxesaroundeachobject,transcribingasequenceof
symbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe
objectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple
ofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis
usingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o
computervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor
imagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor
removingobjectsfromimages 12.2.1Preprocessing
Manyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal
inputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturesto
represent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-
processing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe
same,reasonablerange,like[0,1]or[-1,1] Mixingimagesthatliein[0,1]with
imagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave
thesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many
computervisionarchitectures requireimagesofastandardsize,soimagesmustbe
croppedorscaledtoﬁtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust
thesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,
1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically
scalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan
image( ,) Hadsell e t a l .2007
Datasetaugmentation maybeseenasawayofpreprocessingthetrainingset
only.Datasetaugmentationisanexcellentwaytoreducethegeneralization error
ofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow
themodelmanydiﬀerentversionsofthesameinput(forexample,thesameimage
croppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthe
modelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan
ensembleapproach,andhelpstoreducegeneralization error Otherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith
thegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe
amountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof
4 5 3
CHAPTER12.APPLICATIONS
variationinthedatacanbothreducegeneralization errorandreducethesizeof
themodelneededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmaller
models,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing
ofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput
datathatiseasyforahumandesignertodescribeandthatthehumandesigner
isconﬁdenthasnorelevancetothetask.Whentrainingwithlargedatasetsand
largemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust
letthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For
example,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing
step:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky
e t a l .,).2012
12.2.1.1ContrastNormalization
Oneofthemostobvioussourcesofvariationthatcanbesafelyremoved for
manytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe
magnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage Therearemanywaysofquantifyingthecontrastofanimage.Inthecontextof
deeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan
imageorregionofanimage.Supposewehaveanimagerepresentedbyatensor
X∈ Rr c××3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving
thegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe
entireimageisgivenby
1
3 r cr 
i=1c 
j=13 
k=1
X i , j , k−¯ X2(12.1)
where ¯ Xisthemeanintensityoftheentireimage:
¯ X=1
3 r cr 
i=1c 
j=13 
k=1X i , j , k (12.2)
Globalcontrastnormalization(GCN)aimstopreventimagesfromhaving
varyingamountsofcontrastbysubtractingthemeanfromeachimage, then
rescalingitsothatthe standarddeviation across its pixelsis equaltosome
constant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan
changethecontrastofazero-contrastimage(onewhosepixelsallhaveequal
intensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation
content.Dividingbythetruestandarddeviationusuallyaccomplishesnothing
4 5 4
CHAPTER12.APPLICATIONS
morethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This
motivatesintroducingasmall,positiveregularizationparameter λtobiasthe
estimateofthestandarddeviation.Alternately,onecanconstrainthedenominator
tobeatleast .Givenaninputimage X,GCNproducesanoutputimage X,
deﬁnedsuchthat
X
i , j , k= sX i , j , k−¯ X
max
 ,
λ+1
3 r cr
i=1c
j=13
k=1
X i , j , k−¯ X2 .(12.3)
Datasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely
tocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe
topracticallyignorethesmalldenominator problembysetting λ= 0andavoid
divisionby0inextremelyrarecasesbysetting toanextremelylowvaluelike
10−8 Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a
dataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant
intensity,makingaggressiveregularizationmoreuseful ()used Coates e t a l .2011
 λ = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10 Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011
orchosentomakeeachindividualpixelhavestandarddeviationacrossexamples
closeto1,asdoneby () Goodfellow e t a l .2013a
Thestandarddeviationinequationisjustarescalingofthe 12.3 L2norm
oftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis
preferabletodeﬁneGCNintermsofstandarddeviationratherthan L2norm
becausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN
basedonstandarddeviationallowsthesame stobeusedregardlessofimage
size.However,theobservationthatthe L2normisproportionaltothestandard
deviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping
examplestoasphericalshell.Seeﬁgureforanillustration.Thiscanbea 12.1
usefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections
inspaceratherthanexactlocations.Respondingtomultipledistancesinthe
samedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerent
biases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover Additionally,manyshallowgraphicalmodelshaveproblemswithrepresenting
multipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby
reducingeachexampletoadirectionratherthanadirectionandadistance Counterintuitively,thereisapreprocessingoperationknownasspheringand
itisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata
lieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave
4 5 5
CHAPTER12.APPLICATIONS
− 1 5 0 0 1 5 x 0− 1 5 .0 0 .1 5 .x 1Rawinput
− 1 5 0 0 1 5 x 0GCN, = 10 λ− 2
− 1 5 0 0 1 5 x 0GCN, = 0 λ
Figure12.1:GCNmapsexamplesontoasphere ( L e f t )Rawinputdatamayhaveanynorm ( C e n t e r )GCNwith λ= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse
s= 1and = 10− 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation
ratherthanthe L2norm,theresultingsphereisnottheunitsphere

============================================================

=== CHUNK 115 ===
Palavras: 351
Caracteres: 8353
--------------------------------------------------
( R i g h t )Regularized
GCN,with λ >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe
variationintheirnorm.Weleaveandthesameasbefore s 
equalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas
sphericalcontours.Spheringismorecommonlyknownas .whitening
Globalcontrastnormalization willoftenfailtohighlightimagefeatureswe
wouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge
darkareaandalargebrightarea(suchasacitysquarewithhalftheimagein
theshadowofabuilding)thenglobalcontrastnormalization willensurethereisa
largediﬀerencebetweenthebrightnessofthedarkareaandthebrightnessofthe
lightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout Thismotivateslocalcontrastnormalization.Localcontrastnormalization
ensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover
theimageasawhole.Seeﬁgureforacomparisonofglobalandlocalcontrast 12.2
normalization Variousdeﬁnitionsoflocalcontrastnormalization arepossible.Inallcases,
onemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingby
astandarddeviationofnearbypixels.Insomecases,thisisliterallythemean
andstandarddeviationofallpixelsinarectangularwindowcenteredonthe
pixeltobemodiﬁed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008
andweightedstandarddeviationusingGaussianweightscenteredonthepixelto
bemodiﬁed Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolor
4 5 6
CHAPTER12.APPLICATIONS
Inputimage GCN LCN
Figure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀects
ofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame
scale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local
contrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstant
intensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,
suchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe
normalizationkernelbeingtoohigh channelsseparatelywhileotherscombineinformationfromdiﬀerentchannelsto
normalizeeachpixel( ,) Sermanet e t a l .2012
Localcontrastnormalization canusuallybeimplemented eﬃcientlybyusing
separableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8
localstandarddeviations,thenusingelement-wisesubtractionandelement-wise
divisionondiﬀerentfeaturemaps Localcontrastnormalization isadiﬀerentiable operationandcanalsobeusedas
anonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing
operationappliedtotheinput Aswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal
contrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast
normalization typicallyactsonsmallerwindows,itisevenmoreimportantto
regularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly
thesameaseachother,andthusmorelikelytohavezerostandarddeviation 4 5 7
CHAPTER12.APPLICATIONS
12.2.1.2DatasetAugmentation
Asdescribedinsection,itiseasytoimprovethegeneralization ofaclassiﬁer 7.4
byincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining
examplesthathavebeenmodiﬁedwithtransformationsthatdonotchangethe
class.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenableto
thisform ofdataset augmentationbecause theclass isinvariant toso many
transformationsandtheinputcanbeeasilytransformedwithmanygeometric
operations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,
rotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecialized
computervisionapplications,moreadvancedtransformationsarecommonlyused
fordatasetaugmentation Theseschemesincluderandomperturbationofthe
colorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012
theinput( ,) LeCun e t a l .1998b
12 3 S p eec h R ec ogn i t i o n
Thetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken
naturallanguageutteranceintothecorrespondingsequenceofwordsintendedby
thespeaker.LetX= (x(1),x(2), ,x() T)denotethesequenceofacousticinput
vectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most
speechrecognitionsystemspreprocesstheinputusingspecializedhand-designed
features,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011
fromrawinput.Lety= ( y1 , y2 , , y N)denotethetargetoutputsequence(usually
asequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)
taskconsistsofcreatingafunction f∗
ASRthatcomputesthemostprobablelinguistic
sequencegiventheacousticsequence: y X
f∗
ASR() = argmaxX
yP∗( = ) y X|X (12.4)
where P∗isthetrueconditionaldistributionrelatingtheinputsXtothetargets
y Sincethe1980sanduntilabout2009–2012,state-of-theartspeechrecognition
systemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture
models(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand
phonemes(,),whileHMMsmodeledthesequenceofphonemes Bahl e t a l .1987
TheGMM-HMM modelfamilytreats acousticwaveformsasbeinggenerated
bythefollowingprocess: ﬁrstanHMMgeneratesasequenceofphonemesand
discretesub-phonemicstates(suchasthebeginning,middle,andendofeach
4 5 8
CHAPTER12.APPLICATIONS
phoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof
audiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,
speechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswere
applied,andnumerousASRsystemsfromthelate1980sandearly1990sused
neuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;
Fallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the
performanceofASRbasedonneuralnetsapproximately matchedtheperformance
ofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved
26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993
phonemestodiscriminatebetween), whichwasbetterthanorcomparableto
HMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme
recognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition However,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor
speechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystems
onthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument
forswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both
academicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly
focusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems Later,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition
accuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs
forthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates) Startingin2009,speechresearchersappliedaformofdeeplearningbasedon
unsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas
basedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann
machines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III
Tosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild
deepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM Thesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow
(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates
forthatcenterframe.Trainingsuchdeepnetworkshelpedtosigniﬁcantlyimprove
therecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a
phonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b
analysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone
recognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed
e t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011
byworktoexpandthearchitecturefromphonemerecognition(whichiswhat
TIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012
whichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof
wordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually
4 5 9
CHAPTER12.APPLICATIONS
shiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased
ontechniquessuchasrectiﬁedlinearunitsanddropout(,; Zeiler e t a l .2013Dahl
e t a l .,) Bythattime,severalofthemajorspeechgroupsinindustryhad 2013
startedexploringdeeplearningincollaborationwithacademicresearchers.Hinton
e t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a
arenowdeployedinproductssuchasmobilephones Later,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-
ratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture
ofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither
unnecessaryordidnotbringanysigniﬁcantimprovement

============================================================

=== CHUNK 116 ===
Palavras: 350
Caracteres: 6128
--------------------------------------------------
Thesebreakthroughs inrecognitionperformanceforworderrorrateinspeech
recognitionwereunprecedented (around30%improvement)andwerefollowinga
longperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith
thetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof
trainingsets(seeﬁgure2.4ofDengandYu2014()).Thiscreatedarapidshiftin
thespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly
twoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep
neuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning
algorithmsandarchitectures forASR,whichisstillongoingtoday Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l 2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier
time-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew
two-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone
longvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto
frequencyofspectralcomponents Anotherimportantpush, stillongoing,hasbeentowardsend-to-enddeep
learningspeechrecognitionsystemsthatcompletelyremovetheHMM.Theﬁrst
majorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained
adeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10
phonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves
e t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables
fromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:
ordinarydepthduetoastackoflayers,anddepthduetotimeunfolding This
workbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See
Pascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,
appliedinothersettings Anothercontemporarysteptowardend-to-enddeeplearningASRistoletthe
systemlearnhowto“align”theacoustic-levelinformationwiththephonetic-level
4 6 0
CHAPTER12.APPLICATIONS
information( ,;,) Chorowski e t a l .2014Lu e t a l .2015
12 4 Nat u ra l L an gu a g e Pro c es s i n g
Naturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas
EnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit
specializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimple
programs.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal
description Naturallanguageprocessingincludesapplicationssuchasmachine
translation,inwhichthelearnermustreadasentenceinonehumanlanguageand
emitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications
arebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequences
ofwords,charactersorbytesinanaturallanguage Aswiththeotherapplicationsdiscussedinthischapter,verygenericneural
networktechniquescanbesuccessfullyappliedtonaturallanguageprocessing However,toachieveexcellentperformanceandtoscalewelltolargeapplications,
somedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelof
naturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing
sequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence
ofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal
numberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon
anextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave
beendevelopedtomakemodelsofsuchaspaceeﬃcient,bothinacomputational
andinastatisticalsense 12.4.1-grams n
Alanguagemodeldeﬁnesaprobabilitydistributionoversequencesoftokens
inanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay
beaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The
earliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequences
oftokenscalled-grams.An-gramisasequenceoftokens n n n
Modelsbasedon n-gramsdeﬁnetheconditionalprobabilityofthe n-thtoken
giventhepreceding n−1tokens.Themodelusesproductsoftheseconditional
distributionstodeﬁnetheprobabilitydistributionoverlongersequences:
P x(1 , , x n−1)τ
t n=P x( t| x t n−+1 , , x t−1) .(12.5)
4 6 1
CHAPTER12.APPLICATIONS
Thisdecompositionisjustiﬁedbythechainruleofprobability.Theprobability
distributionovertheinitialsequence P( x1 , , x n−1)maybemodeledbyadiﬀerent
modelwithasmallervalueof n
Training n-grammodelsisstraightforwardbecausethemaximumlikelihood
estimatecanbecomputedsimplybycountinghowmanytimeseachpossible n
gramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore
buildingblockofstatisticallanguagemodelingformanydecades(Jelinekand
Mercer1980Katz1987ChenandGoodman1999 ,;,; ,) Forsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram
for n=2,andtrigramfor n=3 ThesenamesderivefromtheLatinpreﬁxesfor
thecorrespondingnumbersandtheGreeksuﬃx“-gram”denotingsomethingthat
iswritten Usuallywetrainbothan n-grammodelandan n−1 grammodelsimultaneously Thismakesiteasytocompute
P x( t| x t n−+1 , , x t−1) =P n( x t n−+1 , , x t)
P n−1( x t n−+1 , , x t−1)(12.6)
simplybylookinguptwostoredprobabilities Forthistoexactlyreproduce
inferencein P n,wemustomittheﬁnalcharacterfromeachsequencewhenwe
train P n−1 Asanexample,wedemonstratehowatrigrammodelcomputestheprobability
ofthesentence“THEDOGRANAWAY.”Theﬁrstwordsofthesentencecannotbe
handledbythedefaultformulabasedonconditionalprobabilitybecausethereisno
contextatthebeginningofthesentence.Instead,wemustusethemarginalprob-
abilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N) Finally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-
tionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6
weobtain:
P P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N (12.7)
Afundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n
asestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even
thoughthetuple ( x t n−+1 , , x t)mayappearinthetestset.Thiscancausetwo
diﬀerentkindsofcatastrophicoutcomes.When P n−1iszero,theratioisundeﬁned,
sothemodeldoesnotevenproduceasensibleoutput.When P n−1isnon-zerobut
P niszero,thetestlog-likelihoodis−∞ Toavoidsuchcatastrophicoutcomes,
most n-grammodelsemploysomeformofsmoothing.Smoothingtechniques
4 6 2
CHAPTER12.APPLICATIONS
shiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar

============================================================

=== CHUNK 117 ===
Palavras: 351
Caracteres: 8481
--------------------------------------------------
See ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999
techniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext
symbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniform
orDirichletprioroverthecountparameters.Anotherverypopularideaistoform
amixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe
higher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing
morelikelytoavoidcountsofzero.Back-oﬀmethodslook-upthelower-order
n-gramsifthefrequencyofthecontext x t−1 , , x t n−+1istoosmalltousethe
higher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing
contexts x t n k −+ , , x t−1,forincreasing k,untilasuﬃcientlyreliableestimateis
found Classical n-grammodelsareparticularlyvulnerabletothecurseofdimension-
ality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha
massivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset Onewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor
lookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,
similarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely
localpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2
isevenmoreseverethanusual,becauseanytwodiﬀerentwordshavethesamedis-
tancefromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuch
informationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythe
samecontextareusefulforlocalgeneralization T oovercometheseproblems,a
languagemodelmustbeabletoshareknowledgebetweenonewordandother
semanticallysimilarwords Toimprovethestatisticaleﬃciencyof n-grammodels,class-basedlanguage
models(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce
thenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat
areinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe
setofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith
otherwords.ThemodelcanthenusewordclassIDsratherthanindividualword
IDstorepresentthecontextontherightsideoftheconditioningbar.Composite
modelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀare
alsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences
inwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis
lostinthisrepresentation 4 6 3
CHAPTER12.APPLICATIONS
12.4.2NeuralLanguageModels
NeurallanguagemodelsorNLMsare aclassoflanguagemodeldesigned
toovercomethecurseofdimensionalityproblemformodelingnaturallanguage
sequencesbyusingadistributedrepresentationofwords( ,) Bengio e t a l .2001
Unlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize
thattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct
fromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone
word(anditscontext)andothersimilarwordsandcontexts.Thedistributed
representationthemodellearnsforeachwordenablesthissharingbyallowingthe
modeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe
worddogandthewordcatmaptorepresentationsthatsharemanyattributes,then
sentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby
themodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere
aremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,
transferringinformationfromeachtrainingsentencetoanexponentiallylarge
numberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe
modeltogeneralizetoanumberofsentencesthatisexponentialinthesentence
length.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan
exponentialnumberofsimilarsentences Wesometimescallthesewordrepresentationswordembeddings.Inthis
interpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal
tothevocabularysize.Thewordrepresentationsembedthosepointsinafeature
spaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby
aone-hotvector,soeverypairofwordsisatEuclideandistance√
2fromeach
other.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts
(oranypairofwordssharingsome“features”learnedbythemodel)arecloseto
eachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors Figurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow 12.3
howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother Neuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,a
hiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”Usually
NLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause
naturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden
layerhasprovidedamorequalitativelydramaticchangeinthewaythedatais
represented Thebasicideaofusingdistributedrepresentationstoimprovemodelsfor
naturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe
usedwithgraphicalmodelsthathavedistributedrepresentationsintheformof
4 6 4
CHAPTER12.APPLICATIONS
multiplelatentvariables(MnihandHinton2007,) − − − − − 3432302826−14−13−12−11−10−9−8−7−6
CanadaEuropeOntario
NorthEnglish
CanadianUnionAfricanAfrica
BritishFrance
RussianChina
GermanyFrench
AssemblyEU JapanIraq
SouthEuropean
350355360365370375380 .171819202122
1995199619971998199920002001
200220032004
20052006200720082009
Figure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural
machinetranslationmodel( ,),zoominginonspeciﬁcareaswhere Bahdanau e t a l .2015
semanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries
appearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D
forthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher
dimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords 12.4.3High-DimensionalOutputs
Inmanynaturallanguageapplications,weoftenwantourmodelstoproduce
words(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge
vocabularies,itcanbeverycomputationally expensivetorepresentanoutput
distributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany
applications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto
representingsuchadistributionistoapplyanaﬃnetransformationfromahidden
representationtotheoutputspace,thenapplythesoftmaxfunction.Suppose
wehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear
componentofthisaﬃnetransformationisverylarge,becauseitsoutputdimension
is|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh
computational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall
|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining
timeaswellastesttime—wecannotcalculateonlythedotproductwiththeweight
vectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer
thusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and
attesttime(tocomputeprobabilities forallorselectedwords).Forspecialized
4 6 5
CHAPTER12.APPLICATIONS
lossfunctions,thegradientcanbecomputedeﬃciently( ,),but Vincent e t a l .2015
thestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes
manydiﬃculties Supposethathisthetophiddenlayerusedtopredicttheoutputprobabilities
ˆy.IfweparametrizethetransformationfromhtoˆywithlearnedweightsW
andlearnedbiasesb,thentheaﬃne-softmaxoutputlayerperformsthefollowing
computations:
a i= b i+
jW i j h j∀∈{ ||} i1 , , V , (12.8)
ˆ y i=ea i
|| V
i=1 eai  (12.9)
Ifhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe
thousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe
computationofmostneurallanguagemodels 12.4.3.1UseofaShortList
Theﬁrstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003
ofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary
sizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()
builtuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost
frequentwords(handledbytheneuralnet)andatail T= V L\ofmorerarewords
(handledbyan n-grammodel) Tobeabletocombinethetwopredictions,the
neuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext
Cbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput
unittoprovideanestimateof P( i C ∈| T ).Theextraoutputcanthenbeusedto
achieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V
P y i C (= |) =1 i∈ L P y i C, i P i C (= | ∈ − L)(1 (∈| T ))
+1 i∈ T P y i C, i P i C (= | ∈ T)(∈| T )(12.10)
where P( y= i C, i| ∈ L)isprovidedbytheneurallanguagemodeland P( y= i|
C, i∈ T) isprovidedbythe n-grammodel.Withslightmodiﬁcation,thisapproach
canalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmax
layer,ratherthanaseparatesigmoidunit

============================================================

=== CHUNK 118 ===
Palavras: 410
Caracteres: 7200
--------------------------------------------------
Anobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-
alizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent
4 6 6
CHAPTER12.APPLICATIONS
words,where,arguably,itistheleastuseful Thisdisadvantagehasstimulated
theexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,
describedbelow 12.4.3.2HierarchicalSoftmax
Aclassicalapproach(,)toreducingthecomputational burden Goodman2001
ofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose
probabilities hierarchically .Insteadofnecessitatinganumberofcomputations
proportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),
the|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand
Bengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage
models Onecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories
ofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc Thesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,
thetreehasdepth O(log|| V) Theprobabilityofachoosingawordisgivenby
theproductoftheprobabilities ofchoosingthebranchleadingtothatwordat
everynodeonapathfromtherootofthetreetotheleafcontainingtheword Figureillustratesasimpleexample ()alsodescribe 12.4 MnihandHinton2009
howtousemultiplepathstoidentifyasinglewordinordertobettermodelwords
thathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves
summationoverallofthepathsthatleadtothatword Topredicttheconditionalprobabilities requiredateachnodeofthetree,we
typicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe
samecontext Casinputtoallofthesemodels.Becausethecorrectoutputis
encodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic
regressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,
correspondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions Becausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowaslog|| V
ratherthan|| V),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnot
onlythegradientwithrespecttotheoutputparametersbutalsothegradients
withrespecttothehiddenlayeractivations Itispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize
theexpectednumberofcomputations Toolsfrominformationtheoryspecifyhow
tochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To
doso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword
isapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in
4 6 7
CHAPTER12.APPLICATIONS
( 1) ( 0)
( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)
w 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7
Figure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , , w 7
organizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords Internalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence
ofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)
containstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}
and{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which
respectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissuﬃcientlybalanced,
themaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof
thenumberofwords|| V: thechoiceofoneoutof|| Vwordscanbeobtainedbydoing
O(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,
computingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,
associatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom
theroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree
towardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct
ofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach
nodeindexedbythepreﬁxofthesebits.Forexample,node(1 ,0)correspondstothe
preﬁx( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:
P w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)
= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)
4 6 8
CHAPTER12.APPLICATIONS
practice,thecomputational savingsaretypicallynotworththeeﬀortbecausethe
computationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation
intheneurallanguagemodel.Forexample,supposethereare lfullyconnected
hiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits
requiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese
words.Inthisexample,thenumberofoperationsneededtocomputethehidden
activationsgrowsasas O( l n2
h)whiletheoutputcomputations growas O( n h n b) Aslongas n b≤ l n h,wecanreducecomputationmorebyshrinking n hthanby
shrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely
exceedsamillionwordsandlog2(106)≈20,itispossibletoreduce n btoabout,20
but n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing
atreewithabranchingfactorof,onecaninsteaddeﬁneatreewithdepthtwo 2
andabranchingfactorof
|| V.Suchatreecorrespondstosimplydeﬁningaset
ofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth
twocapturesmostofthecomputational beneﬁtofthehierarchicalstrategy Onequestionthatremainssomewhatopenishowtobestdeﬁnetheseword
classes,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexisting
hierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005
jointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexact
optimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword
hierarchyisadiscreteone,notamenabletogradient-basedoptimization However,
onecouldusediscreteoptimization toapproximately optimizethepartitionof
wordsintowordclasses Animportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-
tionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewantto
computetheprobabilityofspeciﬁcwords Ofcourse,computingtheprobabilityofall|| Vwordswillremainexpensive
evenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe
mostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot
provideaneﬃcientandexactsolutiontothisproblem Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse
testresultsthansampling-basedmethodswewilldescribenext.Thismaybedue
toapoorchoiceofwordclasses 12.4.3.3ImportanceSampling
Onewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly
computingthecontributionofthegradientfromallofthewordsthatdonotappear
4 6 9
CHAPTER12.APPLICATIONS
inthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe
model.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,
itispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced
inequation,thegradientcanbewrittenasfollows: 12.8
∂ P y C log(|)
∂ θ=∂logsoftmax y()a
∂ θ(12.13)
=∂
∂ θlogea y

i ea i(12.14)
=∂
∂ θ( a y−log
iea i) (12.15)
=∂ a y
∂ θ−
iP y i C (= |)∂ a i
∂ θ(12.16)
whereaisthevectorofpre-softmaxactivations(orscores),withoneelement
perword.Theﬁrsttermisthepositivephaseterm(pushing a yup)whilethe
secondtermisthenegativephaseterm(pushing a idownforall i,withweight
P( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith
aMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself

============================================================

=== CHUNK 119 ===
Palavras: 383
Caracteres: 8364
--------------------------------------------------
Samplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,
whichispreciselywhatwearetryingtoavoid Insteadofsamplingfromthemodel,onecansamplefromanotherdistribution,
calledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect
forthebiasintroducedbysamplingfromthewrongdistribution(Bengioand
Sénécal2003BengioandSénécal2008 ,; ,).Thisisanapplicationofamoregeneral
techniquecalledimportancesampling,whichwillbedescribedinmoredetail
insection.Unfortunately,evenexactimportancesamplingisnoteﬃcient 17.2
becauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan
onlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor
thisapplicationiscalledbiasedimportancesampling,wheretheimportance
weightsarenormalizedtosumto1.Whennegativeword n iissampled,the
associatedgradientisweightedby
w i=p n i /q n iN
j=1 p n j /q n j (12.17)
Theseweightsareusedtogivetheappropriateimportancetothe mnegative
samplesfrom qusedtoformtheestimatednegativephasecontributiontothe
4 7 0
CHAPTER12.APPLICATIONS
gradient:
|| V
i=1P i C(|)∂ a i
∂ θ≈1
mm 
i=1w i∂ a n i
∂ θ (12.18)
Aunigramorabigramdistributionworkswellastheproposaldistribution q.Itis
easytoestimatetheparametersofsuchadistributionfromdata.Afterestimating
theparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently Importancesamplingisnotonlyusefulforspeedingupmodelswithlarge
softmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge
sparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n
choice.Anexampleisabagofwords.Abagofwordsisasparsevectorv
where v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe
document.Alternately, v icanindicatethenumberoftimesthatword iappears Machinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain
foravarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto
maketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight
mostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto
everyelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa
computational beneﬁttousingsparseoutputs,becausethemodelmaychooseto
makethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto
becomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero Dauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing
importancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionfor
the“positivewords”(thosethatarenon-zerointhetarget)andanequalnumber
of“negativewords.”Thenegativewordsarechosenrandomly,usingaheuristicto
samplewordsthataremorelikelytobemistaken Thebiasintroducedbythis
heuristicoversamplingcanthenbecorrectedusingimportanceweights Inallofthesecases,thecomputational complexityofgradientestimationfor
theoutputlayerisreducedtobeproportionaltothenumberofnegativesamples
ratherthanproportionaltothesizeoftheoutputvector 12.4.3.4Noise-ContrastiveEstimationandRankingLoss
Otherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-
tionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly
exampleistherankinglossproposedbyCollobertandWeston2008a(),which
viewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto
makethescoreofthecorrectword a yberankedhighincomparisontotheother
4 7 1
CHAPTER12.APPLICATIONS
scores a i.Therankinglossproposedthenis
L=
imax(01 ,− a y+ a i) (12.19)
Thegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is
greaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith
thiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which
areusefulinsomeapplications,includingspeechrecognitionandtextgeneration
(includingconditionaltextgenerationtaskssuchastranslation) Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-
contrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6
beensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;
andKavukcuoglu2013,) 12.4.4CombiningNeuralLanguageModelswith-grams n
Amajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels
achievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)
whilerequiringverylittlecomputationtoprocessanexample(bylookingup
onlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees
toaccessthecounts,thecomputationusedfor n-gramsisalmostindependent
ofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameters
typicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels
thatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle
embeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing
thecomputationtimeperexample.Someothermodels,suchastiledconvolutional
networks,canaddparameterswhilereducingthedegreeofparametersharing
inordertomaintainthesameamountofcomputation However,typicalneural
networklayersbasedonmatrixmultiplication useanamountofcomputation
proportionaltothenumberofparameters Oneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble
consistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio
e t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003
theensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearning
providesmanywaysofcombiningtheensemblemembers’predictions,including
uniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()
extendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels Itisalsopossibletopairaneuralnetworkwithamaximumentropymodeland
trainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining
4 7 2
CHAPTER12.APPLICATIONS
aneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe
output,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare
indicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese
variablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity
ishuge—thenewportionofthearchitecturecontainsupto|| s Vnparameters—but
theamountofaddedcomputationneededtoprocessaninputisminimalbecause
theextrainputsareverysparse 12.4.5NeuralMachineTranslation
Machinetranslationisthetaskofreadingasentenceinonenaturallanguageand
emittingasentencewiththeequivalentmeaninginanotherlanguage Mac hine
translationsystemsofteninvolvemanycomponents.Atahighlevel,thereis
oftenonecomponentthatproposesmanycandidatetranslations.Manyofthese
translationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.For
example,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish
directlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggests
manyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecond
componentofthetranslationsystem,alanguagemodel,evaluatestheproposed
translations,andcanscore“redapple”asbetterthan“applered.”
Theearliestuseofneuralnetworksformachinetranslationwastoupgradethe
languagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk
e t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad
usedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor
machinetranslationincludenotjusttraditionalback-oﬀ n-grammodels(Jelinek
andMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum
entropylanguagemodels(,),inwhichanaﬃne-softmaxlayer Berger e t a l .1996
predictsthenextwordgiventhepresenceoffrequent-gramsinthecontext n
Traditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage
sentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven
aninputsentence,itmakessensetoextendthenaturallanguagemodeltobe
conditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1
thatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditional
distributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable
oralistofvariables ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014
machinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , ,t k
inthetargetlanguagegivenaphrases1 ,s2 , ,s ninthesourcelanguage.The
MLPestimates P(t1 ,t2 , ,s n).TheestimateformedbythisMLP
replacestheestimateprovidedbyconditional-grammodels n
4 7 3
CHAPTER12.APPLICATIONS
D e c ode rO ut put ob j e c t   ( E ngl i s h 
s e nt e nc e )
I nt e r m e di at e ,   s e m a n t i c   r e pr e s e nt a t i o n
Sourc e   ob j e c t   ( F r e nc h  s e n t e nc e   or   i m a g e )E nc ode r
Figure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface
representation(suchasasequenceofwordsoranimage)andasemanticrepresentation

============================================================

=== CHUNK 120 ===
Palavras: 352
Caracteres: 7876
--------------------------------------------------
Byusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping
fromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as
theinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden
representationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat
translatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust
tomachinetranslationbutalsotocaptiongenerationfromimages AdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe
preprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewould
liketouseamodelthatcanaccommodatevariablelengthinputsandvariable
lengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4
ofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence
givensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4
whentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequence
andemitsadatastructurethatsummarizestheinputsequence.Wecallthis
summarythe“context” C.Thecontext Cmaybealistofvectors,oritmaybea
vectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN
(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional
network(KalchbrennerandBlunsom2013,) Asecondmodel,usuallyanRNN,
thenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This
generalideaofanencoder-decoderframeworkformachinetranslationisillustrated
inﬁgure.12.5
Inordertogenerateanentiresentenceconditionedonthesourcesentence,the
modelmusthaveawaytorepresenttheentiresourcesentence Earliermodels
wereonlyabletorepresentindividualwordsorphrases Fromarepresentation
4 7 4
CHAPTER12.APPLICATIONS
learningpointofview,itcanbeusefultolearnarepresentationinwhichsentences
thathavethesamemeaninghavesimilarrepresentationsregardlessofwhether
theywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas
exploredﬁrstusingacombinationofconvolutionsandRNNs(Kalchbrennerand
Blunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed
translations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever
e t a l ,).2014Jean()scaledthesemodelstolargervocabularies 2014
12.4.5.1UsinganAttentionMechanismandAligningPiecesofData
α( t − 1 )α( t − 1 )α( ) tα( ) tα( + 1 ) tα( + 1 ) t
h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c
× × × × × ×+
Figure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015
essentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage
offeaturevectorsh( ) twithweights α( ) t.Insomeapplications,thefeaturevectorshare
hiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The
weights α( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval
[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage
approximatesreadingthatonespeciﬁctimestepprecisely.Theweights α( ) tareusually
producedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion
ofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly
indexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The
attentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximation
thatcanbetrainedwithexistingoptimizationalgorithms Usingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofavery
longsentenceofsay60wordsisverydiﬃcult Itcanbeachievedbytraininga
suﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho
e t a l .()and2014aSutskever2014 e t a l .().However,amoreeﬃcientapproachis
toreadthewholesentenceorparagraph(togetthecontextandthegistofwhat
4 7 5
CHAPTER12.APPLICATIONS
isbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime
focusingonadiﬀerentpartoftheinputsentenceinordertogatherthesemantic
detailsthatarerequiredtoproducethenextoutputword Thatisexactlythe
ideathat ()ﬁrstintroduced.Theattentionmechanismused Bahdanau e t a l .2015
tofocusonspeciﬁcpartsoftheinputsequenceateachtimestepisillustratedin
ﬁgure.12.6
Wecanthinkofanattention-basedsystemashavingthreecomponents:
1.Aprocessthat“ r e a d s”rawdata(suchassourcewordsinasourcesentence),
andconvertsthemintodistributedrepresentations,withonefeaturevector
associatedwitheachwordposition 2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe
understoodasa“” containingasequenceoffacts,whichcanbe m e m o r y
retrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall
ofthem 3.Aprocessthat“”thecontentofthememorytosequentiallyperform e x p l o i t s
atask,ateachtimestephavingtheabilityputattentiononthecontentof
onememoryelement(orafew,withadiﬀerentweight) Thethirdcomponentgeneratesthetranslatedsentence Whenwordsinasentencewritteninonelanguagearealignedwithcorrespond-
ingwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate
thecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna
kindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe
wordembeddingsinanother(Kočiský2014 e t a l .,),yieldingloweralignmenterror
ratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable Thereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,
2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcient
cross-lingualalignment( ,)allowstrainingonlargerdatasets Gouws e t a l .2014
12.4.6HistoricalPerspective
TheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart
e t a l .()inoneoftheﬁrstexplorationsofback-propagation, withsymbols 1986a
correspondingtotheidentityoffamilymembersandtheneuralnetworkcapturing
therelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets
suchas(Colin,Mother,Victoria) The ﬁrstlayeroftheneuralnetworklearned
arepresentationofeachfamilymember.Forexample, thefeaturesforColin
4 7 6
CHAPTER12.APPLICATIONS
mightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas
in,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas
computinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe
desiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois
themotherofColin Theideaofforminganembeddingforasymbolwasextendedtotheideaofan
embeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned
usingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks Thehistoryofnaturallanguageprocessingismarkedbytransitionsinthe
popularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Following
thisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural
networkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented
theinputasasequenceofcharacters Bengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced
neurallanguagemodels,whichproduceinterpretable wordembeddings.These
neuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbols
inthe1980stomillionsofwords(includingpropernounsandmisspellings)in
modernapplications.Thiscomputational scalingeﬀortledtotheinventionofthe
techniquesdescribedaboveinsection.12.4.3
Initially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded
improvedlanguage modeling performance( ,).Tothisday, Bengio e t a l .2001
newtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,
2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015
modelingindividualbytesofUnicodecharacters Theideasbehindneurallanguagemodelshavebeenextendedintoseveral
naturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004
Collobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,
sometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,
2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross
tasks Two-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-
alyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality
reductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-
cationtovisualizationwordembeddingsbyJosephTurianin2009 4 7 7
CHAPTER12.APPLICATIONS
12

============================================================

=== CHUNK 121 ===
Palavras: 366
Caracteres: 12276
--------------------------------------------------
5 O t h er A p p l i c a t i o n s
Inthissectionwecoverafewothertypesofapplicationsofdeeplearningthat
arediﬀerentfromthestandardobjectrecognition,speechrecognitionandnatural
languageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III
scopeevenfurthertotasksthatremainprimarilyresearchareas 12.5.1RecommenderSystems
Oneofthemajorfamiliesofapplicationsofmachinelearningintheinformation
technologysectoristheabilitytomakerecommendations ofitemstopotential
usersorcustomers.Twomajortypesofapplicationscanbedistinguished:online
advertisinganditemrecommendations (oftentheserecommendations arestillfor
thepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween
auserandanitem,eithertopredicttheprobabilityofsomeaction(theuser
buyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which
maydependonthevalueoftheproduct)ifanadisshownorarecommendation is
maderegardingthatproducttothatuser.Theinternetiscurrentlyﬁnancedin
greatpartbyvariousformsofonlineadvertising Therearemajorpartsofthe
economythatrelyononlineshopping CompaniesincludingAmazonandeBay
usemachinelearning,includingdeeplearning,fortheirproductrecommendations Sometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude
selectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto
watch,recommendingjokes,recommendingadvicefromexperts,matchingplayers
forvideogames,ormatchingpeopleindatingservices Often,thisassociationproblemishandledlikeasupervisedlearningproblem:
givensomeinformationabouttheitemandabouttheuser,predicttheproxyof
interest(userclicksonad,userentersarating,userclicksona“like”button,user
buysproduct,userspendssomeamountofmoneyontheproduct,userspends
timevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera
regressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic
classiﬁcationproblem(predictingtheconditionalprobabilityofsomediscrete
event) Theearlyworkonrecommendersystemsreliedonminimalinformationas
inputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the
onlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof
thetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1
anduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and
4 7 8
CHAPTER12.APPLICATIONS
user2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong
cuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder
thenameofcollaborativeﬁltering.Bothnon-parametric approaches(suchas
nearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof
preferences)andparametricmethodsarepossible.Parametricmethodsoftenrely
onlearningadistributedrepresentation(alsocalledanembedding)foreachuser
andforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa
simpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent
ofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween
theuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat
dependonlyoneithertheuserIDortheitemID).LetˆRbethematrixcontaining
ourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith
itemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively
akindofbiasforeachuser(representinghowgrumpyorpositivethatuseris
ingeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear
predictionisthusobtainedasfollows:
ˆ R u , i= b u+ c i+
jA u , j B j , i (12.20)
Typicallyonewantstominimizethesquarederrorbetweenpredictedratings
ˆ R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe
convenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoor
three),ortheycanbeusedtocompareusersoritemsagainsteachother,just
likewordembeddings One waytoobtaintheseembeddingsisbyperforminga
singularvaluedecompositionofthematrixRofactualtargets(suchasratings) ThiscorrespondstofactorizingR=UDV(oranormalizedvariant)intothe
productoftwofactors,thelowerrankmatricesA=UDandB=V.One
problemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,
asiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid
payinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum
ofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-
basedoptimization TheSVDandthebilinearpredictionofequation both12.20
performedverywellinthecompetitionfortheNetﬂixprize( , BennettandLanning
2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsby
alargesetofanonymoususers Manymachinelearningexpertsparticipatedin
thiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof
researchinrecommendersystemsusingadvancedmachinelearningandyielded
improvementsinrecommendersystems.Eventhoughitdidnotwinbyitself,
thesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels
4 7 9
CHAPTER12.APPLICATIONS
presentedbymostofthecompetitors,includingthewinners( ,; Töscher e t a l .2009
Koren2009,) Beyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrst
usesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirected
probabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement
oftheensembleofmethodsthatwontheNetﬂixcompetition(Töscher2009 e t a l .,;
Koren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix
havealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand
Mnih2008,) However,thereisabasiclimitationofcollaborativeﬁlteringsystems:whena
newitemoranewuserisintroduced,itslackofratinghistorymeansthatthere
isnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or
thedegreeofassociationbetween,say,thatnewuserandexistingitems.This
iscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving
thecold-startrecommendation problemistointroduceextrainformationabout
theindividualusersanditems.Forexample,thisextrainformationcouldbeuser
proﬁleinformationorfeaturesofeachitem Systems thatusesuchinformation
arecalledcontent-basedrecommendersystems.Themappingfromarich
setofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha
deeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,) Specializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso
beenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical
audiotracks,formusicrecommendation (vandenOörd2013 e t a l .,).Inthatwork,
theconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding
fortheassociatedsong.Thedotproductbetweenthissongembeddingandthe
embeddingforauseristhenusedtopredictwhetherauserwilllistentothesong 12.5.1.1ExplorationVersusExploitation
Whenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary
supervisedlearningandintotherealmofreinforcementlearning.Manyrecom-
mendationproblemsaremostaccuratelydescribedtheoreticallyascontextual
bandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010
usetherecommendation systemtocollectdata,wegetabiasedandincomplete
viewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems
theywererecommendedandnottotheotheritems Inaddition,insomecases
wemaynotgetanyinformationonusersforwhomnorecommendation hasbeen
made(forexample,withadauctions,itmaybethatthepriceproposedforan
4 8 0
CHAPTER12.APPLICATIONS
adwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe
adisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat
outcomewouldhaveresultedfromrecommendinganyoftheotheritems.This
wouldbeliketrainingaclassiﬁerbypickingoneclassˆ yforeachtrainingexample
x(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and
thenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,
eachexampleconveyslessinformationthaninthesupervisedcasewherethetrue
label yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot
careful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions
evenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada
verylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn
aboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning
whereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement
learningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits
scenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly
asingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe
sensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In
thegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight
havebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm
contextualbanditsreferstothecasewheretheactionistakeninthecontextof
someinputvariablethatcaninformthedecision.Forexample,weatleastknow
theuseridentity,andwewanttopickanitem.Themappingfromcontextto
actionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata
distribution(whichnowdependsontheactionsofthelearner)isacentralresearch
issueinthereinforcementlearningandbanditsliterature Reinforcementlearningrequireschoosingatradeoﬀbetweenexplorationand
exploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,
bestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward Explorationreferstotakingactionsspeciﬁcallyinordertoobtainmoretraining
data.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot
knowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent
policyandcontinuetakingaction ainordertoberelativelysureofobtaininga
rewardof1.However,wemayalsowanttoexplorebytryingaction a.Wedonot
knowwhatwillhappenifwetryaction a.Wehopetogetarewardof,butwe 2
runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge 0
Explorationcanbeimplementedinmanyways,rangingfromoccasionally
takingrandomactionsintendedtocovertheentirespaceofpossibleactions,to
model-basedapproachesthatcomputeachoiceofactionbasedonitsexpected
rewardandthemodel’samountofuncertaintyaboutthatreward 4 8 1
CHAPTER12.APPLICATIONS
Manyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation Oneofthemostprominentfactorsisthetimescaleweareinterestedin Ifthe
agenthasonlyashortamountoftimetoaccruereward,thenweprefermore
exploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith
moreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmore
knowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward
moreexploitation Supervised learninghas notradeoﬀ between explorationand exploitation
becausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeach
input.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetter
thanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput Anotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidesthe
exploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparing
diﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner
andtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto
evaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.The
policyitselfdetermineswhichinputswillbeseen ()present Dudik e t a l .2011
techniquesforevaluatingcontextualbandits 12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-
swering
Deeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine
translationandnaturallanguageprocessingduetotheuseofembeddingsfor
symbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,
2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords
andconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor
relationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor
thispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced
representations 12.5.2.1Knowledge,RelationsandQuestionAnswering
Oneinterestingresearchdirectionisdetermininghowdistributedrepresentations
canbetrainedtocapturetherelationsbetweentwoentities.Theserelations
allowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother Inmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs
thatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset
4 8 2
CHAPTER12.APPLICATIONS
donot.Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities
{1 ,2 ,3}bydeﬁningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis
relationisdeﬁned,wecanuseitlikeaverb.Because(1 ,2)∈ S,wesaythat1is
lessthan2.Because(2 ,1)∈ S,wecannotsaythat2islessthan1.Ofcourse,the
entitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnea
relation containingtupleslike(,)

============================================================

=== CHUNK 122 ===
Palavras: 404
Caracteres: 6601
--------------------------------------------------
is_a_type_of dogmammal
InthecontextofAI,wethinkofarelationasasentenceinasyntactically
simpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,
whiletwoargumentstotherelationplaytheroleofitssubjectandobject.These
sentencestaketheformofatripletoftokens
(subjectverbobject) , , (12.21)
withvalues
(entityi ,relation j ,entityk) (12.22)
Wecanalsodeﬁneanattribute,aconceptanalogoustoarelation,buttaking
onlyoneargument:
(entity i ,attribute j) (12.23)
Forexample,wecoulddeﬁnethehas_furattribute,andapplyittoentitieslike
dog Manyapplicationsrequirerepresentingrelationsandreasoningaboutthem Howshouldwebestdothiswithinthecontextofneuralnetworks Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations
betweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage Therearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon
structureforthesedatabasesistherelationaldatabase,whichstoresthissame
kindofinformation, alb eit notformattedasthreetokensentences.Whena
databaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor
expertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,
wecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral
oneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized
knowledgebases,likeGeneOntology.2Representationsforentitiesandrelations
canbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample
andmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes
e t a l .,).2013a
1R e s p e c t i v e l y a v a i l a b l e   f ro m t h e s e   w e b   s i t e s : f r e e b a s e c o m / o p e n c y c , w o r d n e t s e ,
2g e n e o n t o l o g y o r g
4 8 3
CHAPTER12.APPLICATIONS
Inadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain Acommonapproachistoextendneurallanguagemodelstomodelentitiesand
relations.Neurallanguagemodelslearnavectorthatprovidesadistributed
representationofeachword.Theyalsolearnaboutinteractionsbetweenwords,
suchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions
ofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning
anembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling
languageandmodelingknowledgeencodedasrelationsissoclosethatresearchers
havetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases
naturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or
combiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b
possibilitiesexistfortheparticularparametrization associatedwithsuchamodel Earlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton
2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),
oftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities Forexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor
entitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator
onentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes
e t a l .,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis 2012
putinthemachinerythatcombinestheminordertomodeltheirjointdistribution Apracticalshort-termapplicationofsuchmodelsislinkprediction:predict-
ingmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew
facts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave
beenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably
themajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l (),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015
application Evaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcult
becausewehaveonlyadatasetofpositiveexamples(factsthatareknownto
betrue) Ifthemodelproposesafactthatisnotinthedataset,weareunsure
whetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown
fact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe
modelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts
thatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples
thatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue
factandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity
intherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat
10%metriccountshowmanytimesthemodelranksa“correct”factamongthe
top10%ofallcorruptedversionsofthatfact 4 8 4
CHAPTER12.APPLICATIONS
Anotherapplicationofknowledgebasesanddistributedrepresentationsfor
themisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,
2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate
one,insomecontext Eventually,knowledgeofrelationscombinedwithareasoningprocessand
understandingofnaturallanguagecouldallowustobuildageneralquestion
answeringsystem.Ageneralquestionansweringsystemmustbeabletoprocess
inputinformationandrememberimportantfacts,organizedinawaythatenables
ittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblem
whichcanonlybesolvedinrestricted“toy”environments.Currently,thebest
approachtorememberingandretrievingspeciﬁcdeclarativefactsistousean
explicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12
ﬁrstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,) e t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015
theinputintothememoryandtoproducetheanswergiventhecontentsofthe
memory Deeplearninghasbeenappliedtomanyotherapplicationsbesidestheones
describedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould
beimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage
ofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible
asofthiswriting Thisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II
networks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these
methodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofa
modelthatapproximates somedesiredfunction.Withenoughtrainingdata,this
approachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III
territoryofresearch—methodsthataredesignedtoworkwithlesstrainingdata
ortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcult
andnotasclosetobeingsolvedasthesituationswehavedescribedsofar 4 8 5
P a rt I I I
DeepLearningResearch
486
This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes
t o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y In t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d
learning problems — how t o learn t o map one v e c t or t o another, given e nough
e x amples of t he mapping

============================================================

=== CHUNK 123 ===
Palavras: 392
Caracteres: 1336
--------------------------------------------------
N ot all problems w e might w ant t o s olve f all in t o t his c ategory W e ma y
wis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle
mis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples
f r om r e lated t as k s A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial
applications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d
data t o ac hieve go o d accuracy In t his part of t he b o ok, w e dis c us s s ome of
t he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary
f or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of
t as k s A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or
s e mi-s up e r v is e d learning Man y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d
learning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat
deep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of
t as k s In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d
learning and s ome of t he p opular t hought ab out how w e c an make progres s in t his
ﬁ e ld

============================================================

=== CHUNK 124 ===
Palavras: 374
Caracteres: 1282
--------------------------------------------------
A c e ntral c aus e of t he diﬃculties with uns upervis e d learning is t he high di-
mens iona lit y of t he r andom v ariables being mo deled This brings t wo dis t inct
c hallenges : a s t atis t ical c hallenge and a c omputational c hallenge The s t a t i s t i c a l
c h a l l e ng e r e gards generalization: t he num b e r of c onﬁgurations we may wan t t o
dis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and
t his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly
ha v e ( or us e with b ounded c omputational r e s ources ) The c o m p u t a t i o na l c h a l l e ng e
as s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or
learning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit
probabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially
with t he n um b e r of dimens ion s With probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o
p e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on • I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter

============================================================

=== CHUNK 125 ===
Palavras: 356
Caracteres: 1230
--------------------------------------------------
It r e gards 19
t he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other
v ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r
4 8 7
a , b and c In order t o e v e n c ompute s uch c onditional probabilities one needs
t o s um ov e r t he v alues of t he v ariables c , as well as c ompute a normalization
c ons t an t whic h s ums o v e r t he v alues of a and c • I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition
f unction is dis c us s e d mos t ly in c hapter N ormalizing c ons t ants of proba- 18
bilit y f unctions c ome up in inference ( ab o v e ) as well as in learning Man y
probabilis t ic mo dels in v olve s uc h a normalizing c ons t ant U nfortun ately ,
learning s uc h a mo del often r e q uires c omputing t he gradient of t he loga-
r ithm of t he partition f unction with r e s p e c t t o t he mo del parameters That
c omputation is generally as intractable as c omputing t he partition f unction
its e lf Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17
t e n us e d t o deal with t he partition f unction ( c omputing it or its gradient)

============================================================

=== CHUNK 126 ===
Palavras: 364
Caracteres: 1452
--------------------------------------------------
U nfortun ately , MCMC metho ds s uﬀer when t he mo des of t he mo del dis t r ibu-
t ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces
( s e c t ion ) 17.5
One wa y t o c onfront t hes e intractable c omputations is t o approximate t hem,
and many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he
b o ok A nother interes t in g w ay , als o dis c us s e d here, w ould b e t o av oid t hes e
in t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire
s uc h c omputations are t hus v e r y app e aling Several generativ e mo dels ha v e b e e n
prop os e d in r e c e nt y e ars , with t hat motiv ation A wide v ariety of c ontemporary
approac hes t o generativ e mo deling are dis c us s e d in c hapter 20
P art is t he mos t imp ortant f or a r e s e arc her—s om e one who wan t s t o un- I I I
ders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he ﬁ e ld of deep
learning, and pus h t he ﬁ e ld f orward t ow ards t r ue artiﬁcial intelligence 4 8 8
C h a p t e r 1 3
L i n e ar F act or Mo d e l s
Manyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel
oftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto
predictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many
ofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|

============================================================

=== CHUNK 127 ===
Palavras: 366
Caracteres: 4762
--------------------------------------------------
Theselatentvariablesprovideanothermeansofrepresentingthedata.Distributed
representationsbased onlatent variablescanobtain alloftheadvantagesof
representationlearningthatwehaveseenwithdeepfeedforwardandrecurrent
networks Inthischapter,wedescribesomeofthesimplestprobabilisticmodelswith
latentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding
blocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,;
Roweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They
alsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat
themoreadvanceddeepmodelswillextendfurther Alinearfactormodelisdeﬁnedbytheuseofastochastic,lineardecoder
functionthatgeneratesbyaddingnoisetoalineartransformationof x h
Thesemodelsareinterestingbecausetheyallowustodiscoverexplanatory
factorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder
madethesemodelssomeoftheﬁrstlatentvariablemodelstobeextensivelystudied Alinearfactormodeldescribesthedatagenerationprocessasfollows.First,
wesampletheexplanatoryfactorsfromadistribution h
h∼ p ,() h (13.1)
where p( h)isafactorialdistribution,with p( h) =
i p( h i),sothatitiseasyto
489
CHAPTER13.LINEARFACTORMODELS
samplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:
x W h b = ++noise (13.2)
wherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions) Thisisillustratedinﬁgure.13.1
h 1 h 1 h 2 h 2 h 3 h 3
x 1 x 1 x 2 x 2 x 3 x 3
x h n  o i s  e x h n  o i s  e = W + + b = W + + b
Figure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in
whichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof
independentlatentfactors h,plussomenoise.Diﬀerentmodels,suchasprobabilistic
PCA,factoranalysisorICA,makediﬀerentchoicesabouttheformofthenoiseandof
theprior p() h
13.1ProbabilisticPCAandFactorAnalysis
ProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear
factormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2
diﬀerinthechoicesmadeforthenoisedistributionandthemodel’spriorover
latentvariablesbeforeobserving h x
In f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994
priorisjusttheunitvarianceGaussian
h 0 ∼N(; h , I) (13.3)
whiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent,
given h.Speciﬁcally, the noiseisassumed tobedrawnfroma diagonalco-
variance Gaussian distribution,with covariancematrix ψ=diag( σ2),with
σ2= [ σ2
1 , σ2
2 , , σ2
n]avectorofper-variablevariances Theroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween
thediﬀerentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta
multivariatenormalrandomvariable,with
x∼N(; x b W W ,+) ψ (13.4)
490
CHAPTER13.LINEARFACTORMODELS
InordertocastPCAinaprobabilisticframework, wecanmakeaslight
modiﬁcationtothefactoranalysismodel,makingtheconditionalvariances σ2
i
equaltoeachother.Inthatcasethecovarianceof xisjust W W+ σ2I,where
σ2isnowascalar.Thisyieldstheconditionaldistribution
x∼N(; x b W W ,+ σ2I) (13.5)
orequivalently
x h z = W ++ b σ (13.6)
where z∼N( z; 0 , I)isGaussiannoise ()thenshowan TippingandBishop1999
iterativeEMalgorithmforestimatingtheparameters and W σ2 This pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost
variationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall
residual r e c o nst r u c t i o n e r r o r σ2.Asshownby (), TippingandBishop1999
probabilisticPCAbecomesPCAas σ→0.Inthatcase,theconditionalexpected
valueof hgiven xbecomesanorthogonalprojectionof x b−ontothespace
spannedbythecolumnsof,likeinPCA d W
As σ→0,thedensitymodeldeﬁnedbyprobabilisticPCAbecomesverysharp
aroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe
modelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster
nearahyperplane 13.2IndependentComponentAnalysis(ICA)
Independentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning
algorithms( ,; ,; ,; Herault andAns1984Jutten andHerault1991Comon1994
Hyvärinen1999Hyvärinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,) Itisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved
signalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform
theobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan
merelydecorrelatedfromeachother.1
ManydiﬀerentspeciﬁcmethodologiesarereferredtoasICA.Thevariant
thatismostsimilartotheothergenerativemodelswehavedescribedhereisa
variant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992
priordistributionovertheunderlyingfactors, p( h),mustbeﬁxedaheadoftimeby
theuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma
1Seesectionforadiscussionofthediﬀerencebetweenuncorrelatedvariablesandindepen- 3.8
dentvariables

============================================================

=== CHUNK 128 ===
Palavras: 359
Caracteres: 7918
--------------------------------------------------
491
CHAPTER13.LINEARFACTORMODELS
nonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning
themodelthenproceedsasusual,usingmaximumlikelihood Themotivationforthisapproachisthatbychoosing p( h)tobeindependent,
wecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent Thisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto
recoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each
trainingexampleisonemomentintime,each x iisonesensor’sobservationof
themixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For
example,wemighthave npeoplespeakingsimultaneously.Ifwehave ndiﬀerent
microphonesplacedindiﬀerentlocations,ICAcandetectthechangesinthevolume
betweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso
thateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused
inneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical
signalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject’s
headareusedtomeasuremanyelectricalsignalscomingfromthebody.The
experimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom
thesubject’sheartandeyesarestrongenoughtoconfoundmeasurementstaken
atthesubject’sscalp.Thesignalsarriveattheelectrodesmixedtogether,so
ICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals
originatinginthebrain,andtoseparatesignalsindiﬀerentbrainregionsfrom
eachother Asmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise
inthegenerationof xratherthanusingadeterministicdecoder.Mostdonot
usethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof
h= W− 1xindependentfromeachother.Manycriteriathataccomplishthisgoal
arepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe
anexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis
problematicoperationbyconstrainingtobeorthogonal W
AllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h)
isanindependentpriorwithGaussiancomponents,then Wisnotidentiﬁable Wecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery
diﬀerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,
thatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe
modelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe
userexplicitlyspeciﬁesthedistribution,atypicalchoiceistouse p( h i) =d
d h iσ( h i) Typicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than
doestheGaussiandistribution,sowecanalsoseemostimplementations ofICA
aslearningsparsefeatures 492
CHAPTER13.LINEARFACTORMODELS
ManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe
phrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples
fromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but
donothaveanywayofrepresenting p( h),andthusdonotimposeadistribution
over p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof
h= W− 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis
accomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore
oftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating
dataorestimatingitsdensity JustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin
chapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14
weuseanonlinearfunction ftogeneratetheobserveddata.SeeHyvärinenand
Pajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith
ensemblelearningby ()and () RobertsandEverson2001Lappalainen e t a l .2000
AnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent
c o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014
ofinvertibletransformations(encoderstages)thathavethepropertythatthe
determinantoftheJacobianofeachtransformationcanbecomputedeﬃciently Thismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts
totransformthedataintoaspacewhereithasafactorizedmarginaldistribution,
butismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder
isassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto
generatesamplesfromthemodel(byﬁrstsamplingfrom p( h)andthenapplying
thedecoder) Anothergeneralization ofICAistolearngroupsoffeatures,withstatistical
dependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyvärinenand
Hoyer1999Hyvärinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen
tobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso
possibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping
groupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar
features.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns
Gaborﬁlters,suchthatneighboringfeatureshavesimilarorientation,locationor
frequency.ManydiﬀerentphaseoﬀsetsofsimilarGaborfunctionsoccurwithin
eachregion,sothatpoolingoversmallregionsyieldstranslationinvariance 13.3SlowFeatureAnalysis
Sl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom
493
CHAPTER13.LINEARFACTORMODELS
timesignalstolearninvariantfeatures( ,) WiskottandSejnowski2002
Slowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness
principle.Theideaisthattheimportantcharacteristicsofsceneschangevery
slowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa
scene.Forexample,incomputervision,individualpixelvaluescanchangevery
rapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel
willrapidlychangefromblacktowhiteandbackagainasthezebra’sstripespass
overthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe
imagewillnotchangeatall,andthefeaturedescribingthezebra’spositionwill
changeslowly Wethereforemaywishtoregularizeourmodeltolearnfeatures
thatchangeslowlyovertime Theslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied
toawidevarietyofmodels(,;,; ,; Hinton1989Földiák1989Mobahi e t a l .2009
BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany
diﬀerentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe
introducedbyaddingatermtothecostfunctionoftheform
λ
tL f(( x( + 1 ) t)( , f x( ) t)) (13.7)
where λisahyperparameter determiningthestrengthoftheslownessregularization
term, tistheindexintoatimesequenceofexamples, fisthefeatureextractor
toberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t)
and f( x( + 1 ) t).Acommonchoiceforisthemeansquareddiﬀerence L
Slowfeatureanalysisisaparticularlyeﬃcientapplicationoftheslowness
principle.Itiseﬃcientbecauseitisappliedtoalinearfeatureextractor,andcan
thusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea
generativemodelperse,inthesensethatitdeﬁnesalinearmapbetweeninput
spaceandfeaturespacebutdoesnotdeﬁneaprioroverfeaturespaceandthus
doesnotimposeadistributiononinputspace p() x
TheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeﬁning f( x; θ)
tobealineartransformation,andsolvingtheoptimization problem
min
θE t(( f x( + 1 ) t) i− f( x( ) t) i)2(13.8)
subjecttotheconstraints
E t f( x( ) t) i= 0 (13.9)
and
E t[( f x( ) t)2
i] = 1 (13.10)
494
CHAPTER13.LINEARFACTORMODELS
Theconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe
problemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature
valuesandobtainadiﬀerentsolutionwithequalvalueoftheslownessobjective Theconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe
pathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0
areordered,withtheﬁrstfeaturebeingtheslowest.Tolearnmultiplefeatures,we
mustalsoaddtheconstraint
∀ i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 (13.11)
Thisspeciﬁesthatthelearnedfeaturesmustbelinearlydecorrelated fromeach
other.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe
oneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing
reconstructionerror, to forcethe featurestodiversify, but thisdecorrelation
mechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA
problemmaybesolvedinclosedformbyalinearalgebrapackage

============================================================

=== CHUNK 129 ===
Palavras: 350
Caracteres: 8587
--------------------------------------------------
SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis
expansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe
quadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear
SFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors
byrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis
expansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron
topofthatexpansion Whentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith
quadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith
thoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained
onvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep
SFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented
byneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA
thusseemstobeareasonablybiologicallyplausiblemodel AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich
featuresSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical
predictions,onemustknowaboutthedynamicsoftheenvironmentintermsof
conﬁguration space (e.g., inthe caseofrandom motion inthe 3-Drendered
environment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability
distributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow
theunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe
optimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA
appliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions 495
CHAPTER13.LINEARFACTORMODELS
Thisisincomparisontootherlearningalgorithmswherethecostfunctiondepends
highlyonspeciﬁcpixelvalues,makingitmuchmorediﬃculttodeterminewhat
featuresthemodelwilllearn DeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose
estimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome
thebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited
itsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and
that,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,
itwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom
onetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof
whethertheobject’svelocityishighorlow,buttheslownessprincipleencourages
themodeltoignorethepositionofobjectsthathavehighvelocity 13.4SparseCoding
Spar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996
beenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction
mechanism Strictlyspeaking,theterm“sparsecoding”referstotheprocessof
inferringthevalueof hinthismodel,while“sparsemodeling”referstotheprocess
ofdesigningandlearningthemodel,buttheterm“sparsecoding”isoftenusedto
refertoboth Likemostotherlinearfactormodels,itusesalineardecoderplusnoiseto
obtainreconstructionsof x,asspeciﬁedinequation.Morespeciﬁcally,sparse 13.2
codingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith
isotropicprecision: β
p , ( ) = (; + x h| N x W h b1
βI) (13.12)
Thedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen
andField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized
Student- tdistributions.Forexample,theLaplacepriorparametrized intermsof
thesparsitypenaltycoeﬃcientisgivenby λ
p h( i) = Laplace( h i;0 ,2
λ) =λ
4e−1
2λ h| i|(13.13)
andtheStudent-priorby t
p h( i) ∝1
(1+h2
i
ν)ν +1
2 (13.14)
496
CHAPTER13.LINEARFACTORMODELS
Trainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the
trainingalternatesbetweenencodingthedataandtrainingthedecodertobetter
reconstructthedatagiventheencoding.Thisapproachwillbejustiﬁedfurtheras
aprincipledapproximation tomaximumlikelihoodlater,insection.19.3
FormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction
thatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder
thatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder
isanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek
thesinglemostlikelycodevalue:
h∗= () = argmax f x
hp ( ) h x| (13.15)
Whencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12
optimization problem:
argmax
hp( ) h x| (13.16)
=argmax
hlog( ) p h x| (13.17)
=argmin
hλ|||| h 1+ β||− || x W h2
2 , (13.18)
wherewehavedroppedtermsnotdependingon handdividedbypositivescaling
factorstosimplifytheequation Duetotheimpositionofan L1normon h,thisprocedurewillyieldasparse
h∗(Seesection).7.1.2
Totrainthemodelratherthanjustperforminference,wealternatebetween
minimization withrespectto handminimization withrespectto W.Inthis
presentation,wetreat βasahyperparameter.Typicallyitissetto1becauseits
roleinthisoptimization problemissharedwith λandthereisnoneedforboth
hyperparameters.Inprinciple,wecouldalsotreat βasaparameterofthemodel
andlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend
on hbutdodependon β.Tolearn β,thesetermsmustbeincluded,or βwill
collapseto.0
Notallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|) Oftenwearejustinterestedinlearningadictionaryoffeatureswithactivation
valuesthatwilloftenbezerowhenextractedusingthisinferenceprocedure Ifwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor
anelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially
sparse,onlythefeatureextractoris ()describeapproximate Goodfellow e t a l .2013d
497
CHAPTER13.LINEARFACTORMODELS
inferenceinadiﬀerentmodelfamily,thespikeandslabsparsecodingmodel,for
whichsamplesfromthepriorusuallycontaintruezeros Thesparsecodingapproachcombinedwiththeuseofthenon-parametric
encodercaninprincipleminimizethecombinationofreconstructionerrorand
log-priorbetterthananyspeciﬁcparametricencoder.Anotheradvantageisthat
thereisnogeneralization errortotheencoder.Aparametricencodermustlearn
howtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble
thetrainingdata,alearned,parametricencodermayfailtoﬁndan hthatresults
inaccuratereconstructionorasparsecode.Forthevastmajorityofformulations
ofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization
procedurewillalwaysﬁndtheoptimalcode(unlessdegeneratecasessuchas
replicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts
canstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe
decoderweights,ratherthangeneralization errorintheencoder.Thelackof
generalization errorinsparsecoding’soptimization-based encodingprocessmay
resultinbettergeneralization whensparsecodingisusedasafeatureextractorfor
aclassiﬁerthanwhenaparametricfunctionisusedtopredictthecode.Coates
andNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor
objectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric
encoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l ()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d
extractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer
labelsperclass) Theprimarydisadvantageofthenon-parametric encoderisthatitrequires
greatertimetocompute hgiven xbecausethenon-parametric approachrequires
runninganiterativealgorithm.Theparametricautoencoderapproach,developed
in chapter ,usesonly a ﬁxed n umber of layers, often only one.Another 14
disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe
non-parametric encoder,whichmakesitdiﬃculttopretrainasparsecodingmodel
withanunsupervisedcriterionandthenﬁne-tuneitusingasupervisedcriterion Modiﬁedversionsofsparsecodingthatpermitapproximate derivativesdoexist
butarenotwidelyused( ,) BagnellandBradley2009
Sparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as
showninﬁgure.Thishappensevenwhenthemodelisabletoreconstruct 13.2
thedatawellandprovideusefulfeaturesforaclassiﬁer.Thereasonisthateach
individualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode
resultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated
sample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-
498
CHAPTER13.LINEARFACTORMODELS
Figure13.2: Example samplesandweightsfromaspikeandslabsparsecodingmodel
trainedontheMNISTdataset ( L e f t )Thesamplesfromthemodeldonotresemblethe
trainingexamples.Atﬁrstglance,onemightassumethemodelispoorlyﬁt.The ( R i g h t )
weightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete
digits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior
overfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets
areappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof
generativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure
reproducedwithpermissionfromGoodfellow2013d e t a l .()

============================================================

=== CHUNK 130 ===
Palavras: 364
Caracteres: 7096
--------------------------------------------------
factorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore
sophisticatedshallowmodels 13.5ManifoldInterpretationofPCA
LinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas
learningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997
deﬁningathinpancake-shapedregionofhighprobability—aGaussiandistribution
thatisverynarrowalongsomeaxes,justasapancakeisveryﬂatalongitsvertical
axis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal
axes Thisisillustratedinﬁgure PCAcanbeinterpretedasaligningthis 13.3
pancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation
appliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns
matrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto
xaspossible,
Lettheencoderbe
h x W = ( f) = ( ) x µ− (13.19)
499
CHAPTER13.LINEARFACTORMODELS
Theencodercomputesalow-dimensional representationof h.Withtheautoencoder
view,wehaveadecodercomputingthereconstruction
ˆ x h b V h = ( g) = + (13.20)
Figure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional
manifold.Theﬁgureshowstheupperhalfofthe“pancake”abovethe“manifoldplane”
whichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis
verysmall(arrowpointingoutofplane)andcanbeconsideredlike“noise,”whiletheother
variancesarelarge(arrowsintheplane)andcorrespondto“signal,”andacoordinate
systemforthereduced-dimensiondata Thechoicesoflinearencoderanddecoderthatminimizereconstructionerror
E[||− xˆ x||2] (13.21)
correspondto V= W, µ= b= E[ x]andthecolumnsof Wformanorthonormal
basiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance
matrix
C x µ x µ = [( E −)(−)] (13.22)
InthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe
magnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative) Onecanalsoshowthateigenvalue λ iof Ccorrespondstothevarianceof x
inthedirectionofeigenvector v( ) i.If x∈ RDand h∈ Rdwith d < D,thenthe
500
CHAPTER13.LINEARFACTORMODELS
optimalreconstructionerror(choosing,,andasabove)is µ b V W
min[ E||− xˆ x||2] =D 
i d = + 1λ i (13.23)
Hence,ifthecovariancehasrank d,theeigenvalues λ d + 1to λ Dare0andrecon-
structionerroris0 Furthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby
maximizingthevariancesoftheelementsof h,underorthogonal W,insteadof
minimizingreconstructionerror Linearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe
simplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiﬁersand
linearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear
factormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic
modelsthatperformthesametasksbutwithamuchmorepowerfulandﬂexible
modelfamily 501
C h a p t e r 1 4
A u t o e n co d e rs
An aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput
toitsoutput Internally ,ithasahiddenlayer hthatdescribesa c o deusedto
representtheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an
encoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h) Thisarchitectureispresentedinﬁgure.Ifanautoencodersucceedsinsimply 14.1
learningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead,
autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare
restrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly
inputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize
whichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe
data Modern autoencoders havegeneralized the idea of anencoder and ade-
coderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and
pdecoder( ) x h| Theideaofautoencodershasbeenpartofthehistoricallandscapeofneural
networksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel
1994).Traditionally, autoencoderswereused fordimensionalityreductionor
featurelearning.Recently,theoreticalconnectionsbetweenautoencodersand
latentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative
modeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20
aspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame
techniques,typicallyminibatchgradientdescentfollowinggradientscomputed
byback-propagation Unlikegeneralfeedforwardnetworks,autoencodersmay
alsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning
algorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput
502
CHAPTER14.AUTOENCODERS
totheactivationsonthereconstructedinput.Recirculationisregardedasmore
biologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning
applications xx rrh h
f g
Figure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x
(calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder
hastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto
r) 14.1UndercompleteAutoencoders
Copyingtheinputtotheoutputmaysounduseless,butwearetypicallynot
interestedintheoutputofthe decoder Instead, wehope thattrainingthe
autoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful
properties Onewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto
havesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless
thantheinputdimensioniscalled under c o m p l e t e.Learninganundercomplete
representationforcestheautoencodertocapturethemostsalientfeaturesofthe
trainingdata Thelearningprocessisdescribedsimplyasminimizingalossfunction
L , g f ( x(())) x (14.1)
where Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas
themeansquarederror Whenthedecoderislinearand Listhemeansquarederror,anundercomplete
autoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder
trainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe
trainingdataasaside-eﬀect Autoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc-
tions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu-
5 0 3
CHAPTER14.AUTOENCODERS
nately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder
canlearntoperformthecopyingtaskwithoutextractingusefulinformationabout
thedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder
withaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto
representeachtrainingexample x() iwiththecode i.Thedecodercouldlearnto
maptheseintegerindicesbacktothevaluesofspeciﬁctrainingexamples.This
speciﬁcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-
codertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout
thedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat 14.2RegularizedAutoencoders
Undercomplete autoencoders,withcodedimensionlessthantheinputdimension,
canlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat
theseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare
giventoomuchcapacity Asimilarproblemoccursifthehiddencodeisallowedtohavedimension
equaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas
dimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear
decodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful
aboutthedatadistribution

============================================================

=== CHUNK 131 ===
Palavras: 352
Caracteres: 6960
--------------------------------------------------
Ideally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing
thecodedimensionandthecapacityoftheencoderanddecoderbasedonthe
complexityofdistributiontobemodeled.Regularizedautoencodersprovidethe
abilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder
anddecodershallowandthecodesizesmall,regularizedautoencodersusealoss
functionthatencouragesthemodeltohaveotherpropertiesbesidestheability
tocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe
representation,smallnessofthederivativeoftherepresentation,androbustness
tonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand
overcompletebutstilllearnsomethingusefulaboutthedatadistributionevenif
themodelcapacityisgreatenoughtolearnatrivialidentityfunction Inadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted
asregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables
andequippedwithaninferenceprocedure(forcomputinglatentrepresentations
giveninput)maybeviewedasaparticularformofautoencoder.Twogenerative
modelingapproachesthatemphasizethisconnectionwithautoencodersarethe
descendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b
5 0 4
CHAPTER14.AUTOENCODERS
autoencoder(section)andthegenerativestochasticnetworks(section) 20.10.3 20.12
Thesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput
anddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings
arenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize
theprobabilityofthetrainingdataratherthantocopytheinputtotheoutput 1 S p a rse A u t o en co d ers
Asparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa
sparsitypenaltyΩ( h)onthecodelayer h,inadditiontothereconstructionerror:
L , g f ( x(()))+Ω() x h (14.2)
where g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder
output Sparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch
asclassiﬁcation.Anautoencoderthathasbeenregularizedtobesparsemust
respondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather
thansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe
copyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful
featuresasabyproduct Wecanthink ofthepenalty Ω( h)simplyasaregularizertermaddedto
afeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput
(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask
(with asupervised learning ob jective) thatdepends on thesesparsefeatures Unlikeotherregularizerssuchasweightdecay,thereisnotastraightforward
Bayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1
withweightdecayandotherregularizationpenaltiescanbeinterpretedasa
MAPapproximationtoBayesianinference,withtheaddedregularizingpenalty
correspondingtoapriorprobabilitydistributionoverthemodelparameters.In
thisview,regularizedmaximumlikelihoodcorrespondstomaximizing p( θ x|),
whichisequivalenttomaximizing log p( x θ|)+log p( θ) The log p( x θ|)term
istheusualdatalog-likelihoodtermandthelog p( θ)term,thelog-priorover
parameters,incorporatesthepreferenceoverparticularvaluesof θ.Thisviewwas
describedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6
becausetheregularizerdependsonthedataandisthereforebydeﬁnitionnota
priorintheformalsenseoftheword.Wecanstillthinkoftheseregularization
termsasimplicitlyexpressingapreferenceoverfunctions Ratherthanthinkingofthesparsitypenaltyasaregularizerforthecopying
task,wecanthinkoftheentiresparseautoencoderframeworkasapproximating
5 0 5
CHAPTER14.AUTOENCODERS
maximumlikelihood trainingofagenerativemodel thathaslatentvariables Supposewehaveamodelwithvisiblevariables xandlatentvariables h,with
anexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto
pmodel( h)asthemodel’spriordistributionoverthelatentvariables,representing
themodel’sbeliefspriortoseeing x.Thisisdiﬀerentfromthewaywehave
previouslyusedtheword“prior,”torefertothedistribution p( θ)encodingour
beliefsaboutthemodel’sparametersbeforewehaveseenthetrainingdata.The
log-likelihoodcanbedecomposedas
log pmodel() = log x
hpmodel( ) h x , (14.3)
Wecanthinkoftheautoencoderasapproximatingthissumwithapointestimate
forjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative
model(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather
thantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof
view,withthischosen,wearemaximizing h
log pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4)
Thelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior,
pmodel( h i) =λ
2e−| λ h i|, (14.5)
correspondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan
absolutevaluepenalty,weobtain
Ω() = h λ
i| h i| (14.6)
−log pmodel() = h
i
λ h| i|−logλ
2
= Ω()+const h (14.7)
wheretheconstanttermdependsonlyon λandnot h.Wetypicallytreat λasa
hyperparameteranddiscardtheconstanttermsinceitdoesnotaﬀecttheparameter
learning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From
thispointofviewofsparsityasresultingfromtheeﬀectof pmodel( h)onapproximate
maximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat
all Itisjustaconsequenceofthemodel’sdistributionoveritslatentvariables Thisviewprovidesadiﬀerentmotivationfortraininganautoencoder:itisaway
ofapproximately trainingagenerativemodel.Italsoprovidesadiﬀerentreasonfor
5 0 6
CHAPTER14.AUTOENCODERS
whythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent
variablesthatexplaintheinput Earlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008
variousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty
andthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected
probabilisticmodel p( x) =1
Z˜ p( x).Theideaisthatminimizing log Zpreventsa
probabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity
on anautoencoder preventstheautoencoderfrom having lowreconstruction
erroreverywhere.Inthiscase, theconnectionisonthelevelofanintuitive
understandingofageneralmechanismratherthanamathematical correspondence Theinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina
directedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward Onewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders
wasintroducedin ().Theideaistouserectiﬁedlinearunitsto Glorot e t a l .2011b
producethecodelayer.Withapriorthatactuallypushestherepresentationsto
zero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage
numberofzerosintherepresentation 2 D en o i s i n g A u t o en co d ers
Ratherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder Ω 
thatlearnssomethingusefulbychangingthereconstructionerrortermofthecost
function Traditionally,autoencodersminimizesomefunction
L , g f ( x(())) x (14.8)
where Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas
the L2normoftheirdiﬀerence This encourages g f◦tolearntobemerelyan
identityfunctioniftheyhavethecapacitytodoso

============================================================

=== CHUNK 132 ===
Palavras: 371
Caracteres: 6140
--------------------------------------------------
A orDAEinsteadminimizes denoising aut o e nc o der
L , g f ( x((˜ x))) , (14.9)
where ˜ xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising
autoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir
input Denoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x),
asshown by  () and ().Denoising AlainandBengio2013Bengio  e t a l .2013c
5 0 7
CHAPTER14.AUTOENCODERS
autoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge
asabyproductofminimizingreconstructionerror.Theyarealsoanexampleof
howovercomplete,high-capacity modelsmaybeusedasautoencoderssolong
ascareistakentopreventthemfromlearningtheidentityfunction Denoising
autoencodersarepresentedinmoredetailinsection.14.5
1 4 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es
Anotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse Ω
autoencoders,
L , g f , , ( x(()))+Ω( x h x) (14.10)
butwithadiﬀerentformof:Ω
Ω( ) = h x , λ
i||∇ x h i||2 (14.11)
Thisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x
changesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces
theautoencodertolearnfeaturesthatcaptureinformationaboutthetraining
distribution Anautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der
orCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,
manifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail
insection.14.7
14.3RepresentationalPower,LayerSizeandDepth
Autoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer
decoder.However,thisisnotarequirement.Infact,usingdeepencodersand
decodersoﬀersmanyadvantages Recallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1
wardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages
alsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork
asisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually
beneﬁtfromdepth Onemajoradvantageofnon-trivialdepthisthattheuniversalapproximator
theoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden
layercanrepresentanapproximationofanyfunction(withinabroadclass)toan
5 0 8
CHAPTER14.AUTOENCODERS
arbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans
thatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity
functionalongthedomainofthedataarbitrarilywell.However,themappingfrom
inputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary
constraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat
leastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any
mappingfrominputtocodearbitrarilywell,givenenoughhiddenunits Depthcanexponentiallyreducethecomputational costofrepresentingsome
functions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata
neededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1
depthinfeedforwardnetworks Experimentally,deepautoencodersyieldmuchbettercompressionthancorre-
spondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,) Acommonstrategyfortrainingadeepautoencoderistogreedilypretrain
thedeeparchitecturebytrainingastackofshallowautoencoders,soweoften
encountershallowautoencoders,evenwhentheultimategoalistotrainadeep
autoencoder 14.4StochasticEncodersandDecoders
Autoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput
unittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor
autoencoders Asdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4
andthelossfunctionofafeedforwardnetworkistodeﬁneanoutputdistribution
p( y x|)andminimizethenegativelog-likelihood−log p( y x|).Inthatsetting, y
wasavectoroftargets,suchasclasslabels Inthecaseofanautoencoder, xisnowthetargetaswellastheinput.However,
wecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay
thinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|) We
maythentraintheautoencoderbyminimizing −log pdecoder( ) x h|.Theexact
formofthislossfunctionwillchangedependingontheformof pdecoder.Aswith
traditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize
themeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative
log-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues
correspondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid
outputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon 5 0 9
CHAPTER14.AUTOENCODERS
Typically,theoutputvariablesaretreatedasbeingconditionallyindependent
given hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome
techniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs
withcorrelations xx rrh h
p e n c o d e r ( ) h x| p d e c o d e r ( ) x h|
Figure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe
decoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat
theiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder
and pdecoder( ) x h|forthedecoder Tomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen
previously,wecanalsogeneralizethenotionofan e nc o di ng f unc t i o n f( x)to
an e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedinﬁgure.14.2
Anylatentvariablemodel pmodel( ) h x ,deﬁnesastochasticencoder
pencoder( ) = h x| pmodel( ) h x| (14.12)
andastochasticdecoder
pdecoder( ) = x h| pmodel( ) x h| (14.13)
Ingeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional
distributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l ()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015
willtendtomakethemcompatibleasymptotically(withenoughcapacityand
examples) 14.5DenoisingAutoencoders
The denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted
datapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint
asitsoutput TheDAEtrainingprocedureisillustratedinﬁgure.Weintroducea 14.3
corruptionprocess C(˜x x|)whichrepresentsaconditional distrib utionover
5 1 0
CHAPTER14.AUTOENCODERS
˜ x ˜ x L Lh h
fg
xxC ( ˜ x x| )
Figure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,
whichistrainedtoreconstructthecleandatapoint xfromitscorruptedversion˜ x

============================================================

=== CHUNK 133 ===
Palavras: 358
Caracteres: 6010
--------------------------------------------------
Thisisaccomplishedbyminimizingtheloss L=−log pdecoder( x h|= f(˜ x)),where
˜ xisacorruptedversionofthedataexample x,obtainedthroughagivencorruption
process C(˜ x x|).Typicallythedistribution pdecoderisafactorialdistributionwhosemean
parametersareemittedbyafeedforwardnetwork g
corruptedsamples ˜ x,givenadatasample x.Theautoencoderthenlearnsa
r e c o nst r u c t i o n di st r i but i o n preconstruct( x|˜ x)estimatedfromtrainingpairs
( x ,˜ x),asfollows:
1 Sampleatrainingexamplefromthetrainingdata Sampleacorruptedversion˜ xfrom C(˜ x x|= ) x 3.Use( x ,˜ x)asatrainingexampleforestimatingtheautoencoderreconstruction
distribution preconstruct( x|˜x) = pdecoder( x h|)with htheoutputofencoder
f(˜ x)and pdecodertypicallydeﬁnedbyadecoder g() h
Typicallywecansimplyperformgradient-basedapproximate minimization (such
asminibatchgradientdescent)onthenegativelog-likelihood−log pdecoder( x h|) Solongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward
network andmay be trainedwith exactlythesame techniques as anyother
feedforwardnetwork WecanthereforeviewtheDAEasperformingstochasticgradientdescenton
thefollowingexpectation:
− E x∼ˆ p d a t a() x E˜ x∼ C(˜x| x)log pdecoder( = ( x h| f˜ x))(14.14)
where ˆ pdata() xisthetrainingdistribution 5 1 1
CHAPTER14.AUTOENCODERS
x˜ x
g f◦
˜ x
C ( ˜ x x| )
x
Figure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint˜xbackto
theoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara
low-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption
process C(˜x x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates
howonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors
|| g( f(˜ x))−|| x2,thereconstruction g( f(˜ x)) estimates E x ,˜ x∼ p dat a()( x C˜x x|)[ x|˜ x].Thevector
g( f(˜x))−˜ xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(˜x))
estimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenriseto˜ x.The
autoencoderthuslearnsavectorﬁeld g( f( x))− xindicatedbythegreenarrows.This
vectorﬁeldestimatesthescore∇ xlog pdata( x)uptoamultiplicativefactorthatisthe
averagerootmeansquarereconstructionerror 5 1 2
CHAPTER14.AUTOENCODERS
1 4 1 E s t i m a t i n g t h e S co re
Scorematching(,)isanalternativetomaximumlikelihood.It Hyvärinen2005
providesaconsistentestimatorofprobabilitydistributionsbasedonencouraging
themodeltohavethesame sc o r easthedatadistributionateverytrainingpoint
x.Inthiscontext,thescoreisaparticulargradientﬁeld:
∇ xlog() p x (14.15)
Scorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4
regardingautoencoders,itissuﬃcienttounderstandthatlearningthegradient
ﬁeldoflog pdataisonewaytolearnthestructureof pdataitself AveryimportantpropertyofDAEsisthat theirtrainingcriterion(with
conditionallyGaussian p( x h|))makes theautoencoder learnavectorﬁeld
( g( f( x))− x)thatestimatesthescoreofthedatadistribution.Thisisillustrated
inﬁgure.14.4
Denoisingtrainingofaspeciﬁckindofautoencoder(sigmoidalhiddenunits,
linear reconstr uction units) usingGaussiannoiseand meansquared erroras
thereconstructioncostisequivalent(,)totrainingaspeciﬁckind Vincent2011
ofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits Thiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1
discussionitsuﬃcestoknowthatitisamodelthatprovidesanexplicit pmodel( x; θ) WhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun
2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding
autoencoder.Withaﬁxednoiselevel,regularizedscorematchingisnotaconsistent
estimator;itinsteadrecoversablurredversionofthedistribution.However,if
thenoiselevelischosentoapproach0whenthenumberofexamplesapproaches
inﬁnity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin
moredetailinsection.18.5
OtherconnectionsbetweenautoencodersandRBMsexist.Scorematching
appliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror
combinedwitharegularizationtermsimilartothecontractivepenaltyofthe
CAE(Swersky2011BengioandDelalleau2009 e t a l .,) ()showedthatanautoen-
codergradientprovidesanapproximationtocontrastivedivergencetrainingof
RBMs Forcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand
reconstructiondistributionyieldsanestimatorofthescorethatisapplicableto
generalencoderanddecoderparametrizations ( ,).This AlainandBengio2013
meansagenericencoder-decoderarchitecturemaybemadetoestimatethescore
5 1 3
CHAPTER14.AUTOENCODERS
bytrainingwiththesquarederrorcriterion
|| g f((˜ x x))−||2(14.16)
andcorruption
C(˜ x=˜x x|) = (N˜ x x ;= µ , σΣ = 2I) (14.17)
withnoisevariance σ2.Seeﬁgureforanillustrationofhowthisworks 14.5
Figure14.5:Vectorﬁeldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold
nearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe
reconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability
accordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorﬁeldhaszeros
atbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima
ofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof
localmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof
thegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength
ofthearrows)islarge,itmeansthatprobabilitycanbesigniﬁcantlyincreasedbymoving
inthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability Theautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions Whereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore
accurate.Figurereproducedwithpermissionfrom () AlainandBengio2013
Ingeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe
input xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis
5 1 4
CHAPTER14.AUTOENCODERS
whytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011
where g( f( x))− xmaybeobtainedbytakingthederivativeofanotherfunction

============================================================

=== CHUNK 134 ===
Palavras: 367
Caracteres: 7679
--------------------------------------------------
KamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by
identifyingafamilyofshallowautoencoderssuchthat g( f( x))− xcorrespondsto
ascoreforallmembersofthefamily Sofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent
aprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas
agenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed
later,insection.20.11
1 4 1 Hi st o r i c a l P e r spec t i v e
TheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987
and ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001
images.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise However,thename“denoisingautoencoder”referstoamodelthatisintendednot
merelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation
as asideeﬀect oflearningto denoise.This ideacame muchlater (Vincent
e t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010
deeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,
sparsecoding,contractiveautoencodersandotherregularizedautoencoders,the
motivationforDAEswastoallowthelearningofaveryhigh-capacity encoder
whilepreventingtheencoderanddecoderfromlearningauselessidentityfunction Priortotheintroduction ofthemodernDAE,InayoshiandKurita2005()
exploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach
minimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting
noiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove
generalization byintroducing the reconstructionerror andtheinjectednoise However,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction
familiesaspowerfulascanthemodernDAE 14.6LearningManifoldswithAutoencoders
Like many other machine learning algorithms, auto encoders exploittheidea
thatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch
manifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3
thisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe
manifoldbutmayhaveunusualbehaviorifgivenaninputthatisoﬀthemanifold 5 1 5
CHAPTER14.AUTOENCODERS
Autoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold Tounderstandhowautoencodersdothis,wemustpresentsomeimportant
characteristicsofmanifolds Animportantcharacterization ofamanifoldisthesetofits t angen t pl anes Atapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis
vectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As
illustratedinﬁgure,theselocaldirectionsspecifyhowonecanchange 14.6 x
inﬁnitesimallywhilestayingonthemanifold Allautoencodertrainingproceduresinvolveacompromisebetweentwoforces:
1.Learningarepresentation hofatrainingexample xsuchthat xcanbe
approximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn
fromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed
notsuccessfullyreconstructinputsthatarenotprobableunderthedata
generatingdistribution Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-
turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe
aregularizationtermaddedtothereconstructioncost.Thesetechniques
generallyprefersolutionsthatarelesssensitivetotheinput Clearly,neitherforcealonewouldbeuseful—copyingtheinputtotheoutput
isnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether
areusefulbecausetheyforcethehiddenrepresentationtocaptureinformation
aboutthestructureofthedatageneratingdistribution.Theimportantprinciple
isthattheautoencodercanaﬀordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d
t o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates
nearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture
alocalcoordinatesystemforthismanifold:onlythevariationstangenttothe
manifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder
learnsamappingfromtheinputspace xtoarepresentationspace,amappingthat
isonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto
changesorthogonaltothemanifold Aone-dimensional exampleisillustratedinﬁgure,showingthat,bymaking 14.7
thereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe
datapoints,wecausetheautoencodertorecoverthemanifoldstructure Tounderstandwhyautoencodersareusefulformanifoldlearning,itisin-
structivetocomparethemtootherapproaches.Whatismostcommonlylearned
tocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear)
5 1 6
CHAPTER14.AUTOENCODERS
Figure14.6: Anillustrationoftheconceptofatangenthyperplane.Herewecreatea
one-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784
pixelsandtransformitbytranslatingitvertically Theamountofverticaltranslation
deﬁnesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath
throughimagespace.Thisplotshowsafewpointsalongthismanifold Forvisualization,
wehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional
manifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches
themanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint Itdeﬁnesthespaceofdirectionsinwhichitispossibletomovewhileremainingon
themanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean
exampletangentlineatonepoint,withanimageshowinghowthistangentdirection
appearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong
thetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels
thatdarken 5 1 7
CHAPTER14.AUTOENCODERS
x 0 x 1 x 2
x0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y
O p t i m a l r e c o n s t r u c t i o n
Figure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall
perturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here
themanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0
lineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction
functioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal
arrowsatthebottomoftheplotindicatethe r( x)− xreconstructiondirectionvector
atthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest“manifold”
(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake
thederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The
contractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is
askedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The
spacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where
thereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints
backontothemanifold themanifold.Sucharepresentationforaparticularexampleisalsocalledits
embedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions
thanthe“ambient”spaceofwhichthemanifoldisalow-dimensionalsubset.Some
algorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly
learnanembeddingforeachtrainingexample,whileotherslearnamoregeneral
mapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany
pointintheambientspace(theinputspace)toitsembedding Manifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat
attempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch
onlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased
onthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample
andedgesconnectingnearneighborstoeachother.Thesemethods(Schölkopf
e t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,;
5 1 8
CHAPTER14.AUTOENCODERS
Figure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph
inwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor
relationships

============================================================

=== CHUNK 135 ===
Palavras: 411
Caracteres: 9607
--------------------------------------------------
Variousprocedurescanthusobtainthetangentplaneassociatedwitha
neighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining
examplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize
sucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber
ofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these
approachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l andNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;
andRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha
tangentplanethatspansthedirectionsofvariationsassociatedwiththediﬀerence
vectorsbetweentheexampleanditsneighbors,asillustratedinﬁgure.14.8
Aglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or
solvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9
largenumberoflocallylinearGaussian-likepatches(or“pancakes,”becausethe
Gaussiansareﬂatinthetangentdirections) However,thereisafundamentaldiﬃcultywithsuchlocalnon-parametric
approachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005
manifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),
onemayneedaverylargenumberoftrainingexamplestocovereachoneof
5 1 9
CHAPTER14.AUTOENCODERS
Figure14.9:Ifthetangentplanes(seeﬁgure)ateachlocationareknown,thenthey 14.6
canbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch
canbethoughtofasalocalEuclideancoordinatesystemorasalocallyﬂatGaussian,or
“pancake,”withaverysmallvarianceinthedirectionsorthogonaltothepancakeanda
verylargevarianceinthedirectionsdeﬁningthecoordinatesystemonthepancake.A
mixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold
Parzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003
variant( ,) Bengio e t a l .2006c
thesevariations,withnochancetogeneralizetounseenvariations.Indeed,these
methodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between
neighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan
haveverycomplicatedstructurethatcanbediﬃculttocapturefromonlylocal
interpolation.Considerforexamplethemanifoldresultingfromtranslationshown
inﬁgure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe
imageistranslated,wewillobservethatonecoordinateencountersapeakora
troughinitsvalueonceforeverypeakortroughinbrightnessintheimage In
otherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage
templatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming
simpleimagetransformations.Thismotivatestheuseofdistributedrepresentations
anddeeplearningforcapturingmanifoldstructure 5 2 0
CHAPTER14.AUTOENCODERS
14.7ContractiveAutoencoders
Thecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab
onthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible:
Ω() = h λ∂ f() x
∂ x2
F (14.18)
ThepenaltyΩ( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe
Jacobianmatrixofpartialderivativesassociatedwiththeencoderfunction Thereisaconnectionbetweenthedenoisingautoencoderandthecontractive
autoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013
input noise, the denoising reconstruction erroris equivalent toacontractive
penaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother
words,denoisingautoencodersmakethereconstructionfunctionresistsmallbut
ﬁnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe
featureextractionfunctionresistinﬁnitesimalperturbationsoftheinput.When
usingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse
withaclassiﬁer,thebestclassiﬁcationaccuracyusuallyresultsfromapplyingthe
contractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x)
alsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1
Thename c o n t r ac t i v earisesfromthewaythattheCAEwarpsspace.Speciﬁ-
cally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged
tomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints Wecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput
neighborhood Toclarify,theCAEiscontractiveonlylocally—allperturbationsofatraining
point xaremappednearto f( x).Globally,twodiﬀerentpoints xand xmaybe
mappedto f( x)and f( x)pointsthatarefartherapartthantheoriginalpoints Itisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see
forexamplewhathappensinthe1-Dtoyexampleofﬁgure).Whenthe 14.7 Ω( h)
penaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto
makethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01
inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa
binarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout
mostofthehypercubethatitssigmoidalhiddenunitscanspan WecanthinkoftheJacobianmatrix Jatapoint xasapproximating the
nonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword
“contractive”moreformally Inthetheoryoflinearoperators,alinearoperator
5 2 1
CHAPTER14.AUTOENCODERS
issaidtobecontractiveifthenormof J xremainslessthanorequaltofor1
allunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere WecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear
approximationof f( x)ateverytrainingpoint xinordertoencourageeachof
theselocallinearoperatortobecomeacontraction Asdescribed insection, regularized autoencoderslearnmanifoldsby 14.6
balancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare
reconstructionerrorandthecontractivepenaltyΩ( h).Reconstructionerroralone
wouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty
alonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x Thecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives
∂ f() x
∂ xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa
smallnumberofdirectionsintheinput,mayhavesigniﬁcantderivatives ThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions
xwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich
approximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a
and ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b
of Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1
somesingularvaluesremainabove,becausethereconstructionerrorpenalty 1
encouragestheCAEtoencodethedirectionswiththemostlocalvariance.The
directionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent
directionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent
directionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE
appliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas
objectsintheimagegraduallychangepose,asshowninﬁgure.Visualizations 14.6
oftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful
transformationsoftheinputimage,asshowninﬁgure.14.10
OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit
ischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes
muchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby
Rifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each
trainedtoreconstructthepreviousautoencoder’shiddenlayer.Thecomposition
oftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas
separatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive
aswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining
theentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit
capturesmanyofthedesirablequalitativecharacteristics Anotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults
5 2 2
CHAPTER14.AUTOENCODERS
Input
pointTangentvectors
LocalPCA(nosharingacrossregions)
Contractiveautoencoder
Figure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA
andbyacontractiveautoencoder.Thelocationonthemanifoldisdeﬁnedbytheinput
imageofadogdrawnfromtheCIFAR-10dataset Thetangentvectorsareestimated
bytheleadingsingularvectorsoftheJacobianmatrix∂ h
∂ xoftheinput-to-codemapping AlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto
formmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter
sharingacrossdiﬀerentlocationsthatshareasubsetofactivehiddenunits TheCAE
tangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas
theheadorlegs).Imagesreproducedwithpermissionfrom () Rifai e t a l .2011c
ifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder
couldconsistofmultiplyingtheinputbyasmallconstant andthedecoder
couldconsistofdividingthecodeby .As approaches,theencoderdrivesthe 0
contractivepenaltyΩ( h)toapproachwithouthavinglearnedanythingaboutthe 0
distribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai
e t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare
standardneuralnetworklayersconsistingofanaﬃnetransformationfollowedby
anelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g
tobethetransposeoftheweightmatrixof f
14.8PredictiveSparseDecomposition
P r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse
codingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric
encoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen
appliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo
(Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell
asforaudio( ,).Themodelconsistsofanencoder Henaﬀ e t a l .2011 f( x)anda
decoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe
5 2 3
CHAPTER14.AUTOENCODERS
optimization algorithm.Trainingproceedsbyminimizing
||− || x g() h2+ λ|| h1+ () γ f ||− h x||2

============================================================

=== CHUNK 136 ===
Palavras: 362
Caracteres: 10389
--------------------------------------------------
(14.19)
Likeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with
respectto handminimization withrespecttothemodelparameters.Minimization
withrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe
costfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent
canobtainreasonablevaluesofinasfewastensteps h
ThetrainingprocedureusedbyPSDisdiﬀerentfromﬁrsttrainingasparse
codingmodelandthentraining f( x)topredictthevaluesofthesparsecoding
features.ThePSDtrainingprocedureregularizesthedecodertouseparameters
forwhichcaninfergoodcodevalues f() x
Predictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e Insection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19
makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding
probabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe
model InpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring
training.Theparametricencoder fisusedtocomputethelearnedfeatureswhen
themodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto
inferring hviagradientdescent.Because fisadiﬀerentiableparametricfunction,
PSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained
withanothercriterion 14.9ApplicationsofAutoencoders
Autoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-
mationretrievaltasks.Dimensionalityreductionwasoneoftheﬁrstapplications
ofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations
forstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained
astackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder
withgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The
resultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand
thelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe
underlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters Lower-dimensionalrepresentationscanimproveperformanceonmanytasks,
suchasclassiﬁcation.Modelsofsmallerspacesconsumelessmemoryandruntime 5 2 4
CHAPTER14.AUTOENCODERS
Manyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear
eachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l ().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008
generalization Onetaskthatbeneﬁtsevenmorethanusualfromdimensionalityreductionis
i nf o r m at i o n r e t r i e v al,thetaskofﬁndingentriesinadatabasethatresemblea
queryentry Thistaskderivestheusualbeneﬁtsfromdimensionalityreduction
thatothertasksdo,butalsoderivestheadditionalbeneﬁtthatsearchcanbecome
extremelyeﬃcientincertainkindsoflowdimensionalspaces.Speciﬁcally, if
wetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-
dimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary
mappingbinarycodevectorstoentries.Thishashtableallowsustoperform
informationretrievalbyreturningalldatabaseentriesthathavethesamebinary
codeasthe query.Wecanalso search overslightlylesssimilar entries very
eﬃciently,justbyﬂippingindividualbitsfromtheencodingofthequery This
approachtoinformationretrievalviadimensionalityreductionandbinarization
iscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas
beenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and
images(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,) Toproducebinarycodesforsemantichashing,onetypicallyusesanencoding
functionwithsigmoidsontheﬁnallayer.Thesigmoidunitsmustbetrainedtobe
saturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish
thisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring
training.Themagnitudeofthenoiseshouldincreaseovertime.Toﬁghtthat
noiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe
magnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs Theideaoflearningahashingfunctionhasbeenfurtherexploredinseveral
directions,includingtheideaoftrainingtherepresentationssoastooptimize
alossmoredirectlylinkedtothetaskofﬁndingnearbyexamplesinthehash
table( ,) NorouziandFleet2011
5 2 5
C h a p t e r 1 5
Represen t at i on L e ar n i n g
Inthischapter,weﬁrstdiscusswhatitmeanstolearnrepresentationsandhow
thenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss
howlearningalgorithmssharestatisticalstrengthacrossdiﬀerenttasks,including
usinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared
representationsareusefultohandlemultiplemodalitiesordomains,ortotransfer
learnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask
representationexists.Finally,westepbackandargueaboutthereasonsforthe
successofrepresentationlearning,startingwiththetheoreticaladvantagesof
distributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand
endingwiththemoregeneralideaofunderlyingassumptionsaboutthedata
generatingprocess,inparticularaboutunderlyingcausesoftheobserveddata Manyinformationprocessingtaskscanbeveryeasyorverydiﬃcultdepending
onhowtheinformationisrepresented.Thisisageneralprincipleapplicableto
dailylife,computerscienceingeneral,andtomachinelearning.Forexample,it
isstraightforwardforapersontodivide210by6usinglongdivision Thetask
becomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman
numeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX
byVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,
permittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More
concretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing
appropriateorinappropriate representations.Forexample,insertinganumber
intothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe
listisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa
red-blacktree Inthecontextofmachinelearning,whatmakesonerepresentationbetterthan
526
CHAPTER15.REPRESENTATIONLEARNING
another?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent
learningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice
ofthesubsequentlearningtask Wecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-
formingakindofrepresentationlearning.Speciﬁcally,thelastlayerofthenetwork
istypicallyalinearclassiﬁer,suchasasoftmaxregressionclassiﬁer.Therestof
thenetworklearnstoprovidearepresentationtothisclassiﬁer.Trainingwitha
supervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but
moresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiﬁcation
taskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput
featuresmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the
lastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiﬁer
(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould
learndiﬀerentpropertiesdependingonthetypeofthelastlayer Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing
anyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation
learningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin
someparticularway.Forexample,supposewewanttolearnarepresentationthat
makesdensityestimationeasier.Distributionswithmoreindependencesareeasier
tomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements
oftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,
unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso
learnarepresentationasasideeﬀect.Regardlessofhowarepresentationwas
obtained,itcanbeusedforanothertask.Alternatively,multipletasks(some
supervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal
representation Mostrepresentationlearningproblemsfaceatradeoﬀbetweenpreservingas
muchinformationabouttheinputaspossibleandattainingniceproperties(such
asindependence) Representationlearningisparticularlyinterestingbecauseitprovidesone
waytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery
largeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining
data.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften
resultsinsevereoverﬁtting.Semi-supervisedlearningoﬀersthechancetoresolve
thisoverﬁttingproblembyalsolearningfromtheunlabeleddata.Speciﬁcally,
wecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese
representationstosolvethesupervisedlearningtask Humansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo
5 2 7
CHAPTER15.REPRESENTATIONLEARNING
notyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman
performance—forexample,thebrainmayuseverylargeensemblesofclassiﬁers
orBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis
abletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways
toleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe
unlabeleddatacanbeusedtolearnagoodrepresentation 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g
Unsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural
networks,enablingresearchersfortheﬁrsttimetotrainadeepsupervisednetwork
withoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We
callthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r -
wi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow
arepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture
theshapeoftheinputdistribution)cansometimesbeusefulforanothertask
(supervisedlearningwiththesameinputdomain) Greedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-
tationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse
codingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris
pretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer
andproducingasoutputanewrepresentationofthedata,whosedistribution(or
itsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler Seealgorithm foraformaldescription 15.1
Greedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong
beenusedtosidestepthediﬃcultyofjointlytrainingthelayersofadeepneuralnet
forasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron
(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery
thatthisgreedylearningprocedurecouldbeusedtoﬁndagoodinitialization for
ajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused
tosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,;
andSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,)

============================================================

=== CHUNK 137 ===
Palavras: 433
Caracteres: 10453
--------------------------------------------------
Priortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth
resultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow
thatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep
architectures,buttheunsupervisedpretrainingapproachwastheﬁrstmethodto
succeed Greedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o -
5 2 8
CHAPTER15.REPRESENTATIONLEARNING
r i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one
pieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se
becausetheseindependentpiecesarethelayersofthenetwork.Speciﬁcally,greedy
layer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile
keepingthepreviousonesﬁxed.Inparticular,thelowerlayers(whicharetrained
ﬁrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -
v i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning
algorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe
onlyaﬁrststepbeforeajointtrainingalgorithmisappliedto ﬁne-t uneallthe
layerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed
asaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout
decreasingtrainingerror)andaformofparameterinitialization Itiscommontousetheword“pretraining”torefernotonlytothepretraining
stageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining
phaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve
trainingasimpleclassiﬁerontopofthefeatureslearnedinthepretrainingphase,
oritmayinvolvesupervisedﬁne-tuningoftheentirenetworklearnedinthe
pretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor
whatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining
schemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm
willobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining
followthisbasicprotocol Greedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization
forotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton
andSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent
variables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006
Boltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative
modelswillbedescribedinchapter.20
Asdiscussedinsection, itisalsopossibletohavegreedylayer-wise 8.7.4
supervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork
iseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral
contexts(,) 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk Onmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial
improvementsintesterrorforclassiﬁcationtasks.Thisobservationwasresponsible
fortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,
5 2 9
CHAPTER15.REPRESENTATIONLEARNING
Al g o r i t hm 1 5 1Greedylayer-wiseunsupervisedpretrainingprotocol Giventhefollowing: Unsupervisedfeaturelearningalgorithm L,whichtakesa
trainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw
inputdataisX,withonerowperexampleandf( 1 )(X)istheoutputoftheﬁrst
stageencoderonX.Inthecasewhereﬁne-tuningisperformed,weusealearner
Twhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised
ﬁne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber
ofstagesis.m
f←Identityfunction
˜XX= 
f o r dok,...,m = 1
f( ) k= (L˜X)
ff←( ) k◦f
˜X←f( ) k(˜X)
e nd f o r
i fﬁne-tuning t he n
ff,, ←T(XY)
e nd i f
Ret ur nf
2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however,
unsupervisedpretrainingeitherdoesnotconferabeneﬁtorevencausesnoticeable
harm ()studiedtheeﬀectofpretrainingonmachinelearning Maetal.2015
modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas
slightlyharmful,butformanytaskswassigniﬁcantlyhelpful.Becauseunsupervised
pretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand
whenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular
task Attheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted
togreedyunsupervisedpretraininginparticular.Thereareother,completely
diﬀerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,
suchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13
trainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel Examplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle
andBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015
objectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly
usingtheinput) Unsupervisedpretrainingcombinestwodiﬀerentideas.First,itmakesuseof
5 3 0
CHAPTER15.REPRESENTATIONLEARNING
theideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave
asigniﬁcantregularizingeﬀectonthemodel(and,toalesserextent,thatitcan
improveoptimization) Second,itmakesuseofthemoregeneralideathatlearning
abouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto
outputs Bothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral
partsofthemachinelearningalgorithmthatarenotentirelyunderstood Theﬁrstidea,thatthechoiceofinitialparametersforadeepneuralnetwork
canhaveastrongregularizingeﬀectonitsperformance, istheleastwellunderstood Atthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe
modelinalocationthatwouldcauseittoapproachonelocalminimumratherthan
another Today,localminimaarenolongerconsideredtobeaseriousproblem
forneuralnetworkoptimization Wenowknowthatourstandardneuralnetwork
trainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains
possiblethatpretraininginitializesthemodelinalocationthatwouldotherwise
beinaccessible—forexample,aregionthatissurroundedbyareaswherethecost
functionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly
averynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe
Hessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse
verysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe
pretrainedparametersareretainedduringthesupervisedtrainingstageislimited Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised
learningandsupervisedlearningratherthantwosequentialstages.Onemay
alsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe
supervisedlearningstagepreservesinformationfromtheunsupervisedlearning
stagebysimplyfreezingthe parameters for thefeature extractorsand using
supervisedlearningonlytoaddaclassiﬁerontopofthelearnedfeatures Theotheridea,thatalearningalgorithmcanuseinformationlearnedinthe
unsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter
understood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised
taskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain
agenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout
wheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,
therepresentationofthewheelswilltakeonaformthatiseasyforthesupervised
learnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel,
soitisnotalwayspossibletopredictwhichtaskswillbeneﬁtfromunsupervised
learninginthisway.Manyaspectsofthisapproacharehighlydependenton
thespeciﬁcmodelsused.Forexample,ifwewishtoaddalinearclassiﬁeron
5 3 1
CHAPTER15.REPRESENTATIONLEARNING
topofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly
separable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This
isanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe
preferable—theconstraintsimposedbytheoutputlayerarenaturallyincluded
fromthestart Fromthepointofviewofunsupervisedpretrainingaslearningarepresentation,
wecanexpectunsupervisedpretrainingtobemoreeﬀectivewhentheinitial
representationispoor Onekeyexampleofthisistheuseofwordembeddings Wordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo
distinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance
of).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2
distancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially
usefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps
becauseimagesalreadylieinarichvectorspacewheredistancesprovidealow
qualitysimilaritymetric Fromthepointofviewofunsupervisedpretrainingasaregularizer,wecan
expectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled
examplesisverysmall.Becausethesourceofinformationaddedbyunsupervised
pretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining
toperformbest whenthe number ofunlabeled examples is very large.The
advantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany
unlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin
2011withunsupervisedpretrainingwinningtwointernationaltransferlearning
competitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011
numberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens
ofexamplesperclass).Theseeﬀectswerealsodocumentedincarefullycontrolled
experimentsbyPaine2014etal.() Otherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining
islikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated Unsupervisedlearningdiﬀersfromregularizerslikeweightdecaybecauseitdoesnot
biasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering
featurefunctionsthatareusefulfortheunsupervisedlearningtask Ifthetrue
underlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput
distribution,unsupervisedlearningcanbeamoreappropriateregularizer Thesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised
pretrainingisknowntocauseanimprovement,andexplainwhatisknownabout
whythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused
toimproveclassiﬁers,andisusuallymostinterestingfromthepointofviewof
5 3 2
CHAPTER15.REPRESENTATIONLEARNING
                                                       
               
                  
Figure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiﬀerent
neuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one
mappingsfromparametervectorstofunctions),withdiﬀerentrandominitializations
andwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiﬀerent
neuralnetwork,ataparticulartimeduringitstrainingprocess.Thisﬁgureisadapted
withpermissionfrom ().Acoordinateinfunctionspaceisaninﬁnite- Erhan e t a l .2010
dimensionalvectorassociatingeveryinputxwithanoutputy

============================================================

=== CHUNK 138 ===
Palavras: 351
Caracteres: 11847
--------------------------------------------------
()made Erhan e t a l .2010
alinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciﬁcx
points.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum
e t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000
(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions
overtheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to
pointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen
usingpretrainingandinanother,non-overlappingregionwhennotusingpretraining Isomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion
correspondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator
hasreducedvariance 5 3 3
CHAPTER15.REPRESENTATIONLEARNING
reducingtestseterror.However,unsupervisedpretrainingcanhelptasksother
thanclassiﬁcation,andcanacttoimproveoptimization ratherthanbeingmerely
aregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror
fordeepautoencoders(HintonandSalakhutdinov2006,) Erhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof
unsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements
totesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe
parametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork
trainingisnon-determinis tic,andconvergestoadiﬀerentfunctioneverytimeit
isrun Trainingmayhaltatapointwherethegradientbecomessmall,apoint
whereearlystoppingendstrainingtopreventoverﬁtting,oratapointwherethe
gradientislargebutitisdiﬃculttoﬁndadownhillstepduetoproblemssuchas
stochasticityorpoorconditioningoftheHessian Neuralnetworksthatreceive
unsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,
whileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See
ﬁgureforavisualizationofthisphenomenon Theregionwherepretrained 15.1
networksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe
estimationprocess,whichcaninturnreducetheriskofsevereover-ﬁtting.In
otherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto
aregionthattheydonotescape,andtheresultsfollowingthisinitialization are
moreconsistentandlesslikelytobeverybadthanwithoutthisinitialization Erhan2010etal.()alsoprovidesomeanswersastopretrainingworks when
best—themeanandvarianceofthetesterrorweremostreducedbypretrainingfor
deepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe
inventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks
(rectiﬁedlinearunits,dropoutandbatchnormalization) solessisknownaboutthe
eﬀectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches Animportantquestionishowunsupervisedpretrainingcanactasaregularizer Onehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover
featuresthatrelatetotheunderlyingcausesthatgeneratetheobserveddata Thisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised
pretraining,andisdescribedfurtherinsection.15.3
Comparedtootherformsofunsupervisedlearning,unsupervisedpretraining
hasthedisadvantagethatitoperateswithtwoseparatetrainingphases Many
regularizationstrategieshavetheadvantageofallowingtheusertocontrolthe
strengthoftheregularizationbyadjustingthevalueofasinglehyperparameter Unsupervisedpretrainingdoesnotoﬀeraclearwaytoadjustthethestrength
oftheregularization arisi ngfromtheunsupervised stage.Instead, thereare
5 3 4
CHAPTER15.REPRESENTATIONLEARNING
verymanyhyperparameters ,whoseeﬀectmaybemeasuredafterthefactbut
isoftendiﬃculttopredictaheadoftime.Whenweperformunsupervisedand
supervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there
isasinglehyperparameter,usuallyacoeﬃcientattachedtotheunsupervised
cost, thatdetermineshowstronglytheunsupervisedobjectivewillregularize
thesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby
decreasingthiscoeﬃcient.Inthecaseofunsupervisedpretraining,thereisnota
wayofﬂexiblyadaptingthestrengthoftheregularization—either thesupervised
modelisinitializedtopretrainedparameters,oritisnot Anotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase
hasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot
bepredictedduringtheﬁrstphase,sothereisalongdelaybetweenproposing
hyperparametersfortheﬁrstphaseandbeingabletoupdatethemusingfeedback
fromthesecondphase.Themostprincipledapproachistousevalidationseterror
inthesupervisedphaseinordertoselectthehyperparameters ofthepretraining
phase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009
likethenumberofpretrainingiterations,aremoreconvenientlysetduringthe
pretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis
notidealbutcomputationally muchcheaperthanusingthesupervisedobjective Today,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe
ﬁeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas
one-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled
setsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain
onceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof
words),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and
thenusethisrepresentationorﬁne-tuneitforasupervisedtaskforwhichthe
trainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered
bybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal ()andremainsincommonusetoday 2011a
Deeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout
orbatchnormalization, areabletoachievehuman-levelperformanceonverymany
tasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-
performunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and
MNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall
datasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform
methodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,
thepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised
pretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch
5 3 5
CHAPTER15.REPRESENTATIONLEARNING
andcontinuestoinﬂuencecontemporaryapproaches.Theideaofpretraininghas
beengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4
commonapproachfortransferlearning.Supervisedpretrainingfortransferlearning
ispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional
networkspretrainedontheImageNetdataset.Practitionerspublishtheparameters
ofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare
publishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,) 2 T ransfer L earni n g an d D om ai n A d ap t at i o n
Transferlearninganddomainadaptationrefertothesituationwherewhathasbeen
learnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization
inanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe
previoussection,wherewetransferredrepresentationsbetweenanunsupervised
learningtaskandasupervisedlearningtask In t r ansf e r l e ar ni ng,thelearnermustperformtwoormorediﬀerenttasks,
butweassumethatmanyofthefactorsthatexplainthevariationsinP 1are
relevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically
understoodinasupervisedlearningcontext,wheretheinputisthesamebutthe
targetmaybeofadiﬀerentnature.Forexample,wemaylearnaboutonesetof
visualcategories,suchascatsanddogs,intheﬁrstsetting,thenlearnabouta
diﬀerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If
thereissigniﬁcantlymoredataintheﬁrstsetting(sampledfromP 1),thenthat
mayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly
veryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions
ofedgesandvisualshapes,theeﬀectsofgeometricchanges,changesinlighting,
etc Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7
adaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures
thatareusefulforthediﬀerentsettingsortasks,correspondingtounderlying
factorsthatappearinmorethanonesetting.Thisisillustratedinﬁgure,with7.2
sharedlowerlayersandtask-dependentupperlayers However, sometimes, whatisshared amongthe diﬀerent tasksisnotthe
semanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech
recognitionsystemneedstoproducevalidsentencesattheoutputlayer,but
theearlierlayersneartheinputmayneedtorecognizeverydiﬀerentversionsof
thesamephonemesorsub-phonemicvocalizationsdependingonwhichperson
isspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers
(neartheoutput)oftheneuralnetwork,andhaveatask-speciﬁcpreprocessing,as
5 3 6
CHAPTER15.REPRESENTATIONLEARNING
illustratedinﬁgure.15.2
Se l e c t i on sw i t c h
h(1)h(1)h(2)h(2)h(3)h(3)yy
h(shared)h(shared)
x(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 )
Figure15.2: Example architectureformulti-taskortransferlearningwhentheoutput
variablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiﬀerent y x 
meaning(andpossiblyevenadiﬀerentdimension)foreachtask(or,forexample,each
user),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection
switch)aretask-speciﬁc,whiletheupperlevelsareshared.Thelowerlevelslearnto
translatetheirtask-speciﬁcinputintoagenericsetoffeatures Intherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to-
outputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution
isslightlydiﬀerent.Forexample,considerthetaskofsentimentanalysis,which
consistsofdeterminingwhetheracommentexpressespositiveornegativesentiment Commentspostedonthewebcomefrommanycategories.Adomainadaptation
scenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof
mediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments
aboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine
thatthereisanunderlyingfunctionthattellswhetheranystatementispositive,
neutralornegative,butofcoursethevocabularyandstylemayvaryfromone
domaintoanother,makingitmorediﬃculttogeneralizeacrossdomains.Simple
unsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery
successfulforsentimentanalysiswithdomainadaptation( ,) Glorotetal.2011b
Arelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform
oftransferlearningduetogradualchangesinthedatadistributionovertime Bothconceptdriftandtransferlearningcanbeviewedasparticularformsof
5 3 7
CHAPTER15.REPRESENTATIONLEARNING
multi-tasklearning.Whilethephrase“multi-tasklearning” typicallyrefersto
supervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable
tounsupervisedlearningandreinforcementlearningaswell Inallofthesecases,theobjectiveistotakeadvantageofdatafromtheﬁrst
settingtoextractinformationthatmaybeusefulwhenlearningorevenwhen
directlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation
learningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe
samerepresentationinbothsettingsallowstherepresentationtobeneﬁtfromthe
trainingdatathatisavailableforbothtasks Asmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound
successinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow
etal.,).Intheﬁrstofthesecompetitions,theexperimentalsetupisthe 2011
following.Eachparticipantisﬁrstgivenadatasetfromtheﬁrstsetting(from
distributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants
mustusethistolearnagoodfeaturespace(mappingtherawinputtosome
representation),suchthatwhenweapplythislearnedtransformationtoinputs
fromthetransfersetting(distributionP 2),alinearclassiﬁercanbetrainedand
generalizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults
foundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand
deeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected
intheﬁrstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond
(transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled
examplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic
generalization performance

============================================================

=== CHUNK 139 ===
Palavras: 370
Caracteres: 6196
--------------------------------------------------
Twoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r o - sho t
l e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample
ofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare
givenatallforthezero-shotlearningtask One-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation
learnstocleanlyseparatetheunderlyingclassesduringtheﬁrststage.Duringthe
transferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany
possibletestexamplesthatallclusteraroundthesamepointinrepresentation
space.Thisworkstotheextentthatthefactorsofvariationcorrespondingto
theseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned
representationspace,andwehavesomehowlearnedwhichfactorsdoanddonot
matterwhendiscriminatingobjectsofcertaincategories Asanexampleofazero-shotlearningsetting,considertheproblemofhaving
alearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems 5 3 8
CHAPTER15.REPRESENTATIONLEARNING
Itmaybepossibletorecognizeaspeciﬁcobjectclassevenwithouthavingseenan
imageofthatobject,ifthetextdescribestheobjectwellenough Forexample,
havingreadthatacathasfourlegsandpointyears,thelearnermightbeableto
guessthatanimageisacat,withouthavingseenacatbefore Zero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(
etal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation
hasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario
asincludingthreerandomvariables:thetraditionalinputsx,thetraditional
outputsortargetsy,andanadditionalrandomvariabledescribingthetask,T Themodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where
Tisadescriptionofthetaskwewishthemodeltoperform Inourexampleof
recognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley
withy= 1indicating“yes”andy= 0indicating“no.”ThetaskvariableTthen
representsquestionstobeansweredsuchas“Isthereacatinthisimage?”Ifwe
haveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe
samespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT Inourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis
importantthatwehavehadunlabeledtextdatacontainingsentencessuchas“cats
havefourlegs”or“catshavepointyears.”
Zero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort
ofgeneralization Forexample,Tcannotbejustaone-hotcodeindicatingan
objectcategory ()provideinsteadadistributedrepresentation Socheretal.2013b
ofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated
witheachcategory Asimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,;
Mikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and
therelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe
otherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith
wordsintheother.Eventhoughwemaynothavelabeledexamplestranslating
wordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa
translationforwordAbecausewehavelearnedadistributedrepresentationfor
wordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and
createdalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples
consistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe
mostsuccessfulifallthreeingredients(thetworepresentationsandtherelations
betweenthem)arelearnedjointly Zero-shotlearningisaparticularformoftransferlearning.Thesameprinciple
explainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation
5 3 9
CHAPTER15.REPRESENTATIONLEARNING
h x = f x ( ) x
x t e s t
y t e s th y = f y ( ) y
y − s pa ce
R e l at i onshi p  b e t w e e n   e m be dde d  p oi n t s   w i t hi n  one   o f   t h e   d o m a i n s
Maps be t w e e n   r e p r e s e n t at i on spac e s  f x
f y
x − s pa ce
( ) pa i r s i n t he t r a i ni ng s et x y ,
f x : enco der f unctio n f o r x
f y : enco der f unctio n f o r y
Figure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand
similarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions
appearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis
applied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints
inxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance
inh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both
ofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled
examples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way
ortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe
representationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning
isthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno
imageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t)
andimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween
representationspaces.Itworksbecause,althoughthatimageandthatwordwerenever
paired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach
other.FigureinspiredfromsuggestionbyHrantKhachatrian 5 4 0
CHAPTER15.REPRESENTATIONLEARNING
inonemodality,arepresentationintheother,andtherelationship(ingeneralajoint
distribution)betweenpairs (xy,)consistingofoneobservationxinonemodality
andanotherobservationyintheothermodality(SrivastavaandSalakhutdino v,
2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,from
ytoitsrepresentation,andtherelationshipbetweenthetworepresentations),
conceptsinonerepresentationareanchoredintheother,andvice-versa,allowing
onetomeaningfully generalizeto newpairs.Theprocedureis illustratedin
ﬁgure.15.3
15 3 S em i - S u p ervi s ed D i s en t a n g l i n g of C au s al F ac t ors
Animportantquestionaboutrepresentationlearningis“whatmakesonerepre-
sentationbetterthananother?”Onehypothesisisthatanidealrepresentation
isoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-
lyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature
spacecorrespondingtodiﬀerentcauses,sothattherepresentationdisentanglesthe
causesfromoneanother.Thishypothesismotivatesapproachesinwhichweﬁrst
seekagoodrepresentationforp(x)

============================================================

=== CHUNK 140 ===
Palavras: 354
Caracteres: 11154
--------------------------------------------------
Sucharepresentationmayalsobeagood
representationforcomputingp(yx|)ifyisamongthemostsalientcausesof
x Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast
the1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,),inmoredetail Forotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure
supervisedlearning,wereferthereadertosection1.2of () Chapelleetal.2006
Inotherapproachestorepresentationlearning,wehaveoftenbeenconcerned
witharepresentationthatiseasytomodel—forexample,onewhoseentriesare
sparse,orindependentfromeachother.Arepresentationthatcleanlyseparates
theunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel However,afurtherpartofthehypothesismotivatingsemi-supervisedlearning
viaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo
propertiescoincide: once weareabletoobtaintheunderlyingexplanationsfor
whatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom
theothers.Speciﬁcally,ifarepresentationhrepresentsmanyoftheunderlying
causesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,
thenitiseasytopredictfrom.yh
First,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised
learningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecase
wherep(x)isuniformlydistributedandwewanttolearnf(x) = E[y|x].Clearly,
observingatrainingsetofvaluesalonegivesusnoinformationabout x p( )y x|
5 4 1
CHAPTER15.REPRESENTATIONLEARNING
xp x ( )y = 1 y = 2 y = 3
Figure15.4:Exampleofadensityoverxthatisamixtureoverthree components Thecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture
components(e.g., naturalobjectclassesinimagedata)arestatisticallysalient,just
modelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor
y Next,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed Considerthesituationwhere xarisesfromamixture,withonemixturecomponent
pervalueofy,asillustratedinﬁgure Ifthemixturecomponentsarewell- 15.4
separated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda
singlelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|) Butmoregenerally,whatcouldmakeandbetiedtogether p( )y x|p()x
Ifyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and
p(yx|)will bestronglytied, andunsupervisedrepresentationlearningthat
triestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa
semi-supervisedlearningstrategy Considertheassumptionthatyisoneofthecausalfactorsofx,andlet
hrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas
structuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x
p,pp (hx) = ( )xh|()h (15.1)
Asaconsequence,thedatahasmarginalprobability
p() = x E hp ( )xh| (15.2)
Fromthisstraightforwardobservation,weconcludethatthebestpossiblemodel
ofx(fromageneralization pointofview)istheonethatuncoverstheabove“true”
5 4 2
CHAPTER15.REPRESENTATIONLEARNING
structure,withhasalatentvariablethatexplainstheobservedvariationsinx The“ideal”representationlearningdiscussedaboveshouldthusrecovertheselatent
factors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe
veryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe
conditionaldistributionofygivenxistiedbyBayes’ruletothecomponentsin
theaboveequation:
p( ) = yx|pp ( )xy|()y
p()x (15.3)
Thusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge
ofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in
situationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove
performance Animportantresearchproblemregardsthefactthatmostobservationsare
formedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but
theunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor
anunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all
salientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking
iteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y
Inpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible
tocaptureallormostofthefactorsofvariationthatinﬂuenceanobservation Forexample,inavisualscene,shouldtherepresentationalwaysencodeallof
thesmallestobjectsinthebackground Itisawell-documented psychological
phenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat
arenotimmediately relevanttothetasktheyareperforming—see,e.g.,Simons
andLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis
determining toencodeineachsituation.Currently,twoofthemainstrategies what
fordealingwithalargenumberofunderlyingcausesaretouseasupervised
learningsignalatthesametimeastheunsupervisedlearningsignalsothatthe
modelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch
largerrepresentationsifusingpurelyunsupervisedlearning Anemergingstrategyforunsupervisedlearningistomodifythedeﬁnitionof
whichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative
modelshavebeentrainedtooptimizeaﬁxedcriterion,oftensimilartomean
squarederror.Theseﬁxedcriteriadeterminewhichcausesareconsideredsalient Forexample,meansquarederrorappliedtothepixelsofanimageimplicitly
speciﬁesthatanunderlyingcauseisonlysalientifitsigniﬁcantlychangesthe
brightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish
tosolveinvolvesinteractingwithsmallobjects.Seeﬁgureforanexample 15.5
5 4 3
CHAPTER15.REPRESENTATIONLEARNING
Input Reconstruction
Figure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas
failedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits
spatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand
arerelevanttotheroboticstask Unfortunately,theautoencoderhaslimitedcapacity,
andthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing
salientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall
pingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger
objects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror Otherdeﬁnitionsofsaliencearepossible.Forexample,ifagroupofpixels
followahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme
brightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient Onewaytoimplementsuchadeﬁnitionofsalienceistousearecentlydeveloped
approachcalled g e ner at i v e adv e r sar i al net w o r k s( ,) Goodfellow etal.2014c
Inthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiﬁer Thefeedforwardclassiﬁerattemptstorecognizeallsamplesfromthegenerative
modelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis
framework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis
highlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail
insection.Forthepurposesofthepresentdiscussion,itissuﬃcientto 20.10.4
understandthattheylearnhowtodeterminewhatissalient () Lotteretal.2015
showedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect
togeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully
generatetheearswhentrainedwiththeadversarialframework.Becausethe
earsarenotextremelybrightordarkcomparedtothesurroundingskin,they
arenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly
5 4 4
CHAPTER15.REPRESENTATIONLEARNING
GroundTruth MSE Adversarial
Figure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof
learningwhichfeaturesaresalient Inthisexample,thepredictivegenerativenetwork
hasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciﬁc
viewingangle ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould
emit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r )
squarederroralone.Becausetheearsdonotcauseanextremediﬀerenceinbrightness
comparedtotheneighboringskin,theywerenotsuﬃcientlysalientforthemodeltolearn
torepresentthem ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof
meansquarederrorandadversarialloss Usingthislearnedcostfunction,theearsare
salientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare
importantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures
graciouslyprovidedby () Lotter e t a l .2015
recognizableshapeandconsistentpositionmeansthatafeedforwardnetwork
caneasilylearntodetectthem,makingthemhighlysalientunderthegenerative
adversarialframework.Seeﬁgureforexampleimages.Generativeadversarial 15.6
networksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented Weexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich
factorstorepresent,anddevelopmechanismsforrepresentingdiﬀerentfactors
dependingonthetask Abeneﬁtoflearningtheunderlyingcausalfactors,aspointedoutbySchölkopf
etal.(),isthatifthetruegenerativeprocesshas 2012 xasaneﬀectandyas
acause,thenmodelingp(x y|)isrobusttochangesinp(y) Ifthecause-eﬀect
relationshipwasreversed,thiswouldnotbetrue,sincebyBayes’rule,p(x y|)
wouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin
distributionduetodiﬀerentdomains,temporalnon-stationarity,orchangesin
thenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe
universeareconstant)whilethemarginaldistributionovertheunderlyingcauses
canchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan
5 4 5
CHAPTER15.REPRESENTATIONLEARNING
beexpectedvialearningagenerativemodelthatattemptstorecoverthecausal
factorsand 4 D i s t ri b u t ed R ep res en t at i on
Distributedrepresentationsofconcepts—representationscomposedofmanyele-
mentsthatcanbesetseparatelyfromeachother—areoneofthemostimportant
toolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause
theycanusenfeatureswithkvaluestodescribekndiﬀerentconcepts.Aswe
haveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits
andprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof
distributedrepresentation Wenowintroduceanadditionalobservation Many
deeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits
canlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as
discussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3
becauseeachdirectioninrepresentationspacecancorrespondtothevalueofa
diﬀerentunderlyingconﬁgurationvariable Anexampleofadistributedrepresentationisavectorofnbinaryfeatures,
whichcantake2nconﬁgurations, eachpotentiallycorrespondingtoadiﬀerent
regionininputspace,asillustratedinﬁgure.Thiscanbecomparedwith 15.7
asymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor
category.Iftherearensymbolsinthedictionary,onecanimaginenfeature
detectors,eachcorrespondingtothedetectionofthepresenceoftheassociated
category.Inthatcaseonlyndiﬀerentconﬁgurations oftherepresentationspace
arepossible,carvingndiﬀerentregionsininputspace,asillustratedinﬁgure.15.8
Suchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan
becapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone
ofthemcanbeactive).Asymbolicrepresentationisaspeciﬁcexampleofthe
broaderclassofnon-distributedrepresentations,whicharerepresentationsthat
maycontainmanyentriesbutwithoutsigniﬁcantmeaningfulseparatecontrolover
eachentry Examplesoflearningalgorithms basedonnon-distributedrepresentations
include:
•Clusteringmethods,includingthek-meansalgorithm:eachinputpointis
assignedtoexactlyonecluster

============================================================

=== CHUNK 141 ===
Palavras: 494
Caracteres: 7766
--------------------------------------------------
•k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples
areassociatedwithagiveninput.Inthecaseofk>1,therearemultiple
5 4 6
CHAPTER15.REPRESENTATIONLEARNING
h 1h 2 h 3
h = [ 1 , , 1 1 ]
h = [ 0 , , 1 1 ]h = [ 1 , , 0 1 ]h = [ 1 , , 1 0 ]
h = [ 0 , , 1 0 ]h = [ 0 , , 0 1 ]h = [ 1 , , 0 0 ]
Figure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation
breaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures
h 1,h 2,andh 3 Eachfeatureisdeﬁnedbythresholdingtheoutputofalearned,linear
transformation.Eachfeaturedivides R2intotwohalf-planes.Leth+
ibethesetofinput
pointsforwhichh i=1andh−
ibethesetofinputpointsforwhichh i=0.Inthis
illustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding
arrowpointingtotheh+
isideoftheboundary.Therepresentationasawholetakes
onauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the
representationvalue[1,1,1]correspondstotheregionh+
1∩h+
2∩h+
3.Comparethistothe
non-distributedrepresentationsinﬁgure.Inthegeneralcaseof 15.8 dinputdimensions,
adistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes ThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diﬀerent
regions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly
nregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany
moreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible
(thereisnoh=0inthisexample)andthatalinearclassiﬁerontopofthedistributed
representationisnotabletoassigndiﬀerentclassidentitiestoeveryneighboringregion;
evenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew
isthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998
layerandaweakclassiﬁerlayercanbeastrongregularizer;aclassiﬁertryingtolearn
theconceptof“person”versus“notaperson”doesnotneedtoassignadiﬀerentclassto
aninputrepresentedas“womanwithglasses”thanitassignstoaninputrepresentedas
“manwithoutglasses.”Thiscapacityconstraintencourageseachclassiﬁertofocusonfew
h iandencouragestolearntorepresenttheclassesinalinearlyseparableway h
5 4 7
CHAPTER15.REPRESENTATIONLEARNING
valuesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom
eachother,sothisdoesnotqualifyasatruedistributedrepresentation •Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is
activatedwhenaninputisgiven •Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or
expertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest
neighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but
thosevaluescannotreadilybecontrolledseparatelyfromeachother •KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):
althoughthedegreeofactivationofeach“supportvector”ortemplateexample
isnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures •Languageortranslationmodelsbasedonn-grams.Thesetofcontexts
(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuﬃxes Aleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample Separateparametersareestimatedforeachleafofthetree(withsomesharing
beingpossible) Forsomeofthesenon-distributedalgorithms,theoutputisnotconstantby
partsbutinsteadinterpolatesbetweenneighboringregions.Therelationship
betweenthenumberofparameters(orexamples)andthenumberofregionsthey
candeﬁneremainslinear Animportantrelatedconceptthatdistinguishesadistributedrepresentation
fromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween
diﬀerentconcepts.Aspuresymbols,“cat”and“dog”areasfarfromeachother
asanyothertwosymbols.However,ifoneassociatesthemwithameaningful
distributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats
cangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation
maycontainentriessuchas“has_fur”or“number_of_legs”thathavethesame
valuefortheembeddingofboth“cat”and“dog.”Neurallanguagemodelsthat
operateondistributedrepresentationsofwordsgeneralizemuchbetterthanother
modelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin
section.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich
semanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis
absentfrompurelysymbolicrepresentations Whenandwhycantherebeastatisticaladvantagefromusingadistributed
representationaspartofalearningalgorithm D istributedrepresentationscan
5 4 8
CHAPTER15.REPRESENTATIONLEARNING
Figure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace
intodiﬀerentregions.Thenearestneighboralgorithmprovidesanexampleofalearning
algorithmbasedonanon-distributedrepresentation.Diﬀerentnon-distributedalgorithms
mayhavediﬀerentgeometry, but theytypicallybreaktheinput spaceintoregions,
w i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed
approachisthat,givenenoughparameters,itcanﬁtthetrainingsetwithoutsolvinga
diﬃcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiﬀerentoutput
i n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels
generalizeonlylocallyviathesmoothnessprior,makingitdiﬃculttolearnacomplicated
functionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast
thiswithadistributedrepresentation,ﬁgure.15.7
5 4 9
CHAPTER15.REPRESENTATIONLEARNING
haveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe
compactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-
distributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,
whichstatesthatifuv≈,thenthetargetfunctionftobelearnedhasthe
propertythatf(u)≈f(v),ingeneral.Therearemanywaysofformalizingsuchan
assumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe
knowthatf(x)≈y,thenwechooseanestimator ˆfthatapproximatelysatisﬁes
theseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby
inputx+.Thisassumptionisclearlyveryuseful,butitsuﬀersfromthecurseof
dimensionality: inordertolearnatargetfunctionthatincreasesanddecreases
manytimesinmanydiﬀerentregions,1wemayneedanumberofexamplesthatis
atleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof
theseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor
eachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol
tovalue However,thisdoesnotallowustogeneralizetonewsymbolsfornew
regions Ifwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing
smooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean
objectregardlessofitslocationintheimage,eventhoughspatialtranslationof
theobjectmaynotcorrespondtosmoothtransformationsintheinputspace Letusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,
thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each
binaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces, as
illustratedinﬁgure.Theexponentiallylargenumberofintersectionsof 15.7 n
ofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed
representationlearnercandistinguish.Howmanyregionsaregeneratedbyan
arrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe
intersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,)
thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis
d
j = 0n
j
= (Ond) (15.4)
Therefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin
thenumberofhiddenunits 1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y
re g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i ﬀ e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e
m i g h t wa n t t o d i ﬀ e r i n f 2dd i ﬀ e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s

============================================================

=== CHUNK 142 ===
Palavras: 353
Caracteres: 12640
--------------------------------------------------
5 5 0
CHAPTER15.REPRESENTATIONLEARNING
Thisprovidesageometricargumenttoexplainthegeneralization powerof
distributedrepresentation:withO(nd)parameters(fornlinear-threshold features
in Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade
noassumptionatallaboutthedata,andusedarepresentationwithoneunique
symbolforeachregion,andseparateparametersforeachsymboltorecognizeits
correspondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd)
examples.Moregenerally,theargumentinfavorofthedistributedrepresentation
couldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe
usenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin
thedistributedrepresentation.Theargumentinthiscaseisthatifaparametric
transformationwithkparameterscanlearnaboutrregionsininputspace,with
kr,andifobtainingsucharepresentationwasusefultothetaskofinterest,then
wecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed
settingwherewewouldneedO(r)examplestoobtainthesamefeaturesand
associatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto
representthemodelmeansthatwehavefewerparameterstoﬁt,andthusrequire
farfewertrainingexamplestogeneralizewell Afurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-
tationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto
distinctlyencodesomanydiﬀerentregions.Forexample,theVCdimensionofa
neuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber
ofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery
manyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode
space,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace
htotheoutputyusingalinearclassiﬁer.Theuseofadistributedrepresentation
combinedwithalinearclassiﬁerthusexpressesapriorbeliefthattheclassesto
berecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors
capturedbyh Wewilltypicallywanttolearncategoriessuchasthesetofall
imagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat
requirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition
thedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall
greencarsandredtrucksasanotherclass Theideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally
validated ()ﬁndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015
trainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery
ofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign Inpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething
thathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe
toplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein
5 5 1
CHAPTER15.REPRESENTATIONLEARNING
-+ =
Figure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles
theconceptofgenderfromtheconceptofwearingglasses Ifwebeginwiththerepre-
sentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe
conceptofamanwithoutglasses,andﬁnallyaddthevectorrepresentingtheconcept
ofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman
withglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto
imagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith
permissionfrom () Radford e t a l .2015
commonisthatonecouldimagine learningabouteachofthemwithouthavingto
seealltheconﬁgurationsofalltheothers ()demonstratedthat Radfordetal.2015
agenerativemodelcanlearnarepresentationofimagesoffaces,withseparate
directionsinrepresentationspacecapturingdiﬀerentunderlyingfactorsofvariation Figuredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9
towhetherthepersonismaleorfemale,whileanothercorrespondstowhether
thepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not
ﬁxedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiﬁers:
gradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically
interestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout
thedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof
glasses,withouthavingtocharacterizealloftheconﬁgurations ofthen−1other
featuresbyexamplescoveringallofthesecombinationsofvalues Thisformof
statisticalseparabilityiswhatallowsonetogeneralizetonewconﬁgurations ofa
person’sfeaturesthathaveneverbeenseenduringtraining 5 5 2
CHAPTER15.REPRESENTATIONLEARNING
15 5 E x p on en t i al Gai n s f rom D ep t h
Wehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1
tors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep
networkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto
improvedstatisticaleﬃciency.Inthissection,wedescribehowsimilarresultsapply
moregenerallytootherkindsofmodelswithdistributedhiddenrepresentations Insection,wesawanexampleofagenerativemodelthatlearnedabout 15.4
theexplanatoryfactorsunderlyingimagesoffaces,includingtheperson’sgender
andwhethertheyarewearingglasses.Thegenerativemodelthataccomplished
thistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect
ashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship
betweentheseabstractexplanatoryfactorsandthepixelsintheimage Inthis
andotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom
eachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery
high-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis
demands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas
functionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough
thecompositionofmanynonlinearities Ithasbeenproveninmanydiﬀerentsettingsthatorganizingcomputation
throughthecompositionofmanynonlinearities andahierarchyofreusedfeatures
cangiveanexponentialboosttostatisticaleﬃciency,ontopoftheexponential
boostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,
withsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with
asinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel
familythatisauniversalapproximator canapproximatealargeclassoffunctions
(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough
hiddenunits However,therequirednumberofhiddenunitsmaybeverylarge Theoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat
therearefamiliesoffunctionsthatcanberepresentedeﬃcientlybyanarchitecture
ofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect
totheinputsize)withinsuﬃcientdepth(depth2ordepth).k−1
Insection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1
approximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle
hiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep
beliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux
andBengio20082010MontúfarandAy2011Montúfar2014Krause ,,; ,;,; etal.,
2013) 5 5 3
CHAPTER15.REPRESENTATIONLEARNING
Insection,wesawthatasuﬃcientlydeepfeedforwardnetworkcanhave 6.4.1
anexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso
beobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic
modelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These
modelsusepolynomialcircuitstocomputetheprobabilitydistributionovera
setofrandomvariables ()showedthatthereexist DelalleauandBengio2011
probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid
needinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014
showedthattherearesigniﬁcantdiﬀerencesbetweeneverytwoﬁnitedepthsof
SPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit
theirrepresentationalpower Anotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive
poweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan
exponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed
toonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal 2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe
casewheretheshallowcircuitmustexactlyreplicateparticularfunctions 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es
Toclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone
representationbetterthananother?Oneanswer,ﬁrstintroducedinsection,is15.3
thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof
variationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour
applications.Moststrategiesforrepresentationlearningarebasedonintroducing
cluesthathelpthelearningtoﬁndtheseunderlyingfactorsofvariations.Theclues
canhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised
learningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually
speciﬁesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,
tomakeuseofabundantunlabeleddata,representationlearningmakesuseof
other,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof
implicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein
ordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat
regularizationstrategiesarenecessarytoobtaingoodgeneralization Whileitis
impossibletoﬁndauniversallysuperiorregularizationstrategy,onegoalofdeep
learningistoﬁndasetoffairlygenericregularizationstrategiesthatareapplicable
toawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable
tosolve 5 5 4
CHAPTER15.REPRESENTATIONLEARNING
Weprovideherealistofthesegenericregularizationstrategies.Thelistis
clearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning
algorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying
factors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d
beenpartiallyexpandedhere •Smoothness:Thisistheassumptionthatf(x+d)≈f(x)forunitdand
small.Thisassumptionallowsthelearnertogeneralizefromtraining
examplestonearbypointsininputspace.Manymachinelearningalgorithms
leveragethisidea,butitisinsuﬃcienttoovercomethecurseofdimensionality •Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome
variablesarelinear.Thisallowsthealgorithmtomakepredictionseven
veryfarfromtheobserveddata,butcansometimesleadtooverlyextreme
predictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe
smoothnessassumptioninsteadmakethelinearityassumption.Theseare
infactdiﬀerentassumptions—linearfunctionswithlargeweightsapplied
tohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal ()forafurtherdiscussionofthelimitationsofthelinearityassumption 2014b
•Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare
motivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying
explanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate
ofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3
supervisedlearningviarepresentationlearning.Learningthestructureofp(x)
requireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|
x)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4
describeshowthisviewmotivatestheuseofdistributedrepresentations,with
separatedirectionsinrepresentationspacecorrespondingtoseparatefactors
ofvariation •Causalfactors:themodelisconstructedinsuchawaythatittreatsthe
factorsofvariationdescribedbythelearnedrepresentationhasthecauses
oftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3
isadvantageousforsemi-supervisedlearningandmakesthelearnedmodel
morerobustwhenthedistributionovertheunderlyingcauseschangesor
whenweusethemodelforanewtask •Depthahierarchical organization ofexplanatory factors , or  :High-level,
abstractconceptscanbedeﬁnedintermsofsimpleconcepts,forminga
hierarchy.From another point of view, the us e ofa deeparchitecture
5 5 5
CHAPTER15.REPRESENTATIONLEARNING
expressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step
program, with eachstep referringbacktothe outputoftheprocessing
accomplishedviaprevioussteps •Sharedfactors across tasks:In thecontextwherewehavemanytasks,
correspondingtodiﬀerentyivariablessharingthesameinput xorwhere
eachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput
x,theassumptionisthateachyiisassociatedwithadiﬀerentsubsetfroma
commonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning
alltheP(yi|x)viaasharedintermediate representationP(h x|)allows
sharingofstatisticalstrengthbetweenthetasks •Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-
centratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous
case,theseregionscanbeapproximatedbylow-dimensional manifoldswith
amuchsmallerdimensionalitythantheoriginalspacewherethedatalives Manymachinelearningalgorithmsbehavesensiblyonlyonthismanifold
( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b
autoencoders,attempttoexplicitlylearnthestructureofthemanifold

============================================================

=== CHUNK 143 ===
Palavras: 365
Caracteres: 8419
--------------------------------------------------
•Naturalclustering:Manymachinelearningalgorithmsassumethateach
connectedmanifoldintheinputspacemaybeassignedtoasingleclass.The
datamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant
withineachoneofthese Thisassumptionmotivatesavarietyoflearning
algorithms,includingtangentpropagation, doublebackprop,themanifold
tangentclassiﬁerandadversarialtraining •Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms
maketheassumptionthatthemostimportantexplanatoryfactorschange
slowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying
explanatoryfactorsthantopredictrawobservationssuchaspixelvalues Seesectionforfurtherdescriptionofthisapproach 13.3
•Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost
inputs—thereisnoneedtouseafeaturethatdetectselephanttrunkswhen
representinganimageofacat.Itisthereforereasonabletoimposeaprior
thatanyfeaturethatcanbeinterpretedas“present”or“absent”shouldbe
absentmostofthetime •SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the
factorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest
5 5 6
CHAPTER15.REPRESENTATIONLEARNING
possibleismarginalindependence,P(h) =
iP(h i),butlineardependencies
orthosecapturedbyashallowautoencoderarealsoreasonableassumptions Thiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga
linearpredictororafactorizedpriorontopofalearnedrepresentation Theconceptofrepresentationlearningtiestogetherallofthemanyforms
ofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep
probabilisticmodelsalllearnandexploitrepresentations.Learning thebest
possiblerepresentationremainsanexcitingavenueofresearch 5 5 7
C h a p t e r 1 6
S t ru ct u r e d Probabilis t i c Mo d e l s
f or D e e p L e ar n i n g
Deeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto
guidetheirdesigneﬀortsanddescribetheiralgorithms.Oneoftheseformalisms
istheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed
structuredprobabilisticmodelsbrieﬂyinsection.Thatbriefpresentationwas 3.14
suﬃcienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto
describesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III
modelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep
learning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes
structuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended
tobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction
beforecontinuingwiththischapter Astructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,
usingagraphtodescribewhichrandomvariablesintheprobabilitydistribution
interactwitheachotherdirectly.Hereweuse“graph”inthegraphtheorysense—a
setofverticesconnectedtooneanotherbyasetofedges.Becausethestructure
ofthemodelisdeﬁnedbyagraph,thesemodelsareoftenalsoreferredtoas
graphicalmodels Thegraphicalmodelsresearchcommunityislargeandhasdevelopedmany
diﬀerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we
providebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,
withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning
researchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,
youmaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert
558
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
maybeneﬁtfromreadingtheﬁnalsectionofthischapter,section,inwhichwe 16.7
highlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning
algorithms.Deeplearningpractitioners tendtouseverydiﬀerentmodelstructures,
learningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest
ofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese
diﬀerencesinpreferencesandexplainthereasonsforthem Inthischapterweﬁrstdescribethechallengesofbuildinglarge-scaleproba-
bilisticmodels Next,wedescribehowtouseagraphtodescribethestructure
ofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany
challenges,itisnotwithoutitsowncomplications Oneofthemajordiﬃcultiesin
graphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract
directly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem We
outlinetwoapproachestoresolvingthisdiﬃcultybylearningaboutthedependen-
ciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5
deeplearningpractitioners placeonspeciﬁcapproachestographicalmodelingin
section.16.7
16.1TheChallengeofUnstructuredModeling
Thegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges
neededtosolveartiﬁcialintelligence.Thismeansbeingabletounderstandhigh-
dimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto
beabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and
documentscontainingmultiplewordsandpunctuationcharacters Classiﬁcationalgorithmscantakeaninputfromsucharichhigh-dimensional
distributionandsummarizeitwithacategoricallabel—whatobjectisinaphoto,
whatwordisspokeninarecording,whattopicadocumentisabout.Theprocess
ofclassiﬁcationdiscardsmostoftheinformationintheinputandproducesa
singleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The
classiﬁerisalsooftenabletoignoremanypartsoftheinput.Forexample,when
recognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof
thephoto Itispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare
oftenmoreexpensivethanclassiﬁcation.Someofthemrequireproducingmultiple
outputvalues.Mostrequireacompleteunderstandingoftheentirestructureof
1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry
e n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c 5 5 9
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
theinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:
•Densityestimation:givenaninput x,themachinelearningsystemreturns
anestimateofthetruedensity p( x)underthedatageneratingdistribution Thisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-
ingoftheentireinput.Ifevenoneelementofthevectorisunusual,the
systemmustassignitalowprobability •Denoising:givenadamagedorincorrectlyobservedinput ˜ x,themachine
learningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample,
themachinelearningsystemmightbeaskedtoremovedustorscratches
fromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe
estimatedcleanexample x)andanunderstandingoftheentireinput(since
evenonedamagedareawillstillrevealtheﬁnalestimateasbeingdamaged) •Missingvalueimputation:giventheobservationsofsomeelementsof x,
themodelisaskedtoreturnestimatesoforaprobabilitydistributionover
someoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs Becausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it
mustunderstandtheentireinput •Sampling:themodelgeneratesnewsamplesfromthedistribution p( x) Applicationsincludespeechsynthesis,i.e.producingnewwaveformsthat
soundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda
goodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn
fromthewrongdistribution,thenthesamplingprocessiswrong Foranexampleofasamplingtaskusingsmallnaturalimages,seeﬁgure.16.1
Modelingarichdistributionoverthousandsormillionsofrandomvariablesisa
challengingtask,bothcomputationally andstatistically.Supposeweonlywanted
tomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit
seemsoverwhelming.Forasmall, 32×32 2 pixelcolor(RGB)image,thereare3 0 7 2
possiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan
theestimatednumberofatomsintheuniverse Ingeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining
ndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof
representing P(x)bystoringalookuptablewithoneprobabilityvalueperpossible
outcomerequires knparameters Thisisnotfeasibleforseveralreasons:
5 6 0
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
Figure16.1:Probabilisticmodelingofnaturalimages ( T o p )Example32×32pixelcolor
imagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom )
drawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears
atthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean
space.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,
ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen
adjustedfordisplay.Figurereproducedwithpermissionfrom ()

============================================================

=== CHUNK 144 ===
Palavras: 407
Caracteres: 4652
--------------------------------------------------
Courville e t a l .2011
5 6 1
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
• M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues
of nand k,representingthedistributionasatablewillrequiretoomany
valuestostore • St a t i s t i c a l e ﬃ c i e nc y:Asthenumberofparametersinamodelincreases,
sodoestheamountoftrainingdataneededtochoosethevaluesofthose
parametersusingastatisticalestimator.Becausethetable-basedmodel
hasanastronomicalnumberofparameters,itwillrequireanastronomically
largetrainingsettoﬁtaccurately.Anysuchmodelwilloverﬁtthetraining
setverybadlyunlessadditionalassumptionsaremadelinkingthediﬀerent
entriesinthetable(forexample,likeinback-oﬀorsmoothed n-grammodels,
section).12.4.1
• R u nt i m e :   t h e c o s t o f i nfe r e nc e: Supposewewanttoperformaninference
taskwhereweuseourmodelofthejointdistribution P(x)tocomputesome
otherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional
distribution P(x 2|x 1).Computingthesedistributionswillrequiresumming
acrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe
intractablememorycostofstoringthemodel • R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample
fromthemodel.Thenaivewaytodothisistosamplesomevalueu∼ U(0 ,1),
theniteratethroughthetable,addinguptheprobabilityvaluesuntilthey
exceed uandreturntheoutcomecorrespondingtothatpositioninthetable Thisrequiresreadingthroughthewholetableintheworstcase,soithas
thesameexponentialcostastheotheroperations Theproblemwiththetable-basedapproachisthatweareexplicitlymodeling
everypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The
probabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis Usually,mostvariablesinﬂuenceeachotheronlyindirectly Forexample,considermodelingtheﬁnishingtimesofateaminarelayrace Supposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof
therace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting
herlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown
lapandhandsthebatontoCarol,whorunstheﬁnallap.Wecanmodeleachof
theirﬁnishingtimesasacontinuousrandomvariable.Alice’sﬁnishingtimedoes
notdependonanyoneelse’s,sinceshegoesﬁrst.Bob’sﬁnishingtimedepends
onAlice’s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice
hascompletedhers IfAliceﬁnishesfaster,Bobwillﬁnishfaster,allelsebeing
5 6 2
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
equal.Finally,Carol’sﬁnishingtimedependsonbothherteammates.IfAliceis
slow,Bobwillprobablyﬁnishlatetoo.Asaconsequence,Carolwillhavequitea
latestartingtimeandthusislikelytohavealateﬁnishingtimeaswell.However,
Carol’sﬁnishingtimedependsonly i ndir e c t l yonAlice’sﬁnishingtimeviaBob’s IfwealreadyknowBob’sﬁnishingtime,wewillnotbeabletoestimateCarol’s
ﬁnishingtimebetterbyﬁndingoutwhatAlice’sﬁnishingtimewas.Thismeans
wecanmodeltherelayraceusingonlytwointeractions: Alice’seﬀectonBoband
Bob’seﬀectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice
andCarolfromourmodel Structuredprobabilisticmodelsprovideaformalframeworkformodelingonly
directinteractionsbetweenrandomvariables.Thisallowsthemodelstohave
signiﬁcantlyfewerparametersandthereforebeestimatedreliablyfromlessdata Thesesmallermodelsalsohavedramatically reducedcomputational costinterms
ofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom
themodel 16.2UsingGraphstoDescribeModelStructure
Structuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof“nodes”or
“vertices”connectedbyedges)torepresentinteractionsbetweenrandomvariables Eachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction Thesedirectinteractionsimplyother,indirectinteractions,butonlythedirect
interactionsneedtobeexplicitlymodeled Thereismore thanone wayto describe theinteractionsin aprobability
distributionusingagraph.Inthefollowingsectionswedescribesomeofthemost
popularandusefulapproaches.Graphicalmodelscanbelargelydividedinto
twocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon
undirectedgraphs 1 D i rect ed Mo d el s
Onekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,
otherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,) Directedgraphicalmodelsarecalled“directed”becausetheiredgesaredirected,
2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm “ B a y e s i a n n e t wo rk ” wh e n o n e wis h e s t o “ e m p h a s i z e
t h e j u d g m e n t a l ” n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i t o h i g h l i g h t t h a t t h e y u s u a l l y
re p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s

============================================================

=== CHUNK 145 ===
Palavras: 398
Caracteres: 8627
--------------------------------------------------
5 6 3
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
t 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol
Figure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice’sﬁnishing
timet 0inﬂuencesBob’sﬁnishingtimet 1,becauseBobdoesnotgettostartrunninguntil
Aliceﬁnishes.Likewise,CarolonlygetstostartrunningafterBobﬁnishes,soBob’s
ﬁnishingtimet 1directlyinﬂuencesCarol’sﬁnishingtimet 2 thatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin
thedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable’s
probabilitydistributionisdeﬁnedintermsoftheother’s.Drawinganarrowfrom
atobmeansthatwedeﬁnetheprobabilitydistributionoverbviaaconditional
distribution,withaasoneofthevariablesontherightsideoftheconditioning
bar.Inotherwords,thedistributionoverbdependsonthevalueofa Continuingwiththerelayraceexamplefromsection,supposewename 16.1
Alice’sﬁnishingtimet 0,Bob’sﬁnishingtimet 1,andCarol’sﬁnishingtimet 2 Aswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends
directlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected
graphicalmodel,illustratedinﬁgure.16.2
Formally,adirectedgraphicalmodeldeﬁnedonvariables xisdeﬁnedbya
directedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel,
andasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where
P aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven
by
p() = Πx i p(x i| P aG(x i)) (16.1)
Inourrelayraceexample,thismeansthat,usingthegraphdrawninﬁgure,16.2
p(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) (16.2)
Thisisourﬁrsttimeseeingastructuredprobabilisticmodelinaction.We
canexaminethecostofusingit,inordertoobservehowstructuredmodelinghas
manyadvantagesrelativetounstructuredmodeling Supposewerepresentedtimebydiscretizingtimerangingfromminute0to
minute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete
variablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha
table,itwouldneedtostore999,999values(100valuesoft 0×100valuesoft 1×
100valuesoft 2,minus1,sincetheprobabilityofoneoftheconﬁgurations ismade
5 6 4
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
redundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we
onlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe
distributionovert 0requires99values,thetabledeﬁningt 1givent 0requires9900
values,andsodoesthetabledeﬁningt 2givent 1.Thiscomestoatotalof19,899
values.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof
parametersbyafactorofmorethan50 Ingeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe
singletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose
webuildadirectedgraphicalmodeloverthesevariables If misthemaximum
numberofvariablesappearing(oneithersideoftheconditioningbar)inasingle
conditionalprobabilitydistribution,thenthecostofthetablesforthedirected
modelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we
getverydramaticsavings Inotherwords,solongaseachvariablehasfewparentsinthegraph,the
distributioncanberepresentedwithveryfewparameters Somerestrictionson
thegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat
operationslikecomputingmarginalorconditionaldistributionsoversubsetsof
variablesareeﬃcient Itisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin
thegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables
areconditionallyindependentfromeachother.Itisalsopossibletomakeother
kindsofsimplifyingassumptions Forexample,supposeweassumeBobalways
runsthesameregardlessofhowAliceperformed.(Inreality,Alice’sperformance
probablyinﬂuencesBob’sperformance—dependingonBob’spersonality,ifAlice
runsespeciallyfastinagivenrace,thismightencourageBobtopushhardand
matchherexceptionalperformance,oritmightmakehimoverconﬁdentandlazy) ThentheonlyeﬀectAlicehasonBob’sﬁnishingtimeisthatwemustaddAlice’s
ﬁnishingtimetothetotalamountoftimewethinkBobneedstorun.This
observationallowsustodeﬁneamodelwith O( k)parametersinsteadof O( k2) However,notethatt 0andt 1arestilldirectlydependentwiththisassumption,
becauset 1representstheabsolutetimeatwhichBobﬁnishes,notthetotaltime
hehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom
t 0tot 1.TheassumptionthatBob’spersonalrunningtimeisindependentfrom
allotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we
encodethisinformationinthedeﬁnitionoftheconditionaldistributionitself.The
conditionaldistributionisnolongera k k×−1elementtableindexedbyt 0andt 1
butisnowaslightlymorecomplicatedformulausingonly k−1parameters.The
directedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeﬁne
5 6 5
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
ourconditionaldistributions.Itonlydeﬁneswhichvariablestheyareallowedto
takeinasarguments 2 Un d i rec t ed Mo d el s
Directedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-
ticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise
knownasMarkovrandomﬁelds(MRFs)orMarkovnetworks(Kinder-
mann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges
areundirected Directedmodelsaremostnaturallyapplicabletosituationswherethereis
aclearreasontodraweacharrowinoneparticulardirection.Oftentheseare
situationswhereweunderstandthecausalityandthecausalityonlyﬂowsinone
direction.Onesuchsituationistherelayraceexample.Earlierrunnersaﬀectthe
ﬁnishingtimesoflaterrunners;laterrunnersdonotaﬀecttheﬁnishingtimesof
earlierrunners Notallsituationswemightwanttomodelhavesuchacleardirectiontotheir
interactions.Whentheinteractionsseemtohavenointrinsicdirection,orto
operateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel Asanexampleofsuchasituation,supposewewanttomodeladistribution
overthreebinaryvariables:whetherornotyouaresick,whetherornotyour
coworkerissick,andwhetherornotyourroommateissick.Asintherelayrace
example,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat
takeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach
other,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa
colddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel
it.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and
thatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof
acoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe
coldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour
roommate Inthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas
itisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional
narrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel Aswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan
edge,thentherandomvariablescorrespondingtothosenodesinteractwitheach
otherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno
arrow,andisnotassociatedwithaconditionalprobabilitydistribution 5 6 6
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
h r h r h y h y h c h c
Figure16.3:Anundirectedgraphrepresentinghowyourroommate’shealthh r,your
healthh y,andyourworkcolleague’s healthh caﬀecteachother.Youandyourroommate
mightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,
butassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan
onlyinfecteachotherindirectlyviayou Wedenotetherandomvariablerepresentingyourhealthash y,therandom
variablerepresentingyourroommate’shealthash r,andtherandomvariable
representingyourcolleague’shealthash c.Seeﬁgureforadrawingofthe 16.3
graphrepresentingthisscenario Formally,anundirectedgraphicalmodelisastructuredprobabilisticmodel
deﬁnedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor φ(C)
(alsocalledacliquepotential) measurestheaﬃnityofthevariablesinthatclique
forbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe
non-negative.Togethertheydeﬁneanunnormalizedprobabilitydistribution
˜ p() = Πx C∈G φ .()C (16.3)
Theunnormalized probabilitydistributioniseﬃcienttoworkwithsolongas
allthecliquesaresmall.Itencodestheideathatstateswithhigheraﬃnityare
morelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe
deﬁnitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem
togetherwillyieldavalidprobabilitydistribution.Seeﬁgureforanexample 16.4
ofreadingfactorizationinformationfromanundirectedgraph Ourexampleofthecoldspreadingbetweenyou,yourroommate,andyour
colleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis
cliquecanbedeﬁnedbyatable,andmighthavevaluesresemblingthese:
h y= 0h y= 1
h c= 021
h c= 1110
3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f
t h e g ra p h

============================================================

=== CHUNK 146 ===
Palavras: 350
Caracteres: 4449
--------------------------------------------------
5 6 7
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
Astateof1indicatesgoodhealth,whileastateof0indicatespoorhealth
(havingbeen infectedwith acold).Both ofyou areusuallyhealthy, sothe
correspondingstatehasthehighestaﬃnity.Thestatewhereonlyoneofyouis
sickhasthelowestaﬃnity,becausethisisararestate.Thestatewherebothof
youaresick(becauseoneofyouhasinfectedtheother)isahigheraﬃnitystate,
thoughstillnotascommonasthestatewherebotharehealthy Tocompletethemodel,wewouldneedtoalsodeﬁneasimilarfactorforthe
cliquecontainingh yandh r 3 T h e P a rt i t i o n F u n ct i o n
Whiletheunnormalized probabilitydistributionisguaranteedtobenon-negative
everywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid
probabilitydistribution,wemustusethecorrespondingnormalizedprobability
distribution:4
p() =x1
Z˜ p()x (16.4)
where Zisthevalue thatresultsintheprobability distributionsummingor
integratingto1:
Z=
˜ p d ()xx (16.5)
Youcanthinkof Zasaconstantwhenthe φfunctionsareheldconstant.Note
thatifthe φfunctionshaveparameters,then Zisafunctionofthoseparameters Itiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace Thenormalizingconstant Zisknownasthepartitionfunction,atermborrowed
fromstatisticalphysics Since Zisanintegralorsumoverallpossiblejointassignmentsofthestatex
itisoftenintractabletocompute Inordertobeabletoobtainthenormalized
probabilitydistributionofanundirectedmodel, themodelstructureandthe
deﬁnitionsofthe φfunctionsmustbeconducivetocomputing Zeﬃciently.In
thecontextofdeeplearning, Zisusuallyintractable Due totheintractability
ofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate
algorithmsarethetopicofchapter.18
Oneimportantconsiderationtokeepinmindwhendesigningundirectedmodels
isthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral
4A d i s t rib u t i o n d e ﬁ n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s
d is t rib u t i on 5 6 8
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
of˜ povertheirdomaindiverges.Forexample,supposewewanttomodelasingle
scalarvariablexwithasinglecliquepotential ∈ R φ x x () = 2.Inthiscase,
Z=
x2d x (16.6)
Sincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto
thischoiceof φ( x) Sometimes thechoiceofsomeparameterofthe φfunctions
determineswhetherthe probabilit ydistribution isdeﬁned.For example, for
φ( x; β) =exp− β x2
,the βparameterdetermineswhether Zexists.Positive β
resultsinaGaussiandistributionoverxbutallothervaluesof βmake φimpossible
tonormalize Onekeydiﬀerencebetweendirectedmodelingandundirectedmodelingisthat
directedmodelsaredeﬁneddirectlyintermsofprobabilitydistributionsfrom
thestart,whileundirectedmodelsaredeﬁnedmorelooselyby φfunctionsthat
arethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone
mustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind
whileworkingwithundirectedmodelsisthatthedomainofeachofthevariables
hasdramaticeﬀectonthekindofprobabilitydistributionthatagivensetof φ
functionscorrespondsto.Forexample,consideran n-dimensionalvector-valued
randomvariable xandanundirectedmodelparametrized byavectorofbiases
b.Supposewehaveonecliqueforeachelementofx, φ( ) i(x i) =exp( b ix i).What
kindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo
nothaveenoughinformation,becausewehavenotyetspeciﬁedthedomainofx Ifx ∈ Rn,thentheintegraldeﬁning Zdivergesandnoprobabilitydistribution
exists.Ifx∈{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with
p(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors
({[1 ,0 , ,1]})then p(x)=softmax ( b),soalarge
valueof b iactuallyreduces p(x j=1)for j= i Often,itispossibletoleverage
theeﬀectofacarefullychosendomainofavariableinordertoobtaincomplicated
behaviorfromarelativelysimplesetof φfunctions.Wewillexploreapractical
applicationofthisidealater,insection.20.6
1 6 4 E n erg y-B a s ed Mo d el s
Manyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-
sumptionthat∀x ,˜ p(x) >0.Aconvenientwaytoenforcethisconditionistouse
an (EBM)where energy-basedmodel
˜ p E () = exp( x −())x (16.7)
5 6 9
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a b c
d e f
Figure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas
1
Zφ a b ,(ab ,) φ b c ,(bc ,) φ a d ,(ad ,) φ b e ,(be ,) φ e f ,(ef ,)foranappropriatechoiceofthe φfunc-
tions

============================================================

=== CHUNK 147 ===
Palavras: 366
Caracteres: 6245
--------------------------------------------------
and E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall
z,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero
foranystatex.Beingcompletely free to choose theenergyfunction makes
learningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse
constrainedoptimization toarbitrarilyimposesomespeciﬁcminimalprobability
value.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5
Theprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero
butneverreachit Anydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-
mann distribution.For this reason, manyenergy-based models are called
Boltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l .,
1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall
amodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The
termBoltzmannmachinewasﬁrstintroducedtodescribeamodelwithexclusively
binaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted
Boltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann
machineswereoriginallydeﬁnedtoencompassbothmodelswithandwithoutla-
tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate
modelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables
aremoreoftencalledMarkovrandomﬁeldsorlog-linearmodels Cliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized
probabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdiﬀerent
cliquesintheundirectedgraphcorrespondtothediﬀerenttermsoftheenergy
function.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov
network:theexponentiationmakeseachtermintheenergyfunctioncorrespond
toafactorforadiﬀerentclique.Seeﬁgureforanexampleofhowtoreadthe 16.5
5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s Z
5 7 0
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a b c
d e f
Figure 16.5:Thisgraph impliesthat E(abcdef , , , , ,)can be writtenas E a b ,(ab ,)+
E b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique
energyfunctions.Notethatwecanobtainthe φfunctionsinﬁgurebysettingeach 16.4 φ
totheexponentialofthecorrespondingnegativeenergy,e.g., φ a b ,(ab ,) =exp(()) − Eab , formoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan
energy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct
ofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto
anotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan
bethoughtofasan“expert”thatdetermineswhetheraparticularsoftconstraint
issatisﬁed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly
alow-dimensionalprojectionoftherandomvariables,butwhencombinedby
multiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh-
dimensionalconstraint Onepartofthedeﬁnitionofanenergy-basedmodelservesnofunctionalpurpose
fromamachinelearningpointofview:the−signinequation.This16.7 −sign
couldbeincorporatedintothedeﬁnitionof E.Formanychoicesofthefunction
E,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The
−signispresentprimarilytopreservecompatibilitybetweenthemachinelearning
literatureandthephysicsliterature.Manyadvancesinprobabilisticmodeling
wereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual,
physicalenergyanddoesnothavearbitrarysign Terminologysuchas“energy”
and“partitionfunction”remainsassociatedwiththesetechniques,eventhough
theirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey
weredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986
referredtonegativeenergyasharmony)havechosentoemitthenegation,but
thisisnotthestandardconvention Manyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute
p m o de l( x)butonly log ˜ p m o de l( x).Forenergy-basedmodelswithlatentvariables h,
thesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,
5 7 1
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a s b a s b
(a) (b)
Figure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis
active,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres
isshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis
throughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens calledthe :freeenergy
F − () = x log
hexp(( )) − E x h , (16.8)
Inthisbook,weusuallypreferthemoregeneral log ˜ p m o de l() xformulation 5 S ep a ra t i o n a n d D - S ep a r a t i o n
Theedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften
needtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions
canbeenabledordisabledbyobservingothervariables.Moreformally,wewould
liketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach
other,giventhevaluesofothersubsetsofvariables Identifyingtheconditionalindependencesinagraphisverysimpleinthecase
ofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph
iscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother
setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat
Aisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath
involvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno
pathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare
separated.Werefertopathsinvolvingonlyunobservedvariablesas“active”and
pathsincludinganobservedvariableas“inactive.”
Whenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin Seeﬁgureforadepictionofhowactiveandinactivepathsinanundirected 16.6
modellookwhendrawninthisway.Seeﬁgureforanexampleofreading 16.7
separationfromanundirectedgraph Similar concepts apply todirected models ,except that inthe context of
directedmodels,theseconceptsarereferredtoasd-separation.The“d”stands
for“dependence.” D-separati onfordirectedgraphsisdeﬁnedthesameasseparation
5 7 2
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a
b c
d
Figure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here
bisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom
atoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb
alsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem

============================================================

=== CHUNK 148 ===
Palavras: 375
Caracteres: 8765
--------------------------------------------------
Therefore,aanddarenotseparatedgivenb forundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother
setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies
thatisindependentfromgiven A B S
Aswithundirectedmodels,wecanexaminetheindependencesimpliedbythe
graphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables
aredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch
pathexists.Indirectednets,determiningwhetherapathisactiveissomewhat
morecomplicated Seeﬁgureforaguidetoidentifyingactivepathsina 16.8
directedmodel.Seeﬁgureforanexampleofreadingsomepropertiesfroma 16.9
graph Itisimportanttorememberthatseparationandd-separationtellusonly
aboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h e g r a p h .Thereisno
requirementthatthegraphimplyallindependencesthatarepresent.Inparticular,
itisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)
torepresentanydistribution.Infact,somedistributionscontainindependences
thatarenotpossibletorepresentwithexistinggraphicalnotation.Context-
speciﬁcindependencesareindependencesthatarepresentdependentonthe
valueofsomevariablesinthenetwork Forexample,consideramodelofthree
binaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,
butwhenais1,bisdeterministicallyequaltoc Encodingthebehaviorwhen
a= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb
andcareindependentwhena.= 0
Ingeneral,agraphwillneverimplythatanindependenceexistswhenitdoes
not.However,agraphmayfailtoencodeanindependence 5 7 3
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a s b
a s b
a
sb a s ba s b
c( a ) ( b )
( c ) ( d )
Figure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom
variablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa Thiskindofpathbecomesblockedifsisobserved Wehavealreadyseenthiskindof
pathintherelayraceexample ( b )aandbareconnectedbya c o m m o n c a u s es.For
example,supposesisavariableindicatingwhetherornotthereisahurricaneandaand
bmeasurethewindspeedattwodiﬀerentnearbyweathermonitoringoutposts.Ifwe
observeveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This
kindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we
expecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected
windata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing
thereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the
pathisactive ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe
collidercase TheV-structurecausesaandbtoberelatedbytheexplainingaway
eﬀect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose
sisavariableindicatingthatyourcolleagueisnotatwork Thevariablearepresents
herbeingsick,whilebrepresentsherbeingonvacation Ifyouobservethatsheisnot
atwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially
likelythatbothhavehappenedatthesametime.Ifyouﬁndoutthatsheisonvacation,
thisfactissuﬃcienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n
sick.Theexplainingawayeﬀecthappensevenifanydescendantof ( d ) sisobserved!For
example,supposethatcisavariablerepresentingwhetheryouhavereceivedareport
fromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases
yourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit
morelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha
V-structureistoobservenoneofthedescendantsofthesharedchild 5 7 4
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a b
c
d e
Figure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples
include:
•aandbared-separatedgiventheemptyset •aandeared-separatedgivenc •dandeared-separatedgivenc Wecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome
variables:
•aandbarenotd-separatedgivenc •aandbarenotd-separatedgivend 5 7 5
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
1 6 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s
Weoftenrefertoaspeciﬁcmachinelearningmodelasbeingundirectedordirected Forexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected Thischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel
isinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d
usingadirectedgraph,ormosteasilydescribedusinganundirectedgraph Directedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-
vantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,
weshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially
dependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto
useeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan
capturethemostindependencesintheprobabilitydistributionorwhichapproach
usesthefewestedgestodescribethedistribution.Thereareotherfactorsthat
canaﬀectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle
probabilitydistribution,wemaysometimesswitchbetweendiﬀerentmodeling
languages.Sometimesadiﬀerentlanguagebecomesmoreappropriateifweobserve
acertainsubsetofvariables,orifwewishtoperformadiﬀerentcomputational
task.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward
approachtoeﬃcientlydrawsamplesfromthemodel(describedinsection)16.3
whiletheundirectedmodelformulationisoftenusefulforderivingapproximate
inferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19
modelsishighlightedinequation).19.56
Everyprobabilitydistributioncanberepresentedbyeitheradirectedmodel
orbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany
distributionbyusinga“completegraph.”Inthecaseofadirectedmodel,the
completegraphisanydirectedacyclicgraphwhereweimposesomeorderingon
therandomvariables,andeachvariablehasallothervariablesthatprecedeitin
theorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete
graphissimplyagraphcontainingasinglecliqueencompassingallofthevariables 16.10
Ofcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome
variablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit
doesnotimplyanyindependences Whenwerepresentaprobabilitydistributionwithagraph,wewanttochoose
agraphthatimpliesasmanyindependencesaspossible,withoutimplyingany
independencesthatdonotactuallyexist Fromthispointofview,somedistributionscanberepresentedmoreeﬃciently
5 7 6
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
Figure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution Hereweshowexampleswithfourrandomvariables ( L e f t )Thecompleteundirectedgraph Intheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph ( R i g h t )
Inthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe
variablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe
ordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom
variables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom usingdirectedmodels,whileotherdistributionscanberepresentedmoreeﬃciently
using undirectedmodels.In other words,directed models canencode some
independencesthatundirectedmodelscannotencode,andviceversa Directedmodelsareabletouseonespeciﬁckindofsubstructurethatundirected
modelscannotrepresentperfectly.Thissubstructureiscalledanimmorality Thestructureoccurswhentworandomvariablesaandbarebothparentsofa
thirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither
direction.(Thename“immorality”mayseemstrange;itwascoinedinthegraphical
modelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel
withgraph Dintoanundirectedmodel,weneedtocreateanewgraph U For
everypairofvariablesxandy,weaddanundirectededgeconnectingxandyto
Uifthereisadirectededge(ineitherdirection)connectingxandyinDorifx
andyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa
moralizedgraph.Seeﬁgureforexamplesofconvertingdirectedmodelsto 16.11
undirectedmodelsviamoralization Likewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel
canrepresentperfectly.Speciﬁcally,adirectedgraphcannotcaptureallofthe D
conditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop
oflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis
asequenceofvariablesconnectedbyundirectededges,withthelastvariablein
thesequenceconnectedbacktotheﬁrstvariableinthesequence Achordisa
connectionbetweenanytwonon-consecutivevariablesinthesequencedeﬁninga
loop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese
loops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding
5 7 7
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
h 1 h 1 h 2 h 2 h 3 h 3
v 1 v 1 v 2 v 2 v 3 v 3a b
ca
cb
h 1 h 1 h 2 h 2 h 3 h 3
v 1 v 1 v 2 v 2 v 3 v 3a b
ca
cb
Figure16.11: Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels
(bottomrow)byconstructingmoralizedgraphs

============================================================

=== CHUNK 149 ===
Palavras: 352
Caracteres: 11156
--------------------------------------------------
( L e f t )Thissimplechaincanbeconverted
toamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The
resultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional
independences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r )
toanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely
ofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive
pathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude
acliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab⊥ ( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany
impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing
edgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew
directdependences 5 7 8
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
a b
d ca b
d ca b
d c
Figure16.12:Convertinganundirectedmodeltoadirectedmodel ( L e f t )Thisundirected
modelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour
withnochords.Speciﬁcally,theundirectedmodelencodestwodiﬀerentindependencesthat
nodirectedmodelcancapturesimultaneously:acbd ⊥|{ ,}andbdac ⊥|{ ,}.To ( C e n t e r )
converttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by
ensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither
addanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis
example,wechoosetoaddtheedgeconnectingaandc.Toﬁnishtheconversion ( R i g h t )
process,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany
directedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,
andalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode
thatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose
alphabeticalorder thesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU ThegraphformedbyaddingchordstoUisknownasachordalortriangulated
graph,becausealltheloopscannowbedescribedintermsofsmaller,triangular
loops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign
directionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein
D,ortheresultdoesnotdeﬁneavaliddirectedprobabilisticmodel.Oneway
toassigndirectionstotheedgesinDistoimposeanorderingontherandom
variables,thenpointeachedgefromthenodethatcomesearlierintheorderingto
thenodethatcomeslaterintheordering.Seeﬁgureforademonstration 7 F a ct o r G ra p h s
Factorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean
ambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In
anundirectedmodel,thescopeofevery φfunctionmustbeaofsomeclique s u b s e t
inthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas
acorrespondingfactorwhosescopeencompassestheentireclique—forexample,
acliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,
ormaycorrespondtothreefactorsthateachcontainonlyapairofthenodes 5 7 9
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
Factorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach φ
function.Speciﬁcally,afactorgraphisagraphicalrepresentationofanundirected
modelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn
ascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected
model Therestofthenodesaredrawnassquares Thesenodescorrespondto
thefactors φoftheunnormalized probabilitydistribution.Variablesandfactors
maybeconnectedwithundirectededges.Avariableandafactorareconnected
inthegraphifandonlyifthevariableisoneoftheargumentstothefactorin
theunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother
factorinthegraph,norcanavariablebeconnectedtoavariable.Seeﬁgure16.13
foranexampleofhowfactorgraphscanresolveambiguityintheinterpretation of
undirectednetworks a b
ca b
cf 1 f 1a b
cf 1 f 1f 2 f 2
f 3 f 3
Figure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation
ofundirectednetworks ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables:
a,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r )
factorgraphhasonefactoroverallthreevariables Anothervalidfactorgraph ( R i g h t )
forthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo
variables.Representation,inference,andlearningareallasymptoticallycheaperinthis
factorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe
sameundirectedgraphtorepresent 16.3SamplingfromGraphicalModels
Graphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel Oneadvantageofdirectedgraphicalmodelsisthatasimpleandeﬃcientproce-
durecalledancestralsamplingcanproduceasamplefromthejointdistribution
representedbythemodel Thebasicideaistosortthevariablesx iinthegraphintoatopologicalordering,
sothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables
5 8 0
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
canthenbesampledinthisorder.Inotherwords,weﬁrstsamplex 1∼ P(x 1),
thensample P(x 2| P aG(x 2)),andsoon,untilﬁnallywesample P(x n| P aG(x n)) Solongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom,
thenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation
guaranteesthatwecanreadtheconditionaldistributionsinequationand16.1
samplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto
sampleavariablebeforeitsparentsareavailable Forsomegraphs,morethanonetopologicalorderingispossible.Ancestral
samplingmaybeusedwithanyofthesetopologicalorderings Ancestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-
tionaliseasy)andconvenient Onedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical
models.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling
operation.Whenwewishtosamplefromasubsetofthevariablesinadirected
graphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-
ingvariablescomeearlierthanthevariablestobesampledintheorderedgraph Inthiscase,wecansamplefromthelocalconditionalprobabilitydistributions
speciﬁedbythemodeldistribution.Otherwise,theconditionaldistributionswe
needtosamplefromaretheposteriordistributionsgiventheobservedvariables Theseposteriordistributionsareusuallynotexplicitlyspeciﬁedandparametrized
inthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere
thisisthecase,ancestralsamplingisnolongereﬃcient Unfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We
cansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis
oftenrequiressolvingintractableinferenceproblems(todeterminethemarginal
distributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing
somanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling
fromanundirectedmodelwithoutﬁrstconvertingittoadirectedmodelseemsto
requireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother
variable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,
drawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass
process.TheconceptuallysimplestapproachisGibbssampling.Supposewe
haveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We
iterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother
variables,from p(x i|x− i).Duetotheseparationpropertiesofthegraphical
model,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately,
afterwehavemadeonepassthroughthegraphicalmodelandsampledall n
variables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe
5 8 1
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
processandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors Asymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom
thecorrectdistribution.Itcanbediﬃculttodeterminewhenthesampleshave
reachedasuﬃcientlyaccurateapproximationofthedesireddistribution.Sampling
techniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin
chapter.17
16.4AdvantagesofStructuredModeling
Theprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow
ustodramatically reducethecostofrepresentingprobabilitydistributionsaswell
aslearningandinference.Samplingisalsoacceleratedinthecaseofdirected
models,whilethesituationcanbecomplicatedwithundirectedmodels.The
primarymechanismthatallowsalloftheseoperationstouselessruntimeand
memoryischoosingtonotmodelcertaininteractions Graphicalmodelsconvey
informationbyleavingedgesout.Anywherethereisnotanedge,themodel
speciﬁestheassumptionthatwedonotneedtomodeladirectinteraction Alessquantiﬁablebeneﬁtofusingstructuredprobabilisticmodelsisthat
theyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof
knowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto
developanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand
inferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,
wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour
data.Wecanthencombinethesediﬀerentalgorithmsandstructuresandobtain
aCartesianproductofdiﬀerentpossibilities.Itwouldbemuchmorediﬃcultto
designend-to-endalgorithmsforeverypossiblesituation 16.5LearningaboutDependencies
Agoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe
observedor“visible” variables v.Oftenthediﬀerentelementsofvarehighly
dependentoneachother.Inthecontextofdeeplearning,theapproachmost
commonlyusedtomodelthesedependenciesistointroduceseverallatentor
“hidden”variables,h.Themodelcanthencapturedependenciesbetweenanypair
ofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and
directdependenciesbetweenandv h j Agoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto
5 8 2
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
haveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge
cliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis
costly—bothinacomputational sense,becausethenumberofparametersthat
mustbestoredinmemoryscalesexponentiallywiththenumberofmembersina
clique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters
requiresawealthofdatatoestimateaccurately Whenthemodelisintendedtocapturedependenciesbetweenvisiblevariables
withdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe
graphmustbedesignedtoconnectthosevariablesthataretightlycoupledand
omitedgesbetweenothervariables.Anentireﬁeldofmachinelearningcalled
structurelearningisdevotedtothisproblemForagoodreferenceonstructure
learning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare
aformofgreedysearch.Astructureisproposed,amodelwiththatstructure
istrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand
penalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges
addedorremovedarethenproposedasthenextstepofthesearch.Thesearch
proceedstoanewstructurethatisexpectedtoincreasethescore Usinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform
discretesearchesandmultipleroundsoftraining.Aﬁxedstructureovervisible
andhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits
toimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter
learningtechniqueswecanlearnamodelwithaﬁxedstructurethatimputesthe
rightstructureonthemarginal p()v
Latentvariableshaveadvantagesbeyondtheirroleineﬃcientlycapturing p(v)

============================================================

=== CHUNK 150 ===
Palavras: 371
Caracteres: 10615
--------------------------------------------------
Thenewvariables halsoprovideanalternativerepresentationforv.Forexample,
asdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6
thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This
meansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo
classiﬁcation Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14
codinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiﬁer,
orascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,
butdeepermodelsandmodelswithdiﬀerentkindsofinteractionscancreateeven
richerdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning
bylearninglatentvariables.Often,givensomemodelofvandh,experimental
observationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor
v 5 8 3
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
16.6InferenceandApproximateInference
Oneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout
howvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask
whatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto
extractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed
tosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels
usingtheprincipleofmaximumlikelihood.Because
log()= p v E h h∼ p (| v )[log( )log( )] p h v ,− p h v| ,(16.9)
weoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof
theseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof
somevariablesgivenothervariables,orpredicttheprobabilitydistributionover
somevariablesgiventhevalueofothervariables Unfortunately,formostinterestingdeepmodels,theseinferenceproblemsare
intractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The
graphstructureallowsustorepresentcomplicated,high-dimensionaldistributions
withareasonablenumberofparameters,butthegraphsusedfordeeplearningare
usuallynotrestrictiveenoughtoalsoalloweﬃcientinference Itisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral
graphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe
complexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem
hasasolutionandﬁndingasolutionifoneexists.Problemsin#Prequirecounting
thenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat
wedeﬁneagraphicalmodeloverthebinaryvariablesina3-SATproblem We
canimposeauniformdistributionoverthesevariables.Wecanthenaddone
binarylatentvariableperclausethatindicateswhethereachclauseissatisﬁed Wecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare
satisﬁed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction
treeoflatentvariables,witheachnodeinthetreereportingwhethertwoother
variablesaresatisﬁed.Theleavesofthistreearethevariablesforeachclause Therootofthetreereportswhethertheentireproblemissatisﬁed Duetothe
uniformdistributionovertheliterals,themarginaldistributionovertherootofthe
reductiontreespeciﬁeswhatfractionofassignmentssatisfytheproblem.While
thisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical
real-worldscenarios Thismotivatestheuseofapproximate inference.In thecontextof deep
learning,thisusuallyreferstovariationalinference,inwhichweapproximate the
5 8 4
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
truedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas
closetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth
inchapter.19
16.7TheDeepLearningApproachtoStructuredProb-
abilisticModels
Deeplearningpractitioners generallyusethesamebasiccomputational toolsas
othermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels However,inthecontextofdeeplearning,weusuallymakediﬀerentdesigndecisions
abouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat
haveaverydiﬀerentﬂavorfrommoretraditionalgraphicalmodels Deeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe
contextofgraphicalmodels,wecandeﬁnethedepthofamodelintermsofthe
graphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa
latentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved
variableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest
depthofanysuch h i.Thiskindofdepthisdiﬀerentfromthedepthinducedby
thecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno
latentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational
graphstodeﬁnetheconditionaldistributionswithinamodel Deeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-
tations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining
shallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways
haveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave
morelatentvariablesthanobservedvariables.Complicated nonlinearinteractions
betweenvariablesareaccomplishedviaindirectconnectionsthatﬂowthrough
multiplelatentvariables Bycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat
areatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat
randomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order
termsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween
variables.Iftherearelatentvariables,theyareusuallyfewinnumber Thewaythatlatentvariablesaredesignedalsodiﬀersindeeplearning.The
deeplearningpractitionertypicallydoesnotintendforthelatentvariablesto
takeonanyspeciﬁcsemanticsaheadoftime—thetrainingalgorithmisfreeto
inventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare
5 8 5
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
usuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization
techniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When
latentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare
oftendesignedwithsomespeciﬁcsemanticsinmind—thetopicofadocument,
theintelligenceofastudent,thediseasecausingapatient’ssymptoms,etc.These
modelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave
moretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare
notreusableinasmanydiﬀerentcontextsasdeepmodels Anotherobviousdiﬀerenceisthekindofconnectivitytypicallyusedinthe
deeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits
thatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween
twogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels
haveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe
individuallydesigned.Thedesignofthemodelstructureistightlylinkedwith
thechoiceofinferencealgorithm.Traditionalapproachestographicalmodels
typicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint
istoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled
loopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery
sparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto
connecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea
distributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo) Distributedrepresentationshavemanyadvantages,butfromthepointofview
ofgraphicalmodelsandcomputational complexity,distributedrepresentations
havethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor
thetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe
relevant.Asaconsequence,oneofthemoststrikingdiﬀerencesbetweenthelarger
graphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat
loopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels
areinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms
eﬃcient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge
numberoflatentvariables,makingeﬃcientnumericalcodeessential.Thisprovides
anadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for
groupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween
twolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented
witheﬃcientmatrixproductoperations,orsparselyconnectedgeneralizations ,like
blockdiagonalmatrixproductsorconvolutions Finally,thedeeplearningapproachtographicalmodelingischaracterizedby
amarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil
allquantitieswemightwantcanbecomputedexactly,weincreasethepowerof
5 8 6
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
themodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels
whosemarginaldistributionscannotbecomputed,andaresatisﬁedsimplytodraw
approximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable
objectivefunctionthatwecannotevenapproximate inareasonableamountof
time,butwearestillabletoapproximately trainthemodelifwecaneﬃciently
obtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach
isoftentoﬁgureoutwhattheminimumamountofinformationweabsolutely
needis,andthentoﬁgureouthowtogetareasonableapproximation ofthat
informationasquicklyaspossible 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e
TherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium
isthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning TheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables
thatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20
seehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe
RBMexempliﬁesmanyofthepracticesusedinawidevarietyofdeepgraphical
models: itsunitsareorganizedintolargegroupscalledlayers,theconnectivity
betweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the
modelisdesignedtoalloweﬃcientGibbssampling,andtheemphasisofthemodel
designisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics
werenotspeciﬁedbythedesigner.Later,insection,wewillrevisittheRBM 20.2
inmoredetail ThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden
units.Itsenergyfunctionis
E ,( v h b ) = −v c−h v−W h , (16.10)
where b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan
seethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction
betweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically
inﬁgure.Asthisﬁguremakesclear,animportantaspectofthismodelis 16.14
thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany
twohiddenunits(hencethe“restricted,”ageneralBoltzmannmachinemayhave
arbitraryconnections) TherestrictionsontheRBMstructureyieldtheniceproperties
p( ) = Π hv| i p(h i|v) (16.11)
5 8 7
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
h 1 h 1 h 2 h 2 h 3 h 3
v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4
Figure16.14:AnRBMdrawnasaMarkovnetwork and
p( ) = Π vh| i p(v i|h) (16.12)
Theindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM
weobtain:
P(h i= 1 ) = |v σ
vW : , i+ b i
, (16.13)
P(h i= 0 ) = 1 |v − σ
vW : , i+ b i

============================================================

=== CHUNK 151 ===
Palavras: 356
Caracteres: 5072
--------------------------------------------------
(16.14)
TogetherthesepropertiesallowforeﬃcientblockGibbssampling,whichalter-
natesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-
ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin
ﬁgure.16.15
Sincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis
easytotakeitsderivatives.Forexample,
∂
∂ W i , jE ,(vh) = −v ih j (16.15)
Thesetwoproperties—eﬃcientGibbssamplingandeﬃcientderivatives—make
trainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18
trainedbycomputingsuchderivativesappliedtosamplesfromthemodel Trainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse
E h h∼ p (| v )[] hasasetoffeaturestodescribe v
Overall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-
icalmodels: representationlearningaccomplishedvialayersoflatentvariables,
combinedwitheﬃcientinteractionsbetweenlayersparametrized bymatrices Thelanguageofgraphicalmodelsprovidesanelegant,ﬂexibleandclearlanguage
fordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,
amongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels 5 8 8
CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING
Figure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith
permissionfrom() LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn
usingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow
representstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare
highlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t )
thistothesamplesandweightsofalinearfactormodel,showninﬁgure.Thesamples 13.2
herearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The
RBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,
theRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v|
sothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable
tohavebothanon-factorialandanon-factorial p() h p( ) h v|
5 8 9
C h a p t e r 1 7
Mon t e C arl o Me t h o d s
Randomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsand
MonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrect
answer(orreportthattheyfailed).Thesealgorithmsconsumearandomamount
ofresources,usuallymemoryortime.Incontrast,MonteCarloalgorithmsreturn
answerswitharandomamountoferror.Theamountoferrorcantypicallybe
reducedbyexpendingmoreresources(usuallyrunningtimeandmemory).Forany
ﬁxedcomputational budget,aMonteCarloalgorithmcanprovideanapproximate
answer Manyproblemsinmachinelearningaresodiﬃcultthatwecanneverexpectto
obtainpreciseanswerstothem.Thisexcludesprecisedeterministicalgorithmsand
LasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithms
orMonteCarloapproximations.Bothapproachesareubiquitousinmachine
learning.Inthischapter,wefocusonMonteCarlomethods 17.1SamplingandMonteCarloMethods
Manyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebased
ondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplesto
formaMonteCarloestimateofsomedesiredquantity 1 Wh y S a m p l i n g Therearemanyreasonsthatwemaywishtodrawsamplesfromaprobability
distribution.Samplingprovidesaﬂexiblewaytoapproximatemanysumsand
590
CHAPTER17.MONTECARLOMETHODS
integralsatreducedcost.Sometimesweusethistoprovideasigniﬁcantspeedupto
acostlybuttractablesum,asinthecasewhenwesubsamplethefulltrainingcost
withminibatches.Inothercases,ourlearningalgorithmrequiresustoapproximate
anintractablesumorintegral,suchasthegradientofthelogpartitionfunctionof
anundirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthe
sensethatwewanttotrainamodelthatcansamplefromthetrainingdistribution 2 B a s i cs o f Mo n t e Ca rl o S a m p l i n g
Whenasumoranintegralcannotbecomputedexactly(forexamplethesum
hasanexponentialnumberoftermsandnoexactsimpliﬁcationisknown)itis
oftenpossibletoapproximate itusingMonteCarlosampling.Theideaistoview
thesumorintegralasifitwasanexpectationundersomedistributionandto
a p p r o x i m a t e t h e e x p e c t a t i o n b y a c o r r e s p o nding a v e r a g e.Let
s=
xp f E () x() = x p[()] f x (17.1)
or
s=
p f d E () x() x x= p[()] f x (17.2)
bethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraint
that pisaprobabilitydistribution(forthesum)oraprobabilitydensity(forthe
integral)overrandomvariable x
Wecanapproximate sbydrawing nsamples x( 1 ), , x( ) nfrom pandthen
formingtheempiricalaverage
ˆ s n=1
nn 
i = 1f( x( ) i) (17.3)
Thisapproximation isjustiﬁedbyafewdiﬀerentproperties.Theﬁrsttrivial
observationisthattheestimator ˆ sisunbiased,since
E[ˆ s n] =1
nn 
i = 1E[( f x( ) i)] =1
nn 
i = 1s s .= (17.4)
Butinaddition,thelawoflargenumbersstatesthatifthesamples x( ) iare
i.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue:
limn→∞ˆ s n= s , (17.5)
5 9 1
CHAPTER17.MONTECARLOMETHODS
providedthatthevarianceoftheindividualterms,Var[ f( x( ) i)],isbounded.Tosee
thismoreclearly,considerthevarianceofˆ s nas nincreases.Thevariance Var[ˆ s n]
decreasesandconvergesto0,solongasVar[( f x( ) i)] <∞:
Var[ˆ s n] =1
n2n
i = 1Var[()] f x (17.6)
=Var[()] f x
n

============================================================

=== CHUNK 152 ===
Palavras: 456
Caracteres: 4119
--------------------------------------------------
(17.7)
ThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonte
CarloaverageorequivalentlytheamountofexpectederroroftheMonteCarlo
approximation.Wecomputeboththeempiricalaverageofthe f( x( ) i)andtheir
empiricalvariance,1andthendividetheestimatedvariancebythenumberof
samples ntoobtainanestimatorofVar[ˆ s n] Thecentrallimittheoremtells
usthatthedistributionoftheaverage, ˆ s n,convergestoanormaldistribution
withmean sandvarianceV a r [ ( ) ] f x
n.Thisallowsustoestimateconﬁdenceintervals
aroundtheestimate ˆ s n,usingthecumulativedistributionofthenormaldensity However,allthisreliesonourabilitytoeasilysamplefromthebasedistribution
p( x),butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefrom
p,analternativeistouseimportancesampling,presentedinsection.A17.2
moregeneralapproachistoformasequenceofestimatorsthatconvergetowards
thedistributionofinterest.ThatistheapproachofMonteCarloMarkovchains
(section).17.3
17.2ImportanceSampling
Animportantstepinthedecompositionoftheintegrand(orsummand)usedbythe
MonteCarlomethodinequationisdecidingwhichpartoftheintegrandshould 17.2
playtheroletheprobability p( x)andwhichpartoftheintegrandshouldplaythe
roleofthequantity f( x) whoseexpectedvalue(underthatprobabilitydistribution)
istobeestimated.Thereisnouniquedecompositionbecause p( x) f( x)canalways
berewrittenas
p f q () x() = x () xp f() x() x
q() x, (17.8)
wherewenowsamplefrom qandaveragep f
q.Inmanycases,wewishtocompute
anexpectationforagiven pandan f,andthefactthattheproblemisspeciﬁed
1Th e u n b i a s e d e s t i m a t o r o f t h e v a ria n c e i s o f t e n p re f e rre d , i n wh i c h t h e s u m o f s q u a re d
d i ﬀ e re n c e s i s d i v i d e d b y i n s t e a d o f n − 1 n
5 9 2
CHAPTER17.MONTECARLOMETHODS
fromthestartasanexpectationsuggeststhatthis pand fwouldbeanatural
choiceofdecomposition.However,theoriginalspeciﬁcationoftheproblemmay
notbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtain
agivenlevelofaccuracy Fortunately,theformoftheoptimalchoice q∗canbe
derivedeasily.Theoptimal q∗correspondstowhatiscalledoptimalimportance
sampling Becauseoftheidentityshowninequation,anyMonteCarloestimator 17.8
ˆ s p=1
nn 
i , = 1 x( ) i∼ pf( x( ) i) (17.9)
canbetransformedintoanimportancesamplingestimator
ˆ s q=1
nn 
i , = 1 x( ) i∼ qp( x( ) i)( f x( ) i)
q( x( ) i) (17.10)
Weseereadilythattheexpectedvalueoftheestimatordoesnotdependon: q
E q[ˆ s q] = E q[ˆ s p] = s (17.11)
However,thevarianceofanimportancesamplingestimatorcanbegreatlysensitive
tothechoiceof.Thevarianceisgivenby q
Var[ˆ s q] = Var[p f() x() x
q() x] /n (17.12)
Theminimumvarianceoccurswhenis q
q∗() = xp f() x|() x|
Z, (17.13)
where Zisthenormalization constant,chosensothat q∗( x)sumsorintegratesto
1asappropriate.Betterimportancesamplingdistributionsputmoreweightwhere
theintegrandislarger.Infact,when f( x)doesnotchangesign,Var[ˆ s q∗]=0,
meaningthat whentheoptimaldistributionisused a s i ng l e s a m p l e i s s u ﬃ c i e nt
Ofcourse,thisisonlybecausethecomputationof q∗hasessentiallysolvedthe
originalproblem,soitisusuallynotpracticaltousethisapproachofdrawinga
singlesamplefromtheoptimaldistribution Anychoiceofsamplingdistribution qisvalid(inthesenseofyieldingthe
correctexpectedvalue)and q∗istheoptimalone(inthesenseofyieldingminimum
variance).Samplingfrom q∗isusuallyinfeasible,butotherchoicesof qcanbe
feasiblewhilestillreducingthevariancesomewhat 5 9 3
CHAPTER17.MONTECARLOMETHODS
Anotherapproachistousebiasedimportancesampling,whichhasthe
advantageofnotrequiringnormalized por q.Inthecaseofdiscretevariables,the
biasedimportancesamplingestimatorisgivenby
ˆ s B I S=n
i = 1p ( x( ) i)
q ( x( ) i )f( x( ) i)
n
i = 1p ( x( ) i )
q ( x( ) i )(17.14)
=n
i = 1p ( x( ) i)
˜ q ( x( ) i)f( x( ) i)
n
i = 1p ( x( ) i)
˜ q ( x( ) i)(17.15)
=n
i = 1˜ p ( x( ) i)
˜ q ( x( ) i )f( x( ) i)
n
i = 1˜ p ( x( ) i )
˜ q ( x( ) i ), (17.16)
where ˜ pand˜ qaretheunnormalized formsof pand qandthe x( ) iarethesamples
from q.Thisestimatorisbiasedbecause E[ˆ s B I S]= s,exceptasymptoticallywhen
n→∞andthedenominator ofequationconvergesto1.Hencethisestimator 17.14
iscalledasymptoticallyunbiased

============================================================

=== CHUNK 153 ===
Palavras: 372
Caracteres: 7220
--------------------------------------------------
Althoughagoodchoiceof qcangreatlyimprovetheeﬃciencyofMonteCarlo
estimation,apoorchoiceof qcanmaketheeﬃciencymuchworse.Goingbackto
equation,weseethatiftherearesamplesof 17.12 qforwhichp f ( ) x| ( ) x|
q ( ) xislarge,
thenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhen
q( x)istinywhileneither p( x)nor f( x)aresmallenoughtocancelit.The q
distributionisusuallychosentobeaverysimpledistributionsothatitiseasy
tosamplefrom.When xishigh-dimensional,thissimplicityin qcausesitto
match por p f||poorly.When q( x( ) i) p( x( ) i)| f( x( ) i)|,importancesampling
collectsuselesssamples(summingtinynumbersorzeros).Ontheotherhand,when
q( x( ) i) p( x( ) i)| f( x( ) i)|,whichwillhappenmorerarely,theratiocanbehuge Becausetheselattereventsarerare,theymaynotshowupinatypicalsample,
yieldingtypicalunderestimationof s,compensatedrarelybygrossoverestimation Suchverylargeorverysmallnumbersaretypicalwhen xishighdimensional,
becauseinhighdimensionthedynamicrangeofjointprobabilities canbevery
large Inspiteofthisdanger,importancesamplinganditsvariantshavebeenfound
veryusefulinmanymachinelearningalgorithms,includingdeeplearningalgorithms Forexample,seetheuseofimportancesamplingtoacceleratetraininginneural
languagemodelswithalargevocabulary(section)orotherneuralnets 12.4.3.3
withalargenumberofoutputs.Seealsohowimportancesamplinghasbeen
usedtoestimateapartitionfunction(thenormalization constantofaprobability
5 9 4
CHAPTER17.MONTECARLOMETHODS
distribution)insection,andtoestimatethelog-likelihoodindeepdirected 18.7
modelssuchasthevariationalautoencoder,insection.Importancesampling 20.10.3
mayalsobeusedtoimprovetheestimateofthegradientofthecostfunctionused
totrainmodelparameterswithstochasticgradientdescent,particularlyformodels
suchasclassiﬁerswheremostofthetotalvalueofthecostfunctioncomesfroma
smallnumberofmisclassiﬁedexamples.Samplingmorediﬃcultexamplesmore
frequentlycanreducethevarianceofthegradientinsuchcases(,) Hinton2006
17.3MarkovChainMonteCarloMethods
Inmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractable
methodfordrawingexactsamplesfromthedistribution p m o de l( x)orfromagood
(lowvariance)importancesamplingdistribution q( x).Inthecontextofdeep
learning,thismostoftenhappenswhen p m o de l( x)isrepresentedbyanundirected
model.Inthesecases,weintroduceamathematical toolcalledaMarkovchain
toapproximately samplefrom p m o de l( x).ThefamilyofalgorithmsthatuseMarkov
chainstoperformMonteCarloestimatesiscalledMarkovchainMonteCarlo
methods(MCMC).MarkovchainMonteCarlomethodsformachinelearningare
describedatgreaterlengthinKollerandFriedman2009() Themoststandard,
genericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodel
doesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenient
to present these techniques assampling froman energy-basedmodel (EBM)
p( x)∝ −exp( E()) xasdescribedinsection.IntheEBMformulation,every 16.2.4
stateisguaranteedtohavenon-zeroprobability.MCMCmethodsareinfact
morebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthat
containzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthe
behaviorofMCMCmethodsmustbeprovenonacase-by-casebasisfordiﬀerent
familiesofsuchdistributions.Inthecontextofdeeplearning,itismostcommon
torelyonthemostgeneraltheoreticalguaranteesthatnaturallyapplytoall
energy-basedmodels Tounderstandwhydrawingsamplesfromanenergy-basedmodelisdiﬃcult,
consideranEBMoverjusttwovariables,deﬁningadistributionab.Inorder p( ,)
tosamplea,wemustdrawafrom p(ab|),andinordertosampleb,wemust
drawitfrom p(ba|).Itseemstobeanintractablechicken-and-eggproblem Directedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperform
ancestralsamplingonesimplysampleseachofthevariablesintopologicalorder,
conditioningoneachvariable’sparents,whichareguaranteedtohavealreadybeen
sampled(section).Ancestralsamplingdeﬁnesaneﬃcient,single-passmethod 16.3
5 9 5
CHAPTER17.MONTECARLOMETHODS
ofobtainingasample InanEBM,wecanavoidthischickenandeggproblembysamplingusinga
Markovchain.ThecoreideaofaMarkovchainistohaveastate xthatbegins
asanarbitraryvalue.Overtime,werandomlyupdate xrepeatedly.Eventually
xbecomes(verynearly)afairsamplefrom p( x).Formally,aMarkovchainis
deﬁnedbyarandomstate xandatransitiondistribution T( x| x)specifying
theprobabilitythatarandomupdatewillgotostate xifitstartsinstate x RunningtheMarkovchainmeansrepeatedlyupdatingthestate xtoavalue x
sampledfrom T( x| x) TogainsometheoreticalunderstandingofhowMCMCmethodswork,itis
usefultoreparametrizetheproblem.First,werestrictourattentiontothecase
wheretherandomvariable xhascountablymanystates.Wecanthenrepresent
thestateasjustapositiveinteger x Diﬀerentintegervaluesof xmapbackto
diﬀerentstatesintheoriginalproblem x
ConsiderwhathappenswhenweruninﬁnitelymanyMarkovchainsinparallel AllofthestatesofthediﬀerentMarkovchainsaredrawnfromsomedistribution
q( ) t( x),where tindicatesthenumberoftimestepsthathaveelapsed.Atthe
beginning, q( 0 )issomedistributionthatweusedtoarbitrarilyinitialize xforeach
Markovchain.Later, q( ) tisinﬂuencedbyalloftheMarkovchainstepsthathave
runsofar.Ourgoalisfor q( ) t() xtoconvergeto p x()
Becausewehavereparametrized theproblemintermsofpositiveinteger x,we
candescribetheprobabilitydistributionusingavector,with q v
q i v (= x ) = i (17.17)
ConsiderwhathappenswhenweupdateasingleMarkovchain’sstate xtoa
newstate x.Theprobabilityofasinglestatelandinginstate xisgivenby
q( + 1 ) t( x) =
xq( ) t()( x T x| x .) (17.18)
Usingourintegerparametrization, wecanrepresenttheeﬀectofthetransition
operatorusingamatrix.Wedeﬁnesothat T A A
A i , j= ( T x= = ) i| x j (17.19)
Usingthisdeﬁnition,wecannowrewriteequation.Ratherthanwritingitin 17.18
termsof qand Ttounderstandhowasinglestateisupdated,wemaynowuse v
and AtodescribehowtheentiredistributionoverallthediﬀerentMarkovchains
(runninginparallel)shiftsasweapplyanupdate:
v( ) t= A v( 1 ) t− (17.20)
5 9 6
CHAPTER17.MONTECARLOMETHODS
ApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythe
matrix Arepeatedly.Inotherwords,wecanthinkoftheprocessasexponentiating
thematrix: A
v( ) t= Atv( 0 ) (17.21)
Thematrix Ahasspecialstructurebecauseeachofitscolumnsrepresentsa
probabilitydistribution.Suchmatricesarecalledstochasticmatrices.Ifthere
isanon-zeroprobabilityoftransitioningfromanystate xtoanyotherstate xfor
somepower t,thenthePerron-Frobeniustheorem(,;Perron1907Frobenius1908,)
guaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan 1
seethatalloftheeigenvaluesareexponentiated:
v( ) t=
V λ Vdiag()− 1tv( 0 )= () Vdiag λtV− 1v( 0 ).(17.22)
Thisprocesscausesalloftheeigenvaluesthatarenotequaltotodecaytozero 1
Undersomeadditionalmildconditions, Aisguaranteedtohaveonlyoneeigenvector
witheigenvalue.Theprocessthusconvergestoa 1 stationarydistribution,
sometimesalsocalledthe .Atconvergence, equilibriumdistribution
v= = A v v , (17.23)
andthissameconditionholdsforeveryadditionalstep.Thisisaneigenvector
equation.Tobeastationarypoint, vmustbeaneigenvectorwithcorresponding
eigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary 1
distribution,repeatedapplicationsofthetransitionsamplingproceduredonot
changethe overthestatesofallthevariousMarkovchains(although d i s t r i b u t i o n
transitionoperatordoeschangeeachindividualstate,ofcourse)

============================================================

=== CHUNK 154 ===
Palavras: 352
Caracteres: 10019
--------------------------------------------------
Ifwehavechosen Tcorrectly,thenthestationarydistribution qwillbeequal
tothedistribution pwewishtosamplefrom.Wewilldescribehowtochoose T
shortly,insection.17.4
MostpropertiesofMarkovChainswithcountablestatescanbegeneralized
tocontinuousvariables.Inthissituation,someauthorscalltheMarkovChain
aHarrischainbutweusethetermMarkovChaintodescribebothconditions Ingeneral,aMarkovchainwithtransitionoperator Twillconverge,undermild
conditions,toaﬁxedpointdescribedbytheequation
q( x) = E x∼ q T( x| x) , (17.24)
whichinthediscretecaseisjustrewritingequation.When17.23 xisdiscrete,
theexpectationcorrespondstoasum,andwhen xiscontinuous,theexpectation
correspondstoanintegral 5 9 7
CHAPTER17.MONTECARLOMETHODS
Regardlessofwhetherthestateiscontinuousordiscrete,allMarkovchain
methodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythe
statebeginstoyieldsamplesfromtheequilibriumdistribution.Runningthe
Markovchainuntilitreachesitsequilibriumdistributioniscalled“burningin”
theMarkovchain.Afterthechainhasreachedequilibrium,asequenceofinﬁnitely
manysamplesmaybedrawnfromfromtheequilibriumdistribution Theyare
identicallydistributedbutanytwosuccessivesampleswillbehighlycorrelated
witheachother.Aﬁnitesequenceofsamplesmaythusnotbeveryrepresentative
oftheequilibriumdistribution.Onewaytomitigatethisproblemistoreturn
onlyevery nsuccessivesamples, sothatourestimateofthestatisticsofthe
equilibriumdistributionisnotasbiasedbythecorrelationbetweenanMCMC
sampleandthenextseveralsamples.Markovchainsarethusexpensivetouse
becauseofthetimerequiredtoburnintotheequilibriumdistributionandthetime
requiredtotransitionfromonesampletoanotherreasonablydecorrelatedsample
afterreachingequilibrium.Ifonedesirestrulyindependentsamples,onecanrun
multipleMarkovchainsinparallel.Thisapproachusesextraparallelcomputation
toeliminatelatency.ThestrategyofusingonlyasingleMarkovchaintogenerate
allsamplesandthestrategyofusingoneMarkovchainforeachdesiredsampleare
twoextremes;deeplearningpractitioners usuallyuseanumberofchainsthatis
similartothenumberofexamplesinaminibatchandthendrawasmanysamples
asareneededfromthisﬁxedsetofMarkovchains.Acommonlyusednumberof
Markovchainsis100 Anotherdiﬃcultyisthatwedonotknowinadvancehowmanystepsthe
Markovchainmustrunbeforereachingitsequilibriumdistribution.Thislengthof
timeiscalledthemixingtime.ItisalsoverydiﬃculttotestwhetheraMarkov
chainhasreachedequilibrium.Wedonothaveapreciseenoughtheoryforguiding
usinansweringthisquestion.Theorytellsusthatthechainwillconverge,butnot
muchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrix A
actingonavectorofprobabilities v,thenweknowthatthechainmixeswhen At
haseﬀectivelylostalloftheeigenvaluesfrom Abesidestheuniqueeigenvalueof.1
Thismeansthatthemagnitudeofthesecondlargesteigenvaluewilldeterminethe
mixingtime.However,inpractice,wecannotactuallyrepresentourMarkovchain
intermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisit
isexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresent
v, A,ortheeigenvaluesof A Duetotheseandotherobstacles,weusuallydo
notknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkov
chainforanamountoftimethatweroughlyestimatetobesuﬃcient,anduse
heuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristic
methodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetween
5 9 8
CHAPTER17.MONTECARLOMETHODS
successivesamples 17.4GibbsSampling
Sofarwehavedescribedhowtodrawsamplesfromadistribution q( x)byrepeatedly
updating x x←∼ T( x| x).However,wehavenotdescribedhowtoensurethat
q( x)isausefuldistribution.Twobasicapproachesareconsideredinthisbook Theﬁrstoneistoderive Tfromagivenlearned p m o de l,describedbelowwiththe
caseofsamplingfromEBMs.Thesecondoneistodirectlyparametrize Tand
learnit,sothatitsstationarydistributionimplicitlydeﬁnesthe p m o de lofinterest Examplesofthissecondapproacharediscussedinsectionsand 20.1220.13
Inthecontextofdeeplearning,wecommonlyuseMarkovchainstodraw
samplesfromanenergy-basedmodeldeﬁningadistribution p m o de l( x).Inthiscase,
wewantthe q( x)fortheMarkovchaintobe p m o de l( x).Toobtainthedesired
q() x,wemustchooseanappropriate T( x| x) AconceptuallysimpleandeﬀectiveapproachtobuildingaMarkovchain
thatsamplesfrom p m o de l( x)istouseGibbssampling,inwhichsamplingfrom
T( x| x)isaccomplishedbyselectingonevariablex iandsamplingitfrom p m o de l
conditionedonitsneighborsintheundirectedgraph Gdeﬁningthestructureof
theenergy-basedmodel.Itisalsopossibletosampleseveralvariablesatthesame
timesolongastheyareconditionallyindependentgivenalloftheirneighbors AsshownintheRBMexampleinsection,allofthehiddenunitsofan 16.7.1
RBMmaybesampledsimultaneouslybecausetheyareconditionallyindependent
fromeachothergivenallofthevisibleunits.Likewise,allofthevisibleunitsmay
besampledsimultaneouslybecausetheyareconditionallyindependentfromeach
othergivenallofthehiddenunits.Gibbssamplingapproachesthatupdatemany
variablessimultaneouslyinthiswayarecalledblockGibbssampling AlternateapproachestodesigningMarkovchainstosamplefrom p m o de lare
possible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinother
disciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling,
itisraretouseanyapproachotherthanGibbssampling.Improvedsampling
techniquesareonepossibleresearchfrontier 17.5TheChallengeofMixingbetweenSeparatedModes
TheprimarydiﬃcultyinvolvedwithMCMCmethodsisthattheyhaveatendency
tomixpoorly.Ideally,successivesamplesfromaMarkovchaindesignedtosample
5 9 9
CHAPTER17.MONTECARLOMETHODS
from p( x)wouldbecompletelyindependentfromeachotherandwouldvisitmany
diﬀerentregionsin xspaceproportionaltotheirprobability.Instead,especially
inhighdimensionalcases,MCMCsamplesbecomeverycorrelated.Werefer
tosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswith
slowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisy
gradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingonthe
probability,withrespecttothestateofthechain(therandomvariablesbeing
sampled) Thechaintendstotakesmallsteps(inthespaceofthestateofthe
Markovchain),fromaconﬁguration x( 1 ) t−toaconﬁguration x( ) t,withtheenergy
E( x( ) t)generallylowerorapproximately equaltotheenergy E( x( 1 ) t−),witha
preferenceformovesthatyieldlowerenergyconﬁgurations Whenstartingfroma
ratherimprobableconﬁguration(higherenergythanthetypicalonesfrom p( x)),
thechaintendstograduallyreducetheenergyofthestateandonlyoccasionally
movetoanothermode.Oncethechainhasfoundaregionoflowenergy(for
example,ifthevariablesarepixelsinanimage,aregionoflowenergymightbe
aconnectedmanifoldofimagesofthesameobject),whichwecallamode,the
chainwilltendtowalkaroundthatmode(followingakindofrandomwalk).Once
inawhileitwillstepoutofthatmodeandgenerallyreturntoitor(ifitﬁnds
anescaperoute)movetowardsanothermode.Theproblemisthatsuccessful
escaperoutesarerareformanyinterestingdistributions,sotheMarkovchainwill
continuetosamplethesamemodelongerthanitshould ThisisveryclearwhenweconsidertheGibbssamplingalgorithm(section).17.4
Inthiscontext,considertheprobabilityofgoingfromonemodetoanearbymode
withinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshape
ofthe“energybarrier” betweenthesemodes.Transitionsbetweentwomodes
thatareseparatedbyahighenergybarrier(aregionoflowprobability)are
exponentiallylesslikely(intermsoftheheightoftheenergybarrier).Thisis
illustratedinﬁgure.Theproblemariseswhentherearemultiplemodeswith 17.1
highprobabilitythatareseparatedbyregionsoflowprobability,especiallywhen
eachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhose
valuesarelargelydeterminedbytheothervariables Asasimpleexample,consideranenergy-basedmodelovertwovariablesaand
b,whicharebothbinarywithasign,takingonvalues −1 1and.If E(ab ,) =− wab
forsomelargepositivenumber w,thenthemodelexpressesastrongbeliefthata
andbhavethesamesign.ConsiderupdatingbusingaGibbssamplingstepwith
a= 1 Theconditionaldistributionoverbisgivenby P(b= 1|a= 1)= σ( w) If wislarge,thesigmoidsaturates,andtheprobabilityofalsoassigningbtobe
1iscloseto1.Likewise,ifa=−1,theprobabilityofassigningbtobe−1is
closeto1.Accordingto P m o de l(ab ,),bothsignsofbothvariablesareequallylikely 6 0 0
CHAPTER17.MONTECARLOMETHODS
Figure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkov
chaininitializedatthemodeinbothcases ( L e f t )Amultivariatenormaldistribution
withtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesare
independent.Amultivariatenormaldistributionwithhighlycorrelatedvariables ( C e n t e r )
ThecorrelationbetweenvariablesmakesitdiﬃcultfortheMarkovchaintomix.Because
theupdateforeachvariablemustbeconditionedontheothervariable,thecorrelation
reducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint ( R i g h t )AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxis-aligned Gibbssamplingmixesveryslowlybecauseitisdiﬃculttochangemodeswhilealtering
onlyonevariableatatime Accordingto P m o de l(ab|),bothvariablesshouldhavethesamesign.Thismeans
thatGibbssamplingwillonlyveryrarelyﬂipthesignsofthesevariables Inmorepracticalscenarios,thechallengeisevengreaterbecausewecarenot
onlyaboutmakingtransitionsbetweentwomodesbutmoregenerallybetween
allthemanymodesthatarealmodelmightcontain.Ifseveralsuchtransitions
arediﬃcultbecauseofthediﬃcultyofmixingbetweenmodes,thenitbecomes
veryexpensivetoobtainareliablesetofsamplescoveringmostofthemodes,and
convergenceofthechaintoitsstationarydistributionisveryslow Sometimesthisproblemcanberesolvedbyﬁndinggroupsofhighlydependent
unitsandupdatingallofthemsimultaneouslyinablock Unfortunately,when
thedependenciesarecomplicated,itcanbecomputationally intractabletodrawa
samplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginally
introducedtosolveisthisproblemofsamplingfromalargegroupofvariables Inthecontextofmodelswithlatentvariables,whichdeﬁneajointdistribution
p m o de l( x h ,),weoftendrawsamplesof xbyalternatingbetweensamplingfrom
p m o de l( x h|)andsamplingfrom p m o de l( h x|).Fromthepointofviewofmixing
6 0 1
CHAPTER17.MONTECARLOMETHODS
Figure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels

============================================================

=== CHUNK 155 ===
Palavras: 365
Caracteres: 7329
--------------------------------------------------
Eachpanelshouldbereadlefttoright,toptobottom ( L e f t )Consecutivesamplesfrom
GibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset Consecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformed
inadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticratherthanrawvisual
features,butitisstilldiﬃcultfortheGibbschaintotransitionfromonemodeofthe
distributiontoanother,forexamplebychangingthedigitidentity.Consecutive ( R i g h t )
ancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsampling
generateseachsampleindependentlyfromtheothers,thereisnomixingproblem rapidly,wewouldlike p m o de l( h x|)tohaveveryhighentropy.However,fromthe
pointofviewoflearningausefulrepresentationof h,wewouldlike htoencode
enoughinformationabout xtoreconstructitwell,whichimpliesthat hand x
shouldhaveveryhighmutualinformation Thesetwogoalsareatoddswitheach
other.Weoftenlearngenerativemodelsthatverypreciselyencode xinto hbut
arenotabletomixverywell.ThissituationarisesfrequentlywithBoltzmann
machines—thesharperthedistributionaBoltzmannmachinelearns,theharder
itisforaMarkovchainsamplingfromthemodeldistributiontomixwell.This
problemisillustratedinﬁgure.17.2
AllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinterest
hasamanifoldstructurewithaseparatemanifoldforeachclass:thedistribution
isconcentratedaroundmanymodesandthesemodesareseparatedbyvastregions
ofhighenergy.Thistypeofdistributioniswhatweexpectinmanyclassiﬁcation
problemsandwouldmakeMCMCmethodsconvergeveryslowlybecauseofpoor
mixingbetweenmodes 6 0 2
CHAPTER17.MONTECARLOMETHODS
1 7 1 T em p eri n g t o Mi x b et w een Mo d es
Whenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsof
lowprobability,itisdiﬃculttomixbetweenthediﬀerentmodesofthedistribution Severaltechniquesforfastermixingarebasedonconstructingalternativeversions
ofthetargetdistributioninwhichthepeaksarenotashighandthesurrounding
valleysarenotaslow.Energy-basedmodelsprovideaparticularlysimplewayto
doso.Sofar,wehavedescribedanenergy-basedmodelasdeﬁningaprobability
distribution
p E () exp( x∝ −()) x (17.25)
Energy-basedmodelsmaybeaugmentedwithanextraparameter βcontrolling
howsharplypeakedthedistributionis:
p β() exp( ()) x∝ − β E x (17.26)
The βparameterisoftendescribedasbeingthereciprocalofthetemperature,
reﬂectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthe
temperaturefallstozeroandrisestoinﬁnity,theenergy-basedmodelbecomes β
deterministic.Whenthetemperaturerisestoinﬁnityand βfallstozero,the
distribution(fordiscrete)becomesuniform x
Typically,amodelistrainedtobeevaluatedat β= 1.However,wecanmake
useofothertemperatures,particularlythosewhere β <1.Temperingisageneral
strategyofmixingbetweenmodesof p 1rapidlybydrawingsampleswith β <1
Markovchainsbasedontemperedtransitions(,)temporarily Neal1994
samplefromhigher-temperaturedistributionsinordertomixtodiﬀerentmodes,
thenresumesamplingfromtheunittemperaturedistribution.Thesetechniques
havebeenappliedtomodelssuchasRBMs (Salakhutdinov2010,).Another
approachistouseparalleltempering(,),inwhichtheMarkovchain Iba2001
simulatesmanydiﬀerentstatesinparallel,atdiﬀerenttemperatures.Thehighest
temperaturestatesmixslowly,whilethelowesttemperaturestates,attemperature
1,provideaccuratesamplesfromthemodel.Thetransitionoperatorincludes
stochasticallyswappingstatesbetweentwodiﬀerenttemperaturelevels,sothata
suﬃcientlyhigh-probabilit ysamplefromahigh-temperatureslotcanjumpintoa
lowertemperatureslot.ThisapproachhasalsobeenappliedtoRBMs(Desjardins
e t a l Althoughtemperingisapromisingapproach,at 2010
thispointithasnotallowedresearcherstomakeastrongadvanceinsolvingthe
challengeofsamplingfromcomplexEBMs.Onepossiblereasonisthatthere
arecriticaltemperaturesaroundwhichthetemperaturetransitionmustbe
veryslow(asthetemperatureisgraduallyreduced)inorderfortemperingtobe
eﬀective 6 0 3
CHAPTER17.MONTECARLOMETHODS
1 7 2 D ep t h Ma y Hel p Mi xi n g
Whendrawingsamplesfromalatentvariablemodel p( h x ,),wehaveseenthatif
p( h x|)encodes xtoowell,thensamplingfrom p( x h|)willnotchange xvery
muchandmixingwillbepoor.Onewaytoresolvethisproblemistomake hbea
deeprepresentation,thatencodesintoinsuchawaythataMarkovchainin x h
thespaceof hcanmixmoreeasily.Manyrepresentationlearningalgorithms,such
asautoencodersandRBMs,tendtoyieldamarginaldistributionover hthatis
moreuniformandmoreunimodalthantheoriginaldatadistributionover x.Itcan
bearguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusing
alloftheavailablerepresentationspace,becauseminimizingreconstructionerror
overthetrainingexampleswillbebetterachievedwhendiﬀerenttrainingexamples
areeasilydistinguishablefromeachotherin h-space,andthuswellseparated Bengio2013a e t a l .()observedthatdeeperstacksofregularizedautoencodersor
RBMsyieldmarginaldistributionsinthetop-level h-spacethatappearedmore
spreadoutandmoreuniform,withlessofagapbetweentheregionscorresponding
todiﬀerentmodes(categories,intheexperiments).TraininganRBMinthat
higher-levelspaceallowedGibbssamplingtomixfasterbetweenmodes.Itremains
howeverunclearhowtoexploitthisobservationtohelpbettertrainandsample
fromdeepgenerativemodels Despitethediﬃcultyofmixing,MonteCarlotechniquesareusefulandare
oftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfront
theintractablepartitionfunctionofundirectedmodels,discussednext 6 0 4
C h a p t e r 1 8
C on f ron t i n g t h e P art i t i on
F u n ct i on
Insectionwesawthatmanyprobabilisticmodels(commonlyknownasundi- 16.2.2
rectedgraphicalmodels)aredeﬁnedbyanunnormalized probabilitydistribution
˜ p(x; θ).Wemustnormalize ˜ pbydividingbyapartitionfunction Z( θ)inorderto
obtainavalidprobabilitydistribution:
p(;) =x θ1
Z() θ˜ p (;)x θ (18.1)
Thepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscrete
variables)overtheunnormalized probabilityofallstates:

˜ p d() x x (18.2)
or 
x˜ p .() x (18.3)
Thisoperationisintractableformanyinterestingmodels Aswewillseeinchapter,severaldeeplearningmodelsaredesignedto 20
haveatractablenormalizingconstant,oraredesignedtobeusedinwaysthatdo
notinvolvecomputing p(x)atall However,othermodelsdirectlyconfrontthe
challengeofintractablepartitionfunctions.Inthischapter,wedescribetechniques
usedfortrainingandevaluatingmodelsthathaveintractablepartitionfunctions 605
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
18.1TheLog-LikelihoodGradient
What makes learning undirectedmodels bymaximumlikelihood particularly
diﬃcultisthatthepartitionfunctiondependsontheparameters.Thegradientof
thelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothe
gradientofthepartitionfunction:
∇ θlog(;) = px θ ∇ θlog ˜ p(;)x θ−∇ θlog() Z θ .(18.4)
Thisisawell-knowndecompositionintothe p o si t i v e phaseand negat i v e
phaseoflearning Formostundirectedmodelsofinterest,thenegativephaseisdiﬃcult.Models
withnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypically
haveatractablepositivephase.Thequintessentialexampleofamodelwitha
straightforwardpositivephaseanddiﬃcultnegativephaseistheRBM,whichhas
hiddenunitsthatareconditionallyindependentfromeachothergiventhevisible
units.Thecasewherethepositivephaseisdiﬃcult,withcomplicatedinteractions
betweenlatentvariables,isprimarilycoveredinchapter.Thischapterfocuses 19
onthediﬃcultiesofthenegativephase Letuslookmorecloselyatthegradientof: log Z
∇ θlog Z (18.5)
=∇ θ Z
Z(18.6)
=∇ θ
x˜ p()x
Z(18.7)
=
x∇ θ˜ p()x
Z

============================================================

=== CHUNK 156 ===
Palavras: 357
Caracteres: 4848
--------------------------------------------------
(18.8)
Formodelsthatguarantee p(x) >0forallx,wecansubstitute exp(log ˜ p())x
for˜ p()x:
x∇ θexp(log ˜ p())x
Z(18.9)
=
xexp(log ˜ p())x∇ θlog ˜ p()x
Z(18.10)
=
x˜ p()x∇ θlog ˜ p()x
Z(18.11)
=
xp()x∇ θlog ˜ p()x (18.12)
606
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
= E x x ∼ p ( )∇ θlog ˜ p .()x (18.13)
Thisderivationmadeuseofsummationoverdiscrete x,butasimilarresult
appliesusingintegrationovercontinuous x.Inthecontinuousversionofthe
derivation,weuseLeibniz’srulefordiﬀerentiationundertheintegralsigntoobtain
theidentity
∇ θ
˜ p d()x x=
∇ θ˜ p d ()x x (18.14)
Thisidentityisapplicableonlyundercertainregularityconditionson˜ pand∇ θ˜ p(x) Inmeasuretheoreticterms,theconditionsare:(i)Theunnormalized distribution˜ p
mustbeaLebesgue-integrablefunctionof xforeveryvalueof θ;(ii)Thegradient
∇ θ˜ p(x)mustexistforall θandalmostall x;(iii)Theremustexistanintegrable
function R( x)thatbounds ∇ θ˜ p(x)inthesensethatmax i|∂
∂ θ i˜ p(x)|≤ R( x)forall
θandalmostall x.Fortunately,mostmachinelearningmodelsofinteresthave
theseproperties Thisidentity
∇ θlog = Z E x x ∼ p ( )∇ θlog ˜ p()x (18.15)
isthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizing
thelikelihoodofmodelswithintractablepartitionfunctions TheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitive
frameworkinwhichwecanthinkofboththepositivephaseandthenegative
phase.Inthepositivephase,weincreaselog ˜ p(x)for xdrawnfromthedata.In
thenegativephase,wedecreasethepartitionfunctionbydecreasinglog ˜ p(x) drawn
fromthemodeldistribution Inthedeeplearningliterature,itiscommontoparametrize log ˜ pintermsof
anenergyfunction(equation).Inthiscase,wecaninterpretthepositive 16.7
phaseaspushingdownontheenergyoftrainingexamplesandthenegativephase
aspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedin
ﬁgure.18.1
18.2StochasticMaximumLikelihoodandContrastive
Divergence
Thenaivewayofimplementing equation istocomputeitbyburningin 18.15
asetofMarkovchainsfromarandominitialization everytimethegradientis
needed.Whenlearningisperformedusingstochasticgradientdescent,thismeans
thechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe
607
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
trainingprocedurepresentedinalgorithm .Thehighcostofburninginthe 18.1
Markovchainsintheinnerloopmakesthisprocedurecomputationally infeasible,
butthisprocedureisthestartingpointthatothermorepracticalalgorithmsaim
toapproximate 1AnaiveMCMCalgorithmformaximizingthelog-likelihood
withanintractablepartitionfunctionusinggradientascent Set,thestepsize,toasmallpositivenumber 
Set k,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100to
trainanRBMonasmallimagepatch whi l enotconverged do
Sampleaminibatchofexamples m {x( 1 ), ,x( ) m}fromthetrainingset g←1
mm
i = 1∇ θlog ˜ p(x( ) i;) θ Initializeasetof msamples {˜x( 1 ), ,˜x( ) m}torandomvalues(e.g.,from
auniformornormaldistribution,orpossiblyadistributionwithmarginals
matchedtothemodel’smarginals) f o r do i k = 1to
f o r do j m = 1to
˜x( ) j←gibbs_update(˜x( ) j) e nd f o r
e nd f o r
gg←−1
mm
i = 1∇ θlog ˜ p(˜x( ) i;) θ θ θ← +  .g
e nd whi l e
WecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachieve
balancebetweentwoforces,onepushinguponthemodeldistributionwherethe
dataoccurs,andanotherpushingdownonthemodeldistributionwherethemodel
samplesoccur.Figureillustratesthisprocess.Thetwoforcescorrespondto 18.1
maximizing log ˜ pandminimizing log Z.Severalapproximations tothenegative
phasearepossible.Eachoftheseapproximationscanbeunderstoodasmaking
thenegativephasecomputationally cheaperbutalsomakingitpushdowninthe
wronglocations Becausethenegativephaseinvolvesdrawingsamplesfromthemodel’sdistri-
bution,wecanthinkofitasﬁndingpointsthatthemodelbelievesinstrongly Becausethenegativephaseactstoreducetheprobabilityofthosepoints,they
aregenerallyconsideredtorepresentthemodel’sincorrectbeliefsabouttheworld Theyarefrequentlyreferredtointheliteratureas“hallucinations” or“fantasy
particles.”Infact,thenegativephasehasbeenproposedasapossibleexplanation
608
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
xp(x )The p o s i t i v e p h a s e
p mo d e l ( ) x
p d a t a ( ) x
xp(x )The n eg a t i v e p h a s e
p mo d e l ( ) x
p d a t a ( ) x
Figure18.1:Theviewofalgorithmashavinga“positivephase”and“negativephase.” 18.1
( L e f t )Inthepositivephase,wesamplepointsfromthedatadistribution,andpushupon
theirunnormalizedprobability.Thismeanspointsthatarelikelyinthedatagetpushed
uponmore ( R i g h t )Inthenegativephase,wesamplepointsfromthemodeldistribution,
andpushdownontheirunnormalizedprobability.Thiscounteractsthepositivephase’s
tendencytojustaddalargeconstanttotheunnormalizedprobabilityeverywhere.When
thedatadistributionandthemodeldistributionareequal,thepositivephasehasthe
samechancetopushupatapointasthenegativephasehastopushdown.Whenthis
occurs,thereisnolongeranygradient(inexpectation)andtrainingmustterminate

============================================================

=== CHUNK 157 ===
Palavras: 355
Caracteres: 8421
--------------------------------------------------
fordreaminginhumansandotheranimals(CrickandMitchison1983,),theidea
beingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollows
thegradientoflog ˜ pwhileexperiencingrealeventswhileawakeandfollowsthe
negativegradientoflog ˜ ptominimize log Zwhilesleepingandexperiencingevents
sampledfromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedto
describealgorithmswithapositiveandnegativephase,butithasnotbeenproven
tobecorrectwithneuroscientiﬁcexperiments.Inmachinelearningmodels,itis
usuallynecessarytousethepositiveandnegativephasesimultaneously,rather
thaninseparatetimeperiodsofwakefulnessandREMsleep Aswewillseein
section,othermachinelearningalgorithmsdrawsamplesfromthemodel 19.5
distributionforotherpurposesandsuchalgorithmscouldalsoprovideanaccount
forthefunctionofdreamsleep Giventhisunderstandingoftheroleofthepositiveandnegativephaseof
learning,wecanattempttodesignalessexpensivealternativetoalgorithm .18.1
ThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkov
chainsfromarandominitialization ateachstep Anaturalsolutionistoinitialize
theMarkovchainsfromadistributionthatisveryclosetothemodeldistribution,
609
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
sothattheburninoperationdoesnottakeasmanysteps The c o n t r ast i v e di v e r g e nc e(CD,orCD- ktoindicateCDwith kGibbssteps)
algorithminitializestheMarkovchainateachstepwithsamplesfromthedata
distribution(Hinton20002010,,).Thisapproachispresentedasalgorithm .18.2
Obtainingsamplesfromthedatadistributionisfree,becausetheyarealready
availableinthedataset.Initially,thedatadistributionisnotclosetothemodel
distribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositive
phasecanstillaccuratelyincreasethemodel’sprobabilityofthedata.Afterthe
positivephasehashadsometimetoact,themodeldistributionisclosertothe
datadistribution,andthenegativephasestartstobecomeaccurate 2Thecontrastivedivergencealgorithm,usinggradientascentas
theoptimization procedure Set,thestepsize,toasmallpositivenumber 
Set k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling
from p(x; θ)tomixwheninitializedfrom pdata.Perhaps1-20totrainanRBM
onasmallimagepatch whi l enotconverged do
Sampleaminibatchofexamples m {x( 1 ), ,x( ) m}fromthetrainingset g←1
mm
i = 1∇ θlog ˜ p(x( ) i;) θ f o r do i m = 1to
˜x( ) i←x( ) i e nd f o r
f o r do i k = 1to
f o r do j m = 1to
˜x( ) j←gibbs_update(˜x( ) j) e nd f o r
e nd f o r
gg←−1
mm
i = 1∇ θlog ˜ p(˜x( ) i;) θ θ θ← +  .g
e nd whi l e
Ofcourse,CDisstillanapproximation tothecorrectnegativephase.The
mainwaythatCDqualitativelyfailstoimplementthecorrectnegativephase
isthatitfailstosuppressregionsofhighprobabilitythatarefarfromactual
trainingexamples.Theseregionsthathavehighprobabilityunderthemodelbut
lowprobabilityunderthedatageneratingdistributionarecalled spur i o us m o des Figureillustrateswhythishappens.Essentially,itisbecausemodesinthe 18.2
modeldistributionthatarefarfromthedatadistributionwillnotbevisitedby
610
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
xp(x )p mo d e l ( ) x
p d a t a ( ) x
Figure18.2: Anillustrationofhowthenegativephaseofcontrastivedivergence(algo-
rithm)canfailtosuppressspuriousmodes.Aspuriousmodeisamodethatis 18.2
presentinthemodeldistributionbutabsentinthedatadistribution.Becausecontrastive
divergenceinitializesitsMarkovchainsfromdatapointsandrunstheMarkovchainfor
onlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefarfromthedata
points.Thismeansthatwhensamplingfromthemodel,wewillsometimesgetsamples
thatdonotresemblethedata.Italsomeansthatduetowastingsomeofitsprobability
massonthesemodes,themodelwillstruggletoplacehighprobabilitymassonthecorrect
modes.Forthepurposeofvisualization,thisﬁgureusesasomewhatsimpliﬁedconcept
ofdistance—thespuriousmodeisfarfromthecorrectmodealongthenumberlinein
R.ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswithasingle x
variablein R.Formostdeepprobabilisticmodels,theMarkovchainsarebasedonGibbs
samplingandcanmakenon-localmovesofindividualvariablesbutcannotmoveallof
thevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsidertheedit
distancebetweenmodes,ratherthantheEuclideandistance.However,editdistanceina
highdimensionalspaceisdiﬃculttodepictina2-Dplot Markovchainsinitializedattrainingpoints,unlessisverylarge k
Carreira-PerpiñanandHinton2005()showed experimentallythatthe CD
estimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatit
convergestodiﬀerentpointsthanthemaximumlikelihoodestimator.Theyargue
thatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitialize
amodelthatcouldlaterbeﬁne-tunedviamoreexpensiveMCMCmethods.Bengio
andDelalleau2009()showedthatCDcanbeinterpretedasdiscardingthesmallest
termsofthecorrectMCMCupdategradient,whichexplainsthebias CDisusefulfortrainingshallowmodelslikeRBMs.Thesecaninturnbe
stackedtoinitializedeepermodelslikeDBNsorDBMs However,CDdoesnot
providemuchhelpfortrainingdeepermodelsdirectly.Thisisbecauseitisdiﬃcult
611
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
toobtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethe
hiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannot
solvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstill
needtoburninaMarkovchainsamplingfromthedistributionoverthehidden
unitsconditionedonthosevisiblesamples TheCDalgorithmcanbethoughtofaspenalizingthemodelforhavinga
Markovchainthatchangestheinputrapidlywhentheinputcomesfromthedata ThismeanstrainingwithCDsomewhatresemblesautoencodertraining.Even
thoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbe
usefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecause
theearliestmodelsinthestackareencouragedtocopymoreinformationupto
theirlatentvariables,therebymakingitavailabletothelatermodels.Thisshould
bethoughtofmoreofasanoften-exploitable sideeﬀectofCDtrainingratherthan
aprincipleddesignadvantage SutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthe
gradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever,
butinpracticethisisnotaseriousproblem AdiﬀerentstrategythatresolvesmanyoftheproblemswithCDistoinitial-
izetheMarkovchainsateachgradientstepwiththeirstatesfromtheprevious
gradientstep.Thisapproachwasﬁrstdiscoveredunderthename st o c hast i c m ax -
i m um l i k e l i ho o d(SML)intheappliedmathematics andstatisticscommunity
(Younes1998,)andlaterindependently rediscoveredunderthename p e r si st e n t
c o n t r ast i v e di v e r g e n c e(PCD,orPCD- ktoindicatetheuseof kGibbssteps
perupdate)inthedeeplearningcommunity(,).Seealgorithm Tieleman2008 18.3
Thebasicideaofthisapproachisthat,solongasthestepstakenbythestochastic
gradientalgorithmaresmall,thenthemodelfromthepreviousstepwillbesimilar
tothemodelfromthecurrentstep.Itfollowsthatthesamplesfromtheprevious
model’sdistributionwillbeveryclosetobeingfairsamplesfromthecurrent
model’sdistribution,soaMarkovchaininitializedwiththesesampleswillnot
requiremuchtimetomix BecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearning
process,ratherthanrestartedateachgradientstep,thechainsarefreetowander
farenoughtoﬁndallofthemodel’smodes.SMListhusconsiderablymore
resistanttoformingmodelswithspuriousmodesthanCDis.Moreover,because
itispossibletostorethestateofallofthesampledvariables,whethervisibleor
latent,SMLprovidesaninitialization pointforboththehiddenandvisibleunits CDisonlyabletoprovideaninitialization forthevisibleunits,andtherefore
requiresburn-infordeepmodels.SMLisabletotraindeepmodelseﬃciently 612
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
Marlin 2010 e t a l .()comparedSMLtomanyoftheothercriteriapresentedin
thischapter.TheyfoundthatSMLresultsinthebesttestsetlog-likelihoodfor
anRBM,andthatiftheRBM’shiddenunitsareusedasfeaturesforanSVM
classiﬁer,SMLresultsinthebestclassiﬁcationaccuracy SMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithm
canmovethemodelfasterthantheMarkovchaincanmixbetweensteps.This
canhappenif kistoosmallor istoolarge.Thepermissiblerangeofvaluesis
unfortunately highlyproblem-dependent.Thereisnoknownwaytotestformally
whetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearning
rateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeable
toobservethatthereismuchmorevarianceinthenegativephasesamplesacross
gradientstepsratherthanacrossdiﬀerentMarkovchains.Forexample,amodel
trainedonMNISTmightsampleexclusively7sononestep.Thelearningprocess
willthenpushdownstronglyonthemodecorrespondingto7s,andthemodel
mightsampleexclusively9sonthenextstep

============================================================

=== CHUNK 158 ===
Palavras: 365
Caracteres: 6195
--------------------------------------------------
3Thestochasticmaximumlikelihood/persistentcontrastive
divergencealgorithmusinggradientascentastheoptimization procedure Set,thestepsize,toasmallpositivenumber 
Set k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling
from p(x; θ+ g)toburnin,startingfromsamplesfrom p(x; θ).Perhaps1for
RBMonasmallimagepatch,or5-50foramorecomplicatedmodellikeaDBM Initializeasetof msamples {˜x( 1 ), ,˜x( ) m}torandomvalues(e.g.,froma
uniformornormaldistribution,orpossiblyadistributionwithmarginalsmatched
tothemodel’smarginals) whi l enotconverged do
Sampleaminibatchofexamples m {x( 1 ), ,x( ) m}fromthetrainingset g←1
mm
i = 1∇ θlog ˜ p(x( ) i;) θ f o r do i k = 1to
f o r do j m = 1to
˜x( ) j←gibbs_update(˜x( ) j) e nd f o r
e nd f o r
gg←−1
mm
i = 1∇ θlog ˜ p(˜x( ) i;) θ θ θ← +  .g
e nd whi l e
Caremustbetakenwhenevaluatingthesamplesfromamodeltrainedwith
SML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain
613
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
initializedfromarandomstartingpointafterthemodelisdonetraining.The
samplespresentinthepersistentnegativechainsusedfortraininghavebeen
inﬂuencedbyseveralrecentversionsofthemodel,andthuscanmakethemodel
appeartohavegreatercapacitythanitactuallydoes BerglundandRaiko2013()performedexperimentstoexaminethebiasand
varianceintheestimateofthegradientprovidedbyCDandSML.CDprovesto
havelowervariancethantheestimatorbasedonexactsampling.SMLhashigher
variance.ThecauseofCD’slowvarianceisitsuseofthesametrainingpoints
inboththepositiveandnegativephase.Ifthenegativephaseisinitializedfrom
diﬀerenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedon
exactsampling AllofthesemethodsbasedonusingMCMCtodrawsamplesfromthemodel
caninprinciplebeusedwithalmostanyvariantofMCMC.Thismeansthat
techniquessuchasSMLcanbeimprovedbyusinganyoftheenhancedMCMC
techniquesdescribedinchapter,suchasparalleltempering( , 17 Desjardins e t a l 2010Cho2010; e t a l .,) Oneapproachtoacceleratingmixingduringlearningreliesnotonchanging
theMonteCarlosamplingtechnologybutratheronchangingtheparametrization
ofthemodelandthecostfunction F ast P CDorFPCD( , TielemanandHinton
2009)involvesreplacingtheparameters θofatraditionalmodelwithanexpression
θ θ= ( )slow+ θ( )fast (18.16)
Therearenowtwiceasmanyparametersasbefore,andtheyareaddedtogether
element-wisetoprovidetheparametersusedbytheoriginalmodeldeﬁnition.The
fastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowing
ittoadaptrapidlyinresponsetothenegativephaseoflearningandpushthe
Markovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,though
thiseﬀectonlyoccursduringlearningwhilethefastweightsarefreetochange Typicallyonealsoappliessigniﬁcantweightdecaytothefastweights,encouraging
themtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslong
enoughtoencouragetheMarkovchaintochangemodes OnekeybeneﬁttotheMCMC-basedmethodsdescribedinthissectionisthat
theyprovideanestimateofthegradientoflog Z,andthuswecanessentially
decomposetheproblemintothelog ˜ pcontributionandthelog Zcontribution Wecanthenuseanyothermethodtotacklelog ˜ p(x),andjustaddournegative
phasegradientontotheothermethod’sgradient.Inparticular,thismeansthat
ourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon
˜ p.Mostoftheothermethodsofdealingwith log Zpresentedinthischapterare
614
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
incompatible withbound-basedpositivephasemethods 18.3Pseudolikelihood
MonteCarloapproximations tothepartitionfunctionanditsgradientdirectly
confrontthepartitionfunction.Otherapproachessidesteptheissue,bytraining
themodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesare
basedontheobservationthatitiseasytocomputeratiosofprobabilities inan
undirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsin
boththenumeratorandthedenominator oftheratioandcancelsout:
p()x
p()y=1
Z˜ p()x
1
Z˜ p()y=˜ p()x
˜ p()y (18.17)
Thepseudolikelihoodisbasedontheobservationthatconditionalprobabilities
takethisratio-basedform,andthuscanbecomputedwithoutknowledgeofthe
partitionfunction.Supposethatwepartition xintoa,bandc,where acontains
thevariableswewanttoﬁndtheconditionaldistributionover,bcontainsthe
variableswewanttoconditionon,andccontainsthevariablesthatarenotpart
ofourquery p( ) = ab|p ,(ab)
p()b=p ,(ab)
a c , p , ,(abc)=˜ p ,(ab)
a c ,˜ p , ,(abc).(18.18)
Thisquantityrequiresmarginalizing outa,whichcanbeaveryeﬃcientoperation
providedthataandcdonotcontainverymanyvariables.Intheextremecase,a
canbeasinglevariableandccanbeempty,makingthisoperationrequireonlyas
manyevaluationsof˜ pastherearevaluesofasinglerandomvariable Unfortunately,inordertocomputethelog-likelihood,weneedtomarginalize
outlargesetsofvariables.Ifthereare nvariablestotal,wemustmarginalizeaset
ofsize.Bythechainruleofprobability, n−1
log() = log( px p x 1)+log( p x 2| x 1)+ +( ··· p x n|x 1 : 1 n −) .(18.19)
Inthiscase,wehavemade amaximallysmall,butccanbeaslargeasx 2 : n.What
ifwesimplymovecintobtoreducethecomputational cost?Thisyieldsthe
pseudolik e l i ho o d(,)objectivefunction,basedonpredictingthevalue Besag1975
offeature x igivenalloftheotherfeatures x − i:
n 
i = 1log( p x i| x − i) (18.20)
615
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
Ifeachrandomvariablehas kdiﬀerentvalues,thisrequiresonly k n×evaluations
of˜ ptocompute,asopposedtothe knevaluationsneededtocomputethepartition
function Thismaylooklikeanunprincipled hack,butitcanbeproventhatestimation
bymaximizingthepseudolikelihoodisasymptoticallyconsistent(,).Mase1995
Ofcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit,
pseudolikelihoodmaydisplaydiﬀerentbehaviorfromthemaximumlikelihood
estimator Itispossibletotradecomputational complexityfordeviationfrommaximum
likelihoodbehaviorbyusingthe g e ner al i z e d pseudolikel i h o o destimator(Huang
andOgata2002,).Thegeneralizedpseudolikelihoodestimatoruses mdiﬀerentsets
S( ) i, i= 1 , , mofindicesofvariablesthatappeartogetherontheleftsideofthe
conditioningbar.Intheextremecaseof m= 1and S( 1 )=1 , , nthegeneralized
pseudolikelihoodrecoversthelog-likelihood Intheextremecaseof m= nand
S( ) i={} i,thegeneralizedpseudolikelihoodrecoversthepseudolikelihood.The
generalizedpseudolikelihoodobjectivefunctionisgivenby
m 
i = 1log( pxS() i|x− S() i)

============================================================

=== CHUNK 159 ===
Palavras: 359
Caracteres: 7487
--------------------------------------------------
(18.21)
Theperformanceofpseudolikelihood-basedapproachesdependslargelyonhow
themodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthat
requireagoodmodelofthefulljoint p(x),suchasdensityestimationandsampling However,itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonly
theconditionaldistributionsusedduringtraining,suchasﬁllinginsmallamounts
ofmissingvalues.Generalizedpseudolikelihoodtechniquesareespeciallypowerfulif
thedatahasregularstructurethatallowsthe Sindexsetstobedesignedtocapture
themostimportantcorrelationswhileleavingoutgroupsofvariablesthatonly
havenegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidely
separatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihood
canbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow S
Oneweaknessofthepseudolikelihoodestimatoristhatitcannotbeusedwith
otherapproximationsthatprovideonlyalowerboundon˜ p(x),suchasvariational
inference,whichwillbecoveredinchapter.Thisisbecause 19 ˜ pappearsinthe
denominator Alowerboundonthedenominator providesonlyanupperboundon
theexpressionasawhole,andthereisnobeneﬁttomaximizinganupperbound Thismakesitdiﬃculttoapplypseudolikelihoodapproachestodeepmodelssuch
asdeepBoltzmannmachines,sincevariationalmethodsareoneofthedominant
approachestoapproximately marginalizing outthemanylayersofhiddenvariables
616
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
thatinteractwitheachother However,pseudolikelihoodisstillusefulfordeep
learning,becauseitcanbeusedtotrainsinglelayermodels,ordeepmodelsusing
approximateinferencemethodsthatarenotbasedonlowerbounds PseudolikelihoodhasamuchgreatercostpergradientstepthanSML,dueto
itsexplicitcomputationofalloftheconditionals.However,generalizedpseudo-
likelihoodandsimilarcriteriacanstillperformwellifonlyonerandomlyselected
conditionaliscomputedperexample(Goodfellow2013b e t a l .,),therebybringing
thecomputational costdowntomatchthatofSML Thoughthepseudolikelihoodestimatordoesnotexplicitlyminimize log Z,it
canstillbethoughtofashavingsomethingresemblinganegativephase.The
denominators ofeachconditionaldistributionresultinthelearningalgorithm
suppressingtheprobabilityofallstatesthathaveonlyonevariablediﬀeringfrom
atrainingexample SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic
eﬃciencyofpseudolikelihood 18.4ScoreMatchingandRatioMatching
Scorematching(,)providesanotherconsistentmeansoftraininga Hyvärinen2005
modelwithoutestimating Zoritsderivatives.Thenamescorematchingcomes
fromterminologyinwhichthederivativesofalogdensitywithrespecttoits
argument,∇ xlog p( x),arecalledits sc o r e.Thestrategyusedbyscorematching
istominimizetheexpectedsquareddiﬀerencebetweenthederivativesofthe
model’slogdensitywithrespecttotheinputandthederivativesofthedata’slog
densitywithrespecttotheinput:
L ,( x θ) =1
2||∇ xlog p m o de l(;) x θ−∇ xlog pdata() x||2
2(18.22)
J() = θ1
2E pdata ( ) x L ,( x θ) (18.23)
θ∗= min
θJ() θ (18.24)
Thisobjectivefunctionavoidsthediﬃcultiesassociatedwithdiﬀerentiating
thepartitionfunction Zbecause Zisnotafunctionof xandtherefore ∇ x Z= 0 Initially,scorematchingappearstohaveanewdiﬃculty: comput ingthescore
ofthedatadistributionrequiresknowledgeofthetruedistributiongenerating
thetrainingdata, pdata.Fortunately,minimizingtheexpectedvalueofis L ,( x θ)
617
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
equivalenttominimizingtheexpectedvalueof
˜ L ,( x θ) =n 
j = 1
∂2
∂ x2
jlog p m o de l(;)+ x θ1
2∂
∂ x jlog p m o de l(;) x θ2
(18.25)
whereisthedimensionalityof n x
Becausescorematchingrequirestakingderivativeswithrespecttox,itisnot
applicabletomodelsofdiscretedata.However,thelatentvariablesinthemodel
maybediscrete Likethepseudolikelihood,scorematchingonlyworkswhenweareableto
evaluate log ˜ p(x)anditsderivativesdirectly.Itisnotcompatiblewithmethods
thatonlyprovidealowerboundonlog ˜ p(x),becausescorematchingrequires
thederivativesandsecondderivativesoflog ˜ p(x)andalowerboundconveysno
informationaboutitsderivatives.Thismeansthatscorematchingcannotbe
appliedtoestimatingmodelswithcomplicatedinteractionsbetweenthehidden
units,suchassparsecodingmodelsordeepBoltzmannmachines.Whilescore
matchingcanbeusedtopretraintheﬁrsthiddenlayerofalargermodel,ithas
notbeenappliedasapretrainingstrategyforthedeeperlayersofalargermodel Thisisprobablybecausethehiddenlayersofsuchmodelsusuallycontainsome
discretevariables Whilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbe
viewedasaversionofcontrastivedivergenceusingaspeciﬁckindofMarkovchain
(,).TheMarkovchaininthiscaseisnotGibbssampling,but Hyvärinen2007a
ratheradiﬀerentapproachthatmakeslocalmovesguidedbythegradient.Score
matchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthe
localmovesapproacheszero Lyu2009()generalizedscorematchingtothediscretecase(butmadeanerror
intheirderivationthatwascorrectedby ()) Marlin e t a l .2010Marlin e t a l ()foundthat 2010 g e ner al i z e d sc o r e m at c hi ng(GSM)doesnotworkinhigh
dimensionaldiscretespaceswheretheobservedprobabilityofmanyeventsis0 Amoresuccessfulapproachtoextendingthebasicideasofscorematching
todiscretedatais r at i o m at c hi ng(,).Ratiomatchingapplies Hyvärinen2007b
speciﬁcallytobinarydata.Ratiomatchingconsistsofminimizingtheaverageover
examplesofthefollowingobjectivefunction:
L( )RM() = x θ ,n 
j = 1
1
1+pmodel ( ; ) x θ
pmodel ( ( ) ) ; ) f x , j θ
2
,(18.26)
618
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
where returnswiththebitatpositionﬂipped.Ratiomatchingavoids f , j( x) x j
thepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:ina
ratiooftwoprobabilities, thepartitionfunctioncancelsout () Marlin e t a l .2010
foundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMinterms
oftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages Likethepseudolikelihoodestimator,ratiomatchingrequires nevaluationsof˜ p
perdatapoint,makingitscomputational costperupdateroughly ntimeshigher
thanthatofSML Aswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofas
pushingdownonallfantasystatesthathaveonlyonevariablediﬀerentfroma
trainingexample.Sinceratiomatchingappliesspeciﬁcallytobinarydata,this
meansthatitactsonallfantasystateswithinHammingdistance1ofthedata Ratiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensional
sparsedata,suchaswordcountvectors.Thiskindofdataposesachallengefor
MCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentin
denseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodel
haslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio
()overcamethisissuebydesigninganunbiasedstochasticapproximation to 2013
ratiomatching.Theapproximation evaluatesonlyarandomlyselectedsubsetof
thetermsoftheobjective,anddoesnotrequirethemodeltogeneratecomplete
fantasysamples SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic
eﬃciencyofratiomatching 18.5DenoisingScoreMatching
Insomecaseswemaywishtoregularizescorematching,byﬁttingadistribution
psmoothed() = x
pdata()( ) y q x y| d y (18.27)
ratherthanthetrue pdata.Thedistribution q( x y|) isacorruptionprocess,usually
onethatformsbyaddingasmallamountofnoiseto x y
Denoisingscorematchingisespeciallyusefulbecauseinpracticeweusuallydo
nothaveaccesstothetrue pdatabutratheronlyanempiricaldistributiondeﬁned
bysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,make
p m o de lintoasetofDiracdistributionscenteredonthetrainingpoints.Smoothing
by qhelpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty
619
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
describedinsection

============================================================

=== CHUNK 160 ===
Palavras: 366
Caracteres: 5607
--------------------------------------------------
()introducedaprocedurefor 5.4.5KingmaandLeCun2010
performingregularizedscorematchingwiththesmoothingdistribution qbeing
normallydistributednoise Recallfromsectionthatseveralautoencodertrainingalgorithmsare 14.5.1
equivalenttoscorematchingordenoisingscorematching.Theseautoencoder
trainingalgorithmsare therefore a wayof overcomingthe partition function
problem 18.6Noise-ContrastiveEstimation
Mosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonot
provideanestimateofthepartitionfunction.SMLandCDestimateonlythe
gradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself Scorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothe
partitionfunctionaltogether Noi se - c o n t r ast i v e   e st i m a t i o n   ( N C E )(Gutmann and Hy varinen2010, )
takesadiﬀerentstrategy.Inthisapproach,theprobabilitydistributionestimated
bythemodelisrepresentedexplicitlyas
log p m o de l() = log ˜ x pmodel(;)+x θ c , (18.28)
where cisexplicitlyintroducedasanapproximationof−log Z( θ).Ratherthan
estimatingonly θ,thenoisecontrastiveestimationproceduretreats casjust
anotherparameterandestimates θand csimultaneously,usingthesamealgorithm
forboth.Theresulting log p m o de l(x)thusmaynotcorrespondexactlytoavalid
probabilitydistribution,butwillbecomecloserandclosertobeingvalidasthe
estimateofimproves c1
Suchanapproachwouldnotbepossibleusingmaximumlikelihoodasthe
criterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetoset
c c arbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution NCEworksbyreducingtheunsupervisedlearningproblemofestimating p(x)
tothatoflearningaprobabilisticbinaryclassiﬁerinwhichoneofthecategories
correspondstothedatageneratedbythemodel.Thissupervisedlearningproblem
isconstructedinsuchawaythatmaximumlikelihoodestimationinthissupervised
1NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisno
needtointroducetheextraparameter c.However,ithasgeneratedthemostinterestasameans
ofestimatingmodelswithdiﬃcultpartitionfunctions 620
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
learningproblemdeﬁnesanasymptoticallyconsistentestimatoroftheoriginal
problem Speciﬁcally,weintroduceaseconddistribution,the noi se di st r i but i o n pnoise(x) Thenoisedistributionshouldbetractabletoevaluateandtosamplefrom We
cannowconstructamodeloverbothxandanew,binaryclassvariable y.Inthe
newjointmodel,wespecifythat
pjoint(= 1) = y1
2, (18.29)
pjoint( = 1) = x| y p m o de l()x , (18.30)
and
pjoint( = 0) = x| y pnoise()x (18.31)
Inotherwords, yisaswitchvariablethatdetermineswhetherwewillgenerate x
fromthemodelorfromthenoisedistribution Wecanconstructasimilarjointmodeloftrainingdata.Inthiscase,the
switchvariabledetermineswhetherwedraw xfromthe dat aorfromthenoise
distribution.Formally, ptrain( y=1)=1
2, ptrain(x| y=1)= pdata(x), and
ptrain( = 0) = x| y pnoise()x Wecannowjustusestandardmaximumlikelihoodlearningonthe sup e r v i se d
learningproblemofﬁtting pjointto ptrain:
θ , c= argmax
θ , cE x , py ∼trainlog pjoint( ) y|x (18.32)
Thedistribution pjointisessentiallyalogisticregressionmodelappliedtothe
diﬀerenceinlogprobabilities ofthemodelandthenoisedistribution:
pjoint(= 1 ) = y |xp m o de l()x
p m o de l()+x pnoise()x(18.33)
=1
1+pnoise ( ) x
pmodel ( ) x(18.34)
=1
1+exp
logpnoise ( ) x
pmodel ( ) x (18.35)
= σ
−logpnoise()x
p m o de l()x
(18.36)
= (log σ p m o de l()log x− pnoise())x (18.37)
621
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
NCEisthussimpletoapplysolongaslog ˜ pmodeliseasytoback-propagate
through,and,asspeciﬁedabove, pnoiseiseasytoevaluate(inordertoevaluate
pjoint)andsamplefrom(inordertogeneratethetrainingdata) NCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables,
butcanworkwellevenifthoserandomvariablescantakeonahighnumberof
values.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditional
distributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu,
2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyone
word WhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomesless
eﬃcient.Thelogisticregressionclassiﬁercanrejectanoisesamplebyidentifying
anyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdown
greatlyafter p m o de lhaslearnedthebasicmarginalstatistics.Imaginelearninga
modelofimagesoffaces,usingunstructuredGaussiannoiseas pnoise.If p m o de l
learnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithout
havinglearnedanythingaboutotherfacialfeatures,suchasmouths Theconstraintthat pnoisemustbeeasytoevaluateandeasytosamplefrom
canbeoverlyrestrictive.When pnoiseissimple,mostsamplesarelikelytobetoo
obviouslydistinctfromthedatatoforce p m o de ltoimprovenoticeably Likescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalower
boundon˜ pisavailable.Suchalowerboundcouldbeusedtoconstructalower
boundon pjoint( y= 1|x),butitcanonlybeusedtoconstructanupperboundon
pjoint( y= 0|x),whichappearsinhalfthetermsoftheNCEobjective.Likewise,
alowerboundon pnoiseisnotuseful,becauseitprovidesonlyanupperboundon
pjoint(= 1 ) y |x Whenthemodeldistributioniscopiedtodeﬁneanewnoisedistributionbefore
eachgradientstep,NCEdeﬁnesaprocedurecalled se l f - c o n t r ast i v e e st i m at i o n,
whose expected gradientis equivalentto the expected gradientofmaximum
likelihood(,).ThespecialcaseofNCEwherethenoisesamples Goodfellow2014
are thosegenerated by themodel suggests thatmaximumlikelihood can be
interpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguish
realityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachieves
somereducedcomputational costbyonlyforcingthemodeltodistinguishreality
fromaﬁxedbaseline(thenoisemodel)

============================================================

=== CHUNK 161 ===
Palavras: 352
Caracteres: 4397
--------------------------------------------------
Usingthesupervisedtaskofclassifyingbetweentrainingsamplesandgenerated
samples(withthemodelenergyfunctionusedindeﬁningtheclassiﬁer)toprovide
agradientonthemodelwasintroducedearlierinvariousforms(Welling e t a l .,
2003bBengio2009;,) 622
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
Noise contrastiveestimation is basedon the idea that agood generative
modelshould be abletodistinguish datafromnoise.Aclosely relatedidea
isthat agood generativemodelshould beabletogenerate samples thatno
classiﬁercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks
(section).20.10.4
18.7EstimatingthePartitionFunction
Whilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneeding
tocomputetheintractablepartitionfunction Z( θ)associatedwithanundirected
graphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimating
thepartitionfunction Estimatingthepartitionfunctioncanbeimportantbecausewerequireitif
wewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantin
e v a l u a t i ngthemodel,monitoringtrainingperformance,andcomparingmodelsto
eachother Forexample,imaginewehavetwomodels:model M Adeﬁningaprobabil-
itydistribution p A(x; θ A)=1
Z A˜ p A(x; θ A)andmodelM Bdeﬁningaprobability
distribution p B(x; θ B)=1
Z B˜ p B(x; θ B).Acommonwaytocomparethemodels
istoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d testdataset.Supposethetestsetconsistsof mexamples { x( 1 ), , x( ) m}.If
i p A(x( ) i; θ A) >
i p B(x( ) i; θ B)orequivalentlyif

ilog p A(x( ) i; θ A)−
ilog p B(x( ) i; θ B) 0 > ,(18.38)
thenwesaythatM AisabettermodelthanM B(or,atleast,itisabettermodel
ofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately,
testingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction Unfortunately,equationseemstorequireevaluatingthelogprobabilitythat 18.38
themodelassignstoeachpoint,whichinturnrequiresevaluatingthepartition
function.Wecansimplifythesituationslightlybyre-arrangingequation18.38
intoaformwhereweneedtoknowonlythe r at i oofthetwomodel’spartition
functions:

ilog p A(x( ) i; θ A)−
ilog p B(x( ) i; θ B) =
i
log˜ p A(x( ) i; θ A)
˜ p B(x( ) i; θ B)
− mlogZ( θ A)
Z( θ B) (18.39)
623
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
Wecanthusdeterminewhether M AisabettermodelthanM Bwithoutknowing
thepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly,
wecanestimatethisratiousingimportancesampling,providedthatthetwomodels
aresimilar If,however,wewantedtocomputetheactualprobabilityofthetestdataunder
either M AorM B,wewouldneedtocomputetheactualvalueofthepartition
functions.Thatsaid,ifweknewtheratiooftwopartitionfunctions, r=Z ( θ B )
Z ( θ A ),
andweknewtheactualvalueofjustoneofthetwo,say Z( θ A),wecouldcompute
thevalueoftheother:
Z( θ B) = ( r Z θ A) =Z( θ B)
Z( θ A)Z( θ A) (18.40)
Asimplewaytoestimatethe partition functionistouse aMonteCarlo
methodsuchassimpleimportancesampling.Wepresenttheapproachinterms
ofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscrete
variablesbyreplacingtheintegralswithsummation.Weuseaproposaldistribution
p 0(x)=1
Z0˜ p 0(x)whichsupportstractablesamplingandtractableevaluationof
boththepartitionfunction Z 0andtheunnormalized distribution˜ p 0()x Z 1=
˜ p 1()x dx (18.41)
=p 0()x
p 0()x˜ p 1()x dx (18.42)
= Z 0
p 0()x˜ p 1()x
˜ p 0()xdx (18.43)
ˆ Z 1=Z 0
KK
k = 1˜ p 1(x( ) k)
˜ p 0(x( ) k)st: .x( ) k∼ p 0 (18.44)
Inthelastline,wemakeaMonteCarloestimator,ˆ Z 1,oftheintegralusingsamples
drawnfrom p 0(x)andthenweighteachsamplewiththeratiooftheunnormalized
˜ p 1andtheproposal p 0 Weseealsothatthisapproachallowsustoestimatetheratiobetweenthe
partitionfunctionsas
1
KK 
k = 1˜ p 1(x( ) k)
˜ p 0(x( ) k)st: (18.45)
Thisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedin
equation.18.39
624
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
Ifthedistribution p 0iscloseto p 1,equationcanbeaneﬀectivewayof 18.44
estimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetime
p 1isbothcomplicated(usuallymultimodal)anddeﬁnedoverahighdimensional
space.Itisdiﬃculttoﬁndatractable p 0thatissimpleenoughtoevaluatewhile
stillbeingcloseenoughto p 1toresultinahighqualityapproximation.If p 0and
p 1arenotclose,mostsamplesfrom p 0willhavelowprobabilityunder p 1and
thereforemake(relatively)negligiblecontributiontothesuminequation.18.44
Havingfewsamples withsigniﬁcantweightsinthis sumwillresultinan
estimatorthatisofpoorqualityduetohighvariance

============================================================

=== CHUNK 162 ===
Palavras: 409
Caracteres: 3696
--------------------------------------------------
This canbeunderstood
quantitativelythroughanestimateofthevarianceofourestimate ˆ Z 1:
ˆVar
ˆ Z 1
=Z 0
K2K 
k = 1
˜ p 1(x( ) k)
˜ p 0(x( ) k)−ˆ Z 12 (18.46)
Thisquantityislargestwhenthereissigniﬁcantdeviationinthevaluesofthe
importanceweights˜ p1 ( x() k)
˜ p0 ( x() k ) Wenowturntotworelatedstrategiesdevelopedtocopewiththechalleng-
ingtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh-
dimensionalspaces: annealedimportancesamplingandbridgesampling.Both
startwiththesimpleimportancesamplingstrategyintroducedaboveandboth
attempttoovercometheproblemoftheproposal p 0beingtoofarfrom p 1by
introducingintermediatedistributionsthatattemptto between b r i d g e t h e g a p p 0
and p 1 1 A n n ea l ed Im p o rt a n ce S a m p l i n g
Insituationswhere D K L( p 0 p 1)islarge(i.e.,wherethereislittleoverlapbetween
p 0and p 1),astrategycalled annealed i m p o r t anc e sampling(AIS)attempts
tobridgethegapbyintroducingintermediate distributions(,;, Jarzynski1997Neal
2001).Considerasequenceofdistributions p η0 , , p η n,with 0 = η 0 < η 1 < <···
η n − 1 < η n= 1sothattheﬁrstandlastdistributionsinthesequenceare p 0and p 1
respectively Thisapproachallowsustoestimatethepartitionfunctionofamultimodal
distributiondeﬁnedoverahigh-dimensionalspace(suchasthedistributiondeﬁned
byatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction
(suchasanRBMwithzeroesforweights)andestimatetheratiobetweenthetwo
model’spartitionfunctions Theestimateofthisratioisbasedontheestimate
oftheratiosofasequenceofmanysimilardistributions,suchasthesequenceof
RBMswithweightsinterpolatingbetweenzeroandthelearnedweights 625
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
WecannowwritetheratioZ1
Z0as
Z 1
Z 0=Z 1
Z 0Z η1
Z η1···Z η n −1
Z η n −1(18.47)
=Z η1
Z 0Z η2
Z η1···Z η n −1
Z η n −2Z 1
Z η n −1(18.48)
=n − 1
j = 0Z η j+1
Z η j(18.49)
Providedthedistributions p η jand p η j + 1,forall0≤≤− j n1,aresuﬃciently
close,wecanreliablyestimateeachofthefactorsZ η j+1
Z η jusingsimpleimportance
samplingandthenusethesetoobtainanestimateofZ1
Z0 Wheredotheseintermediatedistributionscomefrom?Justastheoriginal
proposaldistribution p 0isadesignchoice,soisthesequenceofdistributions
p η1 p η n −1.Thatis,itcanbespeciﬁcallyconstructedtosuittheproblemdomain Onegeneral-purposeandpopularchoicefortheintermediate distributionsisto
usetheweightedgeometricaverageofthetargetdistribution p 1andthestarting
proposaldistribution(forwhichthepartitionfunctionisknown) p 0:
p η j∝ pη j
1 p1 − η j
0 (18.50)
Inordertosamplefromtheseintermediate distributions,wedeﬁneaseriesof
Markovchaintransitionfunctions T η j( x| x) thatdeﬁnetheconditionalprobability
distributionoftransitioningto xgivenwearecurrentlyat x.Thetransition
operator T η j( x| x)isdeﬁnedtoleave p η j() xinvariant:
p η j() = x
p η j( x) T η j( x x|) d x(18.51)
ThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod
(e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepasses
throughalloftherandomvariablesorotherkindsofiterations TheAISsamplingstrategyisthentogeneratesamplesfrom p 0andthenuse
thetransitionoperatorstosequentiallygeneratesamplesfromtheintermediate
distributionsuntilwearriveatsamplesfromthetargetdistribution p 1:
•for k K = 1
–Sample x( ) k
η1∼ p 0()x
626
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
–Sample x( ) k
η2∼ T η1(x( ) k
η2| x( ) k
η1)
– –Sample x( ) k
η n −1∼ T η n −2(x( ) k
η n −1| x( ) k
η n −2)
–Sample x( ) k
η n∼ T η n −1(x( ) k
η n| x( ) k
η n −1)
•end
Forsample k,wecanderivetheimportanceweightbychainingtogetherthe
importanceweightsforthejumpsbetweentheintermediatedistributionsgivenin
equation:18.49
w( ) k=˜ p η1( x( ) k
η1)
˜ p 0( x( ) k
η1)˜ p η2( x( ) k
η2)
˜ p η1( x( ) k
η2)

============================================================

=== CHUNK 163 ===
Palavras: 364
Caracteres: 2054
--------------------------------------------------
.˜ p 1( x( ) k
1)
˜ p η n −1( x( ) k
η n) (18.52)
Toavoidnumericalissuessuchasoverﬂow,itisprobablybesttocompute log w( ) kby
addingandsubtractinglogprobabilities, ratherthancomputing w( ) kbymultiplying
anddividingprobabilities Withthesamplingprocedurethusdeﬁnedandtheimportanceweightsgiven
inequation,theestimateoftheratioofpartitionfunctionsisgivenby: 18.52
Z 1
Z 0≈1
KK
k = 1w( ) k(18.53)
Inordertoverifythatthisproceduredeﬁnesavalidimportancesampling
scheme,wecanshow(,)thattheAISprocedurecorrespondstosimple Neal2001
importancesamplingonanextendedstatespacewithpointssampledoverthe
productspace [ x η1 , , x η n −1 , x 1].Todothis,wedeﬁnethedistributionoverthe
extendedspaceas:
˜ p( x η1 , , x η n −1 , x 1) (18.54)
=˜ p 1( x 1)˜ T η n −1( x η n −1| x 1)˜ T η n −2( x η n −2| x η n −1) .˜ T η1( x η1| x η2) ,(18.55)
where ˜ T aisthereverseofthetransitionoperatordeﬁnedby T a(viaanapplication
ofBayes’rule):
˜ T a( x| x) =p a( x)
p a() xT a( x x|) =˜ p a( x)
˜ p a() xT a( x x|) .(18.56)
Pluggingtheaboveintotheexpressionforthejointdistributionontheextended
statespacegiveninequation,weget:18.55
˜ p( x η1 , , x η n −1 , x 1) (18.57)
627
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
=˜ p 1( x 1)˜ p η n −1( x η n −1)
˜ p η n −1( x 1)T η n −1( x 1| x η n −1)n − 2
i = 1˜ p η i( x η i)
˜ p η i( x η i+1)T η i( x η i+1| x η i)
(18.58)
=˜ p 1( x 1)
˜ p η n −1( x 1)T η n −1( x 1| x η n −1)˜ p η1( x η1)n − 2 
i = 1˜ p η i+1( x η i+1)
˜ p η i( x η i+1)T η i( x η i+1| x η i) (18.59)
Wenowhavemeansofgeneratingsamplesfromthejointproposaldistribution
qovertheextendedsampleviaasamplingschemegivenabove,withthejoint
distributiongivenby:
q( x η1 , , x η n −1 , x 1) = p 0( x η1) T η1( x η2| x η1) T η n −1( x 1| x η n −1) .(18.60)
Wehaveajointdistributionontheextendedspacegivenbyequation.Taking18.59
q( x η1 , , x η n −1 , x 1)astheproposaldistributionontheextendedstatespacefrom
whichwewilldrawsamples,itremainstodeterminetheimportanceweights:
w( ) k=˜ p( x η1 , , x η n −1 , x 1)
q( x η1 , , x η n −1 , x 1)=˜ p 1( x( ) k
1)
˜ p η n −1( x( ) k
η n −1)

============================================================

=== CHUNK 164 ===
Palavras: 372
Caracteres: 6834
--------------------------------------------------
.˜ p η2( x( ) k
η2)
˜ p 1( x( ) k
η1)˜ p η1( x( ) k
η1)
˜ p 0( x( ) k
0).(18.61)
TheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISas
simpleimportancesamplingappliedtoanextendedstateanditsvalidityfollows
immediatelyfromthevalidityofimportancesampling Annealedimportancesampling(AIS)wasﬁrstdiscoveredby () Jarzynski1997
andthenagain,independently,by().Itiscurrentlythemostcommon Neal2001
wayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.The
reasonsforthismayhavemoretodowiththepublicationofaninﬂuentialpaper
(SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthe
partitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthan
withanyinherentadvantagethemethodhasovertheothermethoddescribed
below AdiscussionofthepropertiesoftheAISestimator(e.g..itsvarianceand
eﬃciency)canbefoundin().Neal2001
1 8 2 B ri d g e S a m p l i n g
Bridgesampling ()isanothermethodthat,likeAIS,addressesthe Bennett1976
shortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof
628
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
intermediatedistributions,bridgesamplingreliesonasingledistribution p ∗,known
asthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction,
p 0,andadistribution p 1forwhichwearetryingtoestimatethepartitionfunction
Z 1 Bridgesamplingestimatestheratio Z 1 / Z 0astheratiooftheexpectedimpor-
tanceweightsbetween˜ p 0and˜ p ∗andbetween˜ p 1and˜ p ∗:
Z 1
Z 0≈K 
k = 1˜ p ∗( x( ) k
0)
˜ p 0( x( ) k
0)K 
k = 1˜ p ∗( x( ) k
1)
˜ p 1( x( ) k
1)(18.62)
Ifthebridgedistribution p ∗ischosencarefullytohavealargeoverlapofsupport
withboth p 0and p 1,thenbridgesamplingcanallowthedistancebetweentwo
distributions(ormoreformally, D K L( p 0 p 1))tobemuchlargerthanwithstandard
importancesampling Itcanbeshownthattheoptimalbridgingdistributionisgivenby p( ) op t
∗(x)∝
˜ p0 ( ) ˜ x p1 ( ) x
r ˜ p0 ( ) + ˜ x p1 ( ) xwhere r= Z 1 / Z 0.Atﬁrst,thisappearstobeanunworkablesolution
asitwouldseemtorequiretheveryquantitywearetryingtoestimate, Z 1 / Z 0 However,itispossibletostartwithacoarseestimateof randusetheresulting
bridgedistributiontoreﬁneourestimateiteratively(,).Thatis,we Neal2005
iterativelyre-estimatetheratioanduseeachiterationtoupdatethevalueof r
L i nk e d i m p o r t anc e samplingBothAISandbridgesamplinghavetheirad-
vantages.If D K L( p 0 p 1)isnottoolarge(because p 0and p 1aresuﬃcientlyclose)
bridgesamplingcanbeamoreeﬀectivemeansofestimatingtheratioofpartition
functionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingle
distribution p ∗tobridgethegapthenonecanatleastuseAISwithpotentially
manyintermediate distributionstospanthedistancebetween p 0and p 1.Neal
()showedhowhislinkedimportancesamplingmethodleveragedthepowerof 2005
thebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIS
tosigniﬁcantlyimprovetheoverallpartitionfunctionestimates E st i m at i n g t he par t i t i o n f unc t i o n whi l e t r ai ni ngWhileAIShasbecome
acceptedasthestandardmethodforestimatingthepartitionfunctionformany
undirectedmodels, itissuﬃcientlycomputationally intensivethatitremains
infeasibletouseduringtraining.However,alternativestrategiesthathavebeen
exploredtomaintainanestimateofthepartitionfunctionthroughouttraining
Usingacombinationofbridgesampling,short-chainAISandparalleltempering,
Desjardins2011 e t a l .()devisedaschemetotrackthepartitionfunctionofan
629
CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION
RBMthroughoutthetrainingprocess.Thestrategyisbasedonthemaintenanceof
independentestimatesofthepartitionfunctionsoftheRBMateverytemperature
operatingintheparalleltemperingscheme.Theauthorscombinedbridgesampling
estimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.from
paralleltempering)withAISestimatesacrosstimetocomeupwithalowvariance
estimateofthepartitionfunctionsateveryiterationoflearning Thetoolsdescribedinthischapterprovidemanydiﬀerentwaysofovercoming
theproblemofintractablepartitionfunctions,buttherecanbeseveralother
diﬃcultiesinvolvedintrainingandusinggenerativemodels.Foremostamongthese
istheproblemofintractableinference,whichweconfrontnext 630
C h a p t e r 1 9
ApproximateInference
Manyprobabilisticmodelsarediﬃculttotrainbecauseitisdiﬃculttoperform
inferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisible
variables vandasetoflatentvariables h.Thechallengeofinferenceusually
referstothediﬃcultproblemofcomputing p( h v|)ortakingexpectationswith
respecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihood
learning Manysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestricted
BoltzmannmachinesandprobabilisticPCA,aredeﬁnedinawaythatmakes
inferenceoperationslikecomputing p( h v|),ortakingexpectationswithrespect
toit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhidden
variableshaveintractableposteriordistributions.Exactinferencerequiresan
exponentialamountoftimeinthesemodels.Evensomemodelswithonlyasingle
layer,suchassparsecoding,havethisproblem Inthischapter,weintroduceseveralofthetechniquesforconfrontingthese
intractableinferenceproblems.Later,inchapter,wewilldescribehowtouse 20
thesetechniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable,
suchasdeepbeliefnetworksanddeepBoltzmannmachines Intractableinferenceproblemsindeeplearningusuallyarisefrominteractions
betweenlatentvariablesinastructuredgraphicalmodel.Seeﬁgureforsome19.1
examples.Theseinteractionsmaybeduetodirectinteractionsinundirected
modelsor“explainingaway”interactionsbetweenmutualancestorsofthesame
visibleunitindirectedmodels 631
CHAPTER19.APPROXIMATEINFERENCE
Figure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultof
interactionsbetweenlatentvariablesinastructuredgraphicalmodel.Thesecanbe
duetoedgesdirectlyconnectingonelatentvariabletoanother,orduetolongerpaths
thatareactivatedwhenthechildofaV-structureisobserved ( L e f t )Asemi-restricted
Boltzmannmachine( ,)withconnectionsbetweenhidden OsinderoandHinton2008
units.Thesedirectconnectionsbetweenlatentvariablesmaketheposteriordistribution
intractableduetolargecliquesoflatentvariables.AdeepBoltzmannmachine, ( C e n t e r )
organizedintolayersofvariableswithoutintra-layerconnections,stillhasanintractable
posteriordistributionduetotheconnectionsbetweenlayers.Thisdirectedmodel ( R i g h t )
hasinteractionsbetweenlatentvariableswhenthevisiblevariablesareobserved,because
everytwolatentvariablesareco-parents.Someprobabilisticmodelsareabletoprovide
tractableinferenceoverthelatentvariablesdespitehavingoneofthegraphstructures
depictedabove.Thisispossibleiftheconditionalprobabilitydistributionsarechosento
introduceadditionalindependencesbeyondthosedescribedbythegraph.Forexample,
probabilisticPCAhasthegraphstructureshownintheright,yetstillhassimpleinference
duetospecialpropertiesofthespeciﬁcconditionaldistributionsituses(linear-Gaussian
conditionalswithmutuallyorthogonalbasisvectors)

============================================================

=== CHUNK 165 ===
Palavras: 360
Caracteres: 3986
--------------------------------------------------
6 3 2
CHAPTER19.APPROXIMATEINFERENCE
19.1InferenceasOptimization
Manyapproachestoconfrontingtheproblemofdiﬃcultinferencemakeuseof
theobservationthatexactinferencecanbedescribedasanoptimization problem Approximateinferencealgorithmsmaythenbederivedbyapproximatingthe
underlyingoptimization problem Toconstructtheoptimization problem,assumewehaveaprobabilisticmodel
consistingofobservedvariables vandlatentvariables h.Wewouldliketocompute
thelogprobabilityoftheobserveddata, log p( v; θ).Sometimesitistoodiﬃcult
tocompute log p( v; θ)ifitiscostlytomarginalizeout h.Instead,wecancompute
alowerbound L( v θ , , q)onlog p( v; θ).Thisboundiscalledtheevidencelower
bound(ELBO).Anothercommonlyusednameforthislowerboundisthenegative
variationalfreeenergy.Speciﬁcally,theevidencelowerboundisdeﬁnedtobe
L − ( ) = log(;) v θ , , q p v θ D K L(( )( ;)) q h v| p h v| θ(19.1)
whereisanarbitraryprobabilitydistributionover q h
Becausethediﬀerencebetweenlog p( v)and L( v θ , , q)isgivenbytheKL
divergenceandbecausetheKLdivergenceisalwaysnon-negative,wecanseethat
Lalwayshasatmostthesamevalueasthedesiredlogprobability.Thetwoare
equalifandonlyifisthesamedistributionas q p( ) h v|
Surprisingly,Lcanbeconsiderablyeasiertocomputeforsomedistributions q Simplealgebrashowsthatwecanrearrange Lintoamuchmoreconvenientform:
L − ( ) =log(;) v θ , , q p v θ D K L(( )( ;)) q h v| p h v| θ (19.2)
=log(;) p v θ− E h∼ qlogq( ) h v|
p( ) h v|(19.3)
=log(;) p v θ− E h∼ qlogq( ) h v|
p , ( h v θ ; )
p ( ; ) v θ(19.4)
=log(;) p v θ− E h∼ q[log( )log(;)+log(;)] q h v|− p h v , θ p v θ(19.5)
=− E h∼ q[log( )log(;)] q h v|− p h v , θ (19.6)
Thisyieldsthemorecanonicaldeﬁnitionoftheevidencelowerbound,
L( ) = v θ , , q E h∼ q[log( )]+() p h v , H q (19.7)
Foranappropriatechoiceof q,Listractabletocompute.Foranychoice
of q,Lprovidesalowerboundonthelikelihood.For q( h v|)thatarebetter
6 3 3
CHAPTER19.APPROXIMATEINFERENCE
approximationsof p( h v|),thelowerbound Lwillbetighter,inotherwords,
closertolog p( v) When q( h v|)= p( h v|),theapproximation isperfect,and
L( ) = log(;) v θ , , q p v θ Wecanthusthinkofinferenceastheprocedureforﬁndingthe qthatmaximizes
L.ExactinferencemaximizesLperfectlybysearchingoverafamilyoffunctions
qthatincludes p( h v|).Throughoutthischapter,wewillshowhowtoderive
diﬀerentformsofapproximateinferencebyusingapproximateoptimization to
ﬁnd q.Wecanmaketheoptimization procedurelessexpensivebutapproximate
byrestrictingthefamilyofdistributions qtheoptimization isallowedtosearch
overorbyusinganimperfectoptimization procedurethatmaynotcompletely
maximizebutmerelyincreaseitbyasigniﬁcantamount L
Nomatterwhatchoiceof qweuse,Lisalowerbound.Wecangettighter
orlooserboundsthatarecheaperormoreexpensivetocomputedependingon
howwechoosetoapproachthisoptimization problem Wecanobtainapoorly
matched qbutreducethecomputational costbyusinganimperfectoptimization
procedure,orbyusingaperfectoptimization procedureoverarestrictedfamilyof
qdistributions 19.2ExpectationMaximization
Theﬁrstalgorithmweintroducebasedonmaximizingalowerbound Listhe
expectationmaximization(EM)algorithm,apopulartrainingalgorithmfor
modelswithlatentvariables.WedescribehereaviewontheEMalgorithm
developedby ().Unlikemostoftheotheralgorithmswe NealandHinton1999
describeinthischapter,EMisnotanapproachtoapproximateinference,but
ratheranapproachtolearningwithanapproximate posterior TheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence:
•TheE-step(Expectationstep):Let θ( 0 )denotethevalueoftheparameters
atthebeginningofthestep.Set q( h( ) i| v)= p( h( ) i| v( ) i; θ( 0 ))forall
indices iofthetrainingexamples v( ) iwewanttotrainon(bothbatchand
minibatchvariantsarevalid).Bythiswemean qisdeﬁnedintermsofthe
c u r r e ntparametervalueof θ( 0 );ifwevary θthen p( h v|; θ)willchangebut
q p ( ) h v|willremainequalto( ; h v| θ( 0 )) •The (Maximization step):Completelyorpartiallymaximize M-step

iL( v( ) i, , q θ) (19.8)
6 3 4
CHAPTER19.APPROXIMATEINFERENCE
withrespecttousingyouroptimization algorithmofchoice

============================================================

=== CHUNK 166 ===
Palavras: 354
Caracteres: 6446
--------------------------------------------------
θ
Thiscanbeviewedasacoordinateascentalgorithmtomaximize L.Onone
step,wemaximize Lwithrespectto q,andontheother,wemaximize Lwith
respectto θ
Stochasticgradientascentonlatentvariablemodelscanbeseenasaspecial
caseoftheEMalgorithmwheretheMstepconsistsoftakingasinglegradient
step.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsome
modelfamilies,theMstepcanevenbeperformedanalytically,jumpingallthe
waytotheoptimalsolutionforgiventhecurrent θ q
EventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEM
algorithmasusingapproximate inferenceinsomesense.Speciﬁcally,theM-step
assumesthatthesamevalueof qcanbeusedforallvaluesof θ.Thiswillintroduce
agapbetweenLandthetruelog p( v)astheM-stepmovesfurtherandfurther
awayfromthevalue θ( 0 )usedintheE-step.Fortunately,theE-stepreducesthe
gaptozeroagainasweentertheloopforthenexttime TheEMalgorithmcontainsafewdiﬀerentinsights.First,thereisthebasic
structureofthelearningprocess,inwhichweupdatethemodelparametersto
improvethelikelihoodofacompleteddataset,whereallmissingvariableshave
theirvaluesprovidedbyanestimateoftheposteriordistribution.Thisparticular
insightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescentto
maximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradient
computationsrequiretakingexpectationswithrespecttotheposteriordistribution
overthehiddenunits AnotherkeyinsightintheEMalgorithmisthatwecan
continuetouseonevalueof qevenafterwehavemovedtoadiﬀerentvalueof θ Thisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelarge
M-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplex
toadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecond
insightwhichismoreuniquetotheEMalgorithmisrarelyused 19.3MAPInferenceandSparseCoding
Weusuallyusetheterminferencetorefertocomputingtheprobabilitydistribution
overonesetofvariablesgivenanother.Whentrainingprobabilisticmodelswith
latentvariables,weareusuallyinterestedincomputing p( h v|).Analternative
formofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables,
ratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext
6 3 5
CHAPTER19.APPROXIMATEINFERENCE
oflatentvariablemodels,thismeanscomputing
h∗= argmax
hp ( ) h v| (19.9)
Thisisknownasmaximumaposterioriinference,abbreviatedMAPinference MAPinferenceisusuallynotthoughtofasapproximate inference—it does
computetheexactmostlikelyvalueof h∗.However,ifwewishtodevelopa
learningprocessbasedonmaximizing L( v h , , q),thenitishelpfultothinkofMAP
inferenceasaprocedurethatprovidesavalueof q.Inthissense,wecanthinkof
MAPinferenceasapproximateinference,becauseitdoesnotprovidetheoptimal
q Recallfromsectionthatexactinferenceconsistsofmaximizing 19.1
L( ) = v θ , , q E h∼ q[log( )]+() p h v , H q (19.10)
withrespectto qoveranunrestrictedfamilyofprobabilitydistributions,using
anexactoptimization algorithm.WecanderiveMAPinferenceasaformof
approximateinferencebyrestrictingthefamilyofdistributions qmaybedrawn
from.Speciﬁcally,werequiretotakeonaDiracdistribution: q
q δ ( ) = h v| ( ) h µ− (19.11)
Thismeansthatwecannowcontrol qentirelyvia µ.DroppingtermsofLthat
donotvarywith,weareleftwiththeoptimization problem µ
µ∗= argmax
µlog(= ) p h µ v , , (19.12)
whichisequivalenttotheMAPinferenceproblem
h∗= argmax
hp ( ) h v| (19.13)
WecanthusjustifyalearningproceduresimilartoEM,inwhichwealternate
betweenperformingMAPinferencetoinfer h∗andthenupdate θtoincrease
log p( h∗, v).AswithEM,thisisaformofcoordinateascentonL,wherewe
alternatebetweenusing inference to optimize Lwithrespect to qandusing
parameterupdatestooptimize Lwithrespectto θ.Theprocedureasawholecan
bejustiﬁedbythefactthatLisalowerboundonlog p( v).InthecaseofMAP
inference,thisjustiﬁcationisrathervacuous,becausetheboundisinﬁnitelyloose,
duetotheDiracdistribution’sdiﬀerentialentropyofnegativeinﬁnity.However,
addingnoisetowouldmaketheboundmeaningfulagain µ
6 3 6
CHAPTER19.APPROXIMATEINFERENCE
MAPinferenceiscommonlyusedindeeplearningasbothafeatureextractor
andalearningmechanism.Itisprimarilyusedforsparsecodingmodels Recallfromsectionthatsparsecodingisalinearfactormodelthatimposes 13.4
asparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplace
prior,with
p h( i) =λ
2e−| λ h i| (19.14)
Thevisibleunitsarethengeneratedbyperformingalineartransformationand
addingnoise:
p , β ( ) = (; + x h| N v W h b− 1I) (19.15)
Computingorevenrepresenting p( h v|)isdiﬃcult.Everypairofvariables h i
and h jarebothparentsof v.Thismeansthatwhen visobserved,thegraphical
modelcontainsanactivepathconnecting h iand h j.Allofthehiddenunitsthus
participateinonemassivecliquein p( h v|).IfthemodelwereGaussianthen
theseinteractionscouldbemodeledeﬃcientlyviathecovariancematrix,butthe
sparsepriormakestheseinteractionsnon-Gaussian Because p( h v|)isintractable,soisthecomputationofthelog-likelihoodand
itsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,we
useMAPinferenceandlearntheparametersbymaximizingtheELBOdeﬁnedby
theDiracdistributionaroundtheMAPestimateof h
Ifweconcatenateallofthe hvectorsinthetrainingsetintoamatrix H,and
concatenateallofthevectorsintoamatrix,thenthesparsecodinglearning v V
processconsistsofminimizing
J ,( H W) =
i , j| H i , j|+
i , j
V H W−2
i , j.(19.16)
Mostapplicationsofsparsecodingalsoinvolveweightdecayoraconstrainton
thenormsofthecolumnsof W,inordertopreventthepathologicalsolutionwith
extremelysmallandlarge H W
Wecanminimize Jbyalternatingbetweenminimization withrespectto H
andminimization withrespectto W.Bothsub-problemsareconvex.Infact,
theminimization withrespectto Wisjustalinearregressionproblem.However,
minimization of Jwithrespecttobothargumentsisusuallynotaconvexproblem Minimization withrespectto Hrequiresspecializedalgorithmssuchasthe
feature-signsearchalgorithm(,) Lee e t a l .2007
6 3 7
CHAPTER19.APPROXIMATEINFERENCE
19.4VariationalInferenceandLearning
We have seen how the evidence lo wer bound L( v θ , , q)is a lower bound on
log p( v; θ),howinferencecanbeviewedasmaximizing Lwithrespectto q,and
howlearningcanbeviewedasmaximizing Lwithrespectto θ.Wehaveseen
thattheEMalgorithmallowsustomakelargelearningstepswithaﬁxed qand
thatlearningalgorithmsbasedonMAPinferenceallowustolearnusingapoint
estimateof p( h v|)ratherthaninferringtheentiredistribution.Nowwedevelop
themoregeneralapproachtovariationallearning Thecoreideabehindvariationallearningisthatwecanmaximize Lovera
restrictedfamilyofdistributions q.Thisfamilyshouldbechosensothatitiseasy
tocompute E qlog p( h v ,)

============================================================

=== CHUNK 167 ===
Palavras: 370
Caracteres: 5800
--------------------------------------------------
Atypicalwaytodothisistointroduceassumptions
abouthowfactorizes q
Acommonapproachtovariationallearningistoimposetherestrictionthat q
isafactorialdistribution:
q( ) = h v|
iq h( i| v) (19.17)
Thisiscalledthemeanﬁeldapproach.Moregenerally,wecanimposeanygraphi-
calmodelstructurewechooseon q,toﬂexiblydeterminehowmanyinteractionswe
wantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproach
iscalledstructuredvariationalinference( ,) SaulandJordan1996
Thebeautyofthevariationalapproachisthatwedonotneedtospecifya
speciﬁcparametricformfor q.Wespecifyhowitshouldfactorize,butthenthe
optimization problemdeterminestheoptimalprobabilitydistributionwithinthose
factorizationconstraints.Fordiscretelatentvariables,thisjustmeansthatwe
usetraditionaloptimization techniquestooptimizeaﬁnitenumberofvariables
describingthe qdistribution.Forcontinuouslatentvariables,thismeansthatwe
useabranchofmathematics calledcalculusofvariationstoperformoptimization
overaspaceoffunctions,andactuallydeterminewhichfunctionshouldbeused
torepresent q.Calculusof variations istheorigin ofthenames “variational
learning”and“variationalinference,”thoughthesenamesapplyevenwhenthe
latentvariablesarediscreteandcalculusofvariationsisnotneeded.Inthecase
ofcontinuouslatentvariables,calculusofvariationsisapowerfultechniquethat
removesmuchoftheresponsibilityfromthehumandesignerofthemodel,who
nowmustspecifyonlyhow qfactorizes,ratherthanneedingtoguesshowtodesign
aspeciﬁcthatcanaccuratelyapproximate theposterior q
BecauseL( v θ , , q)isdeﬁnedtobelog p( v; θ)− D K L( q( h v|) p( h v|; θ)),we
canthinkofmaximizing Lwithrespectto qasminimizing D K L( q( h v|) p( h v|)) 6 3 8
CHAPTER19.APPROXIMATEINFERENCE
Inthissense,weareﬁtting qto p However,wearedoingsowiththeopposite
directionoftheKLdivergencethanweareusedtousingforﬁttinganapproximation Whenweusemaximumlikelihoodlearningtoﬁtamodeltodata,weminimize
D K L( p da t a p m o de l).Asillustratedinﬁgure,thismeansthatmaximumlikelihood 3.6
encouragesthemodeltohavehighprobabilityeverywherethatthedatahashigh
probability, whileouroptimization-based inferenceprocedureencourages qto
havelowprobabilityeverywherethetrueposteriorhaslowprobability.Both
directionsoftheKLdivergencecanhavedesirableandundesirableproperties.The
choiceofwhichtousedependsonwhichpropertiesarethehighestpriorityfor
eachapplication Inthecaseoftheinferenceoptimization problem,wechoose
touse D K L( q( h v|) p( h v|))forcomputational reasons.Speciﬁcally,computing
D K L( q( h v|) p( h v|))involvesevaluatingexpectationswithrespectto q,soby
designing qtobesimple,wecansimplifytherequiredexpectations.Theopposite
directionoftheKLdivergencewouldrequirecomputingexpectationswithrespect
tothetrueposterior.Becausetheformofthetrueposteriorisdeterminedby
thechoiceofmodel,wecannotdesignareduced-costapproachtocomputing
D K L(( )( )) p h v| q h v|exactly 19.4.1DiscreteLatentVariables
Variationalinferencewithdiscretelatentvariablesisrelativelystraightforward Wedeﬁneadistribution q,typicallyonewhereeachfactorof qisjustdeﬁned
byalookuptableoverdiscretestates Inthesimplestcase, hisbinaryandwe
makethemeanﬁeldassumptionthatfactorizesovereachindividual q h i.Inthis
casewecanparametrize qwithavectorˆ hwhoseentriesareprobabilities Then
q h( i= 1 ) =| v ˆh i Afterdetermininghowtorepresent q,wesimplyoptimizeitsparameters.In
thecaseofdiscretelatentvariables,thisisjustastandardoptimization problem Inprincipletheselectionof qcouldbedonewithanyoptimization algorithm,such
asgradientdescent Becausethisoptimization mustoccurintheinnerloopofalearningalgorithm,
itmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimization
algorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsin
veryfewiterations.Apopularchoiceistoiterateﬁxedpointequations,inother
words,tosolve
∂
∂ˆh iL= 0 (19.18)
forˆh i.Werepeatedlyupdatediﬀerentelementsofˆhuntilwesatisfyaconvergence
6 3 9
CHAPTER19.APPROXIMATEINFERENCE
criterion Tomakethismoreconcrete,weshowhowtoapplyvariationalinferencetothe
binarysparsecodingmodel(wepresentherethemodeldevelopedbyHenniges
e t a l .()butdemonstratetraditional,genericmeanﬁeldappliedtothemodel, 2010
whiletheyintroduceaspecializedalgorithm).Thisderivationgoesintoconsiderable
mathematical detailandisintendedforthereaderwhowishestofullyresolve
anyambiguityinthehigh-levelconceptualdescriptionofvariationalinferenceand
learningwehavepresentedsofar.Readerswhodonotplantoderiveorimplement
variationallearningalgorithmsmaysafelyskiptothenextsectionwithoutmissing
anynewhigh-levelconcepts.Readerswhoproceedwiththebinarysparsecoding
exampleareencouragedtoreviewthelistofusefulpropertiesoffunctionsthat
commonlyariseinprobabilisticmodelsinsection.Weusetheseproperties 3.10
liberallythroughoutthefollowingderivationswithouthighlightingexactlywhere
weuseeachone Inthebinarysparsecodingmodel,theinput v∈ Rnisgeneratedfromthe
modelbyaddingGaussiannoisetothesumof mdiﬀerentcomponentswhich
caneachbepresentorabsent.Eachcomponentisswitchedonoroﬀbythe
correspondinghiddenunitin h∈{}01 ,m:
p h( i= 1) = ( σ b i) (19.19)
p , ( ) = (; v h| N v W h β− 1) (19.20)
where bisalearnablesetofbiases, Wisalearnableweightmatrix,and βisa
learnable,diagonalprecisionmatrix Trainingthismodelwithmaximumlikelihoodrequirestakingthederivative
withrespecttotheparameters.Considerthederivativewithrespecttooneofthe
biases:
∂
∂ b ilog() p v (19.21)
=∂
∂ b ip() v
p() v(19.22)
=∂
∂ b i
hp ,( h v)
p() v(19.23)
=∂
∂ b i
hp p() h( ) v h|
p() v(19.24)
6 4 0
CHAPTER19.APPROXIMATEINFERENCE
h 1 h 1 h 2 h 2 h 3 h 3
v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4
h 1 h 1
h 2 h 2h 3 h 3
h 4 h 4
Figure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits ( L e f t )Thegraphstructureof p( h v ,).Notethattheedgesaredirected,andthateverytwo
hiddenunitsareco-parentsofeveryvisibleunit.Thegraphstructureof ( R i g h t ) p( h v|)

============================================================

=== CHUNK 168 ===
Palavras: 352
Caracteres: 3554
--------------------------------------------------
Inordertoaccountfortheactivepathsbetweenco-parents,theposteriordistribution
needsanedgebetweenallofthehiddenunits =
h p( ) v h|∂
∂ b ip() h
p() v(19.25)
=
hp( ) h v|∂
∂ b ip() h
p() h(19.26)
= E h∼ | p ( h v )∂
∂ b ilog() p h (19.27)
Thisrequirescomputingexpectationswithrespectto p( h v|).Unfortunately,
p( h v|)isacomplicateddistribution.Seeﬁgureforthegraphstructureof 19.2
p( h v ,)and p( h v|).Theposteriordistributioncorrespondstothecompletegraph
overthehiddenunits,sovariableeliminationalgorithmsdonothelpustocompute
therequiredexpectationsanyfasterthanbruteforce Wecanresolvethisdiﬃcultybyusingvariationalinferenceandvariational
learninginstead Wecanmakeameanﬁeldapproximation:
q( ) = h v|
iq h( i| v) (19.28)
Thelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresent
afactorial qwesimplyneedtomodel mBernoullidistributions q( h i| v).Anatural
waytorepresentthemeansoftheBernoullidistributionsiswithavectorˆhof
probabilities, with q( h i=1| v)=ˆh i.Weimposearestrictionthatˆh iisnever
equalto0orto1,inordertoavoiderrorswhencomputing,forexample, logˆh i Wewillseethatthevariationalinferenceequationsneverassignorto 01 ˆh i
6 4 1
CHAPTER19.APPROXIMATEINFERENCE
analytically.However,inasoftwareimplementation,machineroundingerrorcould
resultinorvalues.Insoftware,wemaywishtoimplementbinarysparse 0 1
codingusinganunrestrictedvectorofvariationalparameters zandobtain ˆ hvia
therelation ˆh= σ( z).Wecanthussafelycompute logˆh ionacomputerbyusing
theidentitylog( σ z i) = (− ζ− z i)relatingthesigmoidandthesoftplus Tobeginourderivationofvariationallearninginthebinarysparsecoding
model,weshowthattheuseofthismeanﬁeldapproximationmakeslearning
tractable Theevidencelowerboundisgivenby
L( ) v θ , , q (19.29)
= E h∼ q[log( )]+() p h v , H q (19.30)
= E h∼ q[log()+log( )log( )] p h p v h|− q h v| (19.31)
= E h∼ qm
i = 1log( p h i)+n
i = 1log( p v i|− h)m
i = 1log( q h i| v)
(19.32)
=m
i = 1
ˆ h i(log( σ b i)log− ˆ h i)+(1−ˆ h i)(log( σ− b i)log(1 − −ˆ h i))
(19.33)
+ E h∼ qn
i = 1log
β i
2 πexp
−β i
2( v i− W i , : h)2
(19.34)
=m
i = 1
ˆ h i(log( σ b i)log− ˆ h i)+(1−ˆ h i)(log( σ− b i)log(1 − −ˆ h i))
(19.35)
+1
2n
i = 1
logβ i
2 π− β i
 v2
i−2 v i W i , :ˆh+
j
 W2
i , jˆ h j+
k j=W i , j W i , kˆh jˆh k


 (19.36)
Whiletheseequationsaresomewhatunappealingaesthetically,theyshowthatL
canbeexpressedinasmallnumberofsimplearithmeticoperations.Theevidence
lowerbound Listhereforetractable.WecanuseLasareplacementforthe
intractablelog-likelihood Inprinciple,wecouldsimplyrungradientascentonboth vand handthis
wouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm Usually,however,wedonotdothis,fortworeasons.First,thiswouldrequire
storing ˆ hforeach v.Wetypicallypreferalgorithmsthatdonotrequireper-
examplememory.Itisdiﬃculttoscalelearningalgorithmstobillionsofexamples
ifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample 6 4 2
CHAPTER19.APPROXIMATEINFERENCE
Second,wewouldliketobeabletoextractthefeatures ˆhveryquickly,inorderto
recognizethecontentof v Inarealisticdeployedsetting,wewouldneedtobe
abletocompute ˆhinrealtime Forboththesereasons,wetypicallydonotusegradientdescenttocompute
themeanﬁeldparameters ˆ h.Instead,werapidlyestimatethemwithﬁxedpoint
equations Theideabehindﬁxedpointequationsisthatweareseekingalocalmaximum
withrespecttoˆh, where ∇ hL( v θ , ,ˆh)= 0.Wecannoteﬃcientlysolvethis
equationwithrespecttoallofˆhsimultaneously.However,wecansolveforasingle
variable:∂
∂ˆh iL( v θ , ,ˆh) = 0 (19.37)
Wecantheniterativelyapplythesolutiontotheequationfor i=1 ,

============================================================

=== CHUNK 169 ===
Palavras: 356
Caracteres: 5005
--------------------------------------------------
, m,
andrepeatthecycleuntilwesatisfyaconvergecriterion.Commonconvergence
criteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveLbymore
thansometoleranceamount,orwhenthecycledoesnotchange ˆhbymorethan
someamount Iteratingmeanﬁeldﬁxedpointequationsisageneraltechniquethatcan
providefastvariationalinferenceinabroadvarietyofmodels.Tomakethismore
concrete,weshowhowtoderivetheupdatesforthebinarysparsecodingmodelin
particular First,wemustwriteanexpressionforthederivativeswithrespecttoˆh i.To
doso,wesubstituteequation intotheleftsideofequation: 19.36 19.37
∂
∂ˆh iL( v θ , ,ˆ h) (19.38)
=∂
∂ˆh i
m
j = 1
ˆ h j(log( σ b j)log− ˆ h j)+(1−ˆ h j)(log( σ− b j)log(1 − −ˆ h j))
(19.39)
+1
2n
j = 1
logβ j
2 π− β j
 v2
j−2 v j W j , :ˆh+
k
 W2
j , kˆh k+
l k=W j , k W j , lˆh kˆh l




(19.40)
=log( σ b i)log− ˆh i− − 1+log(1ˆh i)+1log( − σ− b i) (19.41)
+n
j = 1
 β j
 v j W j , i−1
2W2
j , i−
k i=W j , k W j , iˆh k

 (19.42)
6 4 3
CHAPTER19.APPROXIMATEINFERENCE
= b i−logˆ h i+log(1−ˆh i)+ vβ W : , i−1
2W
: , i β W : , i−
j i=W
: , j β W : , iˆh j .(19.43)
Toapplytheﬁxedpointupdateinferencerule,wesolvefortheˆ h ithatsets
equationto0:19.43
ˆh i= σ
 b i+ vβ W : , i−1
2W
: , i β W : , i−
j i=W
: , j β W : , iˆh j
 .(19.44)
Atthispoint,wecanseethatthereisacloseconnectionbetweenrecurrent
neuralnetworksandinferenceingraphicalmodels.Speciﬁcally,themeanﬁeld
ﬁxedpointequationsdeﬁnedarecurrentneuralnetwork.Thetaskofthisnetwork
istoperforminference.Wehavedescribedhowtoderivethisnetworkfroma
modeldescription,butitisalsopossibletotraintheinferencenetworkdirectly Severalideasbasedonthisthemearedescribedinchapter.20
Inthecaseofbinarysparsecoding,wecanseethattherecurrentnetwork
connectionspeciﬁedbyequationconsistsofrepeatedlyupdatingthehidden 19.44
unitsbasedonthechangingvaluesoftheneighboringhiddenunits.Theinput
alwayssendsaﬁxedmessageof vβ Wtothehiddenunits,butthehiddenunits
constantlyupdatethemessagetheysendtoeachother.Speciﬁcally,twounits ˆh i
andˆ h jinhibiteachotherwhentheirweightvectorsarealigned.Thisisaformof
competition—betweentwohiddenunitsthatbothexplaintheinput,onlytheone
thatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionis
themeanﬁeldapproximation’sattempttocapturetheexplainingawayinteractions
inthebinarysparsecodingposterior.Theexplainingawayeﬀectactuallyshould
causeamulti-modalposterior,sothatifwedrawsamplesfromtheposterior,
somesampleswillhaveoneunitactive,othersampleswillhavetheotherunit
active,butveryfewsampleshavebothactive.Unfortunately,explainingaway
interactionscannotbemodeledbythefactorial qusedformeanﬁeld,sothemean
ﬁeldapproximation isforcedtochooseonemodetomodel.Thisisaninstanceof
thebehaviorillustratedinﬁgure.3.6
Wecanrewriteequationintoanequivalentformthatrevealssomefurther 19.44
insights:
ˆh i= σ
 b i+
 v−
j i=W : , jˆh j

β W : , i−1
2W
: , i β W : , i
 .(19.45)
Inthisreformulation,weseetheinputateachstepasconsistingof v−
j i=W : , jˆh j
ratherthan v.Wecanthusthinkofunit iasattemptingtoencodetheresidual
6 4 4
CHAPTER19.APPROXIMATEINFERENCE
errorin vgiventhecodeoftheotherunits.Wecanthusthinkofsparsecodingas
aniterativeautoencoder,thatrepeatedlyencodesanddecodesitsinput,attempting
toﬁxmistakesinthereconstructionaftereachiteration Inthisexample,wehavederivedanupdaterulethatupdatesasingleunitat
atime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously Somegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsucha
waythatwecansolveformanyentriesofˆhsimultaneously.Unfortunately,binary
sparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristic
techniquecalleddampingtoperformblockupdates.Inthedampingapproach,
wesolvefortheindividuallyoptimalvaluesofeveryelementofˆh,thenmoveallof
thevaluesinasmallstepinthatdirection.Thisapproachisnolongerguaranteed
toincreaseLateachstep,butworkswellinpracticeformanymodels.SeeKoller
andFriedman2009()formoreinformationaboutchoosingthedegreeofsynchrony
anddampingstrategiesinmessagepassingalgorithms 19.4.2CalculusofVariations
Beforecontinuingwithourpresentationofvariationallearning,wemustbrieﬂy
introduceanimportantsetofmathematical toolsusedinvariationallearning:
calculusofvariations Manymachinelearningtechniquesarebasedonminimizingafunction J( θ)by
ﬁndingtheinputvector θ∈ Rnforwhichittakesonitsminimalvalue.Thiscan
beaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthe
criticalpointswhere ∇ θ J( θ) = 0.Insomecases,weactuallywanttosolvefora
function f( x),suchaswhenwewanttoﬁndtheprobabilitydensityfunctionover
somerandomvariable.Thisiswhatcalculusofvariationsenablesustodo Afunction ofa function fisknown asafunctional J[ f].Muchas we
cantakepartialderivativesofafunctionwithrespecttoelementsofitsvector-
valuedargument,wecantakefunctionalderivatives,alsoknownasvariational
derivatives,ofafunctional J[ f]withrespecttoindividualvaluesofthefunction
f( x)atanyspeciﬁcvalueof x.Thefunctionalderivativeofthefunctional Jwith
respecttothevalueofthefunctionatpointisdenoted f xδ
δ f x ( )J

============================================================

=== CHUNK 170 ===
Palavras: 376
Caracteres: 5337
--------------------------------------------------
Acompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeof
thisbook.Forourpurposes,itissuﬃcienttostatethatfordiﬀerentiablefunctions
f g y , () xanddiﬀerentiable functions ( x)withcontinuousderivatives,that
δ
δ f() x
g f , d (() x x) x=∂
∂ yg f , (() x x) (19.46)
6 4 5
CHAPTER19.APPROXIMATEINFERENCE
Togainsomeintuitionforthisidentity,onecanthinkof f( x)asbeingavector
withuncountablymanyelements,indexedbyarealvector x.Inthis(somewhat
incompleteview),theidentityprovidingthefunctionalderivativesisthesameas
wewouldobtainforavector θ∈ Rnindexedbypositiveintegers:
∂
∂ θ i
jg θ( j , j) =∂
∂ θ ig θ( i , i .) (19.47)
Manyresultsinothermachinelearningpublicationsarepresentedusingthemore
generalEuler-Lagrangeequationwhichallows gtodependonthederivatives
of faswellasthevalueof f,butwedonotneedthisfullygeneralformforthe
resultspresentedinthisbook Tooptimizeafunctionwithrespecttoavector,wetakethegradientofthe
functionwithrespecttothevectorandsolveforthepointwhereeveryelementof
thegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingfor
thefunctionwherethefunctionalderivativeateverypointisequaltozero Asanexampleofhowthisprocessworks,considertheproblemofﬁndingthe
probabilitydistributionfunctionover x∈ Rthathasmaximaldiﬀerentialentropy Recallthattheentropyofaprobabilitydistributionisdeﬁnedas p x()
H p[] = − E xlog() p x (19.48)
Forcontinuousvalues,theexpectationisanintegral:
H p[] = −
p x p x d x ()log() (19.49)
Wecannotsimplymaximize H[ p] withrespecttothefunction p( x),becausethe
resultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrange
multipliers toadd aconstraint that p( x)integratesto 1.Also,theentropy
increaseswithoutboundasthevarianceincreases.Thismakesthequestionof
whichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhich
distributionhasmaximalentropyforﬁxedvariance σ2.Finally,theproblem
isunderdetermined becausethedistributioncanbeshiftedarbitrarilywithout
changingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthe
meanofthedistributionbe µ TheLagrangianfunctionalforthisoptimization
problemis
L[] = p λ 1
p x d x()−1
+ λ 2([] )+ E x− µ λ 3
E[( ) x µ−2]− σ2
+[] H p(19.50)
6 4 6
CHAPTER19.APPROXIMATEINFERENCE
=
λ 1 p x λ ()+ 2 p x x λ ()+ 3 p x x µ ()(−)2− p x p x ()log()d x λ− 1− µ λ 2− σ2λ 3 (19.51)
TominimizetheLagrangianwithrespectto p,wesetthefunctionalderivatives
equalto0:
∀ x ,δ
δ p x()L= λ 1+ λ 2 x λ+ 3( ) x µ−2−−1log() = 0 p x .(19.52)
Thisconditionnowtellsusthefunctionalformof p( x).Byalgebraically
re-arrangingtheequation,weobtain
p x() = exp
λ 1+ λ 2 x λ+ 3( ) x µ−2−1 (19.53)
Weneverassumeddirectlythat p( x)wouldtakethisfunctionalform;we
obtainedtheexpressionitselfbyanalyticallyminimizingafunctional.Toﬁnish
theminimization problem,wemustchoosethe λvaluestoensurethatallofour
constraintsaresatisﬁed.Wearefreetochooseany λvalues,becausethegradient
oftheLagrangianwithrespecttothe λvariablesiszerosolongastheconstraints
aresatisﬁed.Tosatisfyalloftheconstraints,wemayset λ 1=1−log σ√
2 π,
λ 2= 0,and λ 3= −1
2 σ2toobtain
p x x µ , σ () = (N;2) (19.54)
Thisisonereasonforusingthenormaldistributionwhenwedonotknowthe
truedistribution.Becausethenormaldistributionhasthemaximumentropy,we
imposetheleastpossibleamountofstructurebymakingthisassumption WhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy,
wefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyfor
ﬁxedvariance.Whatabouttheprobabilitydistributionfunctionthat m i nim i z e s
theentropy?Whydidwenotﬁndasecondcriticalpointcorrespondingtothe
minimum?Thereasonisthatthereisnospeciﬁcfunctionthatachievesminimal
entropy.Asfunctionsplacemoreprobabilitydensityonthetwopoints x= µ+ σ
and x= µ σ−,andplacelessprobabilitydensityonallothervaluesof x,theylose
entropywhilemaintainingthedesiredvariance.However,anyfunctionplacing
exactlyzeromassonallbuttwopointsdoesnotintegratetoone,andisnota
validprobabilitydistribution.Therethusisnosingleminimalentropyprobability
distributionfunction,muchasthereisnosingleminimalpositiverealnumber Instead,wecansaythatthereisasequenceofprobabilitydistributionsconverging
towardputtingmassonlyonthesetwopoints.Thisdegeneratescenariomaybe
6 4 7
CHAPTER19.APPROXIMATEINFERENCE
describedasamixtureofDiracdistributions.BecauseDiracdistributionsare
notdescribedbyasingleprobabilitydistributionfunction,noDiracormixtureof
Diracdistributioncorrespondstoasinglespeciﬁcpointinfunctionspace.These
distributionsarethusinvisibletoourmethodofsolvingforaspeciﬁcpointwhere
thefunctionalderivativesarezero.Thisisalimitationofthemethod.Distributions
suchastheDiracmustbefoundbyothermethods,suchasguessingthesolution
andthenprovingthatitiscorrect 19.4.3ContinuousLatentVariables
Whenourgraphicalmodelcontainscontinuouslatentvariables, wemaystill
performvariationalinferenceandlearningbymaximizing L.However,wemust
nowusecalculusofvariationswhenmaximizing withrespectto L q( ) h v|
Inmostcases,practitioners neednotsolveanycalculusofvariationsproblems
themselves.Instead,thereisageneralequationforthemeanﬁeldﬁxedpoint
updates.Ifwemakethemeanﬁeldapproximation
q( ) = h v|
iq h( i| v) , (19.55)
andﬁx q( h j| v)forall j= i,thentheoptimal q( h i| v)maybeobtainedby
normalizingtheunnormalized distribution
˜ q h( i| v) = exp
E h − i∼ q ( h − i| v )log ˜ p ,( v h)
(19.56)
solongas pdoesnotassignprobabilitytoanyjointconﬁgurationofvariables

============================================================

=== CHUNK 171 ===
Palavras: 353
Caracteres: 3568
--------------------------------------------------
0
Carryingouttheexpectationinsidetheequationwillyieldthecorrectfunctional
formof q( h i| v).Itisonlynecessarytoderivefunctionalformsof qdirectlyusing
calculusofvariationsifonewishestodevelopanewformofvariationallearning;
equationyieldsthemeanﬁeldapproximation foranyprobabilisticmodel 19.56
Equationisaﬁxedpointequation,designedtobeiterativelyappliedfor 19.56
eachvalueof irepeatedlyuntilconvergence.However,italsotellsusmorethan
that.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whether
wearrivetherebyﬁxedpointequationsornot.Thismeanswecantakethe
functionalformfromthatequationbutregardsomeofthevaluesthatappearinit
asparameters,thatwecanoptimizewithanyoptimization algorithmwelike Asanexample,consideraverysimpleprobabilisticmodel,withlatentvariables
h∈ R2andjustonevisiblevariable, v.Supposethat p( h)=N( h;0 , I)and
p( v| h)=N( v; wh;1).Wecouldactuallysimplifythismodelbyintegrating
out h;theresultisjustaGaussiandistributionover v Themodelitselfisnot
6 4 8
CHAPTER19.APPROXIMATEINFERENCE
interesting;wehaveconstructeditonlytoprovideasimpledemonstrationofhow
calculusofvariationsmaybeappliedtoprobabilisticmodeling Thetrueposteriorisgiven,uptoanormalizingconstant,by
p( ) h v| (19.57)
∝ p ,( h v) (19.58)
=( p h 1)( p h 2)( ) p v h| (19.59)
∝exp
−1
2
h2
1+ h2
2+( v h− 1 w 1− h 2 w 2)2
(19.60)
=exp
−1
2
h2
1+ h2
2+ v2+ h2
1 w2
1+ h2
2 w2
2−2 v h 1 w 1−2 v h 2 w 2+2 h 1 w 1 h 2 w 2 (19.61)
Duetothepresenceofthetermsmultiplying h 1and h 2together,wecanseethat
thetrueposteriordoesnotfactorizeover h 1and h 2 Applyingequation,weﬁndthat 19.56
˜ q h( 1| v) (19.62)
=exp
E h 2∼ q ( h 2| v )log ˜ p ,( v h)
(19.63)
=exp
−1
2Eh 2∼ q ( h 2| v )
h2
1+ h2
2+ v2+ h2
1 w2
1+ h2
2 w2
2(19.64)
−2 v h 1 w 1−2 v h 2 w 2+2 h 1 w 1 h 2 w 2] (19.65)
Fromthis,wecanseethatthereareeﬀectivelyonlytwovaluesweneedtoobtain
from q( h 2| v): E h 2∼ | q ( h v )[ h 2]and E h 2∼ | q ( h v )[ h2
2] Writingtheseas h 2and h2
2,
weobtain
˜ q h( 1| v) = exp
−1
2
h2
1+ h2
2+ v2+ h2
1 w2
1+ h2
2 w2
2(19.66)
−2 v h 1 w 1−2 v h 2 w 2+2 h 1 w 1 h 2 w 2]
.(19.67)
Fromthis,wecanseethat˜ qhasthefunctionalformofaGaussian.Wecan
thusconclude q( h v|)=N( h; µ β ,− 1)where µanddiagonal βarevariational
parametersthatwecanoptimizeusinganytechniquewechoose.Itisimportant
torecallthatwedidnoteverassumethat qwouldbeGaussian;itsGaussian
formwasderivedautomatically byusingcalculusofvariationstomaximize qwith
6 4 9
CHAPTER19.APPROXIMATEINFERENCE
respecttoL.Usingthesameapproachonadiﬀerentmodelcouldyieldadiﬀerent
functionalformof q
Thiswasofcourse,justasmallcaseconstructedfordemonstrationpurposes Forexamplesofrealapplicationsofvariationallearningwithcontinuousvariables
inthecontextofdeeplearning,see () Goodfellow e t a l .2013d
19.4.4InteractionsbetweenLearningandInference
Usingapproximate inferenceaspartofalearningalgorithmaﬀectsthelearning
process,andthisinturnaﬀectstheaccuracyoftheinferencealgorithm Speciﬁcally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakes
theapproximating assumptionsunderlyingtheapproximate inferencealgorithm
becomemoretrue.Whentrainingtheparameters,variationallearningincreases
E h∼ qlog( ) p v h , (19.68)
Foraspeciﬁc v,thisincreases p( h v|)forvaluesof hthathavehighprobability
under q( h v|)anddecreases p( h v|)forvaluesof hthathavelowprobability
under q( ) h v|
Thisbehaviorcausesourapproximating assumptionstobecomeself-fulﬁlling
prophecies.Ifwetrainthemodelwithaunimodalapproximate posterior,wewill
obtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewould
haveobtainedbytrainingthemodelwithexactinference

============================================================

=== CHUNK 172 ===
Palavras: 352
Caracteres: 8598
--------------------------------------------------
Computingthetrueamountofharmimposedonamodelbyavariational
approximationisthusverydiﬃcult.Thereexistseveralmethodsforestimating
log p( v).Weoftenestimate log p( v; θ)aftertrainingthemodel,andﬁndthat
thegapwith L( v θ , , q)issmall.Fromthis,wecanconcludethatourvariational
approximationisaccurateforthespeciﬁcvalueof θthatweobtainedfromthe
learningprocess.Weshouldnotconcludethatourvariationalapproximation is
accurateingeneralorthatthevariationalapproximation didlittleharmtothe
learningprocess.Tomeasurethetrueamountofharminducedbythevariational
approximation,wewouldneedtoknow θ∗=max θlog p( v; θ) Itispossiblefor
L( v θ , , q)≈log p( v; θ)andlog p( v; θ)log p( v; θ∗)toholdsimultaneously.If
max qL( v θ ,∗, q)log p( v; θ∗),because θ∗inducestoocomplicatedofaposterior
distributionforour qfamilytocapture, then thelearningprocesswillnever
approach θ∗.Suchaproblemisverydiﬃculttodetect,becausewecanonlyknow
forsurethatithappenedifwehaveasuperiorlearningalgorithmthatcanﬁnd θ∗
forcomparison 6 5 0
CHAPTER19.APPROXIMATEINFERENCE
19.5LearnedApproximateInference
Wehaveseenthatinferencecanbethoughtofasanoptimization procedure
thatincreasesthevalueofafunction L.Explicitlyperformingoptimization via
iterativeproceduressuchasﬁxedpointequationsorgradient-basedoptimization
isoftenveryexpensiveandtime-consuming Manyapproachestoinferenceavoid
thisexpensebylearningtoperformapproximateinference Speciﬁcally ,wecan
thinkoftheoptimization processasafunction fthatmapsaninput vtoan
approximatedistribution q∗=argmaxqL( v , q).Oncewethinkofthemulti-step
iterativeoptimization processasjustbeingafunction,wecanapproximateitwith
aneuralnetworkthatimplementsanapproximation ˆ f(;) v θ 19.5.1Wake-Sleep
Oneofthemaindiﬃcultieswithtrainingamodeltoinfer hfrom visthatwe
donothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givena v,
wedonotknowtheappropriate h.Themappingfrom vto hdependsonthe
choiceofmodelfamily,andevolvesthroughoutthelearningprocessas θchanges Thewake-sleepalgorithm(Hinton1995bFrey1996 e t a l .,; e t a l .,)resolvesthis
problembydrawingsamplesofboth hand vfromthemodeldistribution For
example,inadirectedmodel,thiscanbedonecheaplybyperformingancestral
samplingbeginningat handendingat v.Theinferencenetworkcanthenbe
trainedtoperformthereversemapping: predicting which hcausedthepresent
v.Themaindrawbacktothisapproachisthatwewillonlybeabletotrainthe
inferencenetworkonvaluesof vthathavehighprobabilityunderthemodel.Early
inlearning,themodeldistributionwillnotresemblethedatadistribution,sothe
inferencenetworkwillnothaveanopportunitytolearnonsamplesthatresemble
data Insectionwesawthatonepossibleexplanationfortheroleofdreamsleep 18.2
inhumanbeingsandanimalsisthatdreamscouldprovidethenegativephase
samplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegative
gradientofthelogpartitionfunctionofundirectedmodels.Anotherpossible
explanationforbiologicaldreamingisthatitisprovidingsamplesfrom p( h v ,)
whichcanbeusedtotrainaninferencenetworktopredict hgiven v.Insome
senses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation MonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonly
thepositivephaseofthegradientforseveralstepsthenwithonlythenegative
phaseofthegradientforseveralsteps.Humanbeingsandanimalsareusually
awakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itis
6 5 1
CHAPTER19.APPROXIMATEINFERENCE
notreadilyapparenthowthisschedulecouldsupportMonteCarlotrainingofan
undirectedmodel.Learningalgorithmsbasedonmaximizing Lcanberunwith
prolongedperiodsofimproving qandprolongedperiodsofimproving θ,however Iftheroleofbiologicaldreamingistotrainnetworksforpredicting q,thenthis
explainshowanimalsareabletoremainawakeforseveralhours(thelongerthey
areawake,thegreaterthegapbetweenLandlog p( v),butLwillremainalower
bound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnot
modiﬁedduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,these
ideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreaming
accomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearning
ratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromthe
animal’stransitionmodel,onwhichtotraintheanimal’spolicy.Orsleepmay
servesomeotherpurposenotyetanticipatedbythemachinelearningcommunity 19.5.2OtherFormsofLearnedInference
Thisstrategyoflearnedapproximateinferencehasalsobeenappliedtoother
models.SalakhutdinovandLarochelle2010()showedthatasinglepassina
learnedinferencenetworkcouldyieldfasterinferencethaniteratingthemeanﬁeld
ﬁxedpointequationsinaDBM.Thetrainingprocedureisbasedonrunningthe
inferencenetwork,thenapplyingonestepofmeanﬁeldtoimproveitsestimates,
andtrainingtheinferencenetworktooutputthisreﬁnedestimateinsteadofits
originalestimate Wehavealreadyseeninsectionthatthepredictivesparsedecomposition 14.8
modeltrainsashallowencodernetworktopredictasparsecodefortheinput Thiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itis
possibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencoder
maybeviewedasperforminglearnedapproximate MAPinference.Duetoits
shallowencoder,PSDisnotabletoimplementthekindofcompetitionbetween
unitsthatwehaveseeninmeanﬁeldinference.However,thatproblemcanbe
remediedbytrainingadeepencodertoperformlearnedapproximateinference,as
intheISTAtechnique( ,) GregorandLeCun2010b
Learned approximate inference hasrecently become one of the dominant
approachestogenerativemodeling,intheformofthevariationalautoencoder
(,; ,).Inthiselegantapproach,thereisnoneedto Kingma2013Rezende e t a l .2014
constructexplicittargetsfortheinferencenetwork.Instead,theinferencenetwork
issimplyusedtodeﬁne L,andthentheparametersoftheinferencenetworkare
adaptedtoincrease.Thismodelisdescribedindepthlater,insection L 20.10.3
6 5 2
CHAPTER19.APPROXIMATEINFERENCE
Usingapproximateinference,itispossibletotrainanduseawidevarietyof
models.Manyofthesemodelsaredescribedinthenextchapter 6 5 3
C h a p t e r 2 0
D e e p Ge n e rat i v e Mo d e l s
Inthischapter,wepresentseveralofthespeciﬁckindsofgenerativemodelsthat
canbebuiltandtrainedusingthetechniquespresentedinchapters–.Allof1619
thesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsome
way.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly Othersdonotallowtheevaluationoftheprobabilitydistributionfunction,but
supportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamples
fromthedistribution.Someofthesemodelsarestructuredprobabilisticmodels
describedintermsofgraphsandfactors,usingthelanguageofgraphicalmodels
presentedinchapter Otherscannoteasilybedescribedintermsoffactors, 16
butrepresentprobabilitydistributionsnonetheless 20.1BoltzmannMachines
Boltzmannmachineswereoriginallyintroducedasageneral“connectionist” ap-
proachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlman
e t a l .,;1983Ackley1985Hinton1984HintonandSejnowski1986 e t a l .,; e t a l .,; ,) VariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelong
agosurpassedthepopularityoftheoriginal.Inthissectionwebrieﬂyintroduce
thebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingto
trainandperforminferenceinthemodel WedeﬁnetheBoltzmannmachineovera d-dimensionalbinaryrandomvector
x ∈{0 ,1}d TheBoltzmannmachineisanenergy-basedmodel(section),16.2.4
654
CHAPTER20.DEEPGENERATIVEMODELS
meaningwedeﬁnethejointprobabilitydistributionusinganenergyfunction:
P() = xexp(()) − E x
Z, (20.1)
where E( x)istheenergyfunctionand Zisthepartitionfunctionthatensures
that
x P() = 1 x.TheenergyfunctionoftheBoltzmannmachineisgivenby
E() = x − xU x b−x , (20.2)
where Uisthe“weight”matrixofmodelparametersand bisthevectorofbias
parameters InthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftraining
examples,eachofwhichare n-dimensional.Equationdescribesthejoint 20.1
probabilitydistributionovertheobservedvariables.Whilethisscenarioiscertainly
viable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablesto
thosedescribedbytheweightmatrix.Speciﬁcally,itmeansthattheprobabilityof
oneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesof
theotherunits TheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesare
observed.Inthiscase,thelatentvariables,canactsimilarlytohiddenunitsina
multi-layerperceptronandmodelhigher-orderinteractionsamongthevisibleunits JustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresults
intheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachine
withhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetween
variables.Instead,theBoltzmannmachinebecomesauniversalapproximator of
probabilitymassfunctionsoverdiscretevariables( ,)

============================================================

=== CHUNK 173 ===
Palavras: 355
Caracteres: 5469
--------------------------------------------------
LeRouxandBengio2008
Formally,wedecomposetheunits xintotwosubsets:thevisibleunits vand
thelatent(orhidden)units.Theenergyfunctionbecomes h
E ,( v h v ) = −R v v−W h h−S h b−v c−h .(20.3)
BoltzmannMachineLearningLearningalgorithmsforBoltzmannmachines
areusuallybasedonmaximumlikelihood.AllBoltzmannmachineshavean
intractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap-
proximatedusingthetechniquesdescribedinchapter.18
OneinterestingpropertyofBoltzmannmachineswhentrainedwithlearning
rulesbasedonmaximumlikelihoodisthattheupdateforaparticularweight
connectingtwounitsdependsonlythestatisticsofthosetwounits,collected
underdiﬀerentdistributions: P m o de l( v)and ˆ P da t a( v) P m o de l( h v|).Therestofthe
6 5 5
CHAPTER20.DEEPGENERATIVEMODELS
networkparticipatesinshapingthosestatistics,buttheweightcanbeupdated
withoutknowinganythingabouttherestofthenetworkorhowthosestatisticswere
produced.Thismeansthatthelearningruleis“local,”whichmakesBoltzmann
machinelearningsomewhatbiologicallyplausible Itisconceivablethatifeach
neuronwerearandomvariableinaBoltzmannmachine,thentheaxonsand
dendritesconnectingtworandomvariablescouldlearnonlybyobservingtheﬁring
patternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthe
positivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnection
strengthened.ThisisanexampleofaHebbianlearningrule(,)oftenHebb1949
summarizedwiththemnemonic“ﬁretogether,wiretogether.” Hebbian learning
rulesareamongtheoldesthypothesizedexplanationsforlearninginbiological
systemsandremainrelevanttoday( ,) Giudice e t a l .2009
Otherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseem
torequireustohypothesizetheexistenceofmoremachinerythanthis.For
example,forthebraintoimplementback-propagation inamultilayerperceptron,
itseemsnecessaryforthebraintomaintainasecondarycommunication networkfor
transmittinggradientinformationbackwardsthroughthenetwork.Proposalsfor
biologicallyplausibleimplementations(andapproximations)ofback-propagation
havebeenmade(,;,)butremaintobevalidated,and Hinton2007aBengio2015
Bengio2015()linksback-propagationofgradientstoinferenceinenergy-based
modelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables) ThenegativephaseofBoltzmannmachinelearningissomewhatharderto
explainfromabiologicalpointofview.Asarguedinsection,dreamsleep 18.2
maybeaformofnegativephasesampling.Thisideaismorespeculativethough 20.2RestrictedBoltzmannMachines
Inventedunderthenameharmonium(,),restrictedBoltzmann Smolensky1986
machinesaresomeofthemostcommonbuildingblocksofdeepprobabilisticmodels WehavebrieﬂydescribedRBMspreviously,insection.Herewereviewthe 16.7.1
previousinformationandgointomoredetail.RBMsareundirectedprobabilistic
graphicalmodelscontainingalayerofobservablevariablesandasinglelayerof
latentvariables.RBMsmaybestacked(oneontopoftheother)toformdeeper
models.Seeﬁgureforsomeexamples.Inparticular,ﬁgureashowsthe 20.1 20.1
graphstructureoftheRBMitself.Itisabipartitegraph,withnoconnections
permittedbetweenanyvariablesintheobservedlayerorbetweenanyunitsinthe
latentlayer 6 5 6
CHAPTER20.DEEPGENERATIVEMODELS
h 1 h 1 h 2 h 2 h 3 h 3
v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 h( 1 )
1 h( 1 )
1 h( 1 )
2 h( 1 )
2 h( 1 )
3 h( 1 )
3
v 1 v 1 v 2 v 2 v 3 v 3h( 2 )
1 h( 2 )
1 h( 2 )
2 h( 2 )
2h( 2 )
3h( 2 )
3
h( 1 )
4 h( 1 )
4
(a) (b)
h( 1 )
1h( 1 )
1h( 1 )
2h( 1 )
2h( 1 )
3 h( 1 )
3
v 1 v 1 v 2 v 2 v 3 v 3h( 2 )
1 h( 2 )
1 h( 2 )
2 h( 2 )
2 h( 2 )
3 h( 2 )
3
h( 1 )
4h( 1 )
4
(c)
Figure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines ( a )TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedon
abipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsinthe
otherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamong
thehiddenunits Typicallyeveryvisibleunitisconnectedtoeveryhiddenunitbutit
ispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A ( b )
deepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirected
connections.LikeanRBM,ithasnointralayerconnections.However,aDBNhasmultiple
hiddenlayers,andthusthereareconnectionsbetweenhiddenunitsthatareinseparate
layers.Allofthelocalconditionalprobabilitydistributionsneededbythedeepbelief
networkarecopieddirectlyfromthelocalconditionalprobabilitydistributionsofits
constituentRBMs.Alternatively,wecouldalsorepresentthedeepbeliefnetworkwith
acompletelyundirectedgraph,butitwouldneedintralayerconnectionstocapturethe
dependenciesbetweenparents.AdeepBoltzmannmachineisanundirectedgraphical ( c )
modelwithseverallayersoflatentvariables.LikeRBMsandDBNs,DBMslackintralayer
connections DBMsarelesscloselytiedtoRBMsthanDBNsare Wheninitializinga
DBMfromastackofRBMs,itisnecessarytomodifytheRBMparametersslightly.Some
kindsofDBMsmaybetrainedwithoutﬁrsttrainingasetofRBMs 6 5 7
CHAPTER20.DEEPGENERATIVEMODELS
WebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butas
weseelaterthereareextensionstoothertypesofvisibleandhiddenunits Moreformally,lettheobservedlayerconsistofasetof n vbinaryrandom
variableswhichwerefertocollectivelywiththevectorv.Werefertothelatentor
hiddenlayerof n hbinaryrandomvariablesas h
LikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisan
energy-basedmodelwiththejointprobabilitydistributionspeciﬁedbyitsenergy
function:
P , (= v vh= ) = h1
Zexp(( )) − E v h , (20.4)
TheenergyfunctionforanRBMisgivenby
E ,( v h b ) = −v c−h v−W h , (20.5)
andisthenormalizingconstantknownasthepartitionfunction: Z
Z=
v
hexp ( ) {− E v h ,}

============================================================

=== CHUNK 174 ===
Palavras: 350
Caracteres: 4215
--------------------------------------------------
(20.6)
Itisapparentfromthedeﬁnitionofthepartitionfunction Zthatthenaivemethod
ofcomputing Z(exhaustivelysummingoverallstates)couldbecomputationally
intractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesinthe
probabilitydistributiontocompute Zfaster.InthecaseofrestrictedBoltzmann
machines, ()formallyprovedthatthepartitionfunction LongandServedio2010 Z
isintractable.Theintractablepartitionfunction Zimpliesthatthenormalized
jointprobabilitydistributionisalsointractabletoevaluate P() v
20.2.1ConditionalDistributions
Though P( v)isintractable,thebipartitegraphstructureoftheRBMhasthe
veryspecialpropertythatitsconditionaldistributions P(hv|)and P(vh|)are
factorialandrelativelysimpletocomputeandtosamplefrom Derivingtheconditionaldistributionsfromthejointdistributionisstraightfor-
ward:
P( ) = h v|P ,( h v)
P() v(20.7)
=1
P() v1
Zexp
bv c+h v+W h
(20.8)
=1
Zexp
ch v+W h
(20.9)
6 5 8
CHAPTER20.DEEPGENERATIVEMODELS
=1
Zexp

n h
j = 1c j h j+n h
j = 1vW : , j h j

(20.10)
=1
Zn h
j = 1exp
c j h j+ vW : , j h j
(20.11)
Sinceweareconditioningonthevisibleunits v,wecantreattheseasconstant
withrespecttothedistribution P(hv|).Thefactorialnatureoftheconditional
P(hv|)followsimmediately fromourabilitytowritethejointprobabilityover
thevector hastheproductof(unnormalized) distributionsovertheindividual
elements, h j.Itisnowasimplematterofnormalizingthedistributionsoverthe
individualbinary h j P h( j= 1 ) =| v˜ P h( j= 1 )| v
˜ P h( j= 0 )+| v ˜ P h( j= 1 )| v(20.12)
=exp
c j+ vW : , j
exp0+exp {} { c j+ v W : , j}(20.13)
= σ
c j+ vW : , j (20.14)
Wecannowexpressthefullconditionaloverthehiddenlayerasthefactorial
distribution:
P( ) = h v|n h
j = 1σ
(2 1) (+ h−  c Wv)
j.(20.15)
Asimilarderivationwillshowthattheotherconditionofinteresttous, P( v h|),
isalsoafactorialdistribution:
P( ) = v h|n v
i = 1σ((2 1) (+ )) v−  b W hi (20.16)
20.2.2TrainingRestrictedBoltzmannMachines
BecausetheRBMadmitseﬃcientevaluationanddiﬀerentiation of˜ P( v)and
eﬃcientMCMCsamplingintheformofblockGibbssampling,itcanreadilybe
trainedwithanyofthetechniquesdescribedinchapterfortrainingmodels 18
thathaveintractablepartitionfunctions ThisincludesCD,SML(PCD),ratio
matchingandsoon.Comparedtootherundirectedmodelsusedindeeplearning,
theRBMisrelativelystraightforwardtotrainbecausewecancompute P(h| v)
6 5 9
CHAPTER20.DEEPGENERATIVEMODELS
exactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmann
machine,combineboththediﬃcultyofanintractablepartitionfunctionandthe
diﬃcultyofintractableinference 20.3DeepBeliefNetworks
Deepbeliefnetworks(DBNs)wereoneoftheﬁrstnon-convolutionalmodels
tosuccessfullyadmittrainingofdeeparchitectures(Hinton2006Hinton e t a l .,;,
2007b).Theintroduction ofdeepbeliefnetworksin2006beganthecurrentdeep
learningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodels
wereconsideredtoodiﬃculttooptimize.Kernelmachineswithconvexobjective
functionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstrated
thatdeeparchitecturescanbesuccessful,byoutperformingkernelizedsupport
vectormachinesontheMNISTdataset( ,).Today,deepbelief Hinton e t a l .2006
networkshavemostlyfallenoutoffavorandarerarelyused,evencomparedto
otherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedly
recognizedfortheirimportantroleindeeplearninghistory Deepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables Thelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinary
orreal.Therearenointralayerconnections.Usually,everyunitineachlayeris
connectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstruct
moresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersare
undirected.Theconnectionsbetweenallotherlayersaredirected,withthearrows
pointedtowardthelayerthatisclosesttothedata.Seeﬁgurebforanexample 20.1
ADBNwith lhiddenlayerscontains lweightmatrices: W( 1 ), , W( ) l.It
alsocontains l+1biasvectors: b( 0 ), , b( ) l,with b( 0 )providingthebiasesforthe
visiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenby
P( h( ) l, h( 1 ) l−) exp∝
b( ) lh( ) l+ b( 1 ) l−h( 1 ) l−+ h( 1 ) l−W( ) lh( ) l
,(20.17)
P h(( ) k
i= 1 | h( + 1 ) k) = σ
b( ) k
i+ W( + 1 ) k 
: , i h( + 1 ) k
∀∀∈ − i , k1 ,

============================================================

=== CHUNK 175 ===
Palavras: 377
Caracteres: 6185
--------------------------------------------------
, l2 ,(20.18)
P v( i= 1 | h( 1 )) = σ
b( 0 )
i+ W( 1 )
: , i h( 1 )
∀ i .(20.19)
Inthecaseofreal-valuedvisibleunits,substitute
v∼N
v b;( 0 )+ W( 1 )h( 1 ), β− 1
(20.20)
6 6 0
CHAPTER20.DEEPGENERATIVEMODELS
with βdiagonalfortractability.Generalizations tootherexponentialfamilyvisible
unitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayeris
justanRBM TogenerateasamplefromaDBN,weﬁrstrunseveralstepsofGibbssampling
onthetoptwohiddenlayers.Thisstageisessentiallydrawingasamplefrom
theRBMdeﬁnedbythetoptwohiddenlayers.Wecanthenuseasinglepassof
ancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisible
units Deepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirected
modelsandundirectedmodels Inferenceinadeepbeliefnetworkisintractableduetotheexplainingaway
eﬀectwithineachdirectedlayer,andduetotheinteractionbetweenthetwohidden
layersthathaveundirectedconnections.Evaluatingormaximizingthestandard
evidencelowerboundonthelog-likelihoodisalsointractable,becausetheevidence
lowerboundtakestheexpectationofcliqueswhosesizeisequaltothenetwork
width Evaluatingormaximizingthelog-likelihoodrequiresnotjustconfrontingthe
problemofintractableinferencetomarginalizeoutthelatentvariables,butalso
theproblemofanintractablepartitionfunctionwithintheundirectedmodelof
thetoptwolayers Totrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximize
E v∼ pdatalog p( v)usingcontrastivedivergenceorstochasticmaximumlikelihood TheparametersoftheRBMthendeﬁnetheparametersoftheﬁrstlayerofthe
DBN.Next,asecondRBMistrainedtoapproximatelymaximize
E v∼ pdata Eh( 1 )∼ p( 1 ) ( h( 1 )| v )log p( 2 )( h( 1 )) (20.21)
where p( 1 )istheprobabilitydistributionrepresentedbytheﬁrstRBMand p( 2 )
istheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords,
thesecondRBMistrainedtomodelthedistributiondeﬁnedbysamplingthe
hiddenunitsoftheﬁrstRBM,whentheﬁrstRBMisdrivenbythedata This
procedurecanberepeatedindeﬁnitely,toaddasmanylayerstotheDBNas
desired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBM
deﬁnesanotherlayeroftheDBN.Thisprocedurecanbejustiﬁedasincreasinga
variationallowerboundonthelog-likelihoodofthedataundertheDBN(Hinton
e t a l .,).2006
Inmostapplications,noeﬀortismadetojointlytraintheDBNafterthegreedy
layer-wiseprocedureiscomplete.However,itispossibletoperformgenerative
ﬁne-tuningusingthewake-sleepalgorithm 6 6 1
CHAPTER20.DEEPGENERATIVEMODELS
ThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostofthe
interestinDBNsarosefromtheirabilitytoimproveclassiﬁcationmodels.Wecan
taketheweightsfromtheDBNandusethemtodeﬁneanMLP:
h( 1 )= σ
b( 1 )+ vW( 1 ) (20.22)
h( ) l= σ
b( ) l
i+ h( 1 ) l−W( ) l
∀∈ l2 , , m ,(20.23)
AfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerative
trainingoftheDBN,wemaytraintheMLPtoperformaclassiﬁcationtask.This
additionaltrainingoftheMLPisanexampleofdiscriminativeﬁne-tuning ThisspeciﬁcchoiceofMLPissomewhatarbitrary,comparedtomanyofthe
inferenceequationsinchapterthatarederivedfromﬁrstprinciples.ThisMLP 19
isaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistently
intheliterature.Manyapproximate inferencetechniquesaremotivatedbytheir
abilitytoﬁndamaximally variationallowerboundonthelog-likelihood t i g h t
undersomesetofconstraints.Onecanconstructavariationallowerboundonthe
log-likelihoodusingthehiddenunitexpectationsdeﬁnedbytheDBN’sMLP,but
thisistrueofprobabilitydistributionoverthehiddenunits,andthereisno a ny
reasontobelievethatthisMLPprovidesaparticularlytightbound Inparticular,
theMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.The
MLPpropagatesinformationupwardfromthevisibleunitstothedeepesthidden
units,butdoesnotpropagateanyinformationdownwardorsideways.TheDBN
graphicalmodelhasexplainingawayinteractionsbetweenallofthehiddenunits
withinthesamelayeraswellastop-downinteractionsbetweenlayers Whilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwith
AIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasa
generativemodel Theterm“deepbeliefnetwork”iscommonlyusedincorrectlytorefertoany
kindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics Theterm“deepbeliefnetwork”shouldreferspeciﬁcallytomodelswithundirected
connectionsinthedeepestlayeranddirectedconnectionspointingdownward
betweenallotherpairsofconsecutivelayers Theterm“deepbeliefnetwork”mayalsocausesomeconfusionbecausethe
term“beliefnetwork”issometimesusedtorefertopurelydirectedmodels,while
deepbeliefnetworkscontainanundirectedlayer.Deepbeliefnetworksalsoshare
theacronymDBNwithdynamicBayesiannetworks(DeanandKanazawa1989,),
whichareBayesiannetworksforrepresentingMarkovchains 6 6 2
CHAPTER20.DEEPGENERATIVEMODELS
h( 1 )
1 h( 1 )
1 h( 1 )
2 h( 1 )
2 h( 1 )
3 h( 1 )
3
v 1 v 1 v 2 v 2 v 3 v 3h( 2 )
1 h( 2 )
1 h( 2 )
2 h( 2 )
2h( 2 )
3h( 2 )
3
h( 1 )
4 h( 1 )
4
Figure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer
(bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers Therearenointralayerlayerconnections 20.4DeepBoltzmannMachines
AdeepBoltzmannmachineorDBM(Salakhutdino vandHinton2009a,)is
anotherkindofdeep,generativemodel Unlikethedeepbeliefnetwork(DBN),
itisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayers
oflatentvariables(RBMshavejustone) ButliketheRBM,withineachlayer,
eachofthevariablesaremutuallyindependent,conditionedonthevariablesin
theneighboringlayers.Seeﬁgureforthegraphstructure.DeepBoltzmann 20.2
machineshavebeenappliedtoavarietyoftasksincludingdocumentmodeling
(Srivastava2013 e t a l .,) LikeRBMsandDBNs, DBMstypicallycontainonlybinaryunits—aswe
assumeforsimplicityofourpresentationofthemodel—butitisstraightforward
toincludereal-valuedvisibleunits ADBMisanenergy-basedmodel,meaningthatthethejointprobability
distributionoverthemodelvariablesisparametrized byanenergyfunction E.In
thecaseofadeepBoltzmannmachinewithonevisiblelayer, v,andthreehidden
layers, h( 1 ), h( 2 )and h( 3 ),thejointprobabilityisgivenby:
P
v h ,( 1 ), h( 2 ), h( 3 )
=1
Z() θexp
− E ,( v h( 1 ), h( 2 ), h( 3 );) θ
.(20.24)
Tosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergy
functionisthendeﬁnedasfollows:
E ,( v h( 1 ), h( 2 ), h( 3 );) = θ − vW( 1 )h( 1 )− h( 1 )W( 2 )h( 2 )− h( 2 )W( 3 )h( 3 )

============================================================

=== CHUNK 176 ===
Palavras: 368
Caracteres: 5521
--------------------------------------------------
(20.25)
6 6 3
CHAPTER20.DEEPGENERATIVEMODELS
h( 1 )
1 h( 1 )
1 h( 1 )
2 h( 1 )
2h( 1 )
3h( 1 )
3
v 1 v 1 v 2 v 2h( 2 )
1h( 2 )
1h( 2 )
2h( 2 )
2h( 2 )
3 h( 2 )
3h( 3 )
1 h( 3 )
1 h( 3 )
2 h( 3 )
2
v1
v2h( 2 )
1 h( 2 )
1
h( 2 )
2 h( 2 )
2
h( 2 )
3h( 2 )
3
h( 1 )
1 h( 1 )
1
h( 1 )
2 h( 1 )
2
h( 1 )
3 h( 1 )
3h( 3 )
1 h( 3 )
1
h( 3 )
2 h( 3 )
2
Figure20.3:AdeepBoltzmannmachine,re-arrangedtorevealitsbipartitegraphstructure IncomparisontotheRBMenergyfunction(equation),theDBMenergy 20.5
functionincludesconnectionsbetweenthehiddenunits(latentvariables)inthe
formoftheweightmatrices( W( 2 )and W( 3 )).Aswewillsee,theseconnections
havesigniﬁcantconsequencesforboththemodelbehavioraswellashowwego
aboutperforminginferenceinthemodel IncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon-
nectedtoeveryotherunit),theDBMoﬀerssomeadvantagesthataresimilar
tothoseoﬀeredbytheRBM.Speciﬁcally,asillustratedinﬁgure,theDBM20.3
layerscanbeorganizedintoabipartitegraph,withoddlayersononesideand
evenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthe
variablesintheevenlayer,thevariablesintheoddlayersbecomeconditionally
independent.Ofcourse,whenweconditiononthevariablesintheoddlayers,the
variablesintheevenlayersalsobecomeconditionallyindependent ThebipartitestructureoftheDBMmeansthatwecanapplythesameequa-
tionswehavepreviouslyusedfortheconditionaldistributionsofanRBMto
determinetheconditionaldistributionsinaDBM.Theunitswithinalayerare
conditionallyindependentfromeachothergiventhevaluesoftheneighboring
layers,sothedistributionsoverbinaryvariablescanbefullydescribedbythe
Bernoulliparametersgivingtheprobabilityofeachunitbeingactive.Inour
examplewithtwohiddenlayers,theactivationprobabilities aregivenby:
P v( i= 1 | h( 1 )) = σ
W( 1 )
i , : h( 1 )
, (20.26)
6 6 4
CHAPTER20.DEEPGENERATIVEMODELS
P h(( 1 )
i= 1 | v h ,( 2 )) = σ
vW( 1 )
: , i+ W( 2 )
i , : h( 2 )
(20.27)
and
P h(( 2 )
k= 1 | h( 1 )) = σ
h( 1 )W( 2 )
: , k (20.28)
ThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachine
eﬃcient ThenaiveapproachtoGibbssamplingistoupdateonlyonevariable
atatime.RBMsallowallofthevisibleunitstobeupdatedinoneblockandall
ofthehiddenunitstobeupdatedinasecondblock.Onemightnaivelyassume
thataDBMwith llayersrequires l+1updates,witheachiterationupdatinga
blockconsistingofonelayerofunits.Instead,itispossibletoupdateallofthe
unitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksof
updates,oneincludingallevenlayers(includingthevisiblelayer)andtheother
includingalloddlayers.DuetothebipartiteDBMconnectionpattern,given
theevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbe
sampledsimultaneouslyandindependentlyasablock.Likewise,giventheodd
layers,theevenlayerscanbesampledsimultaneouslyandindependentlyasa
block.Eﬃcientsamplingisespeciallyimportantfortrainingwiththestochastic
maximumlikelihoodalgorithm 20.4.1InterestingProperties
DeepBoltzmannmachineshavemanyinterestingproperties DBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu-
tion P( h v|)issimplerforDBMs.Somewhatcounterintuitively,thesimplicityof
thisposteriordistributionallowsricherapproximationsoftheposterior.Inthecase
oftheDBN,weperformclassiﬁcationusingaheuristicallymotivatedapproximate
inferenceprocedure,inwhichweguessthatareasonablevalueforthemeanﬁeld
expectationofthehiddenunitscanbeprovidedbyanupwardpassthroughthe
networkinanMLPthatusessigmoidactivationfunctionsandthesameweightsas
theoriginalDBN.distribution A ny Q( h)maybeusedtoobtainavariationallower
boundonthelog-likelihood.Thisheuristicprocedurethereforeallowsustoobtain
suchabound.However,theboundisnotexplicitlyoptimizedinanyway,sothe
boundmaybefarfromtight.Inparticular,theheuristicestimateof Qignores
interactionsbetweenhiddenunitswithinthesamelayeraswellasthetop-down
feedbackinﬂuenceofhiddenunitsindeeperlayersonhiddenunitsthatarecloser
totheinput.BecausetheheuristicMLP-basedinferenceprocedureintheDBN
isnotabletoaccountfortheseinteractions, theresulting Qispresumablyfar
6 6 5
CHAPTER20.DEEPGENERATIVEMODELS
fromoptimal.InDBMs,allofthehiddenunitswithinalayerareconditionally
independentgiventheotherlayers Thislackofintralayerinteractionmakesit
possibletouseﬁxedpointequationstoactuallyoptimizethevariationallower
boundandﬁndthetrueoptimalmeanﬁeldexpectations(towithinsomenumerical
tolerance) Theuseofpropermeanﬁeldallowstheapproximate inferenceprocedurefor
DBMstocapturetheinﬂuenceoftop-downfeedbackinteractions Thismakes
DBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrain
isknowntousemanytop-downfeedbackconnections.Becauseofthisproperty,
DBMshavebeenusedascomputational modelsofrealneuroscientiﬁcphenomena
(,; Series e t a l .2010Reichert2011 e t a l .,) OneunfortunatepropertyofDBMsisthatsamplingfromthemisrelatively
diﬃcult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.The
otherlayersareusedonlyattheendofthesamplingprocess,inoneeﬃcient
ancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessaryto
useMCMCacrossalllayers,witheverylayerofthemodelparticipating inevery
Markovchaintransition 20.4.2DBMMeanFieldInference
TheconditionaldistributionoveroneDBMlayergiventheneighboringlayersis
factorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributions
are P( v h|( 1 )), P( h( 1 )| v h ,( 2 ))and P( h( 2 )| h( 1 )).Thedistributionover a l l
hiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers Intheexamplewithtwohiddenlayers, P( h( 1 ), h( 2 )| v)doesnotfactorizeduedue
totheinteractionweights W( 2 )between h( 1 )and h( 2 )whichrenderthesevariables
mutuallydependent

============================================================

=== CHUNK 177 ===
Palavras: 410
Caracteres: 5791
--------------------------------------------------
AswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximate
theDBMposteriordistribution However,unliketheDBN,theDBMposterior
distributionovertheirhiddenunits—whilecomplicated—is easytoapproximate
withavariationalapproximation(asdiscussedinsection), speciﬁcallya 19.4
meanﬁeldapproximation Themeanﬁeldapproximation isasimpleformof
variationalinference,wherewerestricttheapproximatingdistributiontofully
factorialdistributions.InthecontextofDBMs,themeanﬁeldequationscapture
thebidirectionalinteractionsbetweenlayers.Inthissectionwederivetheiterative
approximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton
().2009a
Invariationalapproximations toinference,weapproachthetaskofapproxi-
6 6 6
CHAPTER20.DEEPGENERATIVEMODELS
matingaparticulartargetdistribution—inourcase,theposteriordistributionover
thehiddenunitsgiventhevisibleunits—bysomereasonablysimplefamilyofdis-
tributions.Inthecaseofthemeanﬁeldapproximation, theapproximating family
isthesetofdistributionswherethehiddenunitsareconditionallyindependent Wenowdevelopthemeanﬁeldapproachfortheexamplewithtwohidden
layers.Let Q( h( 1 ), h( 2 )| v)betheapproximation of P( h( 1 ), h( 2 )| v).Themean
ﬁeldassumptionimpliesthat
Q( h( 1 ), h( 2 )| v) =
jQ h(( 1 )
j| v)
kQ h(( 2 )
k| v) .(20.29)
Themeanﬁeldapproximationattemptstoﬁndamemberofthisfamilyof
distributionsthatbestﬁtsthetrueposterior P( h( 1 ), h( 2 )| v) Importantly ,the
inferenceprocessmustberunagaintoﬁndadiﬀerentdistribution Qeverytime
weuseanewvalueof v
Onecanconceiveofmanywaysofmeasuringhowwell Q( h v|)ﬁts P( h v|) Themeanﬁeldapproachistominimize
KL( ) = Q P
hQ( h( 1 ), h( 2 )| v)log
Q( h( 1 ), h( 2 )| v)
P( h( 1 ) , h( 2 )| v)
.(20.30)
Ingeneral,wedonothavetoprovideaparametricformoftheapproximating
distributionbeyondenforcingtheindependenceassumptions.Thevariational
approximationprocedureisgenerallyabletorecoverafunctionalformofthe
approximatedistribution.However,inthecaseofameanﬁeldassumptionon
binaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgenerality
resultingfromﬁxingaparametrization ofthemodelinadvance Weparametrize QasaproductofBernoullidistributions,thatisweassociate
theprobabilityofeachelementof h( 1 )withaparameter.Speciﬁcally,foreach j,
ˆh( 1 )
j= Q( h( 1 )
j= 1| v),where ˆh( 1 )
j∈[0 ,1]andforeach k,ˆh( 2 )
k= Q( h( 2 )
k= 1| v),
where ˆ h( 2 )
k∈[01] ,.Thuswehavethefollowingapproximationtotheposterior:
Q( h( 1 ), h( 2 )| v) =
jQ h(( 1 )
j| v)
kQ h(( 2 )
k| v) (20.31)
=
j(ˆ h( 1 )
j)h( 1 )
j(1−ˆh( 1 )
j)( 1− h( 1 )
j )×
k(ˆh( 2 )
k)h( 2 )
k(1−ˆh( 2 )
k)( 1− h( 2 )
k) (20.32)
Ofcourse,forDBMswithmorelayerstheapproximateposteriorparametrization
canbeextendedintheobviousway,exploitingthebipartitestructureofthegraph
6 6 7
CHAPTER20.DEEPGENERATIVEMODELS
toupdatealloftheevenlayerssimultaneouslyandthentoupdatealloftheodd
layerssimultaneously,followingthesamescheduleasGibbssampling Nowthatwehavespeciﬁedourfamilyofapproximating distributions Q,it
remainstospecifyaprocedureforchoosingthememberofthisfamilythatbest
ﬁts P.Themoststraightforwardwaytodothisistousethemeanﬁeldequations
speciﬁedbyequation.Theseequationswerederivedbysolvingforwherethe 19.56
derivativesofthevariationallowerboundarezero.Theydescribeinanabstract
mannerhowtooptimizethevariationallowerboundforanymodel,simplyby
takingexpectationswithrespectto Q
Applyingthesegeneralequations,weobtaintheupdaterules(again,ignoring
biasterms):
ˆh( 1 )
j= σ
iv i W( 1 )
i , j+
kW( 2 )
j , kˆ h( 2 )
k
, j∀ (20.33)
ˆh( 2 )
k= σ

jW( 2 )
j , kˆh( 1 )
j
 , k .∀ (20.34)
Ataﬁxedpointofthissystemofequations,wehavealocalmaximumofthe
variationallowerbound L( Q).Thustheseﬁxedpointupdateequationsdeﬁnean
iterativealgorithmwherewealternateupdatesofˆh( 1 )
j(usingequation)and20.33
updatesofˆh( 2 )
k(usingequation).OnsmallproblemssuchasMNIST,asfew 20.34
asteniterationscanbesuﬃcienttoﬁndanapproximate positivephasegradient
forlearning,andﬁftyusuallysuﬃcetoobtainahighqualityrepresentationof
asinglespeciﬁcexampletobeusedforhigh-accuracy classiﬁcation.Extending
approximatevariationalinferencetodeeperDBMsisstraightforward 20.4.3DBMParameterLearning
LearningintheDBMmustconfrontboththechallengeofanintractablepartition
function,usingthetechniquesfromchapter,andthechallengeofanintractable 18
posteriordistribution,usingthetechniquesfromchapter.19
Asdescribedinsection,variationalinferenceallowstheconstructionof 20.4.2
adistribution Q( h v|)thatapproximates theintractable P( h v|).Learningthen
proceedsbymaximizing L( v θ , Q ,),thevariationallowerboundontheintractable
log-likelihood, log(;) P v θ
6 6 8
CHAPTER20.DEEPGENERATIVEMODELS
ForadeepBoltzmannmachinewithtwohiddenlayers,isgivenby L
L( ) = Q , θ
i
jv i W( 1 )
i , jˆh( 1 )
j+
j
kˆh( 1 )
j W( 2 )
j , kˆh( 2 )
k− H log()+ Z θ () Q .(20.35)
Thisexpressionstillcontainsthelogpartitionfunction, log Z( θ).Becauseadeep
BoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,the
hardnessresultsforcomputingthepartitionfunctionandsamplingthatapplyto
restrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.Thismeans
thatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequires
approximatemethodssuchasannealedimportancesampling.Likewise,training
themodelrequiresapproximationstothegradientofthelogpartitionfunction.See
chapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained 18
usingstochasticmaximumlikelihood.Manyoftheothertechniquesdescribedin
chapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe 18
abilitytoevaluatetheunnormalized probabilities, ratherthanmerelyobtaina
variationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmann
machinesbecausetheydonotalloweﬃcientsamplingofthehiddenunitsgiventhe
visibleunits—instead,contrastivedivergencewouldrequireburninginaMarkov
chaineverytimeanewnegativephasesampleisneeded

============================================================

=== CHUNK 178 ===
Palavras: 392
Caracteres: 5784
--------------------------------------------------
Thenon-variationalversionofstochasticmaximumlikelihoodalgorithmwas
discussedearlier,insection Variationalstochasticmaximumlikelihoodas 18.2
appliedtotheDBMisgiveninalgorithm .Recallthatwedescribeasimpliﬁed 20.1
varientoftheDBMthatlacksbiasparameters;includingthemistrivial 20.4.4Layer-WisePretraining
Unfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribed
above)fromarandominitialization usuallyresultsinfailure.Insomecases,the
modelfailstolearntorepresentthedistributionadequately.Inothercases,the
DBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancould
beobtainedwithjustanRBM.ADBMwithverysmallweightsinallbuttheﬁrst
layerrepresentsapproximatelythesamedistributionasanRBM Varioustechniquesthatpermitjointtraininghavebeendevelopedandare
describedinsection.However,theoriginalandmostpopularmethodfor 20.4.5
overcomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining Inthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.The
ﬁrstlayeristrainedtomodeltheinputdata.EachsubsequentRBMistrainedto
modelsamplesfromthepreviousRBM’sposteriordistribution Afterallofthe
6 6 9
CHAPTER20.DEEPGENERATIVEMODELS
Algorithm20.1Thevariationalstochasticmaximumlikelihoodalgorithmfor
trainingaDBMwithtwohiddenlayers Set,thestepsize,toasmallpositivenumber 
Set k,thenumberofGibbssteps,highenoughtoallowaMarkovchainof
p( v h ,( 1 ), h( 2 ); θ+ ∆ θ)toburnin,startingfromsamplesfrom p( v h ,( 1 ), h( 2 ); θ) Initializethreematrices,˜ V,˜ H( 1 )and ˜ H( 2 )eachwith mrowssettorandom
values(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedto
themodel’smarginals) whilenotconverged(learningloop)do
Sampleaminibatchof mexamplesfromthetrainingdataandarrangethem
astherowsofadesignmatrix V
Initializematrices ˆ H( 1 )and ˆ H( 2 ),possiblytothemodel’smarginals whilenotconverged(meanﬁeldinferenceloop)do
ˆ H( 1 )← σ
V W( 1 )+ˆ H( 2 )W( 2 ) ˆ H( 2 )← σ
ˆ H( 1 )W( 2 ) endwhile
∆W( 1 )←1
mVˆ H( 1 )
∆W( 2 )←1
mˆ H( 1 )ˆ H( 2 )
for do l k = 1to(Gibbssampling)
Gibbsblock1:
∀ i , j ,˜ V i , jsampledfrom P(˜ V i , j= 1) = σ
W( 1 )
j , :
˜ H( 1 )
i , : ∀ i , j ,˜ H( 2 )
i , jsampledfrom P(˜ H( 2 )
i , j= 1) = σ
˜ H( 1 )
i , : W( 2 )
: , j Gibbsblock2:
∀ i , j ,˜ H( 1 )
i , jsampledfrom P(˜ H( 1 )
i , j= 1) = σ
˜ V i , : W( 1 )
: , j+˜ H( 2 )
i , : W( 2 )
j , : endfor
∆W( 1 )←∆W( 1 )−1
mV˜ H( 1 )
∆W( 2 )←∆W( 2 )−1
m˜ H( 1 )˜ H( 2 )
W( 1 )← W( 1 )+ ∆W( 1 )(thisisacartoonillustration,inpracticeuseamore
eﬀectivealgorithm,suchasmomentumwithadecayinglearningrate)
W( 2 )← W( 2 )+∆ W( 2 )
endwhile
6 7 0
CHAPTER20.DEEPGENERATIVEMODELS
RBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.The
DBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlya
smallchangeinthemodel’sparametersanditsperformanceasmeasuredbythe
log-likelihooditassignstothedata,oritsabilitytoclassifyinputs.Seeﬁgure20.4
foranillustrationofthetrainingprocedure Thisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbears
somepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetof
theparametersateachstep.Thetwomethodsdiﬀerbecausethegreedylayer-wise
trainingprocedureusesadiﬀerentobjectivefunctionateachstep Greedylayer-wisepretrainingofaDBMdiﬀersfromgreedylayer-wisepre-
trainingofaDBN.TheparametersofeachindividualRBMmaybecopiedto
thecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparameters
mustbemodiﬁedbeforeinclusionintheDBM.Alayerinthemiddleofthestack
ofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombined
toformtheDBM,thelayerwillhavebothbottom-upandtop-downinput To
accountforthiseﬀect,SalakhutdinovandHinton2009a()advocatedividingthe
weightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintothe
DBM.Additionally,thebottomRBMmustbetrainedusingtwo“copies”ofeach
visibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeans
thattheweightsareeﬀectivelydoubledduringtheupwardpass.Similarly,thetop
RBMshouldbetrainedwithtwocopiesofthetopmostlayer ObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequires
amodiﬁcationofthestandardSMLalgorithm,whichistouseasmallamountof
meanﬁeldduringthenegativephaseofthejointPCDtrainingstep(Salakhutdinov
andHinton2009a,) Speciﬁcally,theexpectationoftheenergygradientshould
becomputedwithrespecttothemeanﬁelddistributioninwhichalloftheunits
areindependentfromeachother.Theparametersofthismeanﬁelddistribution
shouldbeobtainedbyrunningthemeanﬁeldﬁxedpointequationsforjustone
step.See ()foracomparisonoftheperformanceofcentered Goodfellow e t a l .2013b
DBMswithandwithouttheuseofpartialmeanﬁeldinthenegativephase 20.4.5JointlyTrainingDeepBoltzmannMachines
ClassicDBMsrequiregreedyunsupervisedpretraining,andtoperformclassiﬁcation
well,requireaseparateMLP-basedclassiﬁerontopofthehiddenfeaturesthey
extract.Thishassomeundesirableproperties.Itishardtotrackperformance
duringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhile
trainingtheﬁrstRBM.Thus,itishardtotellhowwellourhyperparameters
6 7 1
CHAPTER20.DEEPGENERATIVEMODELS
d)a) b)
c )
Figure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNIST
dataset(SalakhutdinovandHinton2009aSrivastava2014 ,; e t a l .,).TrainanRBM ( a )
byusingCDtoapproximatelymaximizelog P( v).TrainasecondRBMthatmodels ( b )
h( 1 )andtargetclassybyusingCD- ktoapproximatelymaximizelog P( h( 1 ),y)where
h( 1 )isdrawnfromtheﬁrstRBM’sposteriorconditionedonthedata.Increase kfrom1
to20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately ( c )
maximizelog P(v ,y)usingstochasticmaximumlikelihoodwith k= 5.Delete ( d )yfrom
themodel.Deﬁneanewsetoffeatures h( 1 )and h( 2 )thatareobtainedbyrunningmean
ﬁeldinferenceinthemodellackingy.UsethesefeaturesasinputtoanMLPwhose
structureisthesameasanadditionalpassofmeanﬁeld,withanadditionaloutputlayer
fortheestimateofy.InitializetheMLP’sweightstobethesameastheDBM’sweights

============================================================

=== CHUNK 179 ===
Palavras: 352
Caracteres: 9331
--------------------------------------------------
TraintheMLPtoapproximatelymaximizelog P(y|v)usingstochasticgradientdescent
anddropout.Figurereprintedfrom( ,) Goodfellow e t a l .2013b
6 7 2
CHAPTER20.DEEPGENERATIVEMODELS
areworkinguntilquitelateinthetrainingprocess.Softwareimplementations
ofDBMsneedtohavemanydiﬀerentcomponentsforCDtrainingofindividual
RBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagation
throughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmany
oftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeing
abletoperforminferencewhensomeinputvaluesaremissing Therearetwomainwaystoresolvethejointtrainingproblemofthedeep
Boltzmannmachine.The ﬁrstisthecentereddeepBoltzmann machine
(MontavonandMuller2012,),whichreparametrizes themodelinordertomake
theHessianofthecostfunctionbetter-conditionedatthebeginningofthelearning
process.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wise
pretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihood
andproduceshighqualitysamples.Unfortunately,itremainsunabletocompete
withappropriately regularizedMLPsasaclassiﬁer.Thesecondwaytojointly
trainadeepBoltzmannmachineistouseamulti-predictiondeepBoltzmann
machine(Goodfellow2013b e t a l .,).Thismodelusesanalternativetraining
criterionthatallowstheuseoftheback-propagationalgorithminordertoavoid
theproblemswithMCMCestimatesofthegradient.Unfortunately, thenew
criteriondoesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMC
approach,itdoesleadtosuperiorclassiﬁcationperformanceandabilitytoreason
wellaboutmissinginputs ThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwe
returntothegeneralviewofaBoltzmannmachineasconsistingofasetofunits
xwithaweightmatrix Uandbiases b.Recallfromequationthatheenergy 20.2
functionisgivenby
E() = x − xU x b−x (20.36)
Using diﬀerent sparsity patternsin theweight matrix U, wecan implemen t
structuresofBoltzmannmachines,suchasRBMs,orDBMswithdiﬀerentnumbers
oflayers.Thisisaccomplishedbypartitioning xintovisibleandhiddenunitsand
zeroingoutelementsof Uforunitsthatdonotinteract.ThecenteredBoltzmann
machineintroducesavectorthatissubtractedfromallofthestates: µ
E(; ) = ( ) x U b , − x µ−U x µ x µ (−)(− −)b .(20.37)
Typically µisahyperparameterﬁxedatthebeginningoftraining.Itisusu-
allychosentomakesurethat x µ− ≈0whenthemodelisinitialized.This
reparametrization doesnotchangethesetofprobabilitydistributionsthatthe
modelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescent
appliedtothelikelihood.Speciﬁcally,inmanycases,thisreparametrization results
6 7 3
CHAPTER20.DEEPGENERATIVEMODELS
inaHessianmatrixthatisbetterconditioned ()experimentally Melchior e t a l .2013
conﬁrmedthattheconditioningoftheHessianmatriximproves,andobservedthat
thecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique,
theenhancedgradient(,).Theimprovedconditioningofthe Cho e t a l .2011
Hessianmatrixallowslearningtosucceed,evenindiﬃcultcasesliketraininga
deepBoltzmannmachinewithmultiplelayers TheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti-
predictiondeepBoltzmannmachine(MP-DBM)whichworksbyviewingthemean
ﬁeldequationsasdeﬁningafamilyofrecurrentnetworksforapproximately solving
everypossibleinferenceproblem( ,).Ratherthantraining Goodfellow e t a l .2013b
themodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrent
networkobtainanaccurateanswertothecorrespondinginferenceproblem.The
trainingprocessisillustratedinﬁgure Itconsistsofrandomlysamplinga 20.5
trainingexample,randomlysamplingasubsetofinputstotheinferencenetwork,
andthentrainingtheinferencenetworktopredictthevaluesoftheremaining
units Thisgeneralprincipleofback-propagating throughthecomputational graph
forapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011 e t a l .,;
Brakel2013 e t a l .,).InthesemodelsandintheMP-DBM,theﬁnallossisnot
thelowerboundonthelikelihood.Instead,theﬁnallossistypicallybasedon
theapproximateconditionaldistributionthattheapproximate inferencenetwork
imposesoverthemissingvalues.Thismeansthatthetrainingofthesemodels
issomewhatheuristicallymotivated.Ifweinspectthe p( v)representedbythe
BoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective,
inthesensethatGibbssamplingyieldspoorsamples Back-propagationthroughtheinferencegraphhastwomainadvantages.First,
ittrainsthemodelasitisreallyused—withapproximate inference.Thismeans
thatapproximateinference,forexample,toﬁllinmissinginputs,ortoperform
classiﬁcationdespitethepresenceofmissinginputs,ismoreaccurateintheMP-
DBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurate
classiﬁeronitsown;thebestclassiﬁcationresultswiththeoriginalDBMwere
basedontrainingaseparateclassiﬁertousefeaturesextractedbytheDBM,
ratherthanbyusinginferenceintheDBMtocomputethedistributionoverthe
classlabels.MeanﬁeldinferenceintheMP-DBMperformswellasaclassiﬁer
withoutspecialmodiﬁcations.Theotheradvantageofback-propagating through
approximateinferenceisthatback-propagationcomputestheexactgradientof
theloss.Thisisbetterforoptimization thantheapproximate gradientsofSML
training,whichsuﬀerfrombothbiasandvariance.ThisprobablyexplainswhyMP-
6 7 4
CHAPTER20.DEEPGENERATIVEMODELS
Figure20.5:Anillustrationofthemulti-predictiontrainingprocessforadeepBoltzmann
machine.Eachrowindicatesadiﬀerentexamplewithinaminibatchforthesametraining
step Eachcolumnrepresentsatimestepwithinthemeanﬁeldinferenceprocess For
eachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinference
process.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthe
meanﬁeldinferenceprocess,witharrowsindicatingwhichvariablesinﬂuencewhichother
variablesintheprocess.Inpracticalapplications,weunrollmeanﬁeldforseveralsteps Inthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocess
couldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstothe
inferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessfor
eachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationto
traintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.This
trainsthemeanﬁeldprocessfortheMP-DBMtoproduceaccurateestimates.Figure
adaptedfrom () Goodfellow e t a l .2013b
6 7 5
CHAPTER20.DEEPGENERATIVEMODELS
DBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining Thedisadvantageofback-propagatingthroughtheapproximate inferencegraphis
thatitdoesnotprovideawaytooptimizethelog-likelihood,butratheraheuristic
approximationofthegeneralizedpseudolikelihood TheMP-DBMinspiredtheNADE- k(Raiko2014 e t a l .,)extensiontothe
NADEframework,whichisdescribedinsection.20.10.10
TheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa-
rametersamongmanydiﬀerentcomputational graphs,withthediﬀerencebetween
eachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalso
sharesparametersacrossmanycomputational graphs.InthecaseoftheMP-DBM,
thediﬀerencebetweenthegraphsiswhethereachinputunitisobservedornot Whenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasdropout
does.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.Onecould
imagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunits
ratherthanmakingthemlatent 20.5BoltzmannMachinesforReal-ValuedData
WhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata,
manyapplicationssuchasimageandaudiomodelingseemtorequiretheability
torepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossible
totreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofa
binaryvariable.Forexample, ()treatsgrayscaleimagesinthetraining Hinton2000
setasdeﬁning[0,1]probabilityvalues.Eachpixeldeﬁnestheprobabilityofa
binaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfrom
eachother.Thisisacommonprocedureforevaluatingbinarymodelsongrayscale
imagedatasets.However,itisnotaparticularlytheoreticallysatisfyingapproach,
andbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.In
thissection,wepresentBoltzmannmachinesthatdeﬁneaprobabilitydensityover
real-valueddata 20.5.1Gaussian-BernoulliRBMs
RestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamily
conditionaldistributions(Welling2005 e t a l .,).Ofthese,themostcommonisthe
RBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditional
distributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisa
functionofthehiddenunits 6 7 6
CHAPTER20.DEEPGENERATIVEMODELS
Therearemanywaysofparametrizing Gaussian-Bernoulli RBMs.Onechoice
iswhethertouseacovariancematrixoraprecisionmatrixfortheGaussian
distribution.Herewepresenttheprecisionformulation.Themodiﬁcationtoobtain
thecovarianceformulationisstraightforward Wewishtohavetheconditional
distribution
p , ( ) = (; v h| N v W h β− 1) (20.38)
Wecanﬁndthetermsweneedtoaddtotheenergyfunctionbyexpandingthe
unnormalized logconditionaldistribution:
log (;N v W h β ,− 1) = −1
2( ) v W h −β v W h β (− )+( f) .(20.39)
Here fencapsulatesallthetermsthatareafunctiononlyoftheparameters
andnottherandomvariablesinthemodel.Wecandiscard fbecauseitsonly
roleistonormalizethedistribution,andthepartitionfunctionofwhateverenergy
functionwechoosewillcarryoutthatrole Ifweincludealloftheterms(withtheirsignﬂipped)involving vfromequa-
tioninourenergyfunctionanddonotaddanyothertermsinvolving 20.39 v,then
ourenergyfunctionwillrepresentthedesiredconditional p( ) v h|
Wehavesomefreedomregardingtheotherconditionaldistribution, p( h v|) Notethatequationcontainsaterm 20.39
1
2hWβ W h

============================================================

=== CHUNK 180 ===
Palavras: 233
Caracteres: 3607
--------------------------------------------------
(20.40)
Thistermcannotbeincludedinitsentiretybecauseitincludes h i h jterms.These
correspondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,we
wouldhavealinearfactormodelinsteadofarestrictedBoltzmannmachine.When
designingourBoltzmannmachine,wesimplyomitthese h i h jcrossterms.Omitting
themdoesnotchangetheconditional p( v h|)soequationisstillrespected 20.39
However,westillhaveachoiceaboutwhethertoincludethetermsinvolvingonly
asingle h i.Ifweassumeadiagonalprecisionmatrix,weﬁndthatforeachhidden
unit h iwehaveaterm
1
2h i
jβ j W2
j , i (20.41)
Intheabove,weusedthefactthat h2
i= h ibecause h i∈{0 ,1}.Ifweincludethis
term(withitssignﬂipped)intheenergyfunction,thenitwillnaturallybias h i
tobeturnedoﬀwhentheweightsforthatunitarelargeandconnectedtovisible
unitswithhighprecision.Thechoiceofwhetherornottoincludethisbiasterm
doesnotaﬀectthefamilyofdistributionsthemodelcanrepresent(assumingthat
6 7 7
CHAPTER20.DEEPGENERATIVEMODELS
weincludebiasparametersforthehiddenunits)butitdoesaﬀectthelearning
dynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivations
remainreasonableevenwhentheweightsrapidlyincreaseinmagnitude OnewaytodeﬁnetheenergyfunctiononaGaussian-Bernoulli RBMisthus
E ,( v h) =1
2v( )( ) β v − v βW h b−h(20.42)
butwemayalsoaddextratermsorparametrizetheenergyintermsofthevariance
ratherthanprecisionifwechoose Inthisderivation,wehavenotincludedabiastermonthevisibleunits,butone
couldeasilybeadded.Oneﬁnalsourceofvariabilityintheparametrization ofa
Gaussian-Bernoulli RBMisthechoiceofhowtotreattheprecisionmatrix.Itmay
eitherbeﬁxedtoaconstant(perhapsestimatedbasedonthemarginalprecision
ofthedata)orlearned.Itmayalsobeascalartimestheidentitymatrix,orit
maybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobe
non-diagonal inthiscontext,becausesomeoperationsontheGaussiandistribution
requireinvertingthematrix,andadiagonalmatrixcanbeinvertedtrivially.In
thesectionsahead,wewillseethatotherformsofBoltzmannmachinespermit
modelingthecovariancestructure,usingvarioustechniquestoavoidinvertingthe
precisionmatrix 20.5.2UndirectedModelsofConditionalCovariance
WhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valued
data, ()arguethattheGaussianRBMinductivebiasisnot Ranzato e t a l .2010a
wellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata,
especiallynaturalimages.Theproblemisthatmuchoftheinformationcontent
presentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthan
intherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsand
nottheirabsolutevalueswheremostoftheusefulinformationinimagesresides SincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhe
hiddenunits,itcannotcaptureconditionalcovarianceinformation Inresponse
tothesecriticisms,alternativemodelshavebeenproposedthatattempttobetter
accountforthecovarianceofreal-valueddata.Thesemodelsincludethemeanand
covarianceRBM(mcRBM1),themean-productof t-distribution(mPoT)model
andthespikeandslabRBM(ssRBM) 1Th e t e rm “ m c R B M ” i s p ro n o u n c e d b y s a y i n g t h e n a m e o f t h e l e t t e rs M - C- R - B - M ; t h e “ m c ”
i s n o t p ro n o u n c e d l i k e t h e “ M c ” i n “ M c D o n a l d ’ s ”
6 7 8
CHAPTER20.DEEPGENERATIVEMODELS
MeanandCovarianceRBMThemcRBMusesitshiddenunitstoindepen-
dentlyencodetheconditionalmeanandcovarianceofallobservedunits.The
mcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovariance
units.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM TheotherhalfisacovarianceRBM( ,),alsocalledacRBM, Ranzato e t a l .2010a
whosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow

============================================================

=== CHUNK 181 ===
Palavras: 366
Caracteres: 1751
--------------------------------------------------
Speciﬁcally,withbinarymeanunits h( ) mandbinarycovarianceunits h( ) c,the
mcRBMmodelisdeﬁnedasthecombinationoftwoenergyfunctions:
E m c( x h ,( ) m, h( ) c) = E m( x h ,( ) m)+ E c( x h ,( ) c) ,(20.43)
where E misthestandardGaussian-Bernoulli RBMenergyfunction:2
E m( x h ,( ) m) =1
2xx−
jxW : , j h( ) m
j−
jb( ) m
j h( ) m
j ,(20.44)
and E cisthecRBMenergy function that models the conditionalcovariance
information:
E c( x h ,( ) c) =1
2
jh( ) c
j
xr( ) j2
−
jb( ) c
j h( ) c
j .(20.45)
Theparameter r( ) jcorrespondstothecovarianceweightvectorassociatedwith
h( ) c
jand b( ) cisavectorofcovarianceoﬀsets.Thecombinedenergyfunctiondeﬁnes
ajointdistribution:
p m c( x h ,( ) m, h( ) c) =1
Zexp
− E m c( x h ,( ) m, h( ) c)
,(20.46)
andacorrespondingconditionaldistributionovertheobservationsgiven h( ) mand
h( ) casamultivariateGaussiandistribution:
p m c( x h|( ) m, h( ) c) = N
 x C;m c
x h|

jW : , j h( ) m
j
 , Cm c
x h|
 .(20.47)
Notethatthecovariancematrix Cm c
x h|=
j h( ) c
j r( ) jr( ) j+ I− 1
isnon-diagonal
andthat WistheweightmatrixassociatedwiththeGaussianRBMmodelingthe
2Th i s v e rs i o n o f t h e Ga u s s i a n - B e rn o u l l i R B M e n e rg y f u n c t i o n a s s u m e s t h e i m a g e d a t a h a s
z e ro m e a n , p e r p i x e l P i x e l o ﬀ s e t s c a n e a s i l y b e a d d e d t o t h e m o d e l t o a c c o u n t f o r n o n z e ro p i x e l
m e a n s 6 7 9
CHAPTER20.DEEPGENERATIVEMODELS
conditionalmeans.ItisdiﬃculttotrainthemcRBMviacontrastivedivergenceor
persistentcontrastivedivergencebecauseofitsnon-diagonal conditionalcovariance
structure.CDandPCDrequiresamplingfromthejointdistributionof x h ,( ) m, h( ) c
which,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals

============================================================

=== CHUNK 182 ===
Palavras: 357
Caracteres: 4016
--------------------------------------------------
However,inthemcRBM,samplingfrom p m c( x h|( ) m, h( ) c)requirescomputing
( Cm c)− 1ateveryiterationoflearning.Thiscanbeanimpracticalcomputational
burdenforlargerobservations ()avoiddirectsampling RanzatoandHinton2010
fromtheconditional p m c( x h|( ) m, h( ) c)bysamplingdirectlyfromthemarginal
p( x)usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfree Neal1993
energy Mean-ProductofStudent’s-distributions t Themean-productofStudent’s
t-distribution(mPoT)model( ,)extendsthePoTmodel( Ranzato e t a l .2010b Welling
e t a l .,)inamannersimilartohowthemcRBMextendsthecRBM.This 2003a
isachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussian
RBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionoverthe
observationisamultivariateGaussian(withnon-diagonal covariance)distribution;
however,unlikethemcRBM,thecomplementaryconditionaldistributionoverthe
hiddenvariablesisgivenbyconditionallyindependentGammadistributions.The
GammadistributionG( k , θ) isaprobabilitydistributionoverpositiverealnumbers,
withmean k θ.Itisnotnecessarytohaveamoredetailedunderstandingofthe
GammadistributiontounderstandthebasicideasunderlyingthemPoTmodel ThemPoTenergyfunctionis:
E m P o T( x h ,( ) m, h( ) c) (20.48)
= E m( x h ,( ) m)+
j
h( ) c
j
1+1
2
r( ) jx2
+(1− γ j)log h( ) c
j
(20.49)
where r( ) jisthecovarianceweightvectorassociatedwithunit h( ) c
jand E m( x h ,( ) m)
isasdeﬁnedinequation.20.44
JustaswiththemcRBM,themPoTmodelenergyfunctionspeciﬁesamul-
tivariateGaussian,withaconditionaldistributionover xthathasnon-diagonal
covariance.LearninginthemPoTmodel—again,likethemcRBM—iscompli-
catedbytheinabilityto samplefromthenon-diagonal Gaussianconditional
p m P o T( x h|( ) m, h( ) c),so ()alsoadvocatedirectsamplingof Ranzato e t a l .2010b
p() xviaHamiltonian(hybrid)MonteCarlo 6 8 0
CHAPTER20.DEEPGENERATIVEMODELS
SpikeandSlabRestrictedBoltzmannMachinesSpikeandslabrestricted
Boltzmannmachines( ,)orssRBMsprovideanothermeans Courville e t a l .2011
ofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs,
ssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonian
MonteCarlomethods.LikethemcRBMandthemPoTmodel,thessRBM’sbinary
hiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseof
auxiliaryreal-valuedvariables ThespikeandslabRBMhastwosetsofhiddenunits:binaryspikeunits h,
andreal-valuedslabunits s.Themeanofthevisibleunitsconditionedonthe
hiddenunitsisgivenby( h s) W.Inotherwords,eachcolumn W : , ideﬁnesa
componentthatcanappearintheinputwhen h i= 1.Thecorrespondingspike
variableh idetermineswhetherthatcomponentispresentatall.Thecorresponding
slabvariables ideterminestheintensityofthatcomponent,ifitispresent.When
aspikevariableisactive,thecorrespondingslabvariableaddsvariancetothe
inputalongtheaxisdeﬁnedby W : , i.Thisallowsustomodelthecovarianceofthe
inputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergence
withGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix Formally,thessRBMmodelisdeﬁnedviaitsenergyfunction:
E s s( ) = x s h , , −
ixW : , i s i h i+1
2x
Λ+
iΦ i h i
x (20.50)
+1
2
iα i s2
i−
iα i µ i s i h i−
ib i h i+
iα i µ2
i h i ,(20.51)
where b iistheoﬀsetofthespike h iandΛisadiagonalprecisionmatrixonthe
observations x.Theparameter α i >0isascalarprecisionparameterforthe
real-valuedslabvariable s i.Theparameter Φ iisanon-negativediagonalmatrix
thatdeﬁnesan h-modulatedquadraticpenaltyon x.Each µ iisameanparameter
fortheslabvariable s i Withthejointdistributiondeﬁnedviatheenergyfunction,itisrelatively
straightforwardto derivethessRBM conditionaldistributions.For example,
bymarginalizing outtheslabvariables s,theconditionaldistributionoverthe
observationsgiventhebinaryspikevariablesisgivenby: h
p s s( )= x h|1
P() h1
Z
exp ( ) {− E x s h , ,} d s(20.52)
=N
x C;s s
x h|
iW : , i µ i h i , Cs s
x h|
(20.53)
6 8 1
CHAPTER20.DEEPGENERATIVEMODELS
where Cs s
x h|=
Λ+
iΦ i h i−
iα− 1
i h i W : , i W
: , i− 1.Thelastequalityholdsonlyif
thecovariancematrix Cs s
x h|ispositivedeﬁnite

============================================================

=== CHUNK 183 ===
Palavras: 384
Caracteres: 7453
--------------------------------------------------
Gatingbythespikevariablesmeansthatthetruemarginaldistributionover
hsissparse.Thisisdiﬀerentfromsparsecoding,wheresamplesfromthemodel
“almostnever”(inthemeasuretheoreticsense)containzerosinthecode,andMAP
inferenceisrequiredtoimposesparsity ComparingthessRBMtothemcRBMandthemPoTmodels,thessRBM
parametrizes theconditionalcovarianceoftheobservationinasigniﬁcantlydiﬀerent
way.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservation
as
j h( ) c
j r( ) jr( ) j+ I− 1
,usingtheactivationofthehiddenunits h j >0to
enforceconstraintsontheconditionalcovarianceinthedirection r( ) j.Incontrast,
thessRBMspeciﬁestheconditionalcovarianceoftheobservationsusingthehidden
spikeactivations h i= 1topinchtheprecisionmatrixalongthedirectionspeciﬁed
bythecorrespondingweightvector ThessRBMconditionalcovarianceisvery
similartothatgivenbyadiﬀerentmodel:theproductofprobabilisticprincipal
componentsanalysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercomplete
setting,sparseactivationswiththessRBMparametrization permitsigniﬁcant
variance(abovethenominalvariancegivenbyΛ− 1)onlyintheselecteddirections
ofthesparselyactivated h i InthemcRBMormPoTmodels,anovercomplete
representationwouldmeanthattocapturevariationinaparticulardirectionin
theobservationspacerequiresremovingpotentiallyallconstraintswithpositive
projectioninthatdirection This wouldsuggestthatthesemodelsarelesswell
suitedtotheovercompletesetting TheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachine
isthatsomesettingsoftheparameterscancorrespondtoacovariancematrix
thatisnotpositivedeﬁnite.Suchacovariancematrixplacesmoreunnormalized
probabilityonvaluesthatarefartherfromthemean,causingtheintegralover
allpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimple
heuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Using
constrainedoptimization toexplicitlyavoidtheregionswheretheprobabilityis
undeﬁnedisdiﬃculttodowithoutbeingoverlyconservativeandalsopreventing
themodelfromaccessinghigh-performingregionsofparameterspace Qualitatively,convolutionalvariantsofthessRBMproduceexcellentsamples
ofnaturalimages.Someexamplesareshowninﬁgure.16.1
ThessRBMallowsforseveralextensions.Includinghigher-orderinteractions
andaverage-poolingoftheslabvariables( ,)enablesthemodel Courville e t a l .2014
tolearnexcellentfeaturesforaclassiﬁerwhenlabeleddataisscarce Addinga
6 8 2
CHAPTER20.DEEPGENERATIVEMODELS
termtotheenergyfunctionthatpreventsthepartitionfunctionfrombecoming
undeﬁnedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellow
e t a l .,),alsoknownasS3C 2013d
20.6ConvolutionalBoltzmannMachines
Asseeninchapter,extremelyhighdimensionalinputssuchasimagesplace 9
greatstrainonthecomputation,memoryandstatisticalrequirementsofmachine
learningmodels.Replacingmatrixmultiplication bydiscreteconvolutionwitha
smallkernelisthestandardwayofsolvingtheseproblemsforinputsthathave
translationinvariantspatialortemporalstructure () DesjardinsandBengio2008
showedthatthisapproachworkswellwhenappliedtoRBMs Deepconvolutionalnetworksusuallyrequireapoolingoperationsothatthe
spatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworks
oftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled Itisunclearhowtogeneralizethistothesettingofenergy-basedmodels.We
couldintroduceabinarypoolingunitpover nbinarydetectorunits dandenforce
p=max i d ibysettingtheenergyfunctiontobe∞wheneverthatconstraintis
violated.Thisdoesnotscalewellthough,asitrequiresevaluating 2ndiﬀerent
energyconﬁgurations tocomputethenormalization constant.Forasmall 3×3
poolingregionthisrequires 29= 512energyfunctionevaluationsperpoolingunit Lee2009 e t a l .()developedasolutiontothisproblemcalledprobabilistic
maxpooling(nottobeconfusedwith“stochasticpooling,”whichisatechnique
forimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).The
strategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitsso
atmostonemaybeactiveatatime.Thismeansthereareonly n+ 1total
states(onestateforeachofthe ndetectorunitsbeingon,andanadditionalstate
correspondingtoallofthedetectorunitsbeingoﬀ).Thepoolingunitisonif
andonlyifoneofthedetectorunitsison.Thestatewithallunitsoﬀisassigned
energyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethat
has n+1states,orequivalentlyasamodelthathas n+1variablesthatassigns
energytoallbutjointassignmentsofvariables ∞ n+1
Whileeﬃcient,probabilisticmaxpoolingdoesforcethedetectorunitstobe
mutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontexts
oraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupport
overlappingpoolingregions.Overlapping poolingregionsareusuallyrequired
toobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothis
constraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann
6 8 3
CHAPTER20.DEEPGENERATIVEMODELS
machines Lee2009 e t a l .()demonstratedthatprobabilisticmaxpoolingcouldbeused
tobuildconvolutionaldeepBoltzmannmachines.3Thismodelisabletoperform
operationssuchasﬁllinginmissingportionsofitsinput.Whileintellectually
appealing,thismodelischallengingtomakeworkinpractice,andusuallydoes
notperformaswellasaclassiﬁerastraditionalconvolutionalnetworkstrained
withsupervisedlearning Manyconvolutionalmodelsworkequallywellwithinputsofmanydiﬀerent
spatialsizes.ForBoltzmannmachines,itisdiﬃculttochangetheinputsize
foravarietyofreasons Thepartitionfunctionchangesasthesizeoftheinput
changes.Moreover,manyconvolutionalnetworksachievesizeinvariancebyscaling
upthesizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscaling
Boltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneural
networkscanuseaﬁxednumberofpoolingunitsanddynamicallyincreasethe
sizeoftheirpoolingregionsinordertoobtainaﬁxed-sizerepresentationofa
variable-sizedinput.ForBoltzmannmachines,largepoolingregionsbecometoo
expensiveforthenaiveapproach The approachof()ofmaking Lee e t a l .2009
eachofthedetectorunitsinthesamepoolingregionmutuallyexclusivesolves
thecomputational problems,butstilldoesnotallowvariable-sizepoolingregions Forexample,supposewelearnamodelwith 2×2probabilisticmaxpoolingover
detectorunitsthatlearnedgedetectors Thisenforcestheconstraintthatonly
oneoftheseedgesmayappearineach2×2region.Ifwethenincreasethesizeof
theinputimageby50%ineachdirection,wewouldexpectthenumberofedgesto
increasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregionsby
50%ineachdirectionto3×3,thenthemutualexclusivityconstraintnowspeciﬁes
thateachoftheseedgesmayonlyappearonceina3×3region.Aswegrow
amodel’sinputimageinthisway,themodelgeneratesedgeswithlessdensity Ofcourse,theseissuesonlyarisewhenthemodelmustusevariableamountsof
poolinginordertoemitaﬁxed-sizeoutputvector.Modelsthatuseprobabilistic
maxpoolingmaystillacceptvariable-sizedinputimagessolongastheoutputof
themodelisafeaturemapthatcanscaleinsizeproportionaltotheinputimage Pixelsattheboundaryoftheimagealsoposesomediﬃculty,whichisexac-
erbatedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.If
wedonotimplicitlyzero-padtheinput,thentherearefewerhiddenunitsthan
visibleunits,andthevisibleunitsattheboundaryoftheimagearenotmodeled
3Th e p u b l i c a t i o n d e s c rib e s t h e m o d e l a s a “ d e e p b e l i e f n e t w o rk ” b u t b e c a u s e i t c a n b e d e s c rib e d
a s a p u re l y u n d i re c t e d m o d e l with t ra c t a b l e l a y e r- wis e m e a n ﬁ e l d ﬁ x e d p o i n t u p d a t e s , i t b e s t ﬁ t s
t h e d e ﬁ n i t i o n o f a d e e p B o l t z m a n n m a c h i n e

============================================================

=== CHUNK 184 ===
Palavras: 357
Caracteres: 8587
--------------------------------------------------
6 8 4
CHAPTER20.DEEPGENERATIVEMODELS
wellbecausetheylieinthereceptiveﬁeldoffewerhiddenunits.However,ifwedo
implicitlyzero-padtheinput,thenthehiddenunitsattheboundaryaredrivenby
fewerinputpixels,andmayfailtoactivatewhenneeded 20.7BoltzmannMachinesforStructuredorSequential
Outputs
Inthestructuredoutputscenario,wewishtotrainamodelthatcanmapfrom
someinput xtosomeoutput y,andthediﬀerententriesof yarerelatedtoeach
otherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask,
yisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance Anaturalwaytorepresenttherelationshipsbetweentheentriesin yisto
useaprobabilitydistribution p(y| x).Boltzmannmachines,extendedtomodel
conditionaldistributions,cansupplythisprobabilisticmodel ThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeused
notjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelatter
case,ratherthanmappinganinput xtoanoutput y,themodelmustestimatea
probabilitydistributionoverasequenceofvariables, p(x( 1 ), ,x( ) τ).Conditional
Boltzmannmachinescanrepresentfactorsoftheform p(x( ) t|x( 1 ), ,x( 1 ) t−)in
ordertoaccomplishthistask Animportantsequencemodelingtaskforthevideogameandﬁlmindustry
ismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters Thesesequencesareoftencollectedusingmotioncapturesystemstorecordthe
movementsofactors.Aprobabilisticmodelofacharacter’smovementallows
thegenerationofnew, previouslyunseen, but realisticanimations.Tosolve
thissequencemodelingtask,Taylor2007 e t a l .()introducedaconditionalRBM
modeling p( x( ) t| x( 1 ) t−, , x( ) t m−)forsmall m.ThemodelisanRBMover
p( x( ) t)whosebiasparametersarealinearfunctionofthepreceding mvaluesof x Whenweconditionondiﬀerentvaluesof x( 1 ) t−andearliervariables,wegetanew
RBMoverx.TheweightsintheRBMoverxneverchange,butbyconditioningon
diﬀerentpastvalues,wecanchangetheprobabilityofdiﬀerenthiddenunitsinthe
RBMbeingactive.Byactivatinganddeactivatingdiﬀerentsubsetsofhiddenunits,
wecanmakelargechangestotheprobabilitydistributioninducedonx Other
variantsofconditionalRBM(,)andothervariantsofsequence Mnih e t a l .2011
modelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever ,;
e t a l .,;2009Boulanger-Lewandowski2012 e t a l .,) Anothersequencemodelingtaskistomodelthedistributionoversequences
6 8 5
CHAPTER20.DEEPGENERATIVEMODELS
ofmusicalnotesusedtocomposesongs.Boulanger-Lewandowski2012 e t a l .()
introducedtheRNN-RBM sequencemodelandappliedittothistask.The
RNN-RBMisagenerativemodelofasequenceofframes x( ) tconsistingofanRNN
thatemitstheRBMparametersforeachtimestep.Unlikepreviousapproaches
inwhichonlythebiasparametersoftheRBMvariedfromonetimesteptothe
next,theRNN-RBMusestheRNNtoemitalloftheparametersoftheRBM,
includingtheweights.Totrainthemodel,weneedtobeabletoback-propagate
thegradientofthelossfunctionthroughtheRNN.Thelossfunctionisnotapplied
directlytotheRNNoutputs.Instead,itisappliedtotheRBM.Thismeansthat
wemustapproximately diﬀerentiatethelosswithrespecttotheRBMparameters
usingcontrastivedivergenceorarelatedalgorithm This approximate gradient
maythenbeback-propagated throughtheRNNusingtheusualback-propagation
throughtimealgorithm 20.8OtherBoltzmannMachines
ManyothervariantsofBoltzmannmachinesarepossible Boltzmannmachinesmaybeextendedwithdiﬀerenttrainingcriteria.Wehave
focusedonBoltzmannmachinestrainedtoapproximately maximizethegenerative
criterion log p( v).ItisalsopossibletotraindiscriminativeRBMsthataimto
maximize log p( y| v)instead( ,).Thisapproachoften LarochelleandBengio2008
performsthebestwhenusingalinearcombinationofboththegenerativeand
thediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerful
supervisedlearnersasMLPs,atleastusingexistingmethodology MostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractions
intheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmany
termsandeachindividualtermonlyincludestheproductbetweentworandom
variables.Anexampleofsuchatermis v i W i , j h j.Itisalsopossibletotrain
higher-orderBoltzmannmachines(,)whoseenergyfunctionterms Sejnowski1987
involvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweena
hiddenunitandtwodiﬀerentimagescanmodelspatialtransformationsfromone
frameofvideotothenext(MemisevicandHinton20072010,,).Multiplication bya
one-hotclassvariablecanchangetherelationshipbetweenvisibleandhiddenunits
dependingonwhichclassispresent( ,).Onerecentexample NairandHinton2009
oftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwogroupsof
hiddenunits,withonegroupofhiddenunitsthatinteractwithboththevisible
units vandtheclasslabel y,andanothergroupofhiddenunitsthatinteractonly
withthe vinputvalues(,).Thiscanbeinterpretedasencouraging Luo e t a l .2011
6 8 6
CHAPTER20.DEEPGENERATIVEMODELS
somehiddenunitstolearntomodeltheinputusingfeaturesthatarerelevantto
theclassbutalsotolearnextrahiddenunitsthatexplainnuisancedetailsthat
arenecessaryforthesamplesof vtoberealisticbutdonotdeterminetheclass
oftheexample.Anotheruseofhigher-orderinteractionsistogatesomefeatures Sohn2013 e t a l .()introducedaBoltzmannmachinewiththird-orderinteractions
withbinarymaskvariablesassociatedwitheachvisibleunit.Whenthesemasking
variablesaresettozero,theyremovetheinﬂuenceofavisibleunitonthehidden
units.Thisallowsvisibleunitsthatarenotrelevanttotheclassiﬁcationproblem
toberemovedfromtheinferencepathwaythatestimatestheclass Moregenerally,theBoltzmannmachineframeworkisarichspaceofmodels
permittingmanymoremodelstructuresthanhavebeenexploredsofar.Developing
anewformofBoltzmannmachinerequiressomemorecareandcreativitythan
developinganewneuralnetworklayer,becauseitisoftendiﬃculttoﬁndanenergy
functionthatmaintainstractabilityofallofthediﬀerentconditionaldistributions
neededtousetheBoltzmannmachine,butdespitethisrequiredeﬀorttheﬁeld
remainsopentoinnovation 20.9Back-PropagationthroughRandomOperations
Traditionalneuralnetworksimplementadeterministictransformationofsome
inputvariables x.Whendevelopinggenerativemodels,weoftenwishtoextend
neuralnetworkstoimplementstochastictransformationsof x.Onestraightforward
waytodothisistoaugmenttheneuralnetworkwithextrainputs zthatare
sampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussian
distribution.Theneuralnetworkcanthencontinuetoperformdeterministic
computationinternally, butthefunction f( x z ,)willappearstochasticto an
observerwhodoesnothaveaccessto z.Providedthat fiscontinuousand
diﬀerentiable,wecanthencomputethegradientsnecessaryfortrainingusing
back-propagationasusual Asanexample,letusconsidertheoperationconsistingofdrawingsamplesy
fromaGaussiandistributionwithmeanandvariance µ σ2:
y∼N( µ , σ2) (20.54)
Becauseanindividualsampleofyisnotproducedbyafunction,butratherby
asamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseem
counterintuitivetotakethederivativesof ywithrespecttotheparametersof
itsdistribution, µand σ2.However, wecanrewritethesamplingprocessas
6 8 7
CHAPTER20.DEEPGENERATIVEMODELS
transforminganunderlyingrandomvaluez∼N( z;0 ,1)toobtainasamplefrom
thedesireddistribution:
y µ σ z = + (20.55)
Wearenowabletoback-propagatethroughthesamplingoperation,byregard-
ingitasadeterministicoperationwithanextrainputz.Crucially,theextrainput
isarandomvariablewhosedistributionisnotafunctionofanyofthevariables
whosederivativeswewanttocalculate The resulttellsushowaninﬁnitesimal
changein µor σwouldchangetheoutputifwecouldrepeatthesamplingoperation
againwiththesamevalueofz Beingabletoback-propagate throughthissamplingoperationallowsusto
incorporateitintoalargergraph.Wecanbuildelementsofthegraphontopofthe
outputofthesamplingdistribution.Forexample,wecancomputethederivatives
ofsomelossfunction J( y).Wecanalsobuildelementsofthegraphwhoseoutputs
aretheinputsortheparametersofthesamplingoperation.Forexample,wecould
buildalargergraphwith µ= f( x; θ)and σ= g( x; θ).Inthisaugmentedgraph,
wecanuseback-propagationthroughthesefunctionstoderive∇ θ J y() TheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli-
cable.Wecanexpressanyprobabilitydistributionoftheform p(y; θ)or p(y| x; θ)
as p(y| ω),where ωisavariablecontainingbothparameters θ,andifapplicable,
theinputs x.Givenavalue ysampledfromdistribution p(y| ω),where ωmayin
turnbeafunctionofothervariables,wecanrewrite
y y ∼ p(| ω) (20.56)
as
y z ω = ( f;) , (20.57)
where zisasourceofrandomness.Wemaythencomputethederivativesof ywith
respectto ωusingtraditionaltoolssuchastheback-propagation algorithmapplied
to f,solongas fiscontinuousanddiﬀerentiable almosteverywhere.Crucially, ω
mustnotbeafunctionof z,and zmustnotbeafunctionof ω.Thistechnique
isoftencalledthereparametrizationtrick,stochasticback-propagationor
perturbationanalysis

============================================================

=== CHUNK 185 ===
Palavras: 389
Caracteres: 4595
--------------------------------------------------
Therequirementthat fbecontinuousanddiﬀerentiableofcourserequires y
tobecontinuous.Ifwewishtoback-propagate throughasamplingprocessthat
producesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradienton
ω,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCE
algorithm(,),discussedinsection Williams1992 20.9.1
6 8 8
CHAPTER20.DEEPGENERATIVEMODELS
Inneuralnetworkapplications,wetypicallychoose ztobedrawnfromsome
simpledistribution,suchasaunituniformorunitGaussiandistribution,and
achievemorecomplexdistributionsbyallowingthedeterministicportionofthe
networktoreshapeitsinput Theideaofpropagatinggradientsoroptimizingthroughstochasticoperations
datesbacktothemid-twentiethcentury(,;,)andwas Price1958Bonnet1964
ﬁrstusedformachinelearninginthecontextofreinforcementlearning(,Williams
1992) Morerecently,ithasbeenappliedtovariationalapproximations(Opper
andArchambeau2009,)andstochasticorgenerativeneuralnetworks(Bengio
e t a l .,;,; 2013bKingma2013KingmaandWelling2014baRezende2014 ,,; e t a l .,;
Goodfellow2014c e t a l .,).Manynetworks,suchasdenoisingautoencodersor
networksregularized with dropout, are also naturally designedto take noise
asaninputwithoutrequiringanyspecialreparametrization tomakethenoise
independentfromthemodel 20.9.1Back-PropagatingthroughDiscreteStochasticOperations
Whenamodelemitsadiscretevariable y,thereparametrization trickisnot
applicable.Suppose thatthemodel takesinputs xandparameters θ, both
encapsulatedinthevector ω,andcombinesthemwithrandomnoise ztoproduce
y:
y z ω = ( f;) (20.58)
Because yisdiscrete, fmustbeastepfunction.Thederivativesofastepfunction
arenotusefulatanypoint.Rightateachstepboundary,thederivativesare
undeﬁned,butthatisasmallproblem.Thelargeproblemisthatthederivatives
arezeroalmosteverywhere,ontheregionsbetweenstepboundaries.Thederivatives
ofanycostfunction J( y)thereforedonotgiveanyinformationforhowtoupdate
themodelparameters θ
TheREINFORCEalgorithm(REwardIncrement=Non-negativeFactor ×
OﬀsetReinforcement×Characteristic Eligibility)providesaframeworkdeﬁninga
familyofsimplebutpowerfulsolutions(,) Thecoreideaisthat Williams1992
eventhough J( f( z; ω))isastepfunctionwithuselessderivatives,theexpected
cost E z z∼ p ( ) J f((;)) z ωisoftenasmoothfunctionamenabletogradientdescent Althoughthatexpectationistypicallynottractablewhen yishigh-dimensional
(oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbe
estimatedwithoutbiasusingaMonteCarloaverage.Thestochasticestimateof
thegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimization
techniques 6 8 9
CHAPTER20.DEEPGENERATIVEMODELS
ThesimplestversionofREINFORCEcanbederivedbysimplydiﬀerentiating
theexpectedcost:
E z[()] = J y
yJ p() y() y (20.59)
∂ J E[()] y
∂ ω=
yJ() y∂ p() y
∂ ω(20.60)
=
yJ p() y() y∂ plog() y
∂ ω(20.61)
≈1
mm
y( ) i∼ p , i ( ) y = 1J( y( ) i)∂ plog( y( ) i)
∂ ω.(20.62)
Equationreliesontheassumptionthat 20.60 Jdoesnotreference ωdirectly.Itis
trivialtoextendtheapproachtorelaxthisassumption.Equationexploits20.61
thederivativeruleforthelogarithm,∂ p l o g ( ) y
∂ ω=1
p ( ) y∂ p ( ) y
∂ ω.Equation gives20.62
anunbiasedMonteCarloestimatorofthegradient Anywherewewrite p( y)inthissection,onecouldequallywrite p( y x|).This
isbecause p( y)isparametrized by ω,and ωcontainsboth θand x,if xispresent OneissuewiththeabovesimpleREINFORCEestimatoristhatithasavery
highvariance,sothatmanysamplesof yneedtobedrawntoobtainagood
estimatorofthegradient,orequivalently,ifonlyonesampleisdrawn,SGDwill
convergeveryslowlyandwillrequireasmallerlearningrate.Itispossibleto
considerablyreducethevarianceofthatestimatorbyusingvariancereduction
methods(,;,).Theideaistomodifytheestimatorso Wilson1984L’Ecuyer1994
thatitsexpectedvalueremainsunchangedbutitsvariancegetreduced Inthe
contextofREINFORCE,theproposedvariancereductionmethodsinvolvethe
computationofabaselinethatisusedtooﬀset J( y).Notethatanyoﬀset b( ω)
thatdoesnotdependon ywouldnotchangetheexpectationoftheestimated
gradientbecause
E p ( ) y∂ plog() y
∂ ω
=
yp() y∂ plog() y
∂ ω(20.63)
=
y∂ p() y
∂ ω(20.64)
=∂
∂ ω
yp() = y∂
∂ ω1 = 0 ,(20.65)
6 9 0
CHAPTER20.DEEPGENERATIVEMODELS
whichmeansthat
E p ( ) y
(()()) J y− b ω∂ plog() y
∂ ω
= E p ( ) y
J() y∂ plog() y
∂ ω
− b E() ω p ( ) y∂ plog() y
∂ ω
(20.66)
= E p ( ) y
J() y∂ plog() y
∂ ω (20.67)
Furthermore,wecanobtaintheoptimal b( ω) bycomputingthevarianceof( J( y)−
b( ω))∂ p l o g ( ) y
∂ ωunder p( y)andminimizingwithrespectto b( ω).Whatweﬁndis
thatthisoptimalbaseline b∗() ω iisdiﬀerentforeachelement ω iofthevector: ω
b∗() ω i=Ep ( ) y
J() y∂ p l o g ( ) y
∂ ω i2
E p ( ) y
∂ p l o g ( ) y
∂ ω i2

============================================================

=== CHUNK 186 ===
Palavras: 364
Caracteres: 7195
--------------------------------------------------
(20.68)
Thegradientestimatorwithrespectto ω ithenbecomes
(()() J y− b ω i)∂ plog() y
∂ ω i(20.69)
where b( ω) iestimatestheabove b∗( ω) i.Theestimate bisusuallyobtainedby
addingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimate
E p ( ) y[ J( y)∂ p l o g ( ) y
∂ ω i2]and E p ( ) y
∂ p l o g ( ) y
∂ ω i2
foreachelementof ω.Theseextra
outputscanbetrainedwiththemeansquarederrorobjective,usingrespectively
J( y)∂ p l o g ( ) y
∂ ω i2and∂ p l o g ( ) y
∂ ω i2astargetswhen yissampledfrom p( y),foragiven
ω.Theestimate bmaythenberecoveredbysubstitutingtheseestimatesinto
equation ()preferredtouseasinglesharedoutput 20.68MnihandGregor2014
(acrossallelements iof ω)trainedwiththetarget J( y),usingasbaseline b( ω)≈
E p ( ) y[()] J y Variancereductionmethodshavebeenintroducedinthereinforcementlearning
context( ,; Sutton e t a l .2000WeaverandTao2001,),generalizingpreviouswork
onthecaseofbinaryrewardbyDayan1990Bengio2013bMnih () See e t a l .(),
andGregor2014Ba2014Mnih2014Xu2015 (), e t a l .(), e t a l .(),or e t a l .()for
examplesofmodernusesoftheREINFORCEalgorithmwithreducedvariancein
thecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaseline
b( ω) ( , ()foundthatthescaleof MnihandGregor2014 J( y)− b( ω))couldbe
adjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbya
movingaverageduringtraining,asakindofadaptivelearningrate,tocounter
theeﬀectofimportantvariationsthatoccurduringthecourseoftraininginthe
6 9 1
CHAPTER20.DEEPGENERATIVEMODELS
magnitudeofthisquantity ()calledthisheuristic MnihandGregor2014 variance
normalization REINFORCE-basedestimatorscanbeunderstoodasestimatingthegradient
bycorrelatingchoicesof ywithcorrespondingvaluesof J( y).Ifagoodvalueof y
isunlikelyunderthecurrentparametrization, itmighttakealongtimetoobtainit
bychance,andgettherequiredsignalthatthisconﬁgurationshouldbereinforced 20.10DirectedGenerativeNets
Asdiscussedinchapter,directedgraphicalmodelsmakeupaprominentclass 16
ofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopular
withinthegreatermachinelearningcommunity,withinthesmallerdeeplearning
communitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodels
suchastheRBM Inthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthat
havetraditionallybeenassociatedwiththedeeplearningcommunity Wehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirected
model.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethought
ofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearners
inthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsample
generationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirected
models 20.10.1SigmoidBeliefNets
Sigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodel Neal1990
withaspeciﬁckindofconditionalprobabilitydistribution.Ingeneral,wecan
thinkofasigmoidbeliefnetworkashavingavectorofbinarystates s,witheach
elementofthestateinﬂuencedbyitsancestors:
p s( i) = σ

j < iW j , i s j+ b i
 (20.70)
Themostcommonstructureofsigmoidbeliefnetworkisonethatisdivided
intomanylayers,withancestralsamplingproceedingthroughaseriesofmany
hiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureis
verysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginningof
6 9 2
CHAPTER20.DEEPGENERATIVEMODELS
thesamplingprocessareindependentfromeachother,ratherthansampledfrom
arestrictedBoltzmannmachine Suchastructureisinterestingforavarietyof
reasons.Onereasonisthatthestructureisauniversalapproximator ofprobability
distributionsoverthevisibleunits,inthesensethatitcanapproximate any
probabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth,
evenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthe
visiblelayer(SutskeverandHinton2008,) Whilegeneratingasampleofthevisibleunitsisveryeﬃcientinasigmoid
beliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiven
thevisibleunitsisintractable.Meanﬁeldinferenceisalsointractablebecausethe
variationallowerboundinvolvestakingexpectationsofcliquesthatencompass
entirelayers.Thisproblemhasremaineddiﬃcultenoughtorestrictthepopularity
ofdirecteddiscretenetworks Oneapproachforperforminginferenceinasigmoidbeliefnetworkistoconstruct
adiﬀerentlowerboundthatisspecializedforsigmoidbeliefnetworks(,Saul e t a l 1996).Thisapproachhasonlybeenappliedtoverysmallnetworks.Another
approachistouselearnedinferencemechanismsasdescribedinsection.The19.5
Helmholtzmachine(Dayan1995DayanandHinton1996 e t a l .,; ,)isasigmoidbelief
networkcombinedwithaninferencenetworkthatpredictstheparametersofthe
meanﬁelddistributionoverthehiddenunits.Modernapproaches( ,Gregor e t a l 2014MnihandGregor2014 ; ,)tosigmoidbeliefnetworksstillusethisinference
networkapproach.Thesetechniquesremaindiﬃcultduetothediscretenatureof
thelatentvariables.Onecannotsimplyback-propagate throughtheoutputofthe
inferencenetwork,butinsteadmustusetherelativelyunreliablemachineryforback-
propagatingthroughdiscretesamplingprocesses,describedinsection.Recent 20.9.1
approachesbasedonimportancesampling,reweightedwake-sleep(Bornscheinand
Bengio2015 Bornschein2015 ,)andbidirectional Helmholtzmachines( e t a l .,)
makeitpossibletoquicklytrainsigmoidbeliefnetworksandreachstate-of-the-art
performanceonbenchmarktasks Aspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatent
variables.Learninginthiscaseiseﬃcient,becausethereisnoneedtomarginalize
latentvariablesoutofthelikelihood Afamilyofmodelscalledauto-regressive
networksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariables
besidesbinaryvariablesandotherstructuresofconditionaldistributionsbesideslog-
linearrelationships.Auto-regressive networksaredescribedlater,insection.20.10.7
6 9 3
CHAPTER20.DEEPGENERATIVEMODELS
20.10.2DiﬀerentiableGeneratorNets
Manygenerativemodelsarebasedontheideaofusingadiﬀerentiablegenerator
network.Themodeltransformssamplesoflatentvariables ztosamples xor
todistributionsoversamples xusingadiﬀerentiablefunction g( z; θ( ) g)whichis
typicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariational
autoencoders, whichpair thegeneratornetwithaninferencenet,generative
adversarial networks, which pair the generator net work witha discriminator
network,andtechniquesthattraingeneratornetworksinisolation Generatornetworksareessentiallyjustparametrized computational procedures
forgeneratingsamples,wherethearchitectureprovidesthefamilyofpossible
distributionstosamplefromandtheparametersselectadistributionfromwithin
thatfamily Asanexample,thestandardprocedurefordrawingsamplesfromanormal
distributionwithmean µandcovariance Σistofeedsamples zfromanormal
distributionwithzeromeanandidentitycovarianceintoaverysimplegenerator
network.Thisgeneratornetworkcontainsjustoneaﬃnelayer:
x z L z = ( g) = + µ (20.71)
whereisgivenbytheCholeskydecompositionof L Σ
Pseudorandomnumbergeneratorscanalsousenonlineartransformationsof
simpledistributions.Forexample,inversetransformsampling(Devroye2013,)
drawsascalar zfrom U(0 ,1)andappliesanonlineartransformationtoascalar
x.Inthiscase g( z)isgivenbytheinverseofthecumulativedistributionfunction
F( x) =x
−∞p( v) d v.Ifweareabletospecify p( x),integrateover x,andinvertthe
resultingfunction,wecansamplefromwithoutusingmachinelearning

============================================================

=== CHUNK 187 ===
Palavras: 364
Caracteres: 6337
--------------------------------------------------
p x()
Togeneratesamplesfrommorecomplicateddistributionsthatarediﬃcult
tospecifydirectly, diﬃculttointegrateover, orwhoseresultingintegralsare
diﬃculttoinvert,weuseafeedforwardnetworktorepresentaparametricfamily
ofnonlinearfunctions g,andusetrainingdatatoinfertheparametersselecting
thedesiredfunction Wecanthinkof gasprovidinganonlinearchangeofvariablesthattransforms
thedistributionoverintothedesireddistributionover z x
Recallfromequationthat,forinvertible,diﬀerentiable,continuous, 3.47 g
p z() = z p x(()) g zdet(∂ g
∂ z) (20.72)
6 9 4
CHAPTER20.DEEPGENERATIVEMODELS
Thisimplicitlyimposesaprobabilitydistributionover:x
p x() = xp z( g− 1()) xdet(∂ g
∂ z) (20.73)
Ofcourse,thisformulamaybediﬃculttoevaluate,dependingonthechoiceof
g,soweoftenuseindirectmeansoflearning g,ratherthantryingtomaximize
log() p xdirectly Insomecases,ratherthanusing gtoprovideasampleof xdirectly,weuse g
todeﬁneaconditionaldistributionover x.Forexample,wecoulduseagenerator
netwhoseﬁnallayerconsistsofsigmoidoutputstoprovidethemeanparameters
ofBernoullidistributions:
p(x i= 1 ) = () | z g z i (20.74)
Inthiscase,whenweuse gtodeﬁne p( x z|),weimposeadistributionover xby
marginalizing : z
p() = x E z p ( ) x z| (20.75)
Bothapproachesdeﬁneadistribution p g( x)andallowustotrainvarious
criteriaof p gusingthereparametrization trickofsection.20.9
Thetwodiﬀerentapproachestoformulatinggeneratornets—emittingthe
parametersofaconditionaldistributionversusdirectlyemittingsamples—have
complementarystrengthsandweaknesses.Whenthegeneratornetdeﬁnesa
conditionaldistributionover x,itiscapableofgeneratingdiscretedataaswellas
continuousdata.Whenthegeneratornetprovidessamplesdirectly,itiscapableof
generatingonlycontinuousdata(wecouldintroducediscretizationintheforward
propagation, butdoingsowouldmeanthemodelcouldnolongerbetrainedusing
back-propagation).Theadvantagetodirectsamplingisthatwearenolonger
forcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownand
algebraically manipulated byahumandesigner Approachesbasedondiﬀerentiable generatornetworksaremotivatedbythe
successof gradient descentappliedtodiﬀerentiable feedforwardnetworksfor
classiﬁcation Inthecontextofsupervisedlearning,deepfeedforwardnetworks
trainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgiven
enoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccess
transfertogenerativemodeling Generativemodelingseemstobemorediﬃcultthanclassiﬁcationorregression
becausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontext
6 9 5
CHAPTER20.DEEPGENERATIVEMODELS
ofdiﬀerentiablegeneratornets,thecriteriaareintractablebecausethedatadoes
notspecifyboththeinputs zandtheoutputs xofthegeneratornet.Inthecase
ofsupervisedlearning,boththeinputs xandtheoutputs yweregiven,andthe
optimization procedureneedsonlytolearnhowtoproducethespeciﬁedmapping Inthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehow
toarrangespaceinausefulwayandadditionallyhowtomapfromto z z x
Dosovitskiy2015 e t a l .()studiedasimpliﬁedproblem,wherethecorrespondence
between zand xisgiven.Speciﬁcally,thetrainingdataiscomputer-rendered
imageryofchairs.Thelatentvariables zareparametersgiventotherendering
enginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair,
andotherconﬁgurationdetailsthataﬀecttherenderingoftheimage.Usingthis
syntheticallygenerateddata,aconvolutionalnetworkisabletolearntomap z
descriptionsofthecontentofanimageto xapproximationsofrenderedimages Thissuggeststhatcontemporarydiﬀerentiablegeneratornetworkshavesuﬃcient
modelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimization
algorithmshavetheabilitytoﬁtthem.Thediﬃcultyliesindetermininghowto
traingeneratornetworkswhenthevalueof zforeach xisnotﬁxedandknown
aheadofeachtime Thefollowingsectionsdescribeseveralapproachestotrainingdiﬀerentiable
generatornetsgivenonlytrainingsamplesof x
20.10.3VariationalAutoencoders
ThevariationalautoencoderorVAE(,; ,)isa Kingma2013Rezende e t a l .2014
directedmodelthatuseslearnedapproximate inferenceandcanbetrainedpurely
withgradient-basedmethods Togenerateasamplefromthemodel,theVAEﬁrstdrawsasample zfrom
thecodedistribution p m o de l( z).Thesampleisthenrunthroughadiﬀerentiable
generatornetwork g( z).Finally, xissampledfromadistribution p m o de l( x; g( z)) =
p m o de l( x z|) However,duringtraining,theapproximate inferencenetwork(or
encoder) q( z x|)isusedtoobtain zand p m o de l( x z|)isthenviewedasadecoder
network Thekeyinsightbehindvariationalautoencodersisthattheymaybetrained
bymaximizingthevariationallowerboundassociatedwithdatapoint: L() q x
L() = q E z z x ∼ q (| )log p m o de l()+(( )) z x , H qz| x (20.76)
= E z z x ∼ q (| )log p m o de l( ) x z|− D K L(( ) qz| x|| p m o de l())z(20.77)
≤log p m o de l() x (20.78)
6 9 6
CHAPTER20.DEEPGENERATIVEMODELS
Inequation,werecognizetheﬁrsttermasthejointlog-likelihoodofthevisible 20.76
andhiddenvariablesundertheapproximateposterioroverthelatentvariables
(justlikewithEM,exceptthatweuseanapproximateratherthantheexact
posterior).Werecognizealsoasecondterm,theentropyoftheapproximate
posterior When qischosentobeaGaussiandistribution,withnoiseaddedto
apredictedmeanvalue,maximizingthisentropytermencouragesincreasingthe
standarddeviationofthisnoise.Moregenerally,thisentropytermencouragesthe
variationalposteriortoplacehighprobabilitymassonmany zvaluesthatcould
havegenerated x,ratherthancollapsingtoasinglepointestimateofthemost
likelyvalue.Inequation,werecognizetheﬁrsttermasthereconstruction 20.77
log-likelihoodfoundinotherautoencoders.Thesecondtermtriestomakethe
approximateposteriordistribution q(z| x) andthemodelprior p m o de l( z) approach
eachother Traditionalapproachestovariationalinferenceandlearninginfer qviaanopti-
mizationalgorithm,typicallyiteratedﬁxedpointequations(section).These19.4
approachesareslowandoftenrequiretheabilitytocompute E z∼ qlog p m o de l( z x ,)
inclosedform.Themainideabehindthevariationalautoencoderistotraina
parametricencoder(alsosometimescalledaninferencenetworkorrecognition
model)thatproducestheparametersof q.Solongas zisacontinuousvariable,we
canthenback-propagate throughsamplesof zdrawnfrom q( z x|) = q( z; f( x; θ))
inordertoobtainagradientwithrespectto θ.Learningthenconsistssolelyof
maximizing Lwithrespecttotheparametersoftheencoderanddecoder.Allof
theexpectationsinmaybeapproximatedbyMonteCarlosampling

============================================================

=== CHUNK 188 ===
Palavras: 361
Caracteres: 7298
--------------------------------------------------
L
Thevariationalautoencoderapproachiselegant,theoreticallypleasing,and
simpletoimplement.Italsoobtainsexcellentresultsandisamongthestateofthe
artapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfrom
variationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecauses
ofthisphenomenon arenotyetknown.Onepossibilityisthattheblurrinessis
anintrinsiceﬀectofmaximumlikelihood,whichminimizes D K L( p da t a p m o de l).As
illustratedinﬁgure,thismeansthatthemodelwillassignhighprobabilityto 3.6
pointsthatoccurinthetrainingset,butmayalsoassignhighprobabilitytoother
points.Theseotherpointsmayincludeblurryimages.Partofthereasonthatthe
modelwouldchoosetoputprobabilitymassonblurryimagesratherthansome
otherpartofthespaceisthatthevariationalautoencodersusedinpracticeusually
haveaGaussiandistributionfor p m o de l( x; g( z)) Maximizing alowerboundon
thelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoder
withmeansquarederror,inthesensethatithasatendencytoignorefeatures
oftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthe
brightnessofthepixelsthattheyoccupy.ThisissueisnotspeciﬁctoVAEsand
6 9 7
CHAPTER20.DEEPGENERATIVEMODELS
issharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently,
D K L( p da t a p m o de l),asarguedby ()andby().Another Theis e t a l .2015 Huszar2015
troublingissuewithcontemporary VAEmodelsisthattheytendtouseonlyasmall
subsetofthedimensionsof z,asiftheencoderwasnotabletotransformenough
ofthelocaldirectionsininputspacetoaspacewherethemarginaldistribution
matchesthefactorizedprior TheVAEframeworkisverystraightforwardtoextendtoawiderangeofmodel
architectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequire
extremelycarefulmodeldesigntomaintaintractability.VAEsworkverywellwith
adiversefamilyofdiﬀerentiable operators.OneparticularlysophisticatedVAE
isthedeeprecurrentattentionwriterorDRAWmodel( ,) Gregor e t a l .2015
DRAWusesarecurrentencoderandrecurrentdecodercombinedwithanattention
mechanism.ThegenerationprocessfortheDRAWmodelconsistsofsequentially
visitingdiﬀerentsmallimagepatchesanddrawingthevaluesofthepixelsatthose
points.VAEscanalsobeextendedtogeneratesequencesbydeﬁningvariational
RNNs( ,)byusingarecurrentencoderanddecoderwithin Chung e t a l .2015b
theVAEframework.GeneratingasamplefromatraditionalRNNinvolvesonly
non-determinis ticoperationsattheoutputspace.VariationalRNNsalsohave
randomvariabilityatthepotentiallymoreabstractlevelcapturedbytheVAE
latentvariables TheVAEframeworkhasbeenextendedtomaximizenotjustthetraditional
variationallowerbound,butinsteadtheimportanceweightedautoencoder
(,)objective: Burda e t a l .2015
L k() = x , q Ez( 1 ) , , z( ) k∼ | q ( z x )
log1
kk
i = 1p m o de l( x z ,( ) i)
q( z( ) i| x)
.(20.79)
Thisnewobjectiveisequivalenttothetraditionallowerbound Lwhen k=1 However,itmayalsobeinterpretedasforminganestimateofthetruelog p m o de l( x)
usingimportancesamplingof zfromproposaldistribution q( z x|).Theimportance
weightedautoencoderobjectiveisalsoalowerboundonlog p m o de l( x) andbecomes
tighterasincreases k
VariationalautoencodershavesomeinterestingconnectionstotheMP-DBM
andotherapproachesthatinvolveback-propagationthroughtheapproximate
inferencegraph(Goodfellow2013bStoyanov2011Brakel2013 e t a l .,; e t a l .,; e t a l .,) Thesepreviousapproachesrequiredaninferenceproceduresuchasmeanﬁeldﬁxed
pointequationstoprovidethecomputational graph.Thevariationalautoencoder
isdeﬁnedforarbitrarycomputational graphs,whichmakesitapplicabletoawider
rangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoice
6 9 8
CHAPTER20.DEEPGENERATIVEMODELS
ofmodelstothosewithtractablemeanﬁeldﬁxedpointequations.Thevariational
autoencoderalsohastheadvantagethatitincreasesaboundonthelog-likelihood
ofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremore
heuristicandhavelittleprobabilisticinterpretation beyondmakingtheresultsof
approximateinferenceaccurate.Onedisadvantageofthevariationalautoencoder
isthatitlearnsaninferencenetworkforonlyoneproblem,inferring zgiven x Theoldermethodsareabletoperformapproximateinferenceoveranysubsetof
variablesgivenanyothersubsetofvariables,becausethemeanﬁeldﬁxedpoint
equationsspecifyhowtoshareparametersbetweenthecomputational graphsfor
allofthesediﬀerentproblems Oneverynicepropertyofthevariationalautoencoderisthatsimultaneously
trainingaparametricencoderincombinationwiththegeneratornetworkforcesthe
modeltolearnapredictablecoordinatesystemthattheencodercancapture.This
makesitanexcellentmanifoldlearningalgorithm.Seeﬁgureforexamplesof 20.6
low-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthe
casesdemonstratedintheﬁgure,thealgorithmdiscoveredtwoindependentfactors
ofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression 20.10.4GenerativeAdversarialNetworks
GenerativeadversarialnetworksorGANs( ,)areanother Goodfellow e t a l .2014c
generativemodelingapproachbasedondiﬀerentiablegeneratornetworks Generativeadversarialnetworksarebasedonagametheoreticscenarioin
whichthegeneratornetworkmustcompeteagainstanadversary.Thegenerator
networkdirectlyproducessamples x= g( z; θ( ) g).Itsadversary,thediscriminator
network,attemptstodistinguishbetweensamplesdrawnfromthetrainingdata
andsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvalue
givenby d( x; θ( ) d),indicatingtheprobabilitythat xisarealtrainingexample
ratherthanafakesampledrawnfromthemodel Thesimplestwaytoformulatelearningingenerativeadversarialnetworksis
asazero-sumgame,inwhichafunction v( θ( ) g, θ( ) d)determinesthepayoﬀofthe
discriminator.Thegeneratorreceives− v( θ( ) g, θ( ) d)asitsownpayoﬀ.During
learning,eachplayerattemptstomaximizeitsownpayoﬀ,sothatatconvergence
g∗= argmin
gmax
dv g , d () (20.80)
Thedefaultchoiceforis v
v( θ( ) g, θ( ) d) = E x∼ pdatalog()+ d x E x∼ pmodellog(1 ()) − d x .(20.81)
6 9 9
CHAPTER20.DEEPGENERATIVEMODELS
Figure20.6:Examplesoftwo-dimensionalcoordinatesystemsforhigh-dimensionalmani-
folds,learnedbyavariationalautoencoder(KingmaandWelling2014a,).Twodimensions
maybeplotteddirectlyonthepageforvisualization,sowecangainanunderstandingof
howthemodelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievethe
intrinsicdimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenot
examplesfromthetrainingsetbutimages xactuallygeneratedbythemodel p( x z|),
simplybychangingthe2-D“code” z(eachimagecorrespondstoadiﬀerentchoiceof“code”
zona2-Duniformgrid) ( L e f t )Thetwo-dimensionalmapoftheFreyfacesmanifold Onedimensionthathasbeendiscovered(horizontal)mostlycorrespondstoarotationof
theface,whiletheother(vertical)correspondstotheemotionalexpression.The ( R i g h t )
two-dimensionalmapoftheMNISTmanifold Thisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasreal
orfake.Simultaneous ly,thegeneratorattemptstofooltheclassiﬁerintobelieving
itssamplesarereal.Atconvergence,thegenerator’ssamplesareindistinguishable
fromrealdata,andthediscriminatoroutputs1
2everywhere.Thediscriminator
maythenbediscarded ThemainmotivationforthedesignofGANsisthatthelearningprocess
requiresneitherapproximateinferencenorapproximation ofapartitionfunction
gradient.Inthecasewhere max d v( g , d)isconvexin θ( ) g(suchasthecasewhere
optimization isperformeddirectlyinthespaceofprobabilitydensityfunctions)
theprocedureisguaranteedtoconvergeandisasymptoticallyconsistent

============================================================

=== CHUNK 189 ===
Palavras: 361
Caracteres: 11248
--------------------------------------------------
Unfortunately,learninginGANscanbediﬃcultinpracticewhen gand d
arerepresentedbyneuralnetworksandmax d v( g , d)isnotconvex.Goodfellow
7 0 0
CHAPTER20.DEEPGENERATIVEMODELS
()identiﬁednon-convergenceasanissuethatmaycauseGANstounderﬁt 2014
Ingeneral,simultaneousgradientdescentontwoplayers’costsisnotguaranteed
toreachanequilibrium.Considerforexamplethevaluefunction v( a , b)= a b,
whereoneplayercontrols aandincurscost a b,whiletheotherplayercontrols b
andreceivesacost− a b.Ifwemodeleachplayerasmakinginﬁnitesimallysmall
gradientsteps,eachplayerreducingtheirowncostattheexpenseoftheother
player,then aand bgointoastable,circularorbit,ratherthanarrivingatthe
equilibriumpointattheorigin.Notethattheequilibriaforaminimaxgameare
notlocalminimaof v.Instead,theyarepointsthataresimultaneouslyminima
forbothplayers’costs.Thismeansthattheyaresaddlepointsof vthatarelocal
minimawithrespecttotheﬁrstplayer’sparametersandlocalmaximawithrespect
tothesecondplayer’sparameters.Itispossibleforthetwoplayerstotaketurns
increasingthendecreasing vforever,ratherthanlandingexactlyonthesaddle
pointwhereneitherplayeriscapableofreducingitscost.Itisnotknowntowhat
extentthisnon-convergenceproblemaﬀectsGANs Goodfellow2014()identiﬁedanalternativeformulationofthepayoﬀs,inwhich
thegameisnolongerzero-sum,thathasthesameexpectedgradientasmaximum
likelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximum
likelihoodtrainingconverges,thisreformulationoftheGANgameshouldalso
converge,givenenoughsamples.Unfortunately,thisalternativeformulationdoes
notseemtoimproveconvergenceinpractice,possiblyduetosuboptimalityofthe
discriminator,orpossiblyduetohighvariancearoundtheexpectedgradient Inrealisticexperiments,thebest-performingformulationoftheGANgame
isadiﬀerentformulationthatisneitherzero-sumnorequivalenttomaximum
likelihood,introducedby ()withaheuristicmotivation.In Goodfellow e t a l .2014c
thisbest-performingformulation,thegeneratoraimstoincreasethelogprobability
thatthediscriminatormakesamistake,ratherthanaimingtodecreasethelog
probabilitythatthediscriminatormakesthecorrectprediction.Thisreformulation
ismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator’s
costfunctionwithrespecttothediscriminator’slogitstoremainlargeeveninthe
situationwherethediscriminatorconﬁdentlyrejectsallgeneratorsamples Stabilization ofGANlearningremainsanopenproblem Fortunately,GAN
learningperformswellwhenthemodelarchitectureandhyperparametersarecare-
fullyselected ()craftedadeepconvolutionalGAN(DCGAN) Radford e t a l .2015
thatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepre-
sentationspacecapturesimportantfactorsofvariation,asshowninﬁgure.15.9
SeeﬁgureforexamplesofimagesgeneratedbyaDCGANgenerator 20.7
TheGANlearningproblemcanalsobesimpliﬁedbybreakingthegeneration
7 0 1
CHAPTER20.DEEPGENERATIVEMODELS
Figure20.7:ImagesgeneratedbyGANstrainedontheLSUNdataset ( L e f t )Images
ofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadford
e t a l .().ImagesofchurchesgeneratedbyaLAPGANmodel,reproducedwith 2015 ( R i g h t )
permissionfrom () Denton e t a l .2015
processintomanylevelsofdetail.ItispossibletotrainconditionalGANs(Mirza
andOsindero2014,)thatlearntosamplefromadistribution p( x y|)rather
thansimplysamplingfromamarginaldistribution p( x) () Denton e t a l .2015
showedthataseriesofconditionalGANscanbetrainedtoﬁrstgenerateavery
low-resolutionversionofanimage,thenincrementally adddetailstotheimage ThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramid
togeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgenerators
areabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,with
experimentalsubjectsidentifyingupto40%oftheoutputsofthenetworkas
beingrealdata.SeeﬁgureforexamplesofimagesgeneratedbyaLAPGAN 20.7
generator OneunusualcapabilityoftheGANtrainingprocedureisthatitcanﬁtproba-
bilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthan
maximizingthelogprobabilityofspeciﬁcpoints,thegeneratornetlearnstotrace
outamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara-
doxically,thismeansthatthemodelmayassignalog-likelihoodofnegativeinﬁnity
tothetestset,whilestillrepresentingamanifoldthatahumanobserverjudges
tocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageor
adisadvantage,andonemayalsoguaranteethatthegeneratornetworkassigns
non-zeroprobabilitytoallpointssimplybymakingthelastlayerofthegenerator
networkaddGaussiannoisetoallofthegeneratedvalues Generatornetworks
thataddGaussiannoiseinthismannersamplefromthesamedistributionthatone
obtainsbyusingthegeneratornetworktoparametrizethemeanofaconditional
7 0 2
CHAPTER20.DEEPGENERATIVEMODELS
Gaussiandistribution Dropoutseemstobeimportantinthediscriminatornetwork Inparticular,
units shouldbe stochasticallydropped whilecomputingthe gradientfor the
generatornetworktofollow.Followingthegradientofthedeterministicversionof
thediscriminatorwithitsweightsdividedbytwodoesnotseemtobeaseﬀective Likewise,neverusingdropoutseemstoyieldpoorresults WhiletheGANframeworkisdesignedfordiﬀerentiablegeneratornetworks,
similarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self-
supervisedboostingcanbeusedtotrainanRBMgeneratortofoolalogistic
regressiondiscriminator(Welling2002 e t a l .,) 20.10.5GenerativeMomentMatchingNetworks
Generativemomentmatchingnetworks(,; , Li e t a l .2015Dziugaite e t a l 2015)areanotherformofgenerativemodelbasedondiﬀerentiablegenerator
networks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetwork
withanyothernetwork—neither aninferencenetworkasusedwithVAEsnora
discriminatornetworkasusedwithGANs Thesenetworksaretrainedwithatechniquecalledmomentmatching.The
basicideabehindmomentmatchingistotrainthegeneratorinsuchawaythat
manyofthestatisticsofsamplesgeneratedbythemodelareassimilaraspossible
tothoseofthestatisticsoftheexamplesinthetrainingset.Inthiscontext,a
momentisanexpectationofdiﬀerentpowersofarandomvariable.Forexample,
theﬁrstmomentisthemean,thesecondmomentisthemeanofthesquared
values,andsoon.Inmultipledimensions,eachelementoftherandomvectormay
beraisedtodiﬀerentpowers,sothatamomentmaybeanyquantityoftheform
E xΠ i xn i
i (20.82)
where n= [ n 1 , n 2 , , n d]isavectorofnon-negativeintegers Uponﬁrstexamination,thisapproachseemstobecomputationally infeasible Forexample,ifwewanttomatchallthemomentsoftheform x i x j,thenweneed
tominimizethediﬀerencebetweenanumberofvaluesthatisquadraticinthe
dimensionof x.Moreover,evenmatchingalloftheﬁrstandsecondmoments
wouldonlybesuﬃcienttoﬁtamultivariateGaussiandistribution,whichcaptures
onlylinearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksareto
capturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments GANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusinga
7 0 3
CHAPTER20.DEEPGENERATIVEMODELS
dynamicallyupdateddiscriminatorthatautomatically focusesitsattentionon
whicheverstatisticthegeneratornetworkismatchingtheleasteﬀectively Instead,generativemomentmatchingnetworkscanbetrainedbyminimizing
acostfunctioncalledmaximummeandiscrepancy(SchölkopfandSmola,
2002Gretton2012 ; e t a l .,)orMMD.Thiscostfunctionmeasurestheerrorin
theﬁrstmomentsinaninﬁnite-dimens ionalspace,usinganimplicitmapping
tofeaturespacedeﬁnedbyakernelfunctioninordertomakecomputations on
inﬁnite-dimens ionalvectorstractable.TheMMDcostiszeroifandonlyifthetwo
distributionsbeingcomparedareequal Visually,thesamplesfromgenerativemomentmatchingnetworksaresomewhat
disappointing.Fortunately,theycanbeimprovedbycombiningthegenerator
networkwithanautoencoder.First,anautoencoderistrainedtoreconstructthe
trainingset.Next,theencoderoftheautoencoderisusedtotransformtheentire
trainingsetintocodespace.Thegeneratornetworkisthentrainedtogenerate
codesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder UnlikeGANs,thecostfunctionisdeﬁnedonlywithrespecttoabatchof
examplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossible
tomakeatrainingupdateasafunctionofonlyonetrainingexampleoronly
onesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbe
computedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoo
small,MMDcanunderestimatethetrueamountofvariationinthedistributions
beingsampled.Noﬁnitebatchsizeissuﬃcientlylargetoeliminatethisproblem
entirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatch
sizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemany
examplesmustbeprocessedinordertocomputeasinglesmallgradientstep AswithGANs,itispossibletotrainageneratornetusingMMDevenifthat
generatornetassignszeroprobabilitytothetrainingpoints 20.10.6ConvolutionalGenerativeNetworks
Whengeneratingimages,itisoftenusefultouseageneratornetworkthatincludes
aconvolutionalstructure(seeforexampleGoodfellow2014cDosovitskiy e t a l .()or
e t a l .()).Todoso, weusethe“transpose”oftheconvolutionoperator, 2015
describedinsection.Thisapproachoftenyieldsmorerealisticimagesanddoes 9.5
sousingfewerparametersthanusingfullyconnectedlayerswithoutparameter
sharing Convolutionalnetworksforrecognitiontaskshaveinformationﬂowfromthe
imagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel 7 0 4
CHAPTER20.DEEPGENERATIVEMODELS
Asthisimageﬂowsupwardthroughthenetwork,informationisdiscardedasthe
representationoftheimagebecomesmoreinvarianttonuisancetransformations Inageneratornetwork, theoppositeistrue.Richdetailsmustbeaddedas
therepresentationoftheimagetobegeneratedpropagatesthroughthenetwork,
culminatingintheﬁnalrepresentationoftheimage,whichisofcoursetheimage
itself,inallofitsdetailedglory,withobjectpositionsandposesandtexturesand
lighting Theprimarymechanismfordiscardinginformationinaconvolutional
recognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedto
addinformation Wecannotputtheinverseofapoolinglayerintothegenerator
networkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationis
tomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseems
toperformacceptablyistousean“un-pooling”asintroducedbyDosovitskiy e t a l ().Thislayercorrespondstotheinverseofthemax-poolingoperationunder 2015
certainsimplifyingconditions Firs t,thestrideofthemax-poolingoperationis
constrainedtobeequaltothewidthofthepoolingregion.Second,themaximum
inputwithineachpoolingregionisassumedtobetheinputintheupper-left
corner.Finally,allnon-maximal inputswithineachpoolingregionareassumedto
bezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthe
max-poolingoperatortobeinverted.Theinverseun-poolingoperationallocates
atensorofzeros,thencopieseachvaluefromspatialcoordinate ioftheinput
tospatialcoordinate i k×oftheoutput.Theintegervalue kdeﬁnesthesize
ofthepoolingregion.Eventhoughtheassumptionsmotivatingthedeﬁnitionof
theun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearnto
compensateforitsunusualoutput,sothesamplesgeneratedbythemodelasa
wholearevisuallypleasing 20.10.7Auto-RegressiveNetworks
Auto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandom
variables.Theconditionalprobabilitydistributionsinthesemodelsarerepresented
byneuralnetworks(sometimesextremelysimpleneuralnetworkssuchaslogistic
regression).Thegraphstructureofthesemodelsisthecompletegraph.They
decomposeajointprobabilityovertheobservedvariablesusingthechainruleof
probabilitytoobtainaproductofconditionalsoftheform P( x d| x d− 1 ,

============================================================

=== CHUNK 190 ===
Palavras: 353
Caracteres: 3658
--------------------------------------------------
Suchmodelshavebeencalledfully-visibleBayesnetworks(FVBNs)andused
successfully inmany forms, ﬁrstwith logistic regression foreachconditional
distribution(Frey1998,)andthenwithneuralnetworkswithhiddenunits(Bengio
andBengio2000bLarochelleandMurray2011 ,; ,).Insomeformsofauto-
regressivenetworks,suchasNADE( ,),described LarochelleandMurray2011
7 0 5
CHAPTER20.DEEPGENERATIVEMODELS
insection below,wecanintroduceaformofparametersharingthat 20.10.10
bringsbothastatisticaladvantage(feweruniqueparameters)andacomputational
advantage(lesscomputation) Thisisonemoreinstanceoftherecurringdeep
learningmotifof r e u s e o f f e a t u r e s x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )
P x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4
Figure20.8:A fullyvisiblebelief networkpredictsthe i-thvariable fromthe i−1
previousones ( T o p ) ( Bottom ) ThedirectedgraphicalmodelforanFVBN Corresponding
computationalgraph,inthecaseofthelogisticFVBN,whereeachpredictionismadeby
alinearpredictor 20.10.8LinearAuto-RegressiveNetworks
Thesimplestformofauto-regressiv enetworkhasnohiddenunitsandnosharing
ofparametersorfeatures.Each P( x i| x i− 1 , , x 1)isparametrized asalinear
model(linearregressionforreal-valueddata,logisticregressionforbinarydata,
softmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998()
andhas O( d2)parameterswhenthereare dvariablestomodel.Itisillustratedin
ﬁgure.20.8
Ifthevariablesarecontinuous,alinearauto-regressive modelismerelyanother
waytoformulateamultivariateGaussiandistribution,capturinglinearpairwise
interactionsbetweentheobservedvariables Linearauto-regressiv enetworksareessentiallythegeneralization oflinear
classiﬁcationmethodstogenerativemodeling.Theythereforehavethesame
7 0 6
CHAPTER20.DEEPGENERATIVEMODELS
advantagesanddisadvantagesaslinearclassiﬁers.Likelinearclassiﬁers,theymay
betrainedwithconvexlossfunctions,andsometimesadmitclosedformsolutions
(asintheGaussiancase).Likelinearclassiﬁers,themodelitselfdoesnotoﬀer
awayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslike
basisexpansionsoftheinputorthekerneltrick x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )
P x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )
Figure20.9:Aneuralauto-regressivenetworkpredictsthe i-thvariable x ifromthe i−1
previousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenoted h i)
thatarefunctionsof x 1 , , x icanbereusedinpredictingallofthesubsequentvariables
x i + 1 , x i + 2 , 20.10.9NeuralAuto-RegressiveNetworks
Neuralauto-regressiv enetworks( ,,)havethesame BengioandBengio2000ab
left-to-rightgraphicalmodelaslogisticauto-regressiv enetworks(ﬁgure)but20.8
employadiﬀerentparametrization oftheconditionaldistributionswithinthat
graphicalmodelstructure.Thenewparametrization ismorepowerfulinthesense
thatitscapacitycanbeincreasedasmuchasneeded,allowingapproximation of
anyjointdistribution.Thenewparametrization canalsoimprovegeneralization
byintroducingaparametersharingandfeaturesharingprinciplecommontodeep
learningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthe
curseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharing
thesamestructureasﬁgure.Intabulardiscreteprobabilisticmodels,each 20.8
conditionaldistributionisrepresentedbyatableofprobabilities, withoneentry
andoneparameterforeachpossibleconﬁgurationofthevariablesinvolved.By
usinganeuralnetworkinstead,twoadvantagesareobtained:
7 0 7
CHAPTER20.DEEPGENERATIVEMODELS
1.Theparametrization ofeach P( x i| x i− 1 ,

============================================================

=== CHUNK 191 ===
Palavras: 352
Caracteres: 3627
--------------------------------------------------
, x 1)byaneuralnetworkwith
( i−1)× kinputsand koutputs(ifthevariablesarediscreteandtake k
values,encodedone-hot)allowsonetoestimatetheconditionalprobability
withoutrequiringanexponentialnumberofparameters(andexamples),yet
stillisabletocapturehigh-orderdependenciesbetweentherandomvariables 2.Insteadofhavingadiﬀerentneuralnetworkforthepredictionofeach x i,
a connectivityillustratedinﬁgureallowsonetomergeall l e f t - t o - r i g h t 20.9
theneuralnetworksintoone.Equivalently,itmeansthatthehiddenlayer
featurescomputedforpredicting x icanbereusedforpredicting x i k +( k >0) Thehiddenunitsarethusorganizedin g r o u p sthathavetheparticularity
thatalltheunitsinthe i-thgrouponlydependontheinputvalues x 1 , Theparametersusedtocomputethesehiddenunitsarejointlyoptimized
to improvethe prediction ofall thevariables inthe sequence.This is
aninstanceofthe r e u s e p r i nc i p l ethatrecursthroughoutdeeplearningin
scenariosrangingfromrecurrentandconvolutionalnetworkarchitectures to
multi-taskandtransferlearning Each P( x i| x i− 1 , , x 1)canrepresentaconditionaldistributionbyhaving
outputsoftheneuralnetworkpredict p a r a m e t e r softheconditionaldistribution
of x i,asdiscussedinsection.Althoughtheoriginalneuralauto-regressive 6.2.1.1
networkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariate
data(withasigmoidoutputforaBernoullivariableorsoftmaxoutputfora
multinoullivariable)itisnaturaltoextendsuchmodelstocontinuousvariablesor
jointdistributionsinvolvingbothdiscreteandcontinuousvariables 20.10.10NADE
Theneuralautoregressivedensityestimator(NADE)isaverysuccessful
recentformofneuralauto-regressive network(LarochelleandMurray2011,).The
connectivityisthesameasfortheoriginalneuralauto-regressive networkofBengio
andBengio2000b()butNADEintroducesanadditionalparametersharingscheme,
asillustratedinﬁgure.Theparametersofthehiddenunitsofdiﬀerentgroups 20.10
jareshared Theweights W
j , k, ifromthe i-thinput x itothe k-thelementofthe j-thgroup
ofhiddenunit h( ) j
k()aresharedamongthegroups: j i≥
W
j , k, i= W k, i (20.83)
Theremainingweights,where,arezero j < i
7 0 8
CHAPTER20.DEEPGENERATIVEMODELS
x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )
P x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )
W : 1 , W : 1 , W : 1 ,
W : 2 , W : 2 , W : 3 ,
Figure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).The
hiddenunitsareorganizedingroups h( ) jsothatonlytheinputs x 1 , , x iparticipate
incomputing h( ) iandpredicting P( x j| x j− 1 , , x 1),for j > i.NADEisdiﬀerentiated
fromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharing
pattern: W
j , k , i= W k , iisshared(indicatedintheﬁgurebytheuseofthesamelinepattern
foreveryinstanceofareplicatedweight)foralltheweightsgoingoutfrom x itothe k-th
unitofanygroup.Recallthatthevector j i≥ ( W 1 , i , W 2 , i , , W n , i)isdenoted W : , i LarochelleandMurray2011()chosethissharingschemesothatforward
propagationinaNADEmodellooselyresemblesthecomputations performedin
meanﬁeldinferencetoﬁllinmissinginputsinanRBM.Thismeanﬁeldinference
correspondstorunningarecurrentnetworkwithsharedweightsandtheﬁrststep
ofthatinferenceisthesameasinNADE.TheonlydiﬀerenceisthatwithNADE,
theoutputweightsconnectingthehiddenunitstotheoutputareparametrized
independentlyfromtheweightsconnectingtheinputunitstothehiddenunits.In
theRBM,thehidden-to-output weightsarethetransposeoftheinput-to-hidden
weights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestep
ofthemeanﬁeldrecurrentinferencebuttomimic ksteps.Thisapproachiscalled
NADE-(,)

============================================================

=== CHUNK 192 ===
Palavras: 356
Caracteres: 6504
--------------------------------------------------
kRaiko e t a l .2014
Asmentionedpreviously,auto-regressiv enetworksmaybeextendtoprocess
continuous-valueddata.Aparticularlypowerfulandgenericwayofparametrizing
acontinuousdensityisasaGaussianmixture(introducedinsection)with3.9.6
mixtureweights α i(thecoeﬃcientorpriorprobabilityforcomponent i),per-
componentconditionalmean µ iandper-componentconditionalvariance σ2
i A
modelcalledRNADE(,)usesthisparametrization toextendNADE Uria e t a l .2013
torealvalues.Aswithothermixturedensitynetworks,theparametersofthis
7 0 9
CHAPTER20.DEEPGENERATIVEMODELS
distributionareoutputsofthenetwork,withthemixtureweightprobabilities
producedbyasoftmaxunit,andthevariancesparametrized sothattheyare
positive Stochasticgradientdescentcanbenumericallyill-behavedduetothe
interactionsbetweentheconditionalmeans µ iandtheconditionalvariances σ2
i Toreducethisdiﬃculty,()useapseudo-gradientthatreplacesthe Uria e t a l .2013
gradientonthemean,intheback-propagationphase Anotherveryinterestingextensionoftheneuralauto-regressiv earchitectures
getsridoftheneedtochooseanarbitraryorderfortheobservedvariables(Murray
andLarochelle2014,).Inauto-regressive networks,theideaistotrainthenetwork
tobeabletocopewithanyorderbyrandomlysamplingordersandprovidingthe
informationtohiddenunitsspecifyingwhichoftheinputsareobserved(onthe
rightsideoftheconditioningbar)andwhicharetobepredictedandarethus
consideredmissing(ontheleftsideoftheconditioningbar).Thisisnicebecause
itallowsonetouseatrainedauto-regressiv enetworkto p e r f o r m a ny i nfe r e nc e
p r o b l e m(i.e.predictorsamplefromtheprobabilitydistributionoveranysubset
ofvariablesgivenanysubset)extremelyeﬃciently.Finally,sincemanyordersof
variablesarepossible( n!for nvariables)andeachorder oofvariablesyieldsa
diﬀerent,wecanformanensembleofmodelsformanyvaluesof: p o(x|) o
p e nse m bl e() =x1
kk
i = 1p o(x|( ) i) (20.84)
Thisensemblemodelusuallygeneralizesbetterandassignshigherprobabilityto
thetestsetthandoesanindividualmodeldeﬁnedbyasingleordering Inthesamepaper,theauthorsproposedeepversionsofthearchitecture, but
unfortunately thatimmediatelymakescomputationasexpensiveasintheoriginal
neuralauto-regressiv eneuralnetwork( ,).Theﬁrstlayer BengioandBengio2000b
andtheoutputlayercanstillbecomputedin O( n h)multiply-addoperations,
asintheregularNADE,where histhenumberofhiddenunits(thesizeofthe
groups h i,inﬁguresand),whereasitis 20.1020.9 O( n2h)inBengioandBengio
().However,fortheotherhiddenlayers,thecomputationis 2000b O( n2h2)ifevery
“previous”groupatlayer lparticipatesinpredictingthe“next”groupatlayer l+1,
assuming ngroupsof hhiddenunitsateachlayer.Makingthe i-thgroupatlayer
l+1onlydependonthe i-thgroup,asinMurrayandLarochelle2014()atlayer l
reducesitto O n h(2),whichisstilltimesworsethantheregularNADE h
7 1 0
CHAPTER20.DEEPGENERATIVEMODELS
20.11DrawingSamplesfromAutoencoders
Inchapter,wesawthatmanykindsofautoencoderslearnthedatadistribution 14
Therearecloseconnectionsbetweenscorematching,denoisingautoencoders,and
contractiveautoencoders.Theseconnectionsdemonstratethatsomekindsof
autoencoderslearnthedatadistributioninsomeway.Wehavenotyetseenhow
todrawsamplesfromsuchmodels Somekindsofautoencoders,suchasthevariationalautoencoder,explicitly
representaprobabilitydistributionandadmitstraightforwardancestralsampling MostotherkindsofautoencodersrequireMCMCsampling Contractiveautoencodersaredesignedtorecoveranestimateofthetangent
planeofthedatamanifold.Thismeansthatrepeatedencodinganddecodingwith
injectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifai
e t a l ,;2012Mesnil,).Thismanifolddiﬀusiontechniqueisakindof 2012
Markovchain ThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoising
autoencoder 20.11.1MarkovChainAssociatedwithanyDenoisingAutoen-
coder
Theabovediscussionleftopenthequestionofwhatnoisetoinjectandwhere,
inordertoobtainaMarkovchainthatwouldgeneratefromthedistribution
estimatedbytheautoencoder ()showedhowtoconstruct Bengio e t a l .2013c
suchaMarkovchainforgeneralizeddenoisingautoencoders.Generalized
denoisingautoencodersarespeciﬁedbyadenoisingdistributionforsamplingan
estimateofthecleaninputgiventhecorruptedinput EachstepoftheMarkovchainthatgeneratesfromtheestimateddistribution
consistsofthefollowingsub-steps,illustratedinﬁgure:20.11
1.Startingfromthepreviousstate x,injectcorruptionnoise,sampling ˜ xfrom
C(˜ x x|) Encode˜ xinto h= ( f˜ x) Decodetoobtaintheparameters of h ω h= ( g) p g p ( = x | ω ()) = h (x|˜ x) Samplethenextstatefrom x p g p ( = x| ω ()) = h (x|˜ x) 7 1 1
CHAPTER20.DEEPGENERATIVEMODELS
xx˜ x˜ xh h
ωω
ˆ x ˆ xC ( ˜ x x| ) p ( ) x| ωfg
Figure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen-
coder,thatgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythe
denoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruption
process Cinstate x,yielding˜ x,(b)encodingitwithfunction f,yielding h= f(˜ x),
(c)decodingtheresultwithfunction g,yieldingparameters ωforthereconstruction
distribution,and(d)given ω,samplinganewstatefromthereconstructiondistribution
p(x | ω= g( f(˜ x))).Inthetypicalsquaredreconstructionerrorcase, g( h)=ˆ x,which
estimates E[ x|˜ x],corruptionconsistsinaddingGaussiannoiseandsamplingfrom
p(x| ω)consistsinaddingGaussiannoise,asecondtime,tothereconstructionˆ x.The
latternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereas
theinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellasthe
extenttowhichtheestimatorsmoothstheempiricaldistribution(,).Inthe Vincent2011
exampleillustratedhere,onlythe Cand pconditionalsarestochasticsteps( fand gare
deterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder,
asingenerativestochasticnetworks( ,) Bengio e t a l .2014
7 1 2
CHAPTER20.DEEPGENERATIVEMODELS
Bengio2014 e t a l .()showedthatiftheautoencoder p(x |˜x)formsaconsistent
estimatorofthecorrespondingtrueconditionaldistribution,thenthestationary
distributionoftheaboveMarkovchainformsaconsistentestimator(albeitan
implicitone)ofthedatageneratingdistributionof.x
20.11.2ClampingandConditionalSampling
SimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations
(suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri-
bution p(x f|x o),simplybyclampingthe o b s e r v e dunits x fandonlyresampling
the f r e eunits x ogivenx fandthesampledlatentvariables(ifany).Forexample,
MP-DBMscanbeinterpretedasaformofdenoisingautoencoder,andareable
tosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentin
MP-DBMstoperformthesameoperation( ,)

============================================================

=== CHUNK 193 ===
Palavras: 350
Caracteres: 9979
--------------------------------------------------
() Bengio e t a l .2014Alain e t a l .2015
identiﬁedamissingconditionfromProposition1of (),whichis Bengio e t a l .2014
thatthetransitionoperator(deﬁnedbythestochasticmappinggoingfromone
stateofthechaintothenext)shouldsatisfyapropertycalleddetailedbalance,
whichspeciﬁesthataMarkovChainatequilibriumwillremaininequilibrium
whetherthetransitionoperatorisruninforwardorreverse Anexperimentinclampinghalfofthepixels(therightpartoftheimage)and
runningtheMarkovchainontheotherhalfisshowninﬁgure.20.12
7 1 3
CHAPTER20.DEEPGENERATIVEMODELS
Figure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkov
Chainbyresamplingonlythelefthalfateachstep ThesesamplescomefromaGSN
trainedtoreconstructMNISTdigitsateachtimestepusingthewalkbackprocedure 20.11.3Walk-BackTrainingProcedure
Thewalk-backtrainingprocedurewasproposedby ()asaway Bengio e t a l .2013c
toacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders Insteadofperformingaone-stepencode-decodereconstruction,thisprocedure
consistsinalternativemultiplestochasticencode-decodesteps(asinthegenerative
Markovchain)initializedatatrainingexample(justlikewiththecontrastive
divergencealgorithm,describedinsection)andpenalizingthelastprobabilistic 18.2
reconstructions(orallofthereconstructionsalongtheway) Trainingwith kstepsisequivalent(inthesenseofachievingthesamestationary
distribution)astrainingwithonestep,butpracticallyhastheadvantagethat
spuriousmodesfurtherfromthedatacanberemovedmoreeﬃciently 20.12GenerativeStochasticNetworks
GenerativestochasticnetworksorGSNs( ,)aregeneraliza- Bengio e t a l .2014
tionsofdenoisingautoencodersthatincludelatentvariables hinthegenerative
7 1 4
CHAPTER20.DEEPGENERATIVEMODELS
Markovchain,inadditiontothevisiblevariables(usuallydenoted).x
AGSNisparametrized bytwoconditionalprobabilitydistributionswhich
specifyonestepoftheMarkovchain:
1 p(x( ) k|h( ) k)tellshowtogeneratethenextvisiblevariablegiventhecurrent
latentstate.Sucha“reconstructiondistribution”isalsofoundindenoising
autoencoders,RBMs,DBNsandDBMs p(h( ) k|h( 1 ) k−,x( 1 ) k−)tellshowtoupdatethelatentstatevariable,given
thepreviouslatentstateandvisiblevariable DenoisingautoencodersandGSNsdiﬀerfromclassicalprobabilisticmodels
(directedorundirected)inthattheyparametrizethegenerativeprocessitselfrather
thanthemathematical speciﬁcationofthejointdistributionofvisibleandlatent
variables.Instead,thelatterisdeﬁned ,,asthestationary i m p l i c i t l y i f i t e x i s t s
distributionofthegenerativeMarkovchain.Theconditionsforexistenceofthe
stationarydistributionaremildandarethesameconditionsrequiredbystandard
MCMCmethods(seesection).Theseconditionsarenecessarytoguarantee 17.3
thatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransition
distributions(forexample,iftheyweredeterministic) OnecouldimaginediﬀerenttrainingcriteriaforGSNs.Theoneproposedand
evaluatedby ()issimplyreconstructionlog-probabilit yonthe Bengio e t a l .2014
visibleunits,justlikefordenoisingautoencoders.Thisisachievedbyclamping
x( 0 )= xtotheobservedexampleandmaximizingtheprobabilityofgenerating x
atsomesubsequenttimesteps,i.e.,maximizing log p(x( ) k= x|h( ) k),where h( ) k
issampledfromthechain,givenx( 0 )= x Inordertoestimatethegradientof
log p(x( ) k= x|h( ) k)withrespecttotheotherpiecesofthemodel,Bengio e t a l ()usethereparametrization trick,introducedinsection 2014 20.9
Thewalk-backtrainingprotocol(describedinsection)wasused( 20.11.3 Ben-
gio2014 e t a l .,)toimprovetrainingconvergenceofGSNs 20.12.1DiscriminantGSNs
TheoriginalformulationofGSNs( ,)wasmeantforunsupervised Bengio e t a l .2014
learningandimplicitlymodeling p(x)forobserveddatax,butitispossibleto
modifytheframeworktooptimize p( )y| x
Forexample,ZhouandTroyanskaya2014()generalizeGSNsinthisway,by
onlyback-propagatingthereconstructionlog-probabilit yovertheoutputvariables,
keepingtheinputvariablesﬁxed.Theyappliedthissuccessfullytomodelsequences
7 1 5
CHAPTER20.DEEPGENERATIVEMODELS
(proteinsecondarystructure)andintroduceda(one-dimensional) convolutional
structureinthetransitionoperatoroftheMarkovchain.Itisimportantto
rememberthat,foreachstepoftheMarkovchain,onegeneratesanewsequence
foreachlayer,andthatsequenceistheinputforcomputingotherlayervalues(say
theonebelowandtheoneabove)atthenexttimestep HencetheMarkovchainisreallyovertheoutputvariable(andassociatedhigher-
levelhiddenlayers),andtheinputsequenceonlyservestoconditionthatchain,
withback-propagationallowingtolearnhowtheinputsequencecanconditionthe
outputdistributionimplicitlyrepresentedbytheMarkovchain.Itisthereforea
caseofusingtheGSNinthecontextofstructuredoutputs ZöhrerandPernkopf2014()introducedahybridmodelthatcombinesasuper-
visedobjective(asintheabovework)andanunsupervisedobjective(asinthe
originalGSNwork),bysimplyadding(withadiﬀerentweight)thesupervisedand
unsupervisedcostsi.e.,thereconstructionlog-probabilities ofyandxrespectively SuchahybridcriterionhadpreviouslybeenintroducedforRBMsbyLarochelle
andBengio2008().Theyshowimprovedclassiﬁcationperformanceusingthis
scheme 20.13OtherGenerationSchemes
ThemethodswehavedescribedsofaruseeitherMCMCsampling,ancestral
sampling,orsomemixtureofthetwotogeneratesamples Whilethesearethe
mostpopularapproachestogenerativemodeling,theyarebynomeanstheonly
approaches Sohl-Dickstein2015 e t a l .()developedadiﬀusioninversiontrainingscheme
forlearningagenerativemodel,basedonnon-equilibrium thermodynamics.The
approachisbasedontheideathattheprobabilitydistributionswewishtosample
fromhavestructure.Thisstructurecangraduallybedestroyedbyadiﬀusion
processthatincrementally changestheprobabilitydistributiontohave more
entropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytraining
amodelthatgraduallyrestoresthestructuretoanunstructureddistribution Byiterativelyapplyingaprocessthatbringsadistributionclosertothetarget
one,wecangraduallyapproachthattargetdistribution.Thisapproachresembles
MCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample However,themodelisdeﬁnedtobetheprobabilitydistributionproducedby
theﬁnalstepofthechain Inthissense,thereisnoapproximation inducedby
theiterativeprocedure.Theapproachintroducedby () Sohl-Dickstein e t a l .2015
isalsoveryclosetothegenerativeinterpretation ofthedenoisingautoencoder
7 1 6
CHAPTER20.DEEPGENERATIVEMODELS
(section).Aswiththedenoisingautoencoder,diﬀusioninversiontrainsa 20.11.1
transitionoperatorthatattemptstoprobabilisticallyundotheeﬀectofadding
somenoise.Thediﬀerenceisthatdiﬀusioninversionrequresundoingonlyonestep
ofthediﬀusionprocess,ratherthantravelingallthewaybacktoacleandatapoint Thisaddressesthefollowingdilemmapresentwiththeordinaryreconstruction
log-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethe
learneronlyseesconﬁgurations nearthedatapoints,whilewithlargelevelsof
noiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistribution
ishighlycomplexandmulti-modal) Withthediﬀusioninversionobjective,the
learnercanlearntheshapeofthedensityaroundthedatapointsmoreprecisely
aswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints AnotherapproachtosamplegenerationistheapproximateBayesiancom-
putation(ABC)framework(,).Inthisapproach,samplesare Rubin e t a l .1984
rejectedormodiﬁedinordertomakethemomentsofselectedfunctionsofthe
samplesmatchthoseofthedesireddistribution.Whilethisideausesthemoments
ofthesampleslikeinmomentmatching,itisdiﬀerentfrommomentmatching
becauseitmodiﬁesthesamplesthemselves,ratherthantrainingthemodelto
automatically emitsampleswiththecorrectmoments () BachmanandPrecup2015
showedhowtouseideasfromABCinthecontextofdeeplearning,byusingABC
toshapetheMCMCtrajectoriesofGSNs Weexpectthatmanyotherpossibleapproachestogenerativemodelingawait
discovery 20.14EvaluatingGenerativeModels
Researchersstudyinggenerativemodelsoftenneedtocompareonegenerative
modeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerative
modelisbetteratcapturingsomedistributionthanthepre-existingmodels Thiscanbeadiﬃcultandsubtletask.Inmanycases,wecannotactually
evaluatethelogprobabilityofthedataunderthemodel,butonlyanapproximation Inthesecases,itisimportanttothinkandcommunicateclearlyaboutexactlywhat
isbeingmeasured.Forexample,supposewecanevaluateastochasticestimateof
thelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihood
formodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwe
careaboutdeterminingwhichmodelhasabetterinternalrepresentationofthe
distribution,weactuallycannottell,unlesswehavesomewayofdetermininghow
loosetheboundformodelBis.However,ifwecareabouthowwellwecanuse
themodelinpractice,forexampletoperformanomalydetection,thenitisfairto
7 1 7
CHAPTER20.DEEPGENERATIVEMODELS
saythatamodelispreferablebasedonacriterionspeciﬁctothepracticaltaskof
interest,e.g.,basedonrankingtestexamplesandrankingcriteriasuchasprecision
andrecall Anothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetrics
areoftenhardresearchproblemsinandofthemselves.Itcanbeverydiﬃcult
toestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuse
AIStoestimate log Zinordertocompute log ˜ p( x)−log Zforanewmodelwe
havejustinvented.Acomputationally economicalimplementation ofAISmayfail
toﬁndseveralmodesofthemodeldistributionandunderestimate Z,whichwill
resultinusoverestimatinglog p( x).Itcanthusbediﬃculttotellwhetherahigh
likelihoodestimateisduetoagoodmodelorabadAISimplementation Otherﬁeldsofmachinelearningusuallyallowforsomevariationinthepre-
processingofthedata.Forexample,whencomparingtheaccuracyofobject
recognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimages
slightlydiﬀerentlyforeachalgorithmbasedonwhatkindofinputrequirements
ithas.Generativemodelingisdiﬀerentbecausechangesinpreprocessing,even
verysmallandsubtleones,arecompletelyunacceptable Anychangetotheinput
datachangesthedistributiontobecapturedandfundamentallyaltersthetask Forexample,multiplyingtheinputby0.1willartiﬁciallyincreaselikelihoodbya
factorof10 Issueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodels
ontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks

============================================================

=== CHUNK 194 ===
Palavras: 352
Caracteres: 11977
--------------------------------------------------
MNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspoints
inarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthe
grayscalevaluesasprobabilities forabinarysamples.Itisessentialtocompare
real-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonly
tootherbinary-valuedmodels Otherwisethelikelihoodsmeasuredarenotonthe
samespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,while
forreal-valuedmodelsitcanbearbitrarilyhigh,sinceitisthemeasurementofa
density.Amongbinarymodels,itisimportanttocomparemodelsusingexactly
thesamekindofbinarization Forexample,wemightbinarizeagraypixelto0or1
bythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing
1isgivenbythegraypixelintensity.Ifweusetherandombinarization, wemight
binarizethewholedatasetonce,orwemightdrawadiﬀerentrandomexamplefor
eachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthese
threeschemesyieldswildlydiﬀerentlikelihoodnumbers,andwhencomparing
diﬀerentmodelsitisimportantthatbothmodelsusethesamebinarizationscheme
fortrainingandforevaluation.Infact,researcherswhoapplyasinglerandom
7 1 8
CHAPTER20.DEEPGENERATIVEMODELS
binarizationstepshareaﬁlecontainingtheresultsoftherandombinarization, so
thatthereisnodiﬀerenceinresultsbasedondiﬀerentoutcomesofthebinarization
step Becausebeingabletogeneraterealisticsamplesfromthedatadistribution
isoneofthegoalsofagenerativemodel,practitioners oftenevaluategenerative
modelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbythe
researchersthemselves,butbyexperimentalsubjectswhodonotknowthesource
ofthesamples(Denton2015 e t a l .,).Unfortunately,itispossibleforaverypoor
probabilisticmodeltoproduceverygoodsamples.Acommonpracticetoverifyif
themodelonlycopiessomeofthetrainingexamplesisillustratedinﬁgure.16.1
Theideaistoshowforsomeofthegeneratedsamplestheirnearestneighborin
thetrainingset,accordingtoEuclideandistanceinthespaceof x Thistestis
intendedtodetectthecasewherethemodeloverﬁtsthetrainingsetandjust
reproducestraininginstances.Itisevenpossibletosimultaneouslyunderﬁtand
overﬁtyetstillproducesamplesthatindividuallylookgood.Imagineagenerative
modeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethe
trainingimagesofdogs.Suchamodelhasclearlyoverﬁt,becauseitdoesnot
producesimagesthatwerenotinthetrainingset,butithasalsounderﬁt,because
itassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserver
wouldjudgeeachindividualimageofadogtobehighquality.Inthissimple
example,itwouldbeeasyforahumanobserverwhocaninspectmanysamplesto
determinethatthecatsareabsent.Inmorerealisticsettings,agenerativemodel
trainedondatawithtensofthousandsofmodesmayignoreasmallnumberof
modes,andahumanobserverwouldnoteasilybeabletoinspectorremember
enoughimagestodetectthemissingvariation Since thevisual quality ofsamples is not areliable guide, we often also
evaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisis
computationally feasible.Unfortunately,insomecasesthelikelihoodseemsnot
tomeasureanyattributeofthemodelthatwereallycareabout.Forexample,
real-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigning
arbitrarilylowvariancetobackgroundpixelsthatneverchange.Modelsand
algorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,even
thoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacost
approaching negativeinﬁnityis present foranykind of maximum likelihood
problemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsof
MNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstrongly
suggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels Theis2015 e t a l .()reviewmanyoftheissuesinvolvedinevaluatinggenerative
7 1 9
CHAPTER20.DEEPGENERATIVEMODELS
models,includingmanyoftheideasdescribedabove.Theyhighlightthefact
thattherearemanydiﬀerentusesofgenerativemodelsandthatthechoiceof
metricmustmatchtheintendeduseofthemodel.Forexample,somegenerative
modelsarebetteratassigninghighprobabilitytomostrealisticpointswhileother
generativemodelsarebetteratrarelyassigninghighprobabilitytounrealistic
points.Thesediﬀerencescanresultfromwhetheragenerativemodelisdesigned
tominimize D K L( p da t a p m o de l)or D K L( p m o de l p da t a),asillustratedinﬁgure.3.6
Unfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismost
suitedfor,allofthemetricscurrentlyinusecontinuetohaveseriousweaknesses Oneofthemostimportantresearchtopicsingenerativemodelingisthereforenot
justhowtoimprovegenerativemodels,butinfact,designingnewtechniquesto
measureourprogress 20.15Conclusion
Traininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodels
understandtheworldrepresentedinthegiventrainingdata.Bylearningamodel
p m o de l( x)andarepresentation p m o de l( h x|),agenerativemodelcanprovide
answerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariables
in xandcanprovidemanydiﬀerentwaysofrepresenting xbytakingexpectations
of hatdiﬀerentlayersofthehierarchy Generativemodelsholdthepromiseto
provideAIsystemswithaframeworkforallofthemanydiﬀerentintuitiveconcepts
theyneedtounderstand,andtheabilitytoreasonabouttheseconceptsinthe
faceofuncertainty.Wehopethatourreaderswillﬁndnewwaystomakethese
approachesmorepowerfulandcontinuethejourneytounderstandingtheprinciples
thatunderlielearningandintelligence 7 2 0
Bibliograp hy
Abadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,
A.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M.,
Jia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Mané,D.,Monga,R.,
Moore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I.,
Talwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Viégas,F.,Vinyals,O.,Warden,
P.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scale
machinelearningonheterogeneoussystems.Softwareavailablefromtensorﬂow.org.,25
214446,
Ackley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).Alearningalgorithmfor
Boltzmannmachines.CognitiveScience,,147–169 , 9 570654
Alain,G.andBengio,Y.(2013) Whatregularizedauto-encoderslearnfromthedata
generatingdistribution.In .,,, ICLR’2013,arXiv:1211.4246507513514521
Alain,G.,Bengio,Y.,Yao,L.,ÉricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015) GSNs:Generativestochasticnetworks.arXiv:1503.05571.,510713
Anderson,E.(1935).TheIrisesoftheGaspéPeninsula.BulletinoftheAmericanIris
Society,,2–5 5 921
Ba,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisual
attention arXiv:1412.7755691
Bachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswith
collaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachine
Learning,ICML2015,Lille,France,6-11July2015,pages1964–1972 717
Bacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationin
neuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConference
onReinforcementLearningandDecisionMaking(RLDM2015).450
Bagnell,J.A.andBradley,D.M.(2009).Diﬀerentiablesparsecoding.InD.Koller,
D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformation
ProcessingSystems21(NIPS’08),pages113–120.498
721
BIBLIOGRAPHY
Bahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointly
learningtoalignandtranslate.In .,,,,, ICLR’2015,arXiv:1409.047325101397418420
465475476,,
Bahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987) Speechrecognition
withcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage, 2,
219–234.458
Baldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis:
Learningfromexampleswithoutlocalminima.NeuralNetworks,,53–58 2286
Baldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthe
pastandthefutureinproteinsecondarystructureprediction , Bioinformatics 1 5(11),
937–946.395
Baldi, P., Sadowski, P., andWhiteson, D.(2014).Searchingforexoticparticlesin
high-energyphysicswithdeeplearning.Naturecommunications, 526
Ballard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation Nature.452
Barlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295–311 1 147
Barron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidal
function.IEEETrans.onInformationTheory,,930–945 3 9 199
Bartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversity
Press.490
Basilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:Theoryand
Applications.Wiley.490
Bastien,F.,Lamblin,P., Pascanu,R.,Bergstra,J., Goodfellow,I.J.,Bergeron,A.,
Bouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements DeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,,2582214
222446,
Basu,S.andChristensen,J.(2013) Teachingclassiﬁcationboundariestohumans In
AAAI’2013.329
Baxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternational
ConferenceonComputationalLearningTheory(COLT’95),pages311–320,SantaCruz,
California.ACMPress.245
Bayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXiv
e-prints.265
Becker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfaces
inrandom-dotstereograms.Nature,,161–163 3 5 5 541
7 2 2
BIBLIOGRAPHY
Behnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstraction
pyramid.Int.J.ComputationalIntelligenceandApplications,(4),427–438 1 515
Beiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthreshold
logic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson, 1 4(5),1217–
1243.451
Belkin, M.and Niyogi, P.(2002).Laplacianeigenmapsandspectraltechniquesfor
embeddingandclustering InT.Dietterich,S.Becker,andZ.Ghahramani,editors,
AdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA MITPress.244
Belkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionand
datarepresentation.NeuralComputation,(6),1373–1396 , 1 5 164518
Bengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationin
neuralnetworksforfastermodels.arXiv:1511.06297.450
Bengio, S (2000a).Taking onthecurseofdimensionalityinjoint
distributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,special
issueonDataMiningandKnowledgeDiscovery,(3),550–557 1 1 707
Bengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingfor
sequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506.03099 384
Bengio,Y.(1991).ArtiﬁcialNeuralNetworksandtheirApplicationtoSequenceRecognition Ph.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.407
Bengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation,
1 2(8),1889–1900 435
Bengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215,
Dept.IRO,UniversitédeMontréal.467
Bengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,201622
Bengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatistical
LanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience,
pages1–37.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.448
Bengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation TechnicalReportarXiv:1510.02777,UniversitedeMontreal.656
Bengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti-
layerneuralnetworks.In,pages400–406.MITPress.,,, NIPS12 705707708710
Bengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence NeuralComputation,(6),1601–1621 , 2 1 513611
7 2 3
BIBLIOGRAPHY
Bengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-fold
cross-validation.InS.Thrun,L.Saul,andB.Schölkopf,editors,AdvancesinNeural
InformationProcessingSystems16(NIPS’03),Cambridge,MA.MITPress,Cambridge 122
Bengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScale
KernelMachines.19
Bengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul,
Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems
17(NIPS’04),pages129–136.MITPress.,160519
Bengio,Y.andSénécal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsby
importancesampling.InProceedingsofAISTATS2003.470
Bengio,Y.andSénécal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetraining
ofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks, 1 9(4),713–722

============================================================

=== CHUNK 195 ===
Palavras: 352
Caracteres: 11340
--------------------------------------------------
470
Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivated
acousticparametersforcontinuousspeechrecognitionusingartiﬁcialneuralnetworks InProceedingsofEuroSpeech’91.,27459
Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussian
mixturehybridforspeechrecognitionordensityestimation.In,pages175–182 NIPS4
MorganKaufmann.459
Bengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-term
dependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeural
Networks,pages1183–1195, SanFrancisco.IEEEPress.(invitedpaper).403
Bengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswith
gradientdescentisdiﬃcult.IEEETr.NeuralNets.,,, 18401403411
Bengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper-
parameters.LearningConference,Snowbird.435
Bengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel InT.K.Leen,T.G.Dietterich,andV.Tresp,editors, ,pages932–938.MIT NIPS’2000
Press.,,,,,, 18447464466472477482
Bengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilistic
languagemodel.,,1137–1155 , JMLR 3 466472
Bengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convex
neuralnetworks.In ,pages123–130 NIPS’2005 258
Bengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctions
forlocalkernelmachines.In NIPS’2005158
7 2 4
BIBLIOGRAPHY
Bengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows In .MITPress., NIPS’2005 160520
Bengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wise
trainingofdeepnetworks.In .,,,,,, NIPS’20061419201323324528530
Bengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.In
ICML’09.328
Bengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeep
representations.In ICML’2013604
Bengio,Y.,Léonard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradients
throughstochasticneuronsforconditionalcomputation arXiv:1308.3432.,,448450
689691,
Bengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto-
encodersasgenerativemodels.In .,, NIPS’2013507711714
Bengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewand
newperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI),
3 5(8),1798–1828 555
Bengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014) Deepgenerative
stochasticnetworkstrainablebybackprop.In .,,,, ICML’2014711712713714715
Bennett,C.(1976).EﬃcientestimationoffreeenergydiﬀerencesfromMonteCarlodata JournalofComputationalPhysics,(2),245–268 2 2 628
Bennett,J.andLanning,S.(2007).TheNetﬂixprize.479
Berger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropy
approachtonaturallanguageprocessing ComputationalLinguistics 2 2473
Berglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastive
divergenceandpersistentcontrastivedivergence., CoRR a b s/ 1 3 1 2 .6 0 0 2614
Bergstra, J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor Pattern
Classiﬁcation.Ph.D.thesis,UniversitédeMontréal.255
Bergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplex
cell-likenetworks.In NIPS’2009494
Bergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J MachineLearningRes.,,281–305 ,, 1 3 433434435
Bergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian,
J.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpression
compiler.InProc.SciPy.,,,, 2582214222446
7 2 5
BIBLIOGRAPHY
Bergstra,J.,Bardenet,R.,Bengio,Y.,andKégl,B.(2011).Algorithmsforhyper-parameter
optimization.In NIPS’2011436
Berkes,P.andWiskott,L.(2005).Slowfeatureanalysisyieldsarichrepertoireofcomplex
cellproperties JournalofVision 5 495
Bertsekas,D.P.andTsitsiklis,J.(1996).Neuro-DynamicProgramming.AthenaScientiﬁc 106
Besag,J.(1975).Statisticalanalysisofnon-latticedata , TheStatistician 2 4(3),179–195 615
Bishop,C.M.(1994).Mixturedensitynetworks.189
Bishop,C.M.(1995a).Regularizationandcomplexitycontrolinfeed-forwardnetworks InProceedingsInternationalConferenceonArtiﬁcialNeuralNetworksICANN’95,
volume1,page141–148 ,242250
Bishop,C.M.(1995b).TrainingwithnoiseisequivalenttoTikhonovregularization NeuralComputation,(1),108–116 7 242
Bishop,C.M.(2006).PatternRecognitionandMachineLearning.Springer.,98146
Blum,A.L.andRivest,R.L.(1992).Traininga3-nodeneuralnetworkisNP-complete 293
Blumer,A.,Ehrenfeucht,A.,Haussler,D.,andWarmuth,M.K.(1989).Learnabilityand
theVapnik–Chervonenkisdimension JournaloftheACM 3 6 114
Bonnet,G.(1964).Transformationsdessignauxaléatoiresàtraverslessystèmesnon
linéairessansmémoire.AnnalesdesTélécommunications,(9–10),203–220 1 9 689
Bordes, A., Weston, J., Collobert, R., andBengio, Y.(2011).Learningstructured
embeddingsofknowledgebases.In AAAI2011484
Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2012).Jointlearningofwordsand
meaningrepresentationsforopen-textsemanticparsing.AISTATS’2012.,,401484485
Bordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2013a).Asemanticmatchingenergy
functionforlearningwithmulti-relationaldata.MachineLearning:SpecialIssueon
LearningSemantics.483
Bordes,A.,Usunier, N.,Garcia-Duran,A.,Weston,J.,andYakhnenko,O.(2013b) Translatingembeddingsformodelingmulti-relationaldata.InC.Burges,L.Bottou,
M.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformation
ProcessingSystems26,pages2787–2795 CurranAssociates,Inc.484
Bornschein,J.andBengio,Y.(2015).Reweightedwake-sleep.InICLR’2015,
arXiv:1406.2751.693
7 2 6
BIBLIOGRAPHY
Bornschein,J.,Shabanian,S.,Fischer,A.,andBengio,Y.(2015).Trainingbidirectional
Helmholtzmachines.Technicalreport,arXiv:1506.03877.693
Boser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992).Atrainingalgorithmforopti-
malmarginclassiﬁers.InCOLT’92:Proceedingsoftheﬁfthannualworkshopon
Computationallearningtheory,pages144–152,NewYork,NY,USA.ACM.,18141
Bottou,L.(1998).Onlinealgorithmsandstochasticapproximations.InD.Saad,editor,
OnlineLearninginNeuralNetworks.CambridgeUniversityPress,Cambridge,UK.296
Bottou, L.(2011).Frommachinelearning tomachinereasoning.Technicalreport,
arXiv.1102.1808.401
Bottou,L.(2015).Multilayerneuralnetworks.DeepLearningSummerSchool.440
Bottou,L.andBousquet,O.(2008).Thetradeoﬀsoflargescalelearning.In NIPS’2008
282295,
Boulanger-Lewandowski,N.,Bengio,Y.,andVincent,P.(2012).Modelingtemporal
dependenciesinhigh-dimensionalsequences:Applicationtopolyphonicmusicgeneration
andtranscription.In ., ICML’12685686
Boureau,Y.,Ponce,J.,andLeCun,Y.(2010).Atheoreticalanalysisoffeaturepoolingin
visionalgorithms.InProc.InternationalConferenceonMachinelearning(ICML’10) 345
Boureau,Y.,LeRoux,N.,Bach,F.,Ponce,J.,andLeCun,Y.(2011) Askthelocals:
multi-waylocalpoolingforimagerecognition.InProc.InternationalConferenceon
ComputerVision(ICCV’11).IEEE.345
Bourlard,H.andKamp,Y.(1988).Auto-associationbymultilayerperceptronsand
singularvaluedecomposition.BiologicalCybernetics,,291–294 5 9 502
Bourlard,H.andWellekens,C.(1989).Speechpatterndiscriminationandmulti-layered
perceptrons.ComputerSpeechandLanguage,,1–19 3459
Boyd,S.andVandenberghe,L.(2004) .CambridgeUniversity ConvexOptimization
Press,NewYork,NY,USA.93
Brady,M.L.,Raghavan,R.,andSlawny,J.(1989).Back-propagationfailstoseparate
whereperceptronssucceed.IEEETransactionsonCircuitsandSystems, 3 6,665–674 284
Brakel,P.,Stroobandt,D.,andSchrauwen,B.(2013).Trainingenergy-basedmodelsfor
time-seriesimputation.JournalofMachineLearningResearch, 1 4,2771–2797 ,674
698
Brand,M.(2003).Chartingamanifold.In ,pages961–968.MITPress., NIPS’2002 164
518
7 2 7
BIBLIOGRAPHY
Breiman,L.(1994).Baggingpredictors.MachineLearning,(2),123–140 2 4 256
Breiman,L.,Friedman,J.H.,Olshen,R.A.,andStone,C.J.(1984).Classiﬁcationand
RegressionTrees.WadsworthInternationalGroup,Belmont,CA.146
Bridle,J.S.(1990).Alphanets:arecurrent‘neural’networkarchitecturewithahidden
Markovmodelinterpretation.SpeechCommunication,(1),83–92 9 186
Briggman,K.,Denk,W.,Seung,S.,Helmstaedter,M.N.,andTuraga,S.C.(2009) Maximinaﬃnitylearningofimagesegmentation.In ,pages1865–1873 NIPS’2009 360
Brown,P.F.,Cocke,J.,Pietra,S.A.D.,Pietra,V.J.D.,Jelinek,F.,Laﬀerty,J.D.,
Mercer,R.L.,andRoossin,P.S.(1990).Astatisticalapproachtomachinetranslation Computationallinguistics,(2),79–85 1 6 21
Brown,P.F.,Pietra,V.J.D.,DeSouza,P.V.,Lai,J.C.,andMercer,R.L.(1992).Class-
based-grammodelsofnaturallanguage , n ComputationalLinguistics 1 8,467–479 463
Bryson,A.andHo,Y.(1969) Appliedoptimalcontrol: optimization,estimation,and
control.BlaisdellPub.Co.225
Bryson,Jr.,A.E.andDenham,W.F.(1961).Asteepest-ascentmethodforsolving
optimumprogrammingproblems.TechnicalReportBR-1303,RaytheonCompany,
MissleandSpaceDivision.225
Buciluˇa, C.,Caruana,R., and Niculescu-Mizil, A (2006).Model compression.In
Proceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledgediscovery
anddatamining,pages535–541.ACM.448
Burda,Y.,Grosse,R.,andSalakhutdinov,R.(2015).Importanceweightedautoencoders arXivpreprintarXiv:1509.00519.698
Cai,M.,Shi,Y.,andLiu,J.(2013).Deepmaxoutneuralnetworksforspeechrecognition InAutomaticSpeechRecognitionandUnderstanding(ASRU),2013IEEEWorkshop
on,pages291–296.IEEE.194
Carreira-Perpiñan,M.A.andHinton,G.E.(2005).Oncontrastivedivergencelearning InR.G.CowellandZ.Ghahramani,editors,ProceedingsoftheTenthInternational
WorkshoponArtiﬁcialIntelligenceandStatistics(AISTATS’05),pages33–40.Society
forArtiﬁcialIntelligenceandStatistics.611
Caruana,R.(1993).Multitaskconnectionistlearning.InProc.1993ConnectionistModels
SummerSchool,pages372–379.244
Cauchy,A.(1847).Méthodegénéralepourlarésolutiondesystèmesd’équationssimul-
tanées.InCompterendudesséancesdel’académiedessciences,pages536–538 ,83
225
7 2 8
BIBLIOGRAPHY
Cayton,L.(2005).Algorithmsformanifoldlearning.TechnicalReportCS2008-0923,
UCSD.164
Chandola,V.,Banerjee,A.,andKumar,V.(2009).Anomalydetection:Asurvey.ACM
computingsurveys(CSUR),(3),15 4 1102
Chapelle,O.,Weston,J.,andSchölkopf,B.(2003).Clusterkernelsforsemi-supervised
learning.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeural
InformationProcessingSystems15(NIPS’02),pages585–592,Cambridge,MA.MIT
Press.244
Chapelle,O.,Schölkopf,B.,andZien,A.,editors(2006).Semi-SupervisedLearning.MIT
Press,Cambridge,MA.,244541
Chellapilla,K.,Puri,S.,andSimard,P.(2006).HighPerformanceConvolutionalNeural
Networks forDocumentProcessing.In GuyLorette, editor, Tenth International
WorkshoponFrontiersinHandwritingRecognition,LaBaule(France).Universitéde
Rennes1,Suvisoft.http://www.suvisoft.com.,,2427445
Chen,B.,Ting,J.-A.,Marlin,B.M.,anddeFreitas,N.(2010).Deeplearningofinvariant
spatio-temporalfeaturesfromvideo.NIPS*2010DeepLearningandUnsupervised
FeatureLearningWorkshop.360
Chen,S.F.andGoodman,J.T.(1999).Anempiricalstudyofsmoothingtechniquesfor
languagemodeling.Computer,SpeechandLanguage,(4),359–393 ,, 1 3 462463473
Chen,T.,Du,Z.,Sun,N.,Wang,J.,Wu,C.,Chen,Y.,andTemam,O.(2014a).DianNao:
Asmall-footprinthigh-throughputacceleratorforubiquitousmachine-learning.InPro-
ceedingsofthe19thinternationalconferenceonArchitecturalsupportforprogramming
languagesandoperatingsystems,pages269–284.ACM.451
Chen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,Xiao,T.,Xu,B.,Zhang,C.,
andZhang,Z.(2015).MXNet: A ﬂexibleandeﬃcientmachinelearninglibraryfor
heterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274.25
Chen,Y.,Luo,T.,Liu,S.,Zhang,S.,He,L.,Wang,J.,Li,L.,Chen,T.,Xu,Z.,Sun,N.,
etal Microarchitecture (2014b).DaDianNao:Amachine-learningsupercomputer.In
(MICRO),201447thAnnualIEEE/ACMInternationalSymposiumon,pages609–622

============================================================

=== CHUNK 196 ===
Palavras: 351
Caracteres: 10839
--------------------------------------------------
IEEE.451
Chilimbi,T.,Suzue,Y.,Apacible,J.,andKalyanaraman,K.(2014).ProjectAdam:
Buildinganeﬃcientandscalabledeeplearningtrainingsystem.In11thUSENIX
SymposiumonOperatingSystemsDesignandImplementation(OSDI’14).447
Cho,K.,Raiko,T.,andIlin,A.(2010).Paralleltemperingiseﬃcientforlearningrestricted
Boltzmannmachines.In ., IJCNN’2010603614
7 2 9
BIBLIOGRAPHY
Cho,K.,Raiko,T.,andIlin,A.(2011).Enhancedgradientandadaptivelearningratefor
trainingrestrictedBoltzmannmachines.In ,pages105–112 ICML’2011 674
Cho,K.,vanMerriënboer,B.,Gulcehre,C.,Bougares,F.,Schwenk,H.,andBengio,Y (2014a).LearningphraserepresentationsusingRNNencoder-decoderforstatistical
machinetranslation InProceedingsoftheEmpiricialMethodsinNaturalLanguage
Processing(EMNLP2014).,,397474475
Cho,K.,VanMerriënboer,B.,Bahdanau,D.,andBengio,Y.(2014b).Ontheprop-
ertiesofneuralmachinetranslation:Encoder-decoderapproaches , ArXive-prints
a b s/ 1 4 0 9 .1 2 5 9.412
Choromanska,A.,Henaﬀ,M.,Mathieu,M.,Arous,G.B.,andLeCun,Y.(2014) The
losssurfaceofmultilayernetworks.,285286
Chorowski,J.,Bahdanau,D.,Cho,K.,andBengio,Y.(2014) End-to-endcontinuous
speechrecognitionusingattention-basedrecurrentNN:Firstresults.arXiv:1412.1602 461
Christianson,B.(1992).AutomaticHessiansbyreverseaccumulation.IMAJournalof
NumericalAnalysis,(2),135–150 1 2 224
Chrupala,G.,Kadar,A.,andAlishahi,A.(2015).Learninglanguagethroughpictures 412
Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).Empiricalevaluationofgated
recurrentneuralnetworksonsequencemodeling.NIPS’2014DeepLearningworkshop,
arXiv1412.3555 ,412460
Chung,J.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2015a).Gatedfeedbackrecurrent
neuralnetworks.In ICML’15412
Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.,andBengio,Y.(2015b).A
recurrentlatentvariablemodelforsequentialdata.In NIPS’2015698
Ciresan,D.,Meier,U.,Masci,J.,andSchmidhuber,J.(2012).Multi-columndeepneural
networkfortraﬃcsignclassiﬁcation.NeuralNetworks,,333–338 , 3 2 23201
Ciresan,D.C.,Meier,U.,Gambardella,L.M.,andSchmidhuber,J.(2010) Deepbig
simpleneuralnetsforhandwrittendigitrecognition.NeuralComputation, 2 2,1–14 2427446,,
Coates,A.andNg,A.Y.(2011).Theimportanceofencodingversustrainingwithsparse
codingandvectorquantization.In .,, ICML’201127256498
Coates, A.,Lee, H.,andNg,A.Y (2011).Ananalysisofsingle-layernetworksin
unsupervisedfeaturelearning.InProceedingsoftheThirteenthInternationalConference
onArtiﬁcialIntelligenceandStatistics(AISTATS2011).,,363364455
7 3 0
BIBLIOGRAPHY
Coates,A.,Huval,B.,Wang, T.,Wu, D.,Catanzaro, B.,and Andrew,N DeeplearningwithCOTSHPCsystems.InS.DasguptaandD.McAllester,editors,
Proceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),
volume28(3),pages1337–1345 JMLRWorkshopandConferenceProceedings.,,2427
364447,
Cohen,N.,Sharir,O.,andShashua,A.(2015).Ontheexpressivepowerofdeeplearning:
Atensoranalysis.arXiv:1509.05009.554
Collobert,R.(2004).LargeScaleMachineLearning.Ph.D.thesis,UniversitédeParisVI,
LIP6.197
Collobert,R.(2011).Deeplearningforeﬃcientdiscriminativeparsing.InAISTATS’2011 101477,
Collobert,R.andWeston,J.(2008a).Auniﬁedarchitecturefornaturallanguageprocessing:
Deepneuralnetworkswithmultitasklearning.In ., ICML’2008471477
Collobert, R (2008b).A uniﬁed architecture fornatural language
processing:Deepneuralnetworkswithmultitasklearning.In ICML’2008535
Collobert,R.,Bengio,S.,andBengio,Y.(2001) AparallelmixtureofSVMsforvery
largescaleproblems.TechnicalReportIDIAP-RR-01-12,IDIAP.450
Collobert,R.,Bengio,S.,andBengio,Y.(2002).ParallelmixtureofSVMsforverylarge
scaleproblems.NeuralComputation,(5),1105–1114 1 4 450
Collobert,R.,Weston,J.,Bottou,L.,Karlen,M.,Kavukcuoglu,K.,andKuksa,P.(2011a) Naturallanguageprocessing(almost)fromscratch.TheJournalofMachineLearning
Research,,2493–2537 ,,, 1 2 328477535536
Collobert,R.,Kavukcuoglu,K.,andFarabet,C.(2011b).Torch7:AMatlab-likeenviron-
mentformachinelearning.InBigLearn,NIPSWorkshop.,,25214446
Comon,P.(1994).Independentcomponentanalysis-anewconcept?SignalProcessing,
3 6,287–314.491
Cortes,C.andVapnik,V.(1995).Supportvectornetworks.MachineLearning, 2 0,
273–297 ,18141
Couprie,C.,Farabet,C.,Najman,L.,andLeCun,Y.(2013).Indoorsemanticsegmentation
usingdepthinformation.InInternationalConferenceonLearningRepresentations
(ICLR2013).,23201
Courbariaux,M.,Bengio,Y.,andDavid,J.-P.(2015).Lowprecisionarithmeticfordeep
learning.InArxiv:1412.7024,ICLR’2015Workshop.452
Courville,A.,Bergstra,J.,andBengio,Y.(2011).Unsupervisedmodelsofimagesby
spike-and-slabRBMs.In ., ICML’11561681
7 3 1
BIBLIOGRAPHY
Courville,A.,Desjardins,G.,Bergstra,J.,andBengio,Y.(2014).Thespike-and-slab
RBMandextensionstodiscreteandsparsedatadistributions.PatternAnalysisand
MachineIntelligence,IEEETransactionson,(9),1874–1887 3 6 682
Cover,T.M.andThomas,J.A.(2006).ElementsofInformationTheory,2ndEdition Wiley-Interscience.73
Cox,D.andPinto,N.(2011).Beyondsimplefeatures:Alarge-scalefeaturesearch
approachtounconstrainedfacerecognition.InAutomaticFace&GestureRecognition
andWorkshops(FG2011),2011IEEEInternationalConferenceon,pages8–15.IEEE 363
Cramér,H.(1946).Mathematicalmethodsofstatistics.PrincetonUniversityPress.,135
295
Crick,F.H.C.andMitchison,G.(1983).Thefunctionofdreamsleep.Nature, 3 0 4,
111–114.609
Cybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction.Mathematics
ofControl,Signals,andSystems,,303–314 2 198
Dahl,G.E.,Ranzato,M.,Mohamed,A.,andHinton,G.E.(2010).Phonerecognition
withthemean-covariancerestrictedBoltzmannmachine.In NIPS’201023
Dahl,G.E.,Yu,D.,Deng,L.,andAcero,A.(2012).Context-dependentpre-traineddeep
neuralnetworksforlargevocabularyspeechrecognition.IEEETransactionsonAudio,
Speech,andLanguageProcessing,(1),33–42 2 0 459
Dahl,G.E.,Sainath,T.N.,andHinton,G.E.(2013).Improvingdeepneuralnetworks
forLVCSRusingrectiﬁedlinearunitsanddropout.In ICASSP’2013460
Dahl,G.E.,Jaitly,N.,andSalakhutdinov,R.(2014).Multi-taskneuralnetworksfor
QSARpredictions.arXiv:1406.1231.26
Dauphin, Y.andBengio, Y.(2013).Stochasticratiomatchingof RBMsforsparse
high-dimensionalinputs.In.NIPSFoundation NIPS26 619
Dauphin,Y.,Glorot,X.,andBengio,Y.(2011).Large-scalelearningofembeddingswith
reconstructionsampling.In ICML’2011471
Dauphin,Y.,Pascanu,R.,Gulcehre,C.,Cho,K.,Ganguli,S.,andBengio,Y.(2014) Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convex
optimization.In .,, NIPS’2014285286288
Davis,A.,Rubinstein,M.,Wadhwa,N.,Mysore,G.,Durand,F.,andFreeman,W.T (2014).Thevisualmicrophone:Passiverecoveryofsoundfromvideo.ACMTransactions
onGraphics(Proc.SIGGRAPH),(4),79:1–79:10 3 3 452
7 3 2
BIBLIOGRAPHY
Dayan,P.(1990).Reinforcementcomparison.InConnectionistModels:Proceedingsof
the1990ConnectionistSummerSchool,SanMateo,CA.691
Dayan,P.andHinton,G.E.(1996).VarietiesofHelmholtzmachine.NeuralNetworks,
9(8),1385–1403 693
Dayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995).TheHelmholtzmachine Neuralcomputation,(5),889–904 7 693
Dean,J.,Corrado,G.,Monga,R.,Chen,K.,Devin,M.,Le,Q.,Mao,M.,Ranzato,M.,
Senior,A.,Tucker,P.,Yang,K.,andNg,A.Y.(2012).Largescaledistributeddeep
networks.In ., NIPS’201225447
Dean,T.andKanazawa,K.(1989).Amodelforreasoningaboutpersistenceandcausation ComputationalIntelligence,(3),142–150 5 662
Deerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,andHarshman,R.(1990) Indexingbylatentsemanticanalysis.JournaloftheAmericanSocietyforInformation
Science,(6),391–407 , 4 1 477482
Delalleau,O.andBengio,Y.(2011).Shallowvs.deepsum-productnetworks.In.NIPS
19554,
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.(2009).ImageNet: A
Large-ScaleHierarchicalImageDatabase.In CVPR0921
Deng,J.,Berg,A.C.,Li,K.,andFei-Fei,L.(2010a).Whatdoesclassifyingmorethan
10,000imagecategoriestellus?InProceedingsofthe11thEuropeanConferenceon
ComputerVision:PartV,ECCV’10,pages71–84,Berlin,Heidelberg.Springer-Verlag 21
Deng,L.andYu,D.(2014).Deeplearning–methodsandapplications.Foundationsand
TrendsinSignalProcessing.460
Deng,L.,Seltzer,M.,Yu,D.,Acero,A.,Mohamed,A.,andHinton,G.(2010b).Binary
codingofspeechspectrogramsusingadeepauto-encoder.InInterspeech2010,Makuhari,
Chiba,Japan.23
Denil,M.,Bazzani,L.,Larochelle,H.,anddeFreitas,N.(2012).Learningwheretoattend
withdeeparchitecturesforimagetracking.NeuralComputation, 2 4(8),2151–2184 367
Denton,E.,Chintala,S.,Szlam,A.,andFergus,R.(2015).Deepgenerativeimagemodels
usingaLaplacianpyramidofadversarialnetworks.., NIPS702719
Desjardins,G.andBengio,Y.(2008).EmpiricalevaluationofconvolutionalRBMsfor
vision TechnicalReport1327,Départementd’InformatiqueetdeRechercheOpéra-
tionnelle,UniversitédeMontréal.683
7 3 3
BIBLIOGRAPHY
Desjardins, G., Courville, A.C., Bengio, Y., Vincen t, P., andDelalleau, O.(2010) TemperedMarkovchainMonteCarlofortrainingofrestrictedBoltzmannmachines.In
InternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages145–152 ,603
614
Desjardins,G.,Courville,A.,andBengio,Y.(2011).Ontrackingthepartitionfunction NIPS’2011629
Desjardins,G.,Simonyan,K.,Pascanu,R.,(2015) Naturalneuralnetworks AdvancesinNeuralInformationProcessingSystems,pages2062–2070 320
Devlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,andMakhoul,J.(2014).Fast
androbustneuralnetworkjointmodelsforstatisticalmachinetranslation ACL’2014.473
Devroye,L.(2013).Non-UniformRandomVariateGeneration.SpringerLink:Bücher SpringerNewYork.694
DiCarlo,J.J.(2013).Mechanismsunderlyingvisualobjectrecognition:Humansvs neuronsvs.machines.NIPSTutorial.,26366
Dinh,L.,Krueger,D.,andBengio,Y.(2014).NICE:Non-linearindependentcomponents
estimation.arXiv:1410.8516.493
Donahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,
K.,andDarrell,T.(2014).Long-termrecurrentconvolutionalnetworksforvisual
recognitionanddescription.arXiv:1411.4389.102
Donoho,D.L.andGrimes,C.(2003).Hessianeigenmaps:newlocallylinearembedding
techniquesforhigh-dimensional data.TechnicalReport2003-08, Dept.Statistics,
StanfordUniversity.,164519
Dosovitskiy,A.,Springenberg,J.T.,andBrox,T.(2015).Learningtogeneratechairswith
convolutionalneuralnetworks.InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,pages1538–1546 ,,696704705
Doya,K.(1993).Bifurcationsofrecurrentneuralnetworksingradientdescentlearning IEEETransactionsonNeuralNetworks,,75–80., 1401403
Dreyfus, S E.(1962).The numerical solutionofvariational problems.Journalof
MathematicalAnalysisandApplications,,30–45 5 ( 1 )225
Dreyfus,S.E.(1973).Thecomputationalsolutionofoptimalcontrolproblemswithtime
lag.IEEETransactionsonAutomaticControl,,383–385 1 8 ( 4 ) 225
Drucker,H.andLeCun,Y.(1992).Improvinggeneralisationperformanceusingdouble
back-propagation.IEEETransactionsonNeuralNetworks,(6),991–997 3 271
7 3 4
BIBLIOGRAPHY
Duchi,J.,Hazan,E.,andSinger,Y.(2011) Adaptivesubgradientmethodsforonline
learningandstochasticoptimization.JournalofMachineLearningResearch.307
Dudik,M.,Langford,J.,andLi,L.(2011).Doublyrobustpolicyevaluationandlearning InProceedingsofthe28thInternationalConferenceonMachinelearning,ICML’11 482
Dugas,C.,Bengio,Y.,Bélisle,F.,andNadeau,C.(2001)

============================================================

=== CHUNK 197 ===
Palavras: 356
Caracteres: 12516
--------------------------------------------------
Incorporatingsecond-order
functionalknowledgeforbetteroptionpricing.InT.Leen,T.Dietterich,andV.Tresp,
editors, AdvancesinNeural InformationProcessingSystems 13(NIPS’00), pages
472–478.MITPress.,68197
Dziugaite,G.K.,Roy,D.M.,andGhahramani,Z.(2015).Traininggenerativeneuralnet-
worksviamaximummeandiscrepancyoptimization.arXivpreprintarXiv:1505.03906 703
ElHihi,S.andBengio,Y.(1996).Hierarchicalrecurrentneuralnetworksforlong-term
dependencies.In .,, NIPS’1995398407408
Elkahky,A.M.,Song,Y.,andHe,X.(2015).Amulti-viewdeeplearningapproachfor
crossdomainusermodelinginrecommendationsystems InProceedingsofthe24th
InternationalConferenceonWorldWideWeb,pages278–288.480
Elman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceof
startingsmall.Cognition,,781–799 4 8 328
Erhan,D.,Manzagol,P.-A.,Bengio,Y.,Bengio,S.,andVincent,P.(2009).Thediﬃculty
oftrainingdeeparchitecturesandtheeﬀectofunsupervisedpre-training.InProceedings
ofAISTATS’2009.201
Erhan,D.,Bengio,Y.,Courville,A.,Manzagol,P.,Vincent,P.,andBengio,S.(2010) Whydoesunsupervisedpre-traininghelpdeeplearning?J.MachineLearningRes 529533534,,
Fahlman,S.E.,Hinton,G.E.,andSejnowski,T.J.(1983).Massivelyparallelarchitectures
for AI:NETL,thistle, andBoltzmann machines.In Proceedings ofthe National
ConferenceonArtiﬁcialIntelligenceAAAI-83.,570654
Fang,H.,Gupta,S.,Iandola,F.,Srivastava,R.,Deng,L.,Dollár,P.,Gao,J.,He,X.,
Mitchell,M.,Platt,J.C.,Zitnick,C.L.,andZweig,G.(2015).Fromcaptionstovisual
conceptsandback.arXiv:1411.4952.102
Farabet,C.,LeCun,Y.,Kavukcuoglu,K.,Culurciello,E.,Martini,B.,Akselrod,P.,and
Talay,S.(2011).Large-scaleFPGA-basedconvolutionalnetworks.InR.Bekkerman,
M.Bilenko,andJ.Langford, editors,ScalingupMachineLearning:Paralleland
DistributedApproaches.CambridgeUniversityPress.523
7 3 5
BIBLIOGRAPHY
Farabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.(2013).Learninghierarchicalfeatures
forscenelabeling.IEEETransactionsonPatternAnalysisandMachineIntelligence,
3 5(8),1915–1929 ,,23201360
Fei-Fei,L.,Fergus,R.,andPerona,P.(2006).One-shotlearningofobjectcategories IEEETransactionsonPatternAnalysisandMachineIntelligence, 2 8(4),594–611.538
Finn,C.,Tan,X.Y.,Duan,Y.,Darrell,T.,Levine,S.,andAbbeel,P.(2015).Learning
visualfeaturespacesforroboticmanipulationwithdeepspatialautoencoders.arXiv
preprintarXiv:1509.06113.25
Fisher,R.A.(1936).Theuseofmultiplemeasurementsintaxonomicproblems.Annals
ofEugenics,,179–188 , 7 21105
Földiák,P.(1989).Adaptivenetworkforoptimallinearfeatureextraction.InInternational
JointConferenceonNeuralNetworks(IJCNN),volume1,pages401–405,Washington
1989.IEEE,NewYork.494
Franzius,M.,Sprekeler,H.,andWiskott,L.(2007).Slownessandsparsenessleadtoplace,
head-direction,andspatial-viewcells.495
Franzius,M.,Wilbert,N.,andWiskott,L.(2008).Invariantobjectrecognitionwithslow
featureanalysis.InArtiﬁcialNeuralNetworks-ICANN2008,pages961–970.Springer 496
Frasconi,P.,Gori,M.,andSperduti,A.(1997).Ontheeﬃcientclassiﬁcationofdata
structuresbyneuralnetworks.InProc.Int.JointConf.onArtiﬁcialIntelligence.401
Frasconi, P., Gori, M., andSperduti, A.(1998).Ageneralframeworkforadaptive
processingofdatastructures.IEEETransactionsonNeuralNetworks, 9(5),768–786 401
Freund,Y.andSchapire,R.E.(1996a).Experimentswithanewboostingalgorithm.In
MachineLearning:ProceedingsofThirteenthInternationalConference,pages148–156,
USA.ACM.258
Freund,Y.andSchapire,R.E.(1996b).Gametheory,on-linepredictionandboosting.In
ProceedingsoftheNinthAnnualConferenceonComputationalLearningTheory,pages
325–332.258
Frey,B.J.(1998).Graphicalmodelsformachinelearninganddigitalcommunication MITPress.,705706
Frey,B.J.,Hinton,G.E.,andDayan,P.(1996).Doesthewake-sleepalgorithmlearngood
densityestimators?InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advances
inNeuralInformationProcessingSystems8(NIPS’95),pages661–670.MITPress,
Cambridge,MA.651
7 3 6
BIBLIOGRAPHY
Frobenius,G.(1908).Übermatrizenauspositivenelementen,s.B.Preuss.Akad.Wiss Berlin,Germany.597
Fukushima,K.(1975).Cognitron:Aself-organizingmultilayeredneuralnetwork.Biological
Cybernetics,,121–136 ,, 2 0 16226528
Fukushima, K.(1980).Neocognitron:Aself-organizingneuralnetworkmodelfora
mechanismofpatternrecognitionunaﬀectedbyshiftinposition.BiologicalCybernetics,
3 6,193–202 ,,,, 162427226367
Gal,Y.andGhahramani,Z.(2015).BayesianconvolutionalneuralnetworkswithBernoulli
approximatevariationalinference.arXivpreprintarXiv:1506.02158.264
Gallinari,P.,LeCun,Y.,Thiria,S.,andFogelman-Soulie,F.(1987).Memoiresassociatives
distribuees.InProceedingsofCOGNITIVA87,Paris,LaVillette.515
Garcia-Duran,A.,Bordes,A.,Usunier,N.,andGrandvalet,Y.(2015).Combiningtwo
andthree-wayembeddingsmodelsforlinkpredictioninknowledgebases.arXivpreprint
arXiv:1506.00999.484
Garofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,andPallett,D.S.(1993) Darpatimitacoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1 NASASTI/ReconTechnicalReportN,,27403 9 3459
Garson,J.(1900).Themetricsystemofidentiﬁcationofcriminals,asusedinGreat
BritainandIreland.TheJournaloftheAnthropologicalInstituteofGreatBritainand
Ireland,(2),177–227.21
Gers,F.A.,Schmidhuber,J.,andCummins,F.(2000) Learningtoforget:Continual
predictionwithLSTM.Neuralcomputation,(10),2451–2471 , 1 2 410412
Ghahramani,Z.andHinton,G.E.(1996).TheEMalgorithmformixturesoffactor
analyzers.TechnicalReportCRG-TR-96-1,Dpt.ofComp.Sci.,Univ.ofToronto.489
Gillick,D.,Brunk,C.,Vinyals,O.,andSubramanya,A.(2015).Multilinguallanguage
processingfrombytes.arXivpreprintarXiv:1512.00103.477
Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2015).Region-basedconvolutional
networksforaccurateobjectdetectionandsegmentation.426
Giudice,M.D.,Manera,V.,andKeysers,C.(2009).Programmedtolearn?Theontogeny
ofmirrorneurons.,(2),350––363 1 2 656
Glorot,X.andBengio,Y.(2010).Understandingthediﬃcultyoftrainingdeepfeedforward
neuralnetworks.InAISTATS’2010.303
Glorot,X.,Bordes,A.,andBengio,Y.(2011a).Deepsparserectiﬁerneuralnetworks.In
AISTATS’2011.,,,, 16174197226227
7 3 7
BIBLIOGRAPHY
Glorot, X.,Bordes, A.,andBengio, Y.(2011b).Domainadaptationforlarge-scale
sentimentclassiﬁcation:Adeeplearningapproach.In ., ICML’2011507537
Goldberger,J.,Roweis,S.,Hinton,G.E.,andSalakhutdinov,R.(2005).Neighbourhood
componentsanalysis.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeural
InformationProcessingSystems17(NIPS’04).MITPress.115
Gong,S.,McKenna,S.,andPsarrou,A.(2000).DynamicVision:FromImagestoFace
Recognition.ImperialCollegePress.,165519
Goodfellow,I.,Le,Q.,Saxe,A., andNg,A.(2009).Measuringinvariancesindeep
networks.In ,pages646–654 NIPS’2009 255
Goodfellow,I.,Koenig,N.,Muja,M.,Pantofaru,C.,Sorokin,A.,andTakayama,L.(2010) Helpmehelpyou:Interfacesforpersonalrobots.InProc.ofHumanRobotInteraction
(HRI),Osaka,Japan.ACMPress,ACMPress.100
Goodfellow,I.J.(2010).Technicalreport:Multidimensional,downsampledconvolution
forautoencoders.Technicalreport,UniversitédeMontréal.357
Goodfellow,I.J.(2014).Ondistinguishabilitycriteriaforestimatinggenerativemodels InInternationalConferenceonLearningRepresentations,WorkshopsTrack.,,622700
701
Goodfellow,I.J.,Courville,A.,andBengio,Y.(2011).Spike-and-slabsparsecoding
forunsupervisedfeaturediscovery.InNIPSWorkshoponChallengesinLearning
HierarchicalModels.,532538
Goodfellow,I.J.,Warde-Farley,D.,Mirza,M.,Courville,A.,andBengio,Y.(2013a) Maxoutnetworks.InS.DasguptaandD.McAllester,editors,,pages1319– ICML’13
1327.,,,, 193264344365455
Goodfellow,I.J.,Mirza,M.,Courville,A.,andBengio,Y.(2013b).Multi-predictiondeep
Boltzmannmachines.In.NIPSFoundation.,,,,,,, NIPS26 100617671672673674675
698
Goodfellow,I.J.,Warde-Farley,D.,Lamblin,P.,Dumoulin,V.,Mirza,M.,Pascanu,R.,
Bergstra,J.,Bastien,F.,andBengio,Y.(2013c).Pylearn2:amachinelearningresearch
library.arXivpreprintarXiv:1308.4214.,25446
Goodfellow,I.J.,Courville,A.,andBengio,Y.(2013d).Scalingupspike-and-slabmodels
forunsupervisedfeaturelearning.IEEETransactionsonPatternAnalysisandMachine
Intelligence,(8),1902–1914 ,,,, 3 5 497498499650683
Goodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,andBengio,Y.(2014a).Anempirical
investigationofcatastrophicforgetingingradient-basedneuralnetworks.In ICLR’2014
194
7 3 8
BIBLIOGRAPHY
Goodfellow,I.J.,Shlens,J.,andSzegedy,C.(2014b).Explainingandharnessingadver-
sarialexamples., .,,,, CoRR a b s/ 1 4 1 2 .6 5 7 2268269271555556
Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville,A.,andBengio,Y.(2014c).Generativeadversarialnetworks.In NIPS’2014
544689699701704 ,,,,
Goodfellow,I.J.,Bulatov,Y.,Ibarz,J.,Arnoud,S.,andShet,V.(2014d).Multi-digit
numberrecognitionfromStreetViewimageryusingdeepconvolutionalneuralnetworks InInternationalConferenceonLearningRepresentations.,,,,,, 25101201202203391
422449,
Goodfellow,I.J.,Vinyals,O.,andSaxe,A.M.(2015).Qualitativelycharacterizingneural
networkoptimizationproblems.InInternationalConferenceonLearningRepresenta-
tions.,,, 285286287291
Goodman,J.(2001).Classes forfast maximumentropytraining.InInternational
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),Utah.467
Gori,M.andTesi,A.(1992).Ontheproblemoflocalminimainbackpropagation.IEEE
TransactionsonPatternAnalysisandMachineIntelligence, P A M I - 1 4(1),76–86.284
Gosset,W.S.(1908).Theprobableerrorofamean , Biometrika 6(1),1–25.Originally
publishedunderthepseudonym“Student”.21
Gouws,S.,Bengio,Y.,andCorrado,G.(2014).BilBOWA:Fastbilingualdistributed
representationswithoutwordalignments.Technicalreport,arXiv:1410.2455.,476539
Graf,H.P.andJackel,L.D.(1989).Analogelectronicneuralnetworkcircuits.Circuits
andDevicesMagazine,IEEE,(4),44–49 5 451
Graves,A.(2011).Practicalvariationalinferenceforneuralnetworks.In NIPS’2011242
Graves,A.(2012).SupervisedSequenceLabellingwithRecurrentNeuralNetworks.Studies
inComputationalIntelligence.Springer.,,, 374395411460
Graves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.Technicalreport,
arXiv:1308.0850.,,, 190410415420
Graves,A.andJaitly,N.(2014).Towardsend-to-endspeechrecognitionwithrecurrent
neuralnetworks.In ICML’2014410
Graves,A.andSchmidhuber,J.(2005).Framewisephonemeclassiﬁcationwithbidirec-
tionalLSTMandotherneuralnetworkarchitectures.NeuralNetworks, 1 8(5),602–610 395
Graves,A.andSchmidhuber,J.(2009).Oﬄinehandwritingrecognitionwithmultidi-
mensionalrecurrentneuralnetworks.InD.Koller,D.Schuurmans,Y.Bengio,and
L.Bottou,editors, ,pages545–552 NIPS’2008 395
7 3 9
BIBLIOGRAPHY
Graves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J.(2006).Connectionisttemporal
classiﬁcation:Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In
ICML’2006,pages369–376,Pittsburgh,USA.460
Graves,A.,Liwicki,M.,Bunke,H.,Schmidhuber,J.,andFernández,S.(2008).Uncon-
strainedon-linehandwritingrecognitionwithrecurrentneuralnetworks.InJ.Platt,
D.Koller,Y.Singer,andS.Roweis,editors, ,pages577–584 NIPS’2007 395
Graves,A.,Liwicki,M.,Fernández,S.,Bertolami,R.,Bunke,H.,andSchmidhuber,J (2009).Anovelconnectionistsystemforunconstrainedhandwritingrecognition.Pattern
AnalysisandMachineIntelligence,IEEETransactionson,(5),855–868 3 1 410
Graves,A.,Mohamed,A.,andHinton,G.(2013).Speechrecognitionwithdeeprecurrent
neuralnetworks.In ,pages6645–6649 ,,,, ICASSP’2013 395398410411460
Graves,A.,Wayne,G.,andDanihelka,I.(2014a).NeuralTuringmachines arXiv:1410.5401.25
Graves,A.,Wayne,G.,andDanihelka,I.(2014b).NeuralTuringmachines.arXivpreprint
arXiv:1410.5401.418
Grefenstette,E.,Hermann,K.M.,Suleyman,M.,andBlunsom,P.(2015).Learningto
transducewithunboundedmemory.In NIPS’2015418
Greﬀ,K.,Srivastava,R.K.,Koutník,J.,Steunebrink,B.R.,andSchmidhuber,J.(2015) LSTM:asearchspaceodyssey.arXivpreprintarXiv:1503.04069.412
Gregor,K.andLeCun,Y.(2010a).Emergenceofcomplex-likecellsinatemporalproduct
networkwithlocalreceptiveﬁelds.Technicalreport,arXiv:1006.0448.352
Gregor,K.andLeCun,Y.(2010b).Learningfastapproximationsofsparsecoding.In
L.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternational
ConferenceonMachineLearning(ICML-10).ACM.652
Gregor, K.,Danihelka, I.,Mnih,A., Blundell,C.,and Wierstra, D (2014).Deep
autoregressivenetworks.InInternationalConferenceonMachineLearning(ICML’2014) 693
Gregor,K.,Danihelka,I.,Graves,A.,andWierstra,D.(2015).DRAW:Arecurrentneural
networkforimagegeneration.arXivpreprintarXiv:1502.04623.698
Gretton,A.,Borgwardt,K.M.,Rasch,M.J.,Schölkopf,B.,andSmola,A.(2012).A
kerneltwo-sampletest.TheJournalofMachineLearningResearch, 1 3(1),723–773 704
Gülçehre,Ç.andBengio,Y.(2013).Knowledgematters:Importanceofpriorinformation
foroptimization.InInternationalConferenceonLearningRepresentations(ICLR’2013) 25
7 4 0
BIBLIOGRAPHY
Guo,H.andGelfand,S.B.(1992).Classiﬁcationtreeswithneuralnetworkfeature
extraction.NeuralNetworks,IEEETransactionson,(6),923–933

============================================================

=== CHUNK 198 ===
Palavras: 350
Caracteres: 10585
--------------------------------------------------
3 450
Gupta,S.,Agrawal,A.,Gopalakrishnan,K.,andNarayanan,P.(2015).Deeplearning
withlimitednumericalprecision., CoRR a b s/ 1 5 0 2 .0 2 5 5 1452
Gutmann,M.andHyvarinen,A.(2010).Noise-contrastiveestimation: Anewestima-
tionprincipleforunnormalizedstatisticalmodels InProceedingsofTheThirteenth
InternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10).620
Hadsell, R., Sermanet, P., Ben, J.,Erkan,A., Han, J., Muller, U., andLeCun, Y (2007).Onlinelearningforoﬀroadrobots:Spatiallabelpropagationtolearnlong-range
traversability.InProceedingsofRobotics:ScienceandSystems,Atlanta,GA,USA.453
Hajnal,A.,Maass,W.,Pudlak,P.,Szegedy,M.,andTuran,G.(1993).Thresholdcircuits
ofboundeddepth 4 6 199
Håstad,J.(1986).Almostoptimallowerboundsforsmalldepthcircuits.InProceedings
ofthe18thannualACMSymposiumonTheoryofComputing,pages6–20,Berkeley,
California.ACMPress.199
Håstad,J.andGoldmann,M.(1991).Onthepowerofsmall-depththresholdcircuits ComputationalComplexity,,113–129 1 199
Hastie,T.,Tibshirani,R.,andFriedman,J.(2001).Theelementsofstatisticallearning:
datamining,inferenceandprediction SpringerSeriesinStatistics.SpringerVerlag 146
He,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Delvingdeepintorectiﬁers:Surpassing
human-levelperformanceonImageNetclassiﬁcation.arXivpreprintarXiv:1502.01852 28193,
Hebb,D.O.(1949).TheOrganizationofBehavior.Wiley,NewYork.,,1417656
Henaﬀ,M.,Jarrett,K.,Kavukcuoglu,K.,andLeCun,Y.(2011).Unsupervisedlearning
ofsparsefeaturesforscalableaudioclassiﬁcation.In ISMIR’11523
Henderson,J.(2003).Inducinghistoryrepresentationsforbroadcoveragestatistical
parsing.InHLT-NAACL,pages103–110.477
Henderson,J.(2004).Discriminativetrainingofaneuralnetworkstatisticalparser.In
Proceedingsofthe42ndAnnualMeetingonAssociationforComputationalLinguistics,
page95.477
Henniges,M.,Puertas,G.,Bornschein,J.,Eggert,J.,andLücke,J.(2010).Binarysparse
coding.InLatentVariableAnalysisandSignalSeparation,pages450–457.Springer 640
7 4 1
BIBLIOGRAPHY
Herault,J.andAns,B.(1984).Circuitsneuronauxàsynapsesmodiﬁables:Décodagede
messagescompositesparapprentissagenonsupervisé.ComptesRendusdel’Académie
desSciences, ,525––528 2 9 9 ( I I I - 1 3 ) 491
Hinton,G.(2012).Neuralnetworksformachinelearning.Coursera,videolectures.307
Hinton,G.,Deng,L.,Dahl,G.E.,Mohamed,A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,
Nguyen,P.,Sainath,T.,andKingsbury,B.(2012a).Deepneuralnetworksforacoustic
modelinginspeechrecognition.IEEESignalProcessingMagazine, 2 9(6),82–97.,23
460
Hinton,G.,Vinyals,O.,andDean,J.(2015).Distillingtheknowledgeinaneuralnetwork arXivpreprintarXiv:1503.02531.448
Hinton,G.E.(1989).Connectionistlearningprocedures.ArtiﬁcialIntelligence, 4 0,
185–234.494
Hinton,G.E.(1990).Mappingpart-wholehierarchiesintoconnectionistnetworks.Artiﬁcial
Intelligence,(1),47–75 4 6 418
Hinton,G.E.(1999).Productsofexperts.In ICANN’1999571
Hinton,G.E.(2000).Trainingproductsofexpertsbyminimizingcontrastivedivergence TechnicalReportGCNUTR2000-004,GatsbyUnit,UniversityCollegeLondon.,610
676
Hinton,G.E.(2006).Torecognizeshapes,ﬁrstlearntogenerateimages.TechnicalReport
UTMLTR2006-003,UniversityofToronto.,528595
Hinton,G.E.(2007a).Howtodobackpropagationinabrain.Invitedtalkatthe
NIPS’2007DeepLearningWorkshop.656
Hinton,G.E.(2007b) Learningmultiplelayersofrepresentation Trendsincognitive
sciences,(10),428–434 1 1 660
Hinton, G.E.(2010).ApracticalguidetotrainingrestrictedBoltzmannmachines TechnicalReportUTMLTR2010-003,DepartmentofComputerScience,Universityof
Toronto.610
Hinton,G.E.andGhahramani,Z.(1997).Generativemodelsfordiscoveringsparse
distributedrepresentations.PhilosophicalTransactionsoftheRoyalSocietyofLondon 147
Hinton,G.E.andMcClelland, J.L.(1988).Learningrepresentationsbyrecirculation.In
NIPS’1987,pages358–366.502
Hinton,G.E.andRoweis,S.(2003).Stochasticneighborembedding.In NIPS’2002519
7 4 2
BIBLIOGRAPHY
Hinton,G.E.andSalakhutdinov,R.(2006).Reducingthedimensionalityofdatawith
neuralnetworks.Science,(5786),504–507 ,,,, 3 1 3 509524528529534
Hinton,G.E.andSejnowski,T.J.(1986).LearningandrelearninginBoltzmannmachines InD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributedProcessing,
volume1,chapter7,pages282–317.MITPress,Cambridge.,570654
Hinton,G.E.andSejnowski,T.J.(1999).Unsupervisedlearning:foundationsofneural
computation.MITpress.541
Hinton,G.E.andShallice,T.(1991).Lesioninganattractornetwork:investigationsof
acquireddyslexia.Psychologicalreview,(1),74 9 813
Hinton,G.E.andZemel,R.S.(1994).Autoencoders,minimumdescriptionlength,and
Helmholtzfreeenergy.In NIPS’1993502
Hinton,G.E.,Sejnowski,T.J.,andAckley,D.H.(1984).Boltzmannmachines:Constraint
satisfactionnetworksthatlearn.TechnicalReportTR-CMU-CS-84-119,Carnegie-Mellon
University,Dept.ofComputerScience.,570654
Hinton,G.E.,McClelland, J.,andRumelhart,D.(1986) Distributedrepresentations InD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributedProcessing:
ExplorationsintheMicrostructureofCognition,volume1,pages77–109.MITPress,
Cambridge.,,17225526
Hinton,G.E.,Revow,M.,andDayan,P.(1995a).Recognizinghandwrittendigitsusing
mixturesoflinearmodels.InG.Tesauro,D.Touretzky,andT.Leen,editors,Advances
inNeuralInformationProcessingSystems7(NIPS’94),pages1015–1022 MITPress,
Cambridge,MA.489
Hinton,G.E.,Dayan,P.,Frey,B.J.,andNeal,R.M.(1995b).Thewake-sleepalgorithm
forunsupervisedneuralnetworks.Science,,1558–1161 , 2 6 8 504651
Hinton,G.E.,Dayan,P.,andRevow,M.(1997).Modellingthemanifoldsofimagesof
handwrittendigits.IEEETransactionsonNeuralNetworks,,65–74 8499
Hinton,G.E.,Welling,M.,Teh,Y.W.,andOsindero,S.(2001).AnewviewofICA.In
Proceedingsof3rdInternationalConferenceonIndependentComponentAnalysisand
BlindSignalSeparation(ICA’01),pages746–751,SanDiego,CA.491
Hinton,G.E.,Osindero,S.,andTeh,Y.(2006).Afastlearningalgorithmfordeepbelief
nets.NeuralComputation,,1527–1554 ,,,,,,, 1 8 141927143528529660661
Hinton,G.E., Deng,L.,Yu, D.,Dahl,G.E.,Mohamed, A.,Jaitly, N.,Senior,A.,
Vanhoucke,V.,Nguyen,P.,Sainath,T.N.,andKingsbury,B.(2012b).Deepneural
networksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearch
groups.IEEESignalProcess.Mag.,(6),82–97 2 9 101
7 4 3
BIBLIOGRAPHY
Hinton,G.E.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2012c) Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.Technical
report,arXiv:1207.0580.,,238263267
Hinton,G.E.,Vinyals,O.,andDean,J.(2014).Darkknowledge Invitedtalkatthe
BayLearnBayAreaMachineLearningSymposium.448
Hochreiter,S.(1991).UntersuchungenzudynamischenneuronalenNetzen.Diploma
thesis,T.U.München.,,18401403
Hochreiter,S.andSchmidhuber,J.(1995) Simplifyingneuralnetsbydiscoveringﬂat
minima.InAdvancesinNeuralInformationProcessingSystems7,pages529–536.MIT
Press.243
Hochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.NeuralComputation,
9(8),1735–1780 ,,18410411
Hochreiter,S.,Bengio,Y.,andFrasconi,P.(2001).Gradientﬂowinrecurrentnets:the
diﬃcultyoflearninglong-termdependencies.InJ.KolenandS.Kremer,editors,Field
GuidetoDynamicalRecurrentNetworks.IEEEPress.411
Holi,J.L.andHwang,J.-N.(1993).Finiteprecisionerroranalysisofneuralnetwork
hardwareimplementations.Computers,IEEETransactionson,(3),281–290 4 2 451
Holt,J.L.andBaker,T.E.(1991) Backpropagationsimulationsusinglimitedpreci-
sioncalculations.InNeuralNetworks,1991.,IJCNN-91-SeattleInternationalJoint
Conferenceon,volume2,pages121–126.IEEE.451
Hornik,K.,Stinchcombe,M.,andWhite,H.(1989).Multilayerfeedforwardnetworksare
universalapproximators.NeuralNetworks,,359–366 2 198
Hornik,K.,Stinchcombe,M.,andWhite,H.(1990).Universalapproximationofan
unknownmappinganditsderivativesusingmultilayerfeedforwardnetworks.Neural
networks,(5),551–560 3 198
Hsu,F.-H.(2002).BehindDeepBlue:BuildingtheComputerThatDefeatedtheWorld
ChessChampion.PrincetonUniversityPress,Princeton,NJ,USA.2
Huang,F.andOgata,Y.(2002).Generalizedpseudo-likelihoodestimatesforMarkov
randomﬁeldsonlattice.AnnalsoftheInstituteofStatisticalMathematics, 5 4(1),1–18 616
Huang,P.-S.,He,X.,Gao,J.,Deng,L.,Acero,A.,andHeck,L.(2013).Learningdeep
structuredsemanticmodelsforwebsearchusingclickthroughdata.InProceedingsof
the22ndACMinternationalconferenceonConferenceoninformation&knowledge
management,pages2333–2338 ACM.480
Hubel,D.andWiesel,T.(1968).Receptiveﬁeldsandfunctionalarchitectureofmonkey
striatecortex.JournalofPhysiology(London),,215–243 1 9 5 364
7 4 4
BIBLIOGRAPHY
Hubel,D.H.andWiesel,T.N.(1959).Receptiveﬁeldsofsingleneuronsinthecat’s
striatecortex.JournalofPhysiology,,574–591 1 4 8 364
Hubel,D.H.andWiesel, T.N.(1962).Receptiveﬁelds, binocularinteraction,and
functionalarchitectureinthecat’svisualcortex.JournalofPhysiology(London), 1 6 0,
106–154.364
Huszar,F.(2015).How(not)totrainyourgenerativemodel:schedulesampling,likelihood,
adversary arXiv:1511.05101698
Hutter,F.,Hoos,H.,andLeyton-Brown,K.(2011) Sequentialmodel-basedoptimization
forgeneralalgorithmconﬁguration.In.ExtendedversionasUBCTechreport LION-5
TR-2010-10.436
Hyotyniemi,H.(1996).Turingmachinesarerecurrentneuralnetworks.InSTeP’96,pages
13–24.379
Hyvärinen,A.(1999) Surveyonindependentcomponentanalysis.NeuralComputing
Surveys,,94–128 2 491
Hyvärinen,A.(2005).Estimationofnon-normalizedstatisticalmodelsusingscorematching JournalofMachineLearningResearch,,695–709 , 6 513617
Hyvärinen,A.(2007a).Connectionsbetweenscorematching,contrastivedivergence,
andpseudolikelihoodforcontinuous-valuedvariables.IEEETransactionsonNeural
Networks,,1529–1531 1 8 618
Hyvärinen,A.(2007b).Someextensionsofscorematching.ComputationalStatisticsand
DataAnalysis,,2499–2512 5 1 618
Hyvärinen,A.andHoyer,P.O.(1999).Emergenceoftopographyandcomplexcell
propertiesfromnaturalimagesusingextensionsofica.In,pages827–833 NIPS 493
Hyvärinen, A.andPajunen, P.(1999).Nonlinearindependentcomponentanalysis:
Existenceanduniquenessresults.NeuralNetworks,(3),429–439 1 2 493
Hyvärinen,A.,Karhunen,J.,andOja,E.(2001a).IndependentComponentAnalysis Wiley-Interscience.491
Hyvärinen,A.,Hoyer,P.O.,andInki,M.O.(2001b).Topographicindependentcomponent
analysis.NeuralComputation,(7),1527–1558 1 3 493
Hyvärinen,A.,Hurri,J.,andHoyer,P.O.(2009).NaturalImageStatistics:Aprobabilistic
approachtoearlycomputationalvision.Springer-Verlag.370
Iba,Y.(2001).ExtendedensembleMonteCarlo.InternationalJournalofModernPhysics,
C 1 2,623–656.603
7 4 5
BIBLIOGRAPHY
Inayoshi, H (2005).Improved generalizationbyadding both auto-
associationandhidden-layernoisetoneural-network-based-classiﬁers.IEEEWorkshop
onMachineLearningforSignalProcessing,pages141—-146.515
Ioﬀe,S.andSzegedy,C.(2015).Batchnormalization:Acceleratingdeepnetworktraining
byreducinginternalcovariateshift.,,100317320
Jacobs,R.A.(1988) Increasedratesofconvergencethroughlearningrateadaptation Neuralnetworks,(4),295–307

============================================================

=== CHUNK 199 ===
Palavras: 357
Caracteres: 13838
--------------------------------------------------
1 307
Jacobs,R.A.,Jordan,M.I.,Nowlan,S.J.,andHinton,G.E.(1991).Adaptivemixtures
oflocalexperts.NeuralComputation,,79–87., 3189450
Jaeger,H.(2003).Adaptivenonlinearsystemidentiﬁcationwithechostatenetworks.In
AdvancesinNeuralInformationProcessingSystems15.404
Jaeger,H.(2007a).Discoveringmultiscaledynamicalfeatureswithhierarchicalechostate
networks.Technicalreport,JacobsUniversity.398
Jaeger,H.(2007b).Echostatenetwork.Scholarpedia,(9),2330 2 404
Jaeger,H.(2012).Longshort-termmemoryinechostatenetworks:Detailsofasimulation
study.Technicalreport,Technicalreport,JacobsUniversityBremen.405
Jaeger,H.andHaas,H.(2004).Harnessingnonlinearity:Predictingchaoticsystemsand
savingenergyinwirelesscommunication.Science,(5667),78–80., 3 0 4 27404
Jaeger,H.,Lukosevicius,M.,Popovici,D.,andSiewert,U.(2007).Optimizationand
applicationsofechostatenetworkswithleaky-integratorneurons.NeuralNetworks,
2 0(3),335–352.407
Jain,V.,Murray,J.F.,Roth,F.,Turaga,S.,Zhigulin,V.,Briggman,K.L.,Helmstaedter,
M.N.,Denk,W.,andSeung,H.S.(2007).Supervisedlearningofimagerestoration
withconvolutionalnetworks.InComputer Vision,2007.ICCV2007.IEEE11th
InternationalConferenceon,pages1–8.IEEE.359
Jaitly,N.andHinton,G.(2011).Learningabetterrepresentationofspeechsoundwaves
usingrestrictedBoltzmannmachines.InAcoustics, SpeechandSignalProcessing
(ICASSP),2011IEEEInternationalConferenceon,pages5884–5887 IEEE.458
Jaitly,N.andHinton,G.E.(2013).Vocaltractlengthperturbation(VTLP)improves
speechrecognition.In ICML’2013241
Jarrett,K.,Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2009).Whatisthebest
multi-stagearchitectureforobjectrecognition?In.,,,,,, ICCV’09162427174193226
363364523,,
Jarzynski,C.(1997).Nonequilibriumequalityforfreeenergydiﬀerences.Phys.Rev.Lett.,
7 8,2690–2693 ,625628
7 4 6
BIBLIOGRAPHY
Jaynes,E.T.(2003).ProbabilityTheory:TheLogicofScience.CambridgeUniversity
Press.53
Jean,S.,Cho,K.,Memisevic,R.,andBengio,Y.(2014).Onusingverylargetarget
vocabularyforneuralmachinetranslation.arXiv:1412.2007.,474475
Jelinek,F.andMercer,R.L.(1980).InterpolatedestimationofMarkovsourceparameters
fromsparsedata.InE.S.GelsemaandL.N.Kanal,editors,PatternRecognitionin
Practice.North-Holland,Amsterdam.,462473
Jia,Y.(2013).Caﬀe:Anopensourceconvolutionalarchitectureforfastfeatureembedding http://caffe.berkeleyvision.org/.,25214
Jia,Y.,Huang,C.,andDarrell,T.(2012).Beyondspatialpyramids:Receptiveﬁeld
learningforpooledimagefeatures.InComputerVisionandPatternRecognition
(CVPR),2012IEEEConferenceon,pages3370–3377 IEEE.345
Jim,K.-C.,Giles,C.L.,andHorne,B.G.(1996).Ananalysisofnoiseinrecurrentneural
networks: convergenceandgeneralization.IEEETransactionsonNeuralNetworks,
7(6),1424–1438 242
Jordan,M.I.(1998).LearninginGraphicalModels.Kluwer,Dordrecht,Netherlands.18
Joulin,A.andMikolov,T.(2015).Inferringalgorithmicpatternswithstack-augmented
recurrentnets.arXivpreprintarXiv:1503.01007.418
Jozefowicz,R.,Zaremba,W.,andSutskever,I.(2015).Anempiricalevaluationofrecurrent
networkarchitectures.In ., ICML’2015306412
Judd,J.S.(1989).NeuralNetworkDesignandtheComplexityofLearning.MITpress 293
Jutten, C.andHerault, J.(1991).Blindseparationofsources, partI:anadaptive
algorithmbasedonneuromimeticarchitecture.SignalProcessing,,1–10 2 4491
Kahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Gülçehre,c.,Memisevic,R.,Vincent,
P.,Courville,A.,Bengio,Y.,Ferrari,R.C.,Mirza,M.,Jean,S.,Carrier,P.L.,Dauphin,
Y.,Boulanger-Lewandowski,N.,Aggarwal,A.,Zumer, J.,Lamblin,P.,Raymond,
J.-P.,Desjardins,G.,Pascanu,R.,Warde-Farley,D.,Torabi,A.,Sharma,A.,Bengio,
E.,Côté,M.,Konda,K.R.,andWu,Z.(2013).Combiningmodalityspeciﬁcdeep
neuralnetworksforemotionrecognitioninvideo.InProceedingsofthe15thACMon
InternationalConferenceonMultimodalInteraction.201
Kalchbrenner,N.andBlunsom,P.(2013).Recurrentcontinuoustranslationmodels.In
EMNLP’2013.,474475
Kalchbrenner,N.,Danihelka,I.,andGraves,A.(2015) Gridlongshort-termmemory arXivpreprintarXiv:1507.01526.395
7 4 7
BIBLIOGRAPHY
Kamyshanska,H.andMemisevic,R.(2015).Thepotentialenergyofanautoencoder IEEETransactionsonPatternAnalysisandMachineIntelligence.515
Karpathy,A.andLi,F.-F.(2015).Deepvisual-semanticalignmentsforgeneratingimage
descriptions.In .arXiv:1412.2306 CVPR’2015 102
Karpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,andFei-Fei,L.(2014) Large-scalevideoclassiﬁcationwithconvolutionalneuralnetworks.In.CVPR21
Karush,W.(1939).MinimaofFunctionsofSeveralVariableswithInequalitiesasSide
Constraints.Master’sthesis,Dept.ofMathematics,Univ.ofChicago.95
Katz,S.M.(1987).Estimationofprobabilitiesfromsparsedataforthelanguagemodel
componentofaspeechrecognizer IEEETransactionsonAcoustics,Speech,andSignal
Processing, (3),400–401 , A S S P-3 5 462473
Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2008) Fastinferenceinsparsecoding
algorithmswithapplicationstoobjectrecognition.Technicalreport,Computationaland
BiologicalLearningLab,CourantInstitute,NYU.TechReportCBLL-TR-2008-12-01 523
Kavukcuoglu,K.,Ranzato,M.-A.,Fergus,R.,andLeCun,Y.(2009).Learninginvariant
featuresthroughtopographicﬁltermaps.In CVPR’2009523
Kavukcuoglu,K.,Sermanet,P.,Boureau,Y.-L.,Gregor,K.,Mathieu,M.,andLeCun,Y (2010).Learningconvolutionalfeaturehierarchiesforvisualrecognition.In NIPS’2010
364523,
Kelley,H.J.(1960).Gradienttheoryofoptimalﬂightpaths , ARSJournal 3 0(10),
947–954.225
Khan,F.,Zhu,X.,andMutlu,B.(2011).Howdohumansteach:Oncurriculumlearning
andteachingdimension.InAdvancesinNeuralInformationProcessingSystems24
(NIPS’11),pages1449–1457 328
Kim,S.K.,McAfee,L.C.,McMahon, P.L.,andOlukotun,K.(2009).Ahighlyscalable
restrictedBoltzmannmachineFPGAimplementation.InFieldProgrammableLogic
andApplications,2009.FPL2009.InternationalConferenceon,pages367–372.IEEE 451
Kindermann,R.(1980).MarkovRandomFieldsandTheirApplications(Contemporary
Mathematics;V.1).AmericanMathematicalSociety.566
Kingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXiv
preprintarXiv:1412.6980.308
Kingma,D.andLeCun,Y.(2010).Regularizedestimationofimagestatisticsbyscore
matching.In ., NIPS’2010513620
7 4 8
BIBLIOGRAPHY
Kingma,D.,Rezende,D.,Mohamed,S.,andWelling,M.(2014).Semi-supervisedlearning
withdeepgenerativemodels.In NIPS’2014426
Kingma,D.P.(2013).Fastgradient-basedinferencewithcontinuouslatentvariable
modelsinauxiliaryform.Technicalreport,arxiv:1306.0733.,,652689696
Kingma,D.P.andWelling,M.(2014a).Auto-encodingvariationalbayes.InProceedings
oftheInternationalConferenceonLearningRepresentations(ICLR).,689700
Kingma, D.P.andWelling, M.(2014b).Eﬃcientgradient-basedinferencethrough
transformationsbetweenbayesnetsandneuralnets.Technicalreport,arxiv:1402.0480 689
Kirkpatrick,S.,Jr.,C.D.G.,,andVecchi,M.P.(1983).Optimizationbysimulated
annealing.Science,,671–680 2 2 0 327
Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014a).Multimodalneurallanguagemodels ICML’2014102
Kiros,R.,Salakhutdinov,R.,andZemel,R.(2014b).Unifyingvisual-semanticembeddings
withmultimodalneurallanguagemodels ., arXiv:1411.2539[cs.LG]102410
Klementiev,A.,Titov,I.,andBhattarai,B.(2012).Inducingcrosslingualdistributed
representationsofwords.InProceedingsofCOLING2012.,476539
Knowles-Barley,S.,Jones,T.R.,Morgan,J.,Lee,D.,Kasthuri,N.,Lichtman,J.W.,and
Pﬁster,H.(2014).Deeplearningfortheconnectome.GPUTechnologyConference.26
Koller,D.andFriedman, N.(2009).ProbabilisticGraphicalModels:Principlesand
Techniques.MITPress.,,583595645
Konig,Y.,Bourlard,H.,andMorgan,N.(1996).REMAP:Recursiveestimationand
maximizationofaposterioriprobabilities–applicationtotransition-basedconnectionist
speechrecognition.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advancesin
NeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.459
Koren,Y.(2009).TheBellKorsolutiontotheNetﬂixgrandprize.,258480
Kotzias,D.,Denil,M.,deFreitas,N.,andSmyth,P.(2015).Fromgrouptoindividual
labelsusingdeepfeatures.In ACMSIGKDD106
Koutnik,J.,Greﬀ,K.,Gomez,F.,andSchmidhuber,J.(2014).AclockworkRNN.In
ICML’2014.408
Kočiský,T.,Hermann,K.M.,andBlunsom,P.(2014).LearningBilingualWordRepre-
sentationsbyMarginalizingAlignments.InProceedingsofACL.476
Krause,O.,Fischer,A.,Glasmachers,T.,andIgel,C.(2013).Approximationproperties
ofDBNswithbinaryhiddenunitsandreal-valuedvisibleunits.In ICML’2013553
7 4 9
BIBLIOGRAPHY
Krizhevsky,A.(2010).ConvolutionaldeepbeliefnetworksonCIFAR-10.Technicalreport,
UniversityofToronto.UnpublishedManuscript:http://www.cs.utoronto.ca/kriz/conv-
cifar10-aug2010.pdf.446
Krizhevsky,A.andHinton,G.(2009).Learningmultiplelayersoffeaturesfromtiny
images.Technicalreport,UniversityofToronto.,21561
Krizhevsky,A.andHinton,G.E.(2011).Usingverydeepautoencodersforcontent-based
imageretrieval.In.ESANN525
Krizhevsky,A.,Sutskever,I.,andHinton,G.(2012).ImageNetclassiﬁcationwithdeep
convolutionalneuralnetworks.In .,,,,,,, NIPS’2012232427100201371454458
Krueger,K.A.andDayan,P.(2009).Flexibleshaping:howlearninginsmallstepshelps 1 1 0 328
Kuhn,H.W.andTucker,A.W.(1951).Nonlinearprogramming.InProceedingsofthe
SecondBerkeleySymposiumonMathematicalStatisticsandProbability,pages481–492,
Berkeley,Calif.UniversityofCaliforniaPress.95
Kumar,A.,Irsoy,O.,Su,J.,Bradbury,J.,English,R.,Pierce,B.,Ondruska,P.,Iyyer,
M.,Gulrajani,I.,andSocher,R.(2015).Askmeanything:Dynamicmemorynetworks
fornaturallanguageprocessing ., arXiv:1506.07285418485
Kumar,M.P.,Packer,B.,andKoller,D.(2010).Self-pacedlearningforlatentvariable
models.In NIPS’2010328
Lang,K.J.andHinton,G.E.(1988).Thedevelopmentofthetime-delayneuralnetwork
architectureforspeechrecognition.TechnicalReportCMU-CS-88-152,Carnegie-Mellon
University.,,367374407
Lang,K.J.,Waibel,A.H.,andHinton,G.E.(1990).Atime-delayneuralnetwork
architectureforisolatedwordrecognition.Neuralnetworks,(1),23–43 3 374
Langford,J.andZhang,T.(2008).Theepoch-greedyalgorithmforcontextualmulti-armed
bandits.In ,pages1096––1103 NIPS’2008 480
Lappalainen,H.,Giannakopoulos,X.,Honkela,A.,andKarhunen,J.(2000).Nonlinear
independentcomponentanalysisusingensemblelearning:Experimentsanddiscussion InProc.ICA.Citeseer.493
Larochelle, H and Bengi o, Y.(2008).Classiﬁcation usingdiscriminative restricted
Boltzmannmachines.In .,,,, ICML’2008244255530686716
Larochelle,H.andHinton,G.E.(2010).Learningtocombinefovealglimpseswitha
third-orderBoltzmannmachine.InAdvancesinNeuralInformationProcessingSystems
23,pages1243–1251 367
7 5 0
BIBLIOGRAPHY
Larochelle,H.andMurray,I.(2011).TheNeuralAutoregressiveDistributionEstimator InAISTATS’2011.,,705708709
Larochelle,H.,Erhan,D.,andBengio,Y.(2008) Zero-datalearningofnewtasks.In
AAAIConferenceonArtiﬁcialIntelligence.539
Larochelle,H.,Bengio,Y.,Louradour,J.,andLamblin,P.(2009).Exploringstrategiesfor
trainingdeepneuralnetworks.JournalofMachineLearningResearch,,1–40 1 0535
Lasserre,J.A.,Bishop,C.M.,andMinka,T.P.(2006).Principledhybridsofgenerativeand
discriminativemodels.InProceedingsoftheComputerVisionandPatternRecognition
Conference(CVPR’06),pages87–94,Washington,DC,USA.IEEEComputerSociety 244253,
Le,Q.,Ngiam,J.,Chen,Z.,haoChia,D.J.,Koh,P.W.,andNg,A.(2010).Tiled
convolutionalneuralnetworks.InJ.Laﬀerty,C.K.I.Williams,J.Shawe-Taylor,
R.Zemel,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems
23(NIPS’10),pages1279–1287 352
Le,Q.,Ngiam,J.,Coates,A.,Lahiri,A.,Prochnow,B.,andNg,A.(2011).Onoptimization
methodsfordeeplearning.InProc.ICML’2011.ACM.316
Le,Q.,Ranzato,M.,Monga,R.,Devin,M.,Corrado,G.,Chen,K.,Dean,J.,andNg,
A.(2012).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.In
ICML’2012.,2427
LeRoux,N.andBengio,Y.(2008).RepresentationalpowerofrestrictedBoltzmann
machinesanddeepbeliefnetworks.NeuralComputation,(6),1631–1649 , 2 0 553655
LeRoux,N.andBengio,Y.(2010).Deepbeliefnetworksarecompactuniversalapproxi-
mators.NeuralComputation,(8),2192–2207 2 2 553
LeCun,Y.(1985).Uneprocédured’apprentissagepourRéseauàseuilassymétrique.In
Cognitiva85:AlaFrontièredel’IntelligenceArtiﬁcielle,desSciencesdelaConnaissance
etdesNeurosciences,pages599–604,Paris1985.CESTA,Paris.225
LeCun,Y.(1986).Learningprocessesinanasymmetricthresholdnetwork.InF.Fogelman-
Soulié,E.Bienenstock,andG.Weisbuch,editors,DisorderedSystemsandBiological
Organization,pages233–240.Springer-Verlag,LesHouches,France.352
LeCun,Y.(1987).Modèlesconnexionistesdel’apprentissage.Ph.D.thesis,Universitéde
ParisVI.,,18502515
LeCun, Y.(1989).Generalizationandnetworkdesignstrategies.TechnicalReport
CRG-TR-89-4,UniversityofToronto.,330352
7 5 1
BIBLIOGRAPHY
LeCun,Y.,Jackel,L.D.,Boser,B.,Denker,J.S.,Graf,H.P.,Guyon,I.,Henderson,D.,
Howard,R.E.,andHubbard,W.(1989).Handwrittendigitrecognition:Applications
ofneuralnetworkchipsandautomaticlearning.IEEECommunicationsMagazine,
2 7(11),41–46.368
LeCun,Y.,Bottou,L.,Orr,G.B.,andMüller,K.-R.(1998a).Eﬃcientbackprop.In
NeuralNetworks,TricksoftheTrade,LectureNotesinComputerScienceLNCS1524 SpringerVerlag.,310429
LeCun,Y.,Bottou,L.,Bengio,Y.,andHaﬀner,P.(1998b).Gradientbasedlearning
appliedtodocumentrecognition.Proc.IEEE.,,,,,, 16182127371458460
LeCun, Y., Kavukcuoglu, K., andFarabet, C.(2010).Convolutionalnetworksand
applicationsinvision InCircuitsandSystems(ISCAS),Proceedingsof2010IEEE
InternationalSymposiumon,pages253–256.IEEE.371
L’Ecuyer,P.(1994).Eﬃciencyimprovementandvariancereduction.InProceedingsof
the1994WinterSimulationConference,pages122––132.690
Lee,C.-Y.,Xie,S.,Gallagher,P.,Zhang,Z.,andTu,Z.(2014).Deeply-supervisednets arXivpreprintarXiv:1409.5185.326
Lee,H.,Battle,A.,Raina,R.,andNg,A.(2007).Eﬃcientsparsecodingalgorithms InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformation
ProcessingSystems19(NIPS’06),pages801–808.MITPress.637
Lee,H.,Ekanadham,C.,andNg,A.(2008).Sparsedeepbeliefnetmodelforvisualarea
V2.In.NIPS’07255
Lee,H.,Grosse,R.,Ranganath,R.,andNg,A.Y.(2009).Convolutionaldeepbelief
networksforscalableunsupervisedlearningofhierarchicalrepresentations.InL.Bottou
andM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceon
MachineLearning(ICML’09).ACM,Montreal,Canada.,,363683684
Lee,Y.J.andGrauman,K.(2011).Learningtheeasythingsﬁrst:self-pacedvisual
categorydiscovery.In

============================================================

=== CHUNK 200 ===
Palavras: 361
Caracteres: 13198
--------------------------------------------------
CVPR’2011328
Leibniz,G.W.(1676).Memoirusingthechainrule.(CitedinTMME7:2&3p321-332,
2010).225
Lenat,D.B.andGuha,R.V.(1989).Buildinglargeknowledge-basedsystems;representa-
tionandinferenceintheCycproject.Addison-WesleyLongmanPublishingCo.,Inc 2
Leshno,M.,Lin,V.Y.,Pinkus,A.,andSchocken,S.(1993).Multilayerfeedforward
networkswithanonpolynomialactivationfunctioncanapproximateanyfunction NeuralNetworks,,861––867 , 6 198199
7 5 2
BIBLIOGRAPHY
Levenberg,K.(1944).Amethodforthesolutionofcertainnon-linearproblemsinleast
squares.QuarterlyJournalofAppliedMathematics,(2),164–168 I I 312
L’Hôpital,G.F.A.(1696).Analysedesinﬁnimentpetits,pourl’intelligencedeslignes
courbes.Paris:L’ImprimerieRoyale.225
Li,Y.,Swersky,K.,andZemel,R.S.(2015).Generativemomentmatchingnetworks a b s/ 1 5 0 2 .0 2 7 6 1703
Lin,T.,Horne,B.G.,Tino,P.,andGiles,C.L.(1996).Learninglong-termdependencies
isnotasdiﬃcultwithNARXrecurrentneuralnetworks.IEEETransactionsonNeural
Networks,(6),1329–1338 7 407
Lin,Y.,Liu,Z.,Sun,M.,Liu,Y.,andZhu,X.(2015).Learningentityandrelation
embeddingsforknowledgegraphcompletion.InProc.AAAI’15.484
Linde,N.(1992).Themachinethatchangedtheworld,episode3.Documentaryminiseries 2
Lindsey,C.andLindblad,T.(1994).Reviewofhardwareneuralnetworks:auser’s
perspective.InProc.ThirdWorkshoponNeuralNetworks:FromBiologytoHigh
EnergyPhysics,pages195––202,Isolad’Elba,Italy.451
Linnainmaa, S (1976).Taylorexpansionofthe accumulated roundingerror.BIT
NumericalMathematics,(2),146–160 1 6 225
LISA(2008).Deeplearningtutorials:RestrictedBoltzmannmachines.Technicalreport,
LISALab,UniversitédeMontréal.589
Long,P.M.andServedio,R.A.(2010).RestrictedBoltzmannmachinesarehardto
approximatelyevaluateorsimulate.InProceedingsofthe27thInternationalConference
onMachineLearning(ICML’10).658
Lotter,W.,Kreiman,G.,andCox,D.(2015).Unsupervisedlearningofvisualstructure
usingpredictivegenerativenetworks.arXivpreprintarXiv:1511.06380.,544545
Lovelace,A.(1842).NotesuponL.F.Menabrea’s“SketchoftheAnalyticalEngine
inventedbyCharlesBabbage”.1
Lu,L.,Zhang,X.,Cho,K.,andRenals,S.(2015).Astudyoftherecurrentneuralnetwork
encoder-decoderforlargevocabularyspeechrecognition.InProc.Interspeech.461
Lu,T.,Pál,D.,andPál,M.(2010).Contextualmulti-armedbandits.InInternational
ConferenceonArtiﬁcialIntelligenceandStatistics,pages485–492.480
Luenberger,D.G.(1984).LinearandNonlinearProgramming.AddisonWesley.316
Lukoševičius,M.andJaeger,H.(2009).Reservoircomputingapproachestorecurrent
neuralnetworktraining.ComputerScienceReview,(3),127–149 3 404
7 5 3
BIBLIOGRAPHY
Luo,H.,Shen,R.,Niu,C.,andUllrich,C.(2011).Learningclass-relevantfeaturesand
class-irrelevantfeaturesviaahybridthird-orderRBM.InInternationalConferenceon
ArtiﬁcialIntelligenceandStatistics,pages470–478.686
Luo,H.,Carrier,P.L.,Courville,A.,andBengio,Y.(2013).Texturemodelingwith
convolutionalspike-and-slabRBMsanddeepextensions.InAISTATS’2013.102
Lyu,S.(2009).Interpretationandgeneralizationofscorematching.InProceedingsofthe
Twenty-ﬁfthConferenceinUncertaintyinArtiﬁcialIntelligence(UAI’09).618
Ma,J.,Sheridan,R.P.,Liaw,A.,Dahl,G.E.,andSvetnik,V.(2015).Deepneuralnets
asamethodforquantitativestructure–activityrelationships.J.Chemicalinformation
andmodeling.530
Maas,A.L.,Hannun,A.Y.,andNg,A.Y.(2013).Rectiﬁernonlinearitiesimproveneural
networkacousticmodels.InICMLWorkshoponDeepLearningforAudio,Speech,and
LanguageProcessing.193
Maass,W.(1992).Boundsforthecomputationalpowerandlearningcomplexityofanalog
neuralnets(extendedabstract).InProc.ofthe25thACMSymp.TheoryofComputing,
pages335–344.199
Maass,W.,Schnitger,G.,andSontag,E.D.(1994).Acomparisonofthecomputational
powerofsigmoidandBooleanthresholdcircuits.TheoreticalAdvancesinNeural
ComputationandLearning,pages127–151.199
Maass,W.,Natschlaeger,T.,andMarkram,H.(2002).Real-timecomputingwithout
stablestates:Anewframeworkforneuralcomputationbasedonperturbations.Neural
Computation,(11),2531–2560 1 4 404
MacKay,D.(2003) InformationTheory,InferenceandLearningAlgorithms.Cambridge
UniversityPress.73
Maclaurin,D.,Duvenaud,D.,andAdams,R.P.(2015).Gradient-basedhyperparameter
optimizationthroughreversiblelearning.arXivpreprintarXiv:1502.03492.435
Mao,J.,Xu,W.,Yang,Y.,Wang,J.,Huang,Z.,andYuille,A.L.(2015).Deepcaptioning
withmultimodalrecurrentneuralnetworks.In .arXiv:1410.1090 ICLR’2015 102
Marcotte,P.andSavard,G.(1992).Novelapproachestothediscriminationproblem ZeitschriftfürOperationsResearch(Theory),,517–545 3 6 276
Marlin,B.anddeFreitas,N.(2011).Asymptoticeﬃciencyofdeterministicestimatorsfor
discreteenergy-basedmodels:Ratiomatchingandpseudolikelihood.In ., UAI’2011617
619
7 5 4
BIBLIOGRAPHY
Marlin,B.,Swersky,K.,Chen,B.,anddeFreitas,N.(2010).Inductiveprinciplesfor
restrictedBoltzmannmachinelearning.InProceedingsofTheThirteenthInternational
ConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10),volume9,pages
509–516 ,,613618619
Marquardt,D.W.(1963).Analgorithmforleast-squaresestimationofnon-linearparam-
eters.JournaloftheSocietyofIndustrialandAppliedMathematics, 1 1(2),431–441 312
Marr,D.andPoggio,T.(1976).Cooperativecomputationofstereodisparity.Science,
1 9 4.367
Martens, J.(2010).DeeplearningviaHessian-freeoptimization.InL.Bottouand
M.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceon
MachineLearning(ICML-10),pages735–742.ACM.304
Martens,J.andMedabalimi,V.(2014).Ontheexpressiveeﬃciencyofsumproduct
networks arXiv:1411.7717554
Martens,J.andSutskever,I.(2011).LearningrecurrentneuralnetworkswithHessian-free
optimization.InProc.ICML’2011.ACM.413
Mase,S.(1995).Consistencyofthemaximumpseudo-likelihoodestimatorofcontinuous
statespaceGibbsianprocesses.TheAnnalsofAppliedProbability, 5(3),pp.603–612 616
McClelland, J.,Rumelhart,D.,andHinton,G.(1995).Theappealofparalleldistributed
processing.InComputation&intelligence,pages305–341.AmericanAssociationfor
ArtiﬁcialIntelligence.17
McCulloch,W.S.andPitts,W.(1943).Alogicalcalculusofideasimmanentinnervous
activity.BulletinofMathematicalBiophysics,,115–133 , 5 1415
Mead,C.andIsmail,M.(2012).AnalogVLSIimplementationofneuralsystems,volume80 SpringerScience&BusinessMedia.451
Melchior,J.,Fischer,A.,andWiskott,L.(2013).HowtocenterbinarydeepBoltzmann
machines.arXivpreprintarXiv:1311.1354.674
Memisevic,R.andHinton,G.E.(2007).Unsupervisedlearningofimagetransformations InProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07) 686
Memisevic,R.andHinton,G.E.(2010).Learningtorepresentspatialtransformations
withfactoredhigher-orderBoltzmannmachines.NeuralComputation, 2 2(6),1473–1492 686
7 5 5
BIBLIOGRAPHY
Mesnil,G.,Dauphin,Y.,Glorot,X.,Rifai,S.,Bengio,Y.,Goodfellow,I.,Lavoie,E.,
Muller,X.,Desjardins,G.,Warde-Farley,D.,Vincent,P.,Courville,A.,andBergstra,
J.(2011).Unsupervisedandtransferlearningchallenge:adeeplearningapproach.In
JMLRW&CP:Proc.UnsupervisedandTransferLearning,volume7.,,201532538
Mesnil,G.,Rifai,S.,Dauphin,Y.,Bengio,Y.,andVincent,P.(2012).Surﬁngonthe
manifold.LearningWorkshop,Snowbird.711
Miikkulainen,R.andDyer,M.G.(1991).Naturallanguageprocessingwithmodular
PDPnetworksanddistributedlexicon.CognitiveScience,,343–399 1 5 477
Mikolov,T.(2012).StatisticalLanguageModelsbasedonNeuralNetworks.Ph.D.thesis,
BrnoUniversityofTechnology.414
Mikolov,T.,Deoras,A.,Kombrink,S.,Burget,L.,andCernocky,J.(2011a).Empirical
evaluationandcombinationofadvancedlanguagemodelingtechniques.InProc.12than-
nualconferenceoftheinternationalspeechcommunicationassociation(INTERSPEECH
2011).472
Mikolov,T.,Deoras,A.,Povey,D.,Burget,L.,andCernocky,J.(2011b).Strategiesfor
traininglargescaleneuralnetworklanguagemodels.InProc.ASRU’2011.,328472
Mikolov,T.,Chen,K.,Corrado,G.,andDean,J.(2013a).Eﬃcientestimationofwordrep-
resentationsinvectorspace.InInternationalConferenceonLearningRepresentations:
WorkshopsTrack.536
Mikolov,T.,Le,Q.V.,andSutskever,I.(2013b).Exploitingsimilaritiesamonglanguages
formachinetranslation.Technicalreport,arXiv:1309.4168.539
Minka,T.(2005).Divergencemeasuresandmessagepassing.MicrosoftResearchCambridge
UKTechRepMSRTR2005173,(TR-2005-173) 7 2 625
Minsky,M.L.andPapert,S.A.(1969).Perceptrons.MITPress,Cambridge.15
Mirza,M.andOsindero,S.(2014).Conditionalgenerativeadversarialnets.arXivpreprint
arXiv:1411.1784.702
Mishkin,D.and Matas,J.(2015).Allyouneedisagoodinit.arXivpreprint
arXiv:1511.06422.305
Misra,J.andSaha,I.(2010) Artiﬁcialneuralnetworksinhardware:Asurveyoftwo
decadesofprogress.Neurocomputing,(1),239–255 7 4 451
Mitchell,T.M.(1997).MachineLearning.McGraw-Hill,NewYork.99
Miyato,T.,Maeda,S.,Koyama,M.,Nakae,K.,andIshii,S.(2015).Distributional
smoothingwithvirtualadversarialtraining.In.Preprint:arXiv:1507.00677 ICLR 269
7 5 6
BIBLIOGRAPHY
Mnih,A.andGregor, K.(2014).Neuralvariationalinferenceandlearninginbelief
networks.In .,, ICML’2014691692693
Mnih,A.andHinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguage
modelling.InZ.Ghahramani,editor,ProceedingsoftheTwenty-fourthInternational
ConferenceonMachineLearning(ICML’07),pages641–648.ACM.465
Mnih,A.andHinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel InD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeural
InformationProcessingSystems21(NIPS’08),pages1081–1088 467
Mnih,A.andKavukcuoglu,K.(2013).Learningwordembeddingseﬃcientlywithnoise-
contrastiveestimation.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,and
K.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages
2265–2273 CurranAssociates,Inc.,472622
Mnih, A.andTeh, Y W.(2012).Afastandsimple algorithmfortrainingneural
probabilisticlanguagemodels.In ,pages1751–1758 ICML’2012 472
Mnih,V.andHinton,G.(2010).Learningtodetectroadsinhigh-resolutionaerialimages InProceedingsofthe11thEuropeanConferenceonComputerVision(ECCV).102
Mnih,V.,Larochelle,H., andHinton,G.(2011).ConditionalrestrictedBoltzmann
machinesforstructureoutputprediction.InProc.Conf.onUncertaintyinArtiﬁcial
Intelligence(UAI).685
Mnih,V.,Kavukcuoglo,K.,Silver,D.,Graves,A.,Antonoglou,I.,andWierstra,D.(2013) PlayingAtariwithdeepreinforcementlearning.Technicalreport,arXiv:1312.5602.106
Mnih,V.,Heess,N.,Graves,A.,andKavukcuoglu,K.(2014).Recurrentmodelsofvisual
attention.InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Weinberger,
editors, ,pages2204–2212 NIPS’2014 691
Mnih,V.,Kavukcuoglo,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,
A.,Riedmiller,M.,Fidgeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,
Antonoglou,I.,King,H.,Kumaran,D.,Wierstra,D.,Legg,S.,andHassabis,D.(2015) Human-levelcontrolthroughdeepreinforcementlearning.Nature,,529–533 5 1 8 25
Mobahi,H.andFisher, III,J.W.(2015).Atheoreticalanalysisofoptimizationby
Gaussiancontinuation.In AAAI’2015327
Mobahi,H.,Collobert,R.,andWeston,J.(2009).Deeplearningfromtemporalcoherence
invideo.InL.BottouandM.Littman,editors,Proceedingsofthe26thInternational
ConferenceonMachineLearning,pages737–744,Montreal.Omnipress.494
Mohamed,A.,Dahl,G.,andHinton,G.(2009).Deepbeliefnetworksforphonerecognition 459
7 5 7
BIBLIOGRAPHY
Mohamed,A.,Sainath,T.N.,Dahl,G.,Ramabhadran,B.,Hinton,G.E.,andPicheny,
M.A.(2011).Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition.In
Acoustics,SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConference
on,pages5060–5063 IEEE.459
Mohamed,A.,Dahl,G.,andHinton,G.(2012a) Acousticmodelingusingdeepbelief
networks.IEEETrans.onAudio,SpeechandLanguageProcessing, 2 0(1),14–22.459
Mohamed,A.,Hinton,G.,andPenn,G.(2012b).Understandinghowdeepbeliefnetworks
performacousticmodelling.InAcoustics,SpeechandSignalProcessing(ICASSP),
2012IEEEInternationalConferenceon,pages4273–4276 IEEE.459
Moller,M.F.(1993).Ascaledconjugategradientalgorithmforfastsupervisedlearning NeuralNetworks,,525–533 6 316
Montavon,G.andMuller,K.-R.(2012) DeepBoltzmannmachinesandthecentering
trick.InG.Montavon,G.Orr,andK.-R.Müller,editors,NeuralNetworks:Tricksof
theTrade,volume7700ofLectureNotesinComputerScience,pages621–637.Preprint:
http://arxiv.org/abs/1203.3783.673
Montúfar,G.(2014).Universalapproximationdepthanderrorsofnarrowbeliefnetworks
withdiscreteunits.NeuralComputation, 2 6553
Montúfar,G.andAy,N.(2011).Reﬁnementsofuniversalapproximationresultsfor
deepbeliefnetworksandrestrictedBoltzmannmachines.NeuralComputation, 2 3(5),
1306–1319 553
Montufar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y.(2014).Onthenumberoflinear
regionsofdeepneuralnetworks.In .,, NIPS’201419199200
Mor-Yosef,S.,Samueloﬀ,A.,Modan,B.,Navot,D.,andSchenker,J.G.(1990).Ranking
theriskfactorsforcesarean:logisticregressionanalysisofanationwidestudy.Obstet
Gynecol,(6),944–7 7 5 3
Morin,F.andBengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguage
model.InAISTATS’2005.,467469
Mozer,M.C.(1992).Theinductionofmultiscaletemporalstructure.InJ.M.S.Hanson
andR.Lippmann, editors, AdvancesinNeural InformationProcessingSystems4
(NIPS’91),pages275–282,SanMateo,CA.MorganKaufmann.,407408
Murphy,K P.(2012).MachineLearning:a Probabilistic Perspective.MIT Press,
Cambridge,MA,USA.,,6298146
Murray,B.U.I.andLarochelle,H.(2014).Adeepandtractabledensityestimator.In
ICML’2014.,190710
Nair,V.andHinton,G.(2010).RectiﬁedlinearunitsimproverestrictedBoltzmann
machines.In .,, ICML’201016174197
7 5 8
BIBLIOGRAPHY
Nair,V.andHinton,G.E.(2009).3dobjectrecognitionwithdeepbeliefnets.InY.Bengio,
D.Schuurmans,J.D.Laﬀerty,C.K.I.Williams,andA.Culotta,editors,Advancesin
NeuralInformationProcessingSystems22,pages1339–1347

============================================================

=== CHUNK 201 ===
Palavras: 350
Caracteres: 11615
--------------------------------------------------
686
Narayanan,H.andMitter,S.(2010).Samplecomplexityoftestingthemanifoldhypothesis NIPS’2010164
Naumann,U.(2008).OptimalJacobianaccumulationisNP-complete.Mathematical
Programming,(2),427–441 1 1 2 222
Navigli,R.andVelardi,P.(2005) Structuralsemanticinterconnections:aknowledge-
basedapproachtowordsensedisambiguation.IEEETrans.PatternAnalysisand
MachineIntelligence,(7),1075––1086 2 7 485
Neal,R.andHinton,G.(1999).AviewoftheEMalgorithmthatjustiﬁesincremental,
sparse,andothervariants.InM.I.Jordan,editor,LearninginGraphicalModels.MIT
Press,Cambridge,MA.634
Neal,R.M.(1990).Learningstochasticfeedforwardnetworks.Technicalreport.692
Neal,R.M.(1993).ProbabilisticinferenceusingMarkovchainMonte-Carlomethods TechnicalReportCRG-TR-93-1,Dept.ofComputerScience,UniversityofToronto.680
Neal,R.M.(1994).Samplingfrommultimodaldistributionsusingtemperedtransitions TechnicalReport9421,Dept.ofStatistics,UniversityofToronto.603
Neal,R.M.(1996).Bayesian LearningforNeuralNetworks.LectureNotesinStatistics Springer.265
Neal,R.M.(2001).Annealedimportancesampling , StatisticsandComputing 1 1(2),
125–139 ,,625627628
Neal,R.M.(2005).Estimatingratiosofnormalizingconstantsusinglinkedimportance
sampling.629
Nesterov,Y.(1983).Amethodofsolvingaconvexprogrammingproblemwithconvergence
rate O /k ( 12) SovietMathematicsDoklady 2 7 300
Nesterov,Y.(2004).Introductorylecturesonconvexoptimization:abasiccourse.Applied
optimization.KluwerAcademicPubl.,Boston,Dordrecht,London.300
Netzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y.(2011).Reading
digits in naturalimages withunsupervised feature learning.Deep Learning and
UnsupervisedFeatureLearningWorkshop,NIPS.21
Ney,H.andKneser,R.(1993).Improvedclusteringtechniquesforclass-basedstatistical
languagemodelling.InEuropeanConferenceonSpeechCommunicationandTechnology
(Eurospeech),pages973–976,Berlin.463
7 5 9
BIBLIOGRAPHY
Ng,A.(2015) Adviceforapplyingmachinelearning https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.421
Niesler,T.R.,Whittaker,E.W.D.,andWoodland,P.C.(1998).Comparisonofpart-of-
speechandautomaticallyderivedcategory-basedlanguagemodelsforspeechrecognition InInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),
pages177–180.463
Ning,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,andBarbano,P.E.(2005) Towardautomaticphenotypingofdevelopingembryosfromvideos.ImageProcessing,
IEEETransactionson,(9),1360–1371 1 4 360
Nocedal,J.andWright,S.(2006).NumericalOptimization.Springer.,9296
Norouzi,M.andFleet,D.J.(2011).Minimallosshashingforcompactbinarycodes.In
ICML’2011.525
Nowlan,S.J.(1990).Competingexperts:Anexperimentalinvestigationofassociative
mixturemodels.TechnicalReportCRG-TR-90-5,UniversityofToronto.450
Nowlan,S.J.andHinton,G.E.(1992).Simplifyingneuralnetworksbysoftweight-sharing NeuralComputation,(4),473–493 4 139
Olshausen,B.andField,D.J.(2005).HowclosearewetounderstandingV1?Neural
Computation,,1665–1699 1 7 16
Olshausen,B.A.andField,D.J.(1996).Emergenceofsimple-cellreceptiveﬁeldproperties
bylearningasparsecodefornaturalimages Nature, 3 8 1,607–609 ,,, 147255370496
Olshausen,B.A.,Anderson,C.H.,andVanEssen,D.C.(1993).Aneurobiological
modelofvisualattentionandinvariantpatternrecognitionbasedondynamicrouting
ofinformation.J.Neurosci.,(11),4700–4719 1 3 450
Opper,M.andArchambeau,C.(2009).ThevariationalGaussianapproximationrevisited Neuralcomputation,(3),786–792 2 1 689
Oquab,M.,Bottou,L.,Laptev,I.,andSivic,J.(2014).Learningandtransferringmid-level
imagerepresentationsusingconvolutionalneuralnetworks.InComputerVisionand
PatternRecognition(CVPR),2014IEEEConferenceon,pages1717–1724 IEEE.536
Osindero,S.andHinton,G.E.(2008).Modelingimagepatcheswithadirectedhierarchy
ofMarkovrandomﬁelds.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,
AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1121–1128,
Cambridge,MA.MITPress.632
OvidandMartin,C.(2004) Metamorphoses 1
7 6 0
BIBLIOGRAPHY
Paccanaro,A.andHinton,G.E.(2000).Extractingdistributedrepresentationsofconcepts
andrelationsfrompositiveandnegativepropositions.InInternationalJointConference
onNeuralNetworks(IJCNN),Como,Italy.IEEE,NewYork.484
Paine,T.L.,Khorrami,P.,Han,W.,andHuang,T.S.(2014).Ananalysisofunsupervised
pre-traininginlightofrecentadvances.arXivpreprintarXiv:1412.6597.532
Palatucci,M.,Pomerleau,D.,Hinton,G.E.,andMitchell,T.M.(2009).Zero-shot
learningwithsemanticoutputcodes.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,
C.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessing
Systems22,pages1410–1418 CurranAssociates,Inc.539
Parker,D.B.(1985).Learning-logic.TechnicalReportTR-47,CenterforComp.Research
inEconomicsandManagementSci.,MIT.225
Pascanu,R.,Mikolov,T.,andBengio,Y.(2013).Onthediﬃcultyoftrainingrecurrent
neuralnetworks.In .,,,,, ICML’2013289402403408414416
Pascanu,R.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2014a).Howtoconstructdeep
recurrentneuralnetworks.In .,,,,, ICLR’201419265398399410460
Pascanu,R.,Montufar,G.,andBengio,Y.(2014b).Onthenumberofinferenceregions
ofdeepfeedforwardnetworkswithpiece-wiselinearactivations.In ICLR’2014550
Pati,Y.,Rezaiifar,R.,andKrishnaprasad,P.(1993).Orthogonalmatchingpursuit:
Recursivefunctionapproximationwithapplicationstowaveletdecomposition.InPro-
ceedingsofthe27thAnnualAsilomarConferenceonSignals,Systems,andComputers,
pages40–44.255
Pearl,J.(1985).Bayesiannetworks:Amodelofself-activatedmemoryforevidential
reasoning.In Proceedingsofthe7thConferenceofthe CognitiveScience Society,
UniversityofCalifornia,Irvine,pages329–334.563
Pearl,J.(1988) ProbabilisticReasoninginIntelligentSystems: NetworksofPlausible
Inference.MorganKaufmann.54
Perron,O.(1907).Zurtheoriedermatrices.MathematischeAnnalen, 6 4(2),248–263.597
Petersen,K.B.andPedersen,M.S.(2006).Thematrixcookbook.Version20051003.31
Peterson,G.B.(2004).Adayofgreatillumination:B.F.Skinner’sdiscoveryofshaping JournaloftheExperimentalAnalysisofBehavior,(3),317–328 8 2 328
Pham,D.-T.,Garat,P.,andJutten,C.(1992).Separationofamixtureofindependent
sourcesthroughamaximumlikelihoodapproach.In ,pages771–774 EUSIPCO 491
7 6 1
BIBLIOGRAPHY
Pham,P.-H.,Jelaca,D.,Farabet,C.,Martini,B.,LeCun,Y.,andCulurciello,E.(2012) NeuFlow:dataﬂowvisionprocessingsystem-on-a-chip.InCircuitsandSystems(MWS-
CAS),2012IEEE55thInternationalMidwestSymposiumon,pages1044–1047 451
Pinheiro,P.H.O.andCollobert,R.(2014).Recurrentconvolutionalneuralnetworksfor
scenelabeling.In ICML’2014359
Pinheiro,P.H.O.andCollobert,R.(2015).Fromimage-leveltopixel-levellabelingwith
convolutionalnetworks.InConferenceonComputerVisionandPatternRecognition
(CVPR).359
Pinto,N.,Cox,D.D.,andDiCarlo,J.J.(2008).Whyisreal-worldvisualobjectrecognition
hard?PLoSComputBiol, 4456
Pinto,N.,Stone,Z.,Zickler,T.,andCox,D.(2011).Scalingupbiologically-inspired
computervision:Acasestudyinunconstrainedfacerecognition onfacebook.In
ComputerVisionandPatternRecognitionWorkshops(CVPRW),2011IEEEComputer
SocietyConferenceon,pages35–42.IEEE.363
Pollack,J.B.(1990).Recursivedistributedrepresentations.ArtiﬁcialIntelligence, 4 6(1),
77–105.401
Polyak,B.andJuditsky,A.(1992).Accelerationofstochasticapproximationbyaveraging SIAMJ.ControlandOptimization,,838–855 3 0 ( 4 ) 322
Polyak,B.T.(1964).Somemethodsofspeedinguptheconvergenceofiterationmethods USSRComputationalMathematicsandMathematicalPhysics,(5),1–17 4 296
Poole,B.,Sohl-Dickstein,J.,andGanguli,S.(2014) Analyzingnoiseinautoencoders
anddeepnetworks., CoRR a b s/ 1 4 0 6 .1 8 3 1241
Poon,H.andDomingos,P.(2011).Sum-productnetworks:Anewdeeparchitecture.In
ProceedingsoftheTwenty-seventhConferenceinUncertaintyinArtiﬁcialIntelligence
(UAI),Barcelona,Spain.554
Presley,R.K.andHaggard,R.L.(1994).Aﬁxedpointimplementationofthebackpropa-
gationlearningalgorithm.InSoutheastcon’94.CreativeTechnologyTransfer-AGlobal
Aﬀair.,Proceedingsofthe1994IEEE,pages136–138.IEEE.451
Price,R.(1958).AusefultheoremfornonlineardeviceshavingGaussianinputs.IEEE
TransactionsonInformationTheory,(2),69–72 4 689
Quiroga,R.Q.,Reddy,L.,Kreiman,G.,Koch,C.,andFried,I.(2005).Invariantvisual
representationbysingleneuronsinthehumanbrain.Nature, 4 3 5(7045),1102–1107 366
7 6 2
BIBLIOGRAPHY
Radford,A.,Metz,L.,andChintala,S.(2015).Unsupervisedrepresentationlearningwith
deepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434 552701702,,
Raiko,T.,Yao,L.,Cho,K.,andBengio, Y.(2014).Iterativeneuralautoregressive
distributionestimator(NADE-k).Technicalreport,arXiv:1406.1485.,676709
Raina,R.,Madhavan,A.,andNg,A.Y.(2009).Large-scaledeepunsupervisedlearning
usinggraphicsprocessors InL.BottouandM.Littman,editors,Proceedingsofthe
Twenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages873–880,
NewYork,NY,USA.ACM.,27446
Ramsey,F.P.(1926).Truthandprobability.InR.B.Braithwaite,editor,TheFoundations
ofMathematicsandotherLogicalEssays,chapter7,pages156–198.McMasterUniversity
ArchivefortheHistoryofEconomicThought.56
Ranzato,M.andHinton,G.H.(2010).Modelingpixelmeansandcovariancesusing
factorizedthird-orderBoltzmannmachines.In ,pages2551–2558 CVPR’2010 680
Ranzato,M.,Poultney,C.,Chopra,S.,andLeCun,Y.(2007a).Eﬃcientlearningofsparse
representationswithanenergy-basedmodel.In .,,,, NIPS’20061419507528530
Ranzato,M.,Huang,F.,Boureau,Y.,andLeCun,Y.(2007b).Unsupervisedlearningof
invariantfeaturehierarchieswithapplicationstoobjectrecognition.InProceedingsof
theComputerVisionandPatternRecognitionConference(CVPR’07).IEEEPress.364
Ranzato,M.,Boureau,Y.,andLeCun,Y.(2008).Sparsefeaturelearningfordeepbelief
networks.In NIPS’2007507
Ranzato,M.,Krizhevsky,A.,andHinton,G.E.(2010a).Factored3-wayrestricted
Boltzmannmachinesformodelingnaturalimages.InProceedingsofAISTATS2010 678679,
Ranzato,M.,Mnih,V.,andHinton,G.(2010b).Generatingmorerealisticimagesusing
gatedMRFs.In NIPS’2010680
Rao,C.(1945).Informationandtheaccuracyattainableintheestimationofstatistical
parameters.BulletinoftheCalcuttaMathematicalSociety,,81–89., 3 7135295
Rasmus,A.,Valpola,H.,Honkala,M.,Berglund,M.,andRaiko,T.(2015).Semi-supervised
learningwithladdernetwork.arXivpreprintarXiv:1507.02672.,426530
Recht,B.,Re,C.,Wright,S.,andNiu,F.(2011).Hogwild:Alock-freeapproachto
parallelizingstochasticgradientdescent.In NIPS’2011447
Reichert,D.P.,Seriès,P.,andStorkey,A.J.(2011).Neuronaladaptationforsampling-
basedprobabilisticinferenceinperceptualbistability.InAdvancesinNeuralInformation
ProcessingSystems,pages2357–2365 666
7 6 3
BIBLIOGRAPHY
Rezende,D.J.,Mohamed,S.,andWierstra,D.(2014).Stochasticbackpropagation
and approximateinferencein deepgenerative models.In  .Preprint: ICML’2014
arXiv:1401.4082.,,652689696
Rifai, S., Vincent, P., Muller,X., Glorot, X.,andBengio,Y.(2011a).Contractive
auto-encoders:Explicitinvarianceduringfeatureextraction.In .,, ICML’2011521522
523
Rifai,S.,Mesnil,G.,Vincent,P.,Muller,X.,Bengio,Y.,Dauphin,Y.,andGlorot,X (2011b).Higherordercontractiveauto-encoder.In ., ECMLPKDD521522
Rifai,S.,Dauphin,Y.,Vincent,P.,Bengio,Y.,andMuller,X.(2011c) Themanifold
tangentclassiﬁer.In .,, NIPS’2011271272523
Rifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P.(2012).Agenerativeprocessfor
samplingcontractiveauto-encoders.In ICML’2012711
Ringach,D.andShapley,R.(2004).Reversecorrelationinneurophysiology.Cognitive
Science,(2),147–166 2 8 368
Roberts,S.andEverson,R.(2001).Independentcomponentanalysis:principlesand
practice.CambridgeUniversityPress.493
Robinson,A.J.andFallside,F.(1991).Arecurrenterrorpropagationnetworkspeech
recognitionsystem.ComputerSpeechandLanguage,(3),259–274 , 5 27459
Rockafellar,R.T.(1997).Convexanalysis.princetonlandmarksinmathematics.93
Romero,A.,Ballas,N.,EbrahimiKahou,S.,Chassang,A.,Gatta,C.,andBengio,Y (2015).Fitnets:Hintsforthindeepnets.In ICLR’2015,arXiv:1412.6550325
Rosen,J.B.(1960).Thegradientprojectionmethodfornonlinearprogramming.parti

============================================================

=== CHUNK 202 ===
Palavras: 353
Caracteres: 11993
--------------------------------------------------
linearconstraints.JournaloftheSocietyforIndustrialandAppliedMathematics, 8(1),
pp.181–217.93
Rosenblatt,F.(1958).Theperceptron:Aprobabilisticmodelforinformationstorageand
organizationinthebrain.PsychologicalReview,,386–408 ,, 6 5 141527
Rosenblatt,F.(1962).PrinciplesofNeurodynamics.Spartan,NewYork.,1527
Roweis,S.andSaul,L.K.(2000).Nonlineardimensionalityreductionbylocallylinear
embedding.Science,(5500)., 2 9 0164518
Roweis,S.,Saul,L.,andHinton,G.(2002).Globalcoordinationoflocallinearmodels.In
T.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformation
ProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.489
Rubin,D.B.(1984).Bayesianlyjustiﬁableandrelevantfrequencycalculationsfor etal theappliedstatistician TheAnnalsofStatistics 1 2 717
7 6 4
BIBLIOGRAPHY
Rumelhart, D., Hinton, G., andWilliams, R (1986a).Learningrepresentationsby
back-propagatingerrors.Nature,,533–536 ,,,,,,, 3 2 3 141823204225373476482
Rumelhart,D.E.,Hinton,G.E.,andWilliams,R.J.(1986b).Learninginternalrepresen-
tationsbyerrorpropagation.InD.E.RumelhartandJ.L.McClelland, editors,Parallel
DistributedProcessing,volume1,chapter8,pages318–362.MITPress,Cambridge.,21
27225,
Rumelhart,D.E.,McClelland, J.L.,andthePDPResearchGroup(1986c).Parallel
DistributedProcessing:ExplorationsintheMicrostructureofCognition.MITPress,
Cambridge.17
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,
A.,Khosla,A.,Bernstein,M.,Berg,A.C.,andFei-Fei,L.(2014a).ImageNetLarge
ScaleVisualRecognitionChallenge.21
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,
A.,Khosla,A.,Bernstein,M.,(2014b).Imagenetlargescalevisualrecognition etal challenge.arXivpreprintarXiv:1409.0575.28
Russel,S.J.andNorvig,P.(2003).ArtiﬁcialIntelligence:aModernApproach.Prentice
Hall.86
Rust,N.,Schwartz,O.,Movshon,J.A.,andSimoncelli,E.(2005).Spatiotemporal
elementsofmacaqueV1receptiveﬁelds.Neuron,(6),945–956 4 6 367
Sainath,T.,Mohamed,A.,Kingsbury,B.,andRamabhadran,B.(2013).Deepconvolu-
tionalneuralnetworksforLVCSR.In ICASSP2013460
Salakhutdinov,R.(2010).LearninginMarkovrandomﬁeldsusingtemperedtransitions.In
Y.Bengio,D.Schuurmans,C.Williams,J.Laﬀerty,andA.Culotta,editors,Advances
inNeuralInformationProcessingSystems22(NIPS’09).603
Salakhutdinov,R.andHinton,G.(2009a).DeepBoltzmannmachines.InProceedingsof
theInternationalConferenceonArtiﬁcialIntelligenceandStatistics,volume5,pages
448–455 ,,,,,, 2427529663666671672
Salakhutdinov,R.andHinton,G.(2009b).Semantichashing.InInternationalJournalof
ApproximateReasoning.525
Salakhutdinov, R E.(2007a).Learning a nonlinearembedding by
preservingclassneighbourhoodstructure.InProceedingsoftheEleventhInternational
ConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’07),SanJuan,Porto
Rico.Omnipress.527
Salakhutdinov,R.andHinton,G.E.(2007b).Semantichashing.In SIGIR’2007525
7 6 5
BIBLIOGRAPHY
Salakhutdinov,R.andHinton,G.E.(2008).Usingdeepbeliefnetstolearncovariance
kernelsforGaussianprocesses.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,
AdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1249–1256,
Cambridge,MA.MITPress.244
Salakhutdinov,R.andLarochelle,H.(2010).EﬃcientlearningofdeepBoltzmannmachines InProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceand
Statistics(AISTATS2010),JMLRW&CP,volume9,pages693–700.652
Salakhutdinov,R.andMnih,A.(2008).Probabilisticmatrixfactorization.In NIPS’2008
480
Salakhutdinov,R.andMurray,I.(2008).Onthequantitativeanalysisofdeepbelief
networks.InW.W.Cohen,A.McCallum, andS.T.Roweis,editors,Proceedingsof
theTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),volume25,
pages872–879.ACM.,628662
Salakhutdinov,R.,Mnih,A.,andHinton,G.(2007).RestrictedBoltzmannmachinesfor
collaborativeﬁltering.In.ICML480
Sanger, T.D (1994).Neuralnetworklearningcontrolofrobotmanipulatorsusing
graduallyincreasingtaskdiﬃculty.IEEETransactionsonRoboticsandAutomation,
1 0(3).328
Saul,L.K.andJordan,M.I.(1996).Exploitingtractablesubstructuresinintractable
networks.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeural
InformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.638
Saul,L.K.,Jaakkola,T.,andJordan,M.I.(1996).Meanﬁeldtheoryforsigmoidbelief
networks.JournalofArtiﬁcialIntelligenceResearch,,61–76., 427693
Savich,A.W.,Moussa,M.,andAreibi,S.(2007).Theimpactofarithmeticrepresentation
onimplementingmlp-bponfpgas:Astudy.NeuralNetworks,IEEETransactionson,
1 8(1),240–252.451
Saxe,A.M.,Koh,P.W.,Chen,Z.,Bhand,M.,Suresh,B.,andNg,A.(2011).Onrandom
weightsandunsupervisedfeaturelearning.InProc.ICML’2011.ACM.363
Saxe,A.M.,McClelland, J.L.,andGanguli,S.(2013).Exactsolutionstothenonlinear
dynamicsoflearningindeeplinearneuralnetworks.In.,, ICLR285286303
Schaul,T.,Antonoglou,I.,andSilver,D.(2014).Unittestsforstochasticoptimization InInternationalConferenceonLearningRepresentations.309
Schmidhuber,J.(1992).Learningcomplex,extendedsequencesusingtheprincipleof
historycompression.NeuralComputation,(2),234–242 4 398
Schmidhuber,J.(1996).Sequentialneuraltextcompression.IEEETransactionsonNeural
Networks,(1),142–146 7 477
7 6 6
BIBLIOGRAPHY
Schmidhuber,J.(2012).Self-delimitingneuralnetworks.arXivpreprintarXiv:1210.0118 390
Schölkopf,B.andSmola,A.J.(2002).Learningwithkernels:Supportvectormachines,
regularization,optimization,andbeyond.MITpress.704
Schölkopf,B.,Smola,A.,andMüller,K.-R.(1998).Nonlinearcomponentanalysisasa
kerneleigenvalueproblem.NeuralComputation,,1299–1319 , 1 0 164518
Schölkopf,B.,Burges,C.J.C.,andSmola,A.J.(1999).AdvancesinKernelMethods—
SupportVectorLearning.MITPress,Cambridge,MA.,18142
Schölkopf,B.,Janzing,D.,Peters,J.,Sgouritsa,E.,Zhang,K.,andMooij,J.(2012).On
causalandanticausallearning.In ,pages1255–1262 ICML’2012 545
Schuster,M.(1999).Onsupervisedlearningfromsequentialdatawithapplicationsfor
speechrecognition.190
Schuster,M.andPaliwal,K.(1997).Bidirectionalrecurrentneuralnetworks.IEEE
TransactionsonSignalProcessing,(11),2673–2681 4 5 395
Schwenk,H.(2007).Continuousspacelanguagemodels.Computerspeechandlanguage,
2 1,492–518.466
Schwenk,H.(2010).Continuousspacelanguagemodelsforstatisticalmachinetranslation ThePragueBulletinofMathematicalLinguistics,,137–146 9 3 473
Schwenk,H.(2014).CleanedsubsetofWMT’14dataset.21
Schwenk,H.andBengio,Y.(1998).Trainingmethodsforadaptiveboostingofneuralnet-
works.InM.Jordan,M.Kearns,andS.Solla,editors,AdvancesinNeuralInformation
ProcessingSystems10(NIPS’97),pages647–653.MITPress.258
Schwenk, H.andGauvain, J.-L.(2002).Connectionistlanguagemodeling forlarge
vocabularycontinuousspeechrecognition.InInternationalConferenceonAcoustics,
SpeechandSignalProcessing(ICASSP),pages765–768,Orlando,Florida.466
Schwenk,H.,Costa-jussà,M.R.,andFonollosa,J.A.R.(2006).Continuousspace
languagemodelsfortheIWSLT2006task.InInternationalWorkshoponSpoken
LanguageTranslation,pages166–173.473
Seide,F.,Li,G.,andYu,D.(2011).Conversationalspeechtranscriptionusingcontext-
dependentdeepneuralnetworks.InInterspeech2011,pages437–440.23
Sejnowski,T.(1987).Higher-orderBoltzmannmachines.InAIPConferenceProceedings
151onNeuralNetworksforComputing,pages398–403.AmericanInstituteofPhysics
Inc.686
7 6 7
BIBLIOGRAPHY
Series,P.,Reichert,D.P.,andStorkey,A.J.(2010).HallucinationsinCharlesBonnet
syndromeinducedbyhomeostasis:adeepBoltzmannmachinemodel.InAdvancesin
NeuralInformationProcessingSystems,pages2020–2028 666
Sermanet,P.,Chintala,S.,andLeCun,Y.(2012).Convolutionalneuralnetworksapplied
tohousenumbersdigitclassiﬁcation., CoRR a b s/ 1 2 0 4 .3 9 6 8457
Sermanet,P.,Kavukcuoglu,K.,Chintala,S.,andLeCun,Y.(2013).Pedestriandetection
withunsupervisedmulti-stagefeaturelearning.InProc.InternationalConferenceon
ComputerVisionandPatternRecognition(CVPR’13).IEEE.,23201
Shilov,G.(1977).LinearAlgebra.DoverBooksonMathematicsSeries.DoverPublications 31
Siegelmann,H.(1995).ComputationbeyondtheTuringlimit.Science, 2 6 8(5210),
545–548.379
Siegelmann,H.andSontag,E.(1991).Turingcomputabilitywithneuralnets.Applied
MathematicsLetters,(6),77–80 4 379
Siegelmann,H.T.andSontag,E.D.(1995).Onthecomputationalpowerofneuralnets JournalofComputerandSystemsSciences,(1),132–150 , 5 0 379403
Sietsma,J.andDow,R.(1991).Creatingartiﬁcialneuralnetworksthatgeneralize.Neural
Networks,(1),67–79 4 241
Simard,D.,Steinkraus,P.Y.,andPlatt,J.C.(2003).Bestpracticesforconvolutional
neuralnetworks.In ICDAR’2003371
Simard,P.andGraf,H.P.(1994).Backpropagationwithoutmultiplication.InAdvances
inNeuralInformationProcessingSystems,pages232–239.451
Simard,P.,Victorri,B.,LeCun,Y.,andDenker,J.(1992).Tangentprop-Aformalism
forspecifyingselectedinvariancesinanadaptivenetwork.In .,,, NIPS’1991270271272
356
Simard,P.Y.,LeCun,Y.,andDenker,J.(1993).Eﬃcientpatternrecognitionusinga
newtransformationdistance.In.NIPS’92270
Simard,P.Y.,LeCun,Y.A.,Denker,J.S.,andVictorri,B.(1998).Transformation
invarianceinpatternrecognition—tangentdistanceandtangentpropagation.Lecture
NotesinComputerScience, 1 5 2 4270
Simons,D.J.andLevin,D.T.(1998).Failuretodetectchangestopeopleduringa
real-worldinteraction.PsychonomicBulletin&Review,(4),644–649 5 543
Simonyan,K.andZisserman,A.(2015).Verydeepconvolutionalnetworksforlarge-scale
imagerecognition.In.ICLR323
7 6 8
BIBLIOGRAPHY
Sjöberg,J.andLjung,L.(1995).Overtraining,regularizationandsearchingforaminimum,
withapplicationtoneuralnetworks.InternationalJournalofControl, 6 2(6),1391–1407 250
Skinner,B.F.(1958).Reinforcementtoday.AmericanPsychologist,,94–99 1 3328
Smolensky,P.(1986).Informationprocessingindynamicalsystems:Foundationsof
harmonytheory.InD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributed
Processing,volume1,chapter6,pages194–281.MITPress,Cambridge.,,571587656
Snoek,J.,Larochelle,H.,andAdams,R.P.(2012).PracticalBayesianoptimizationof
machinelearningalgorithms.In NIPS’2012436
Socher,R.,Huang,E.H.,Pennington,J.,Ng,A.Y.,andManning,C.D.(2011a).Dynamic
poolingandunfoldingrecursiveautoencodersforparaphrasedetection.In NIPS’2011
401
Socher,R.,Manning,C.,andNg,A.Y.(2011b).Parsingnaturalscenesandnaturallan-
guagewithrecursiveneuralnetworks.InProceedingsoftheTwenty-EighthInternational
ConferenceonMachineLearning(ICML’2011).401
Socher, R., Pennington, J., Huang, E.H., Ng, A Y.,andManning, C.D.(2011c) Semi-supervisedrecursiveautoencoders forpredictingsentimentdistributions.In
EMNLP’2011.401
Socher,R.,Perelygin,A.,Wu,J.Y.,Chuang,J.,Manning,C.D.,Ng,A.Y.,andPotts,
C.(2013a).Recursivedeepmodelsforsemanticcompositionalityoverasentiment
treebank.In EMNLP’2013401
Socher,R.,Ganjoo,M.,Manning,C.D.,andNg,A.Y.(2013b).Zero-shotlearningthrough
cross-modaltransfer.In27thAnnualConferenceonNeuralInformationProcessing
Systems(NIPS2013).539
Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.(2015).Deep
unsupervisedlearningusingnonequilibriumthermodynamics.716
Sohn,K.,Zhou,G.,andLee,H.(2013).Learningandselectingfeaturesjointlywith
point-wisegatedBoltzmannmachines.In ICML’2013687
Solomonoﬀ,R.J.(1989).Asystemforincrementallearningbasedonalgorithmicproba-
bility.328
Sontag,E.D.(1998).VCdimensionofneuralnetworks.NATOASISeriesFComputer
andSystemsSciences,,69–96., 1 6 8547551
Sontag,E.D.andSussman,H.J.(1989).Backpropagationcangiverisetospuriouslocal
minimaevenfornetworkswithouthiddenlayers ComplexSystems 3 284
7 6 9
BIBLIOGRAPHY
Sparkes,B.(1996).TheRedandtheBlack:StudiesinGreekPottery.Routledge.1
Spitkovsky,V.I.,Alshawi,H.,andJurafsky,D.(2010).Frombabystepstoleapfrog:how
“lessismore”inunsuperviseddependencyparsing.InHLT’10.328
Squire,W.andTrapp,G.(1998) Usingcomplexvariablestoestimatederivativesofreal
functions.SIAMRev.,(1),110––112 4 0 439
Srebro,N.andShraibman,A.(2005).Rank,trace-normandmax-norm.InProceedingsof
the18thAnnualConferenceonLearningTheory,pages545–560.Springer-Verlag.238
Srivastava,N.(2013).ImprovingNeuralNetworksWithDropout.Master’sthesis,U Toronto.535
Srivastava,N.andSalakhutdinov,R.(2012).MultimodallearningwithdeepBoltzmann
machines.In NIPS’2012541
Srivastava,N.,Salakhutdinov,R.R.,andHinton,G.E.(2013).Modelingdocumentswith
deepBoltzmannmachines.arXivpreprintarXiv:1309.6865.663
Srivastava,N.,Hinton,G.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2014)

============================================================

=== CHUNK 203 ===
Palavras: 366
Caracteres: 11644
--------------------------------------------------
Dropout:Asimplewaytopreventneuralnetworksfromoverﬁtting.JournalofMachine
LearningResearch,,1929–1958 ,,, 1 5 258265267672
Srivastava,R.K.,Greﬀ,K.,andSchmidhuber,J.(2015).Highwaynetworks arXiv:1505.00387.326
Steinkrau,D.,Simard,P.Y.,andBuck,I.(2005).UsingGPUsformachinelearning
algorithms.201312thInternationalConferenceonDocumentAnalysisandRecognition,
0,1115–1119 445
Stoyanov,V.,Ropson,A.,andEisner,J.(2011).Empiricalriskminimizationofgraphical
modelparametersgivenapproximateinference,decoding,andmodelstructure.In
Proceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics
(AISTATS) JMLRWorkshopandConferenceProceedings ,volume15of ,pages725–733,
FortLauderdale.Supplementarymaterial(4pages)alsoavailable.,674698
Sukhbaatar,S.,Szlam,A.,Weston,J.,andFergus,R.(2015).Weaklysupervisedmemory
networks.arXivpreprintarXiv:1503.08895.418
Supancic,J.andRamanan,D.(2013).Self-pacedlearningforlong-termtracking.In
CVPR’2013.328
Sussillo,D.(2014).Randomwalks:Trainingverydeepnonlinearfeed-forwardnetworks
withsmartinitialization., .,,, CoRR a b s/ 1 4 1 2 .6 5 5 8290303305403
Sutskever,I.(2012).TrainingRecurrentNeuralNetworks.Ph.D.thesis,Departmentof
computerscience,UniversityofToronto.,406413
7 7 0
BIBLIOGRAPHY
Sutskever,I.andHinton,G.E.(2008).Deepnarrowsigmoidbeliefnetworksareuniversal
approximators.NeuralComputation,(11),2629–2636 2 0 693
Sutskever,I.andTieleman,T.(2010).OntheConvergencePropertiesofContrastive
Divergence.InY.W.TehandM.Titterington,editors,Proc.oftheInternational
ConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),volume9,pages789–795 612
Sutskever,I.,Hinton,G.,andTaylor,G.(2009).Therecurrenttemporalrestricted
Boltzmannmachine.In NIPS’2008685
Sutskever,I.,Martens,J.,andHinton,G.E.(2011).Generatingtextwithrecurrent
neuralnetworks.In ,pages1017–1024 ICML’2011 477
Sutskever, I.,Martens,J.,Dahl, G.,andHinton,G.(2013).Ontheimportanceof
initializationandmomentumindeeplearning.In.,, ICML300406413
Sutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwith
neuralnetworks.In .,,,,,, NIPS’2014,arXiv:1409.321525101397410411474475
Sutton,R.andBarto,A.(1998).ReinforcementLearning:AnIntroduction.MITPress 106
Sutton,R.S.,Mcallester,D.,Singh,S.,andMansour,Y.(2000).Policygradientmethods
forreinforcementlearningwithfunctionapproximation.In ,pages1057– NIPS’1999
–1063.MITPress.691
Swersky,K.,Ranzato,M.,Buchman,D.,Marlin,B.,anddeFreitas,N.(2011).On
autoencodersandscorematchingforenergybasedmodels.In .ACM ICML’2011513
Swersky,K.,Snoek,J.,andAdams,R.P.(2014).Freeze-thawBayesianoptimization arXivpreprintarXiv:1406.3896.436
Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,
V.,andRabinovich,A.(2014a).Goingdeeperwithconvolutions.Technicalreport,
arXiv:1409.4842.,,,,,, 2427201258269326347
Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,and
Fergus,R.(2014b).Intriguingpropertiesofneuralnetworks.,ICLR a b s/ 1 3 1 2 .6 1 9 9 268271,
Szegedy,C.,Vanhoucke,V.,Ioﬀe,S.,Shlens,J.,andWojna,Z.(2015).Rethinkingthe
InceptionArchitectureforComputerVision ., ArXive-prints243322
Taigman,Y.,Yang,M.,Ranzato,M.,andWolf,L.(2014).DeepFace:Closingthegapto
human-levelperformanceinfaceveriﬁcation.In CVPR’2014100
Tandy,D.W.(1997).WorksandDays:ATranslationandCommentaryfortheSocial
Sciences.UniversityofCaliforniaPress.1
7 7 1
BIBLIOGRAPHY
Tang,Y.andEliasmith,C.(2010).Deepnetworksforrobustvisualrecognition.In
Proceedingsofthe27thInternationalConferenceonMachineLearning,June21-24,
2010,Haifa,Israel.241
Tang,Y.,Salakhutdinov,R.,andHinton,G.(2012).Deepmixturesoffactoranalysers arXivpreprintarXiv:1206.4635.489
Taylor,G.andHinton,G.(2009).FactoredconditionalrestrictedBoltzmannmachines
formodelingmotionstyle.InL.BottouandM.Littman, editors,Proceedingsof
theTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages
1025–1032, Montreal,Quebec,Canada.ACM.685
Taylor,G.,Hinton,G.E.,andRoweis,S.(2007).Modelinghumanmotionusingbinary
latentvariables.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeural
InformationProcessingSystems19(NIPS’06),pages1345–1352 MITPress,Cambridge,
MA.685
Teh,Y.,Welling,M.,Osindero,S.,andHinton,G.E.(2003).Energy-basedmodels
forsparseovercompleterepresentations.JournalofMachineLearningResearch, 4,
1235–1260 491
Tenenbaum,J.,deSilva,V.,andLangford,J.C.(2000).Aglobalgeometricframework
fornonlineardimensionalityreduction.Science,(5500),2319–2323 ,, 2 9 0 164518533
Theis,L.,vandenOord,A.,andBethge,M.(2015).Anoteontheevaluationofgenerative
models.arXiv:1511.01844.,698719
Thompson,J.,Jain,A.,LeCun,Y.,andBregler,C.(2014).Jointtrainingofaconvolutional
networkandagraphicalmodelforhumanposeestimation.In NIPS’2014360
Thrun,S.(1995).Learningtoplaythegameofchess.In NIPS’1994271
Tibshirani,R.J.(1995).Regressionshrinkageandselectionviathelasso.Journalofthe
RoyalStatisticalSocietyB,,267–288 5 8 236
Tieleman,T.(2008).TrainingrestrictedBoltzmannmachinesusingapproximationsto
thelikelihoodgradient.InW.W.Cohen,A.McCallum, andS.T.Roweis,editors,Pro-
ceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),
pages1064–1071 ACM.612
Tieleman,T.andHinton,G.(2009).Usingfastweightstoimprovepersistentcontrastive
divergence.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixth
InternationalConferenceonMachineLearning(ICML’09),pages1033–1040 614
Tipping,M.E.andBishop,C.M.(1999).Probabilisticprincipalcomponentsanalysis JournaloftheRoyalStatisticalSocietyB,(3),611–622 6 1 491
7 7 2
BIBLIOGRAPHY
Torralba,A.,Fergus,R.,andWeiss,Y.(2008).Smallcodesandlargedatabasesfor
recognition.InProceedingsoftheComputerVisionandPatternRecognitionConference
(CVPR’08),pages1–8.525
Touretzky,D.S.andMinton,G.E.(1985).Symbolsamongtheneurons:Detailsof
aconnectionistinferencearchitecture InProceedingsofthe9thInternationalJoint
ConferenceonArtiﬁcialIntelligence-Volume1,IJCAI’85,pages238–243,SanFrancisco,
CA,USA.MorganKaufmannPublishersInc.17
Tu,K.andHonavar,V.(2011) Ontheutilityofcurriculainunsupervisedlearningof
probabilisticgrammars.In IJCAI’2011328
Turaga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,M.,Briggman,K.,Denk,
W.,andSeung,H.S.(2010).Convolutionalnetworkscanlearntogenerateaﬃnity
graphsforimagesegmentation.NeuralComputation,(2),511–538 2 2 360
Turian,J.,Ratinov,L.,andBengio,Y.(2010).Wordrepresentations:Asimpleand
generalmethodforsemi-supervisedlearning.InProc.ACL’2010,pages384–394.535
Töscher,A.,Jahrer,M.,andBell,R.M.(2009) TheBigChaossolutiontotheNetﬂix
grandprize.480
Uria,B.,Murray,I.,andLarochelle,H.(2013).Rnade:Thereal-valuedneuralautoregres-
sivedensity-estimator.In ., NIPS’2013709710
vandenOörd,A.,Dieleman,S.,andSchrauwen,B.(2013).Deepcontent-basedmusic
recommendation.In NIPS’2013480
vanderMaaten,L.andHinton,G.E.(2008).Visualizingdatausingt-SNE.J.Machine
LearningRes.,., 9477519
Vanhoucke,V.,Senior,A.,andMao,M.Z.(2011).Improvingthespeedofneuralnetworks
onCPUs.InProc.DeepLearningandUnsupervisedFeatureLearningNIPSWorkshop 444452,
Vapnik,V.N.(1982).EstimationofDependencesBasedonEmpiricalData.Springer-
Verlag,Berlin.114
Vapnik,V.N.(1995).TheNatureofStatisticalLearningTheory.Springer,NewYork 114
Vapnik,V.N.andChervonenkis,A.Y.(1971).Ontheuniformconvergenceofrelative
frequenciesofeventstotheirprobabilities.TheoryofProbabilityandItsApplications,
1 6,264–280.114
Vincent,P.(2011) Aconnectionbetweenscorematchinganddenoisingautoencoders NeuralComputation,(7).,, 2 3513515712
7 7 3
BIBLIOGRAPHY
Vincent,P.andBengio,Y.(2003).ManifoldParzenwindows.In .MITPress NIPS’2002
520
Vincent,P.,Larochelle,H.,Bengio,Y.,andManzagol,P.-A.(2008).Extractingand
composingrobustfeatureswithdenoisingautoencoders.In ., ICML2008241515
Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A.(2010).Stacked
denoisingautoencoders:Learningusefulrepresentationsinadeepnetworkwithalocal
denoisingcriterion.J.MachineLearningRes., 1 1515
Vincent,P.,deBrébisson,A.,andBouthillier,X.(2015).Eﬃcientexactgradientupdate
fortrainingdeepnetworkswithverylargesparsetargets.InC.Cortes,N.D.Lawrence,
D.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformation
ProcessingSystems28,pages1108–1116 CurranAssociates,Inc.466
Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., andHinton, G.(2014a) Grammarasaforeignlanguage.Technicalreport,arXiv:1412.7449.410
Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2014b).Showandtell:aneuralimage
captiongenerator.arXiv1411.4555 410
Vinyals,O.,Fortunato,M.,andJaitly,N.(2015a).Pointernetworks.arXivpreprint
arXiv:1506.03134.418
Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015b).Showandtell:aneuralimage
captiongenerator.In .arXiv:1411.4555 CVPR’2015 102
Viola,P.andJones,M.(2001).Robustreal-timeobjectdetection.InInternational
JournalofComputerVision.449
Visin,F.,Kastner,K.,Cho,K.,Matteucci,M.,Courville,A.,andBengio,Y.(2015) ReNet:Arecurrentneuralnetworkbasedalternativetoconvolutionalnetworks.arXiv
preprintarXiv:1505.00393.395
VonMelchner,L.,Pallas,S.L.,andSur,M.(2000).Visualbehaviourmediatedbyretinal
projectionsdirectedtotheauditorypathway.Nature,(6780),871–876 4 0 4 16
Wager,S.,Wang,S.,andLiang,P.(2013).Dropouttrainingasadaptiveregularization InAdvancesinNeuralInformationProcessingSystems26,pages351–359.265
Waibel,A.,Hanazawa,T.,Hinton,G.E.,Shikano,K.,andLang,K.(1989).Phoneme
recognitionusingtime-delayneuralnetworks.IEEETransactionsonAcoustics,Speech,
andSignalProcessing,,328–339 ,, 3 7 374453459
Wan,L.,Zeiler,M.,Zhang,S.,LeCun,Y.,andFergus,R.(2013).Regularizationofneural
networksusingdropconnect.In ICML’2013266
Wang,S.andManning,C.(2013).Fastdropouttraining.In ICML’2013266
7 7 4
BIBLIOGRAPHY
Wang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014a).Knowledgegraphandtextjointly
embedding.InProc.EMNLP’2014.484
Wang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014b) Knowledgegraphembeddingby
translatingonhyperplanes.InProc.AAAI’2014.484
Warde-Farley,D.,Goodfellow,I.J.,Courville,A.,andBengio,Y.(2014).Anempirical
analysisofdropoutinpiecewiselinearnetworks.In .,, ICLR’2014262266267
Wawrzynek,J.,Asanovic,K.,Kingsbury,B.,Johnson,D.,Beck,J.,andMorgan,N (1996).Spert-II:Avectormicroprocessorsystem Computer 2 9 451
Weaver,L.andTao,N.(2001).Theoptimalrewardbaselineforgradient-basedreinforce-
mentlearning.InProc.UAI’2001,pages538–545.691
Weinberger,K.Q.andSaul,L.K.(2004).Unsupervisedlearningofimagemanifoldsby
semideﬁniteprogramming.In ,pages988–995 , CVPR’2004 164519
Weiss, Y., Torralba, A., andFergus, R.(2008).Spectral hashing.In,pagesNIPS
1753–1760 525
Welling,M.,Zemel,R.S.,andHinton,G.E.(2002).Selfsupervisedboosting.InAdvances
inNeuralInformationProcessingSystems,pages665–672.703
Welling, M.,Hinton,G.E., andOsindero, S.(2003a).Learningsparsetopographic
representationswithproductsofStudent-tdistributions.In NIPS’2002680
Welling,M.,Zemel,R.,andHinton,G.E.(2003b).Self-supervisedboosting.InS.Becker,
S.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessing
Systems15(NIPS’02),pages665–672.MITPress.622
Welling,M.,Rosen-Zvi,M.,andHinton,G.E.(2005).Exponentialfamilyharmoniums
withanapplicationtoinformationretrieval.InL.Saul,Y.Weiss,andL.Bottou,
editors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),volume17,
Cambridge,MA.MITPress.676
Werbos, P.J.(1981).Applicationsofadvancesinnonlinearsensitivityanalysis.In
Proceedingsofthe10thIFIPConference,31.8-4.9,NYC,pages762–770.225
Weston,J.,Bengio,S.,andUsunier,N.(2010).Largescaleimageannotation:learningto
rankwithjointword-imageembeddings.MachineLearning,(1),21–35 8 1 401
Weston, J., Chopra, S., andBordes, A.(2014).Memorynetworks.arXivpreprint
arXiv:1410.3916.,418485
Widrow,B.andHoﬀ,M.E.(1960).Adaptiveswitchingcircuits.In1960IREWESCON
ConventionRecord,volume4,pages96–104.IRE,NewYork.,,,15212427
7 7 5
BIBLIOGRAPHY
Wikipedia(2015).Listofanimalsbynumberofneurons—Wikipedia,thefreeencyclopedia

============================================================

=== CHUNK 204 ===
Palavras: 105
Caracteres: 3326
--------------------------------------------------
[Online;accessed4-March-2015].,2427
Williams,C.K.I.andAgakov,F.V.(2002) ProductsofGaussiansandProbabilistic
MinorComponentAnalysis.NeuralComputation,,1169–1182 1 4 ( 5 ) 682
Williams,C.K.I.andRasmussen,C.E.(1996) Gaussianprocessesforregression In
D.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformation
ProcessingSystems8(NIPS’95),pages514–520.MITPress,Cambridge,MA.142
Williams,R.J.(1992).Simplestatisticalgradient-followingalgorithmsconnectionist
reinforcementlearning.MachineLearning,,229–256 , 8 688689
Williams,R.J.andZipser,D.(1989).Alearningalgorithmforcontinuallyrunningfully
recurrentneuralnetworks.NeuralComputation,,270–280 1 223
Wilson,D.R.andMartinez,T.R.(2003).Thegeneralineﬃciencyofbatchtrainingfor
gradientdescentlearning.NeuralNetworks,(10),1429–1451 1 6 279
Wilson,J.R.(1984).Variancereductiontechniquesfordigitalsimulation.American
JournalofMathematicalandManagementSciences,(3),277––312 4 690
Wiskott,L.andSejnowski,T.J.(2002).Slowfeatureanalysis:Unsupervisedlearningof
invariances.NeuralComputation,(4),715–770 1 4 494
Wolpert,D.andMacReady,W.(1997).Nofreelunchtheoremsforoptimization.IEEE
TransactionsonEvolutionaryComputation,,67–82 1293
Wolpert,D.H.(1996).Thelackofaprioridistinctionbetweenlearningalgorithms.Neural
Computation,(7),1341–1390 8 116
Wu,R.,Yan,S.,Shan,Y.,Dang,Q.,andSun,G.(2015).Deepimage:Scalingupimage
recognition.arXiv:1501.02876.447
Wu,Z.(1997).Globalcontinuationfordistancegeometryproblems.SIAMJournalof
Optimization,,814–836 7 327
Xiong,H.Y.,Barash,Y.,andFrey,B.J.(2011).Bayesianpredictionoftissue-regulated
splicingusingRNAsequenceandcellularcontext , Bioinformatics 2 7(18),2554–2562 265
Xu,K.,Ba,J.L.,Kiros,R.,Cho,K.,Courville,A.,Salakhutdinov,R.,Zemel,R.S.,and
Bengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisual
attention.In .,, ICML’2015,arXiv:1502.03044102410691
Yildiz,I.B.,Jaeger,H.,andKiebel,S.J.(2012).Re-visitingtheechostateproperty 3 5405
7 7 6
BIBLIOGRAPHY
Yosinski,J.,Clune,J.,Bengio,Y.,andLipson,H.(2014).Howtransferablearefeatures
indeepneuralnetworks?In ., NIPS’2014325536
Younes,L.(1998).OntheconvergenceofMarkovianstochasticalgorithmswithrapidly
decreasingergodicityrates.InStochasticsandStochasticsModels,pages177–228.612
Yu,D.,Wang,S.,and Deng, L (2010).Sequential labeling using deep-structured
conditionalrandomﬁelds.IEEEJournalofSelectedTopicsinSignalProcessing.323
Zaremba,W.andSutskever,I.(2014).Learningtoexecute.arXiv1410.4615 329
Zaremba,W.andSutskever,I.(2015).ReinforcementlearningneuralTuringmachines arXiv:1505.00521.419
Zaslavsky,T.(1975).FacingUptoArrangements:Face-CountFormulasforPartitions
ofSpacebyHyperplanes.Numberno.154inMemoirsoftheAmericanMathematical
Society.AmericanMathematicalSociety.550
Zeiler,M.D.andFergus,R.(2014).Visualizingandunderstandingconvolutionalnetworks ECCV’146
Zeiler,M.D.,Ranzato,M.,Monga,R.,Mao,M.,Yang,K.,Le,Q.,Nguyen,P.,Senior,
A.,Vanhoucke,V.,Dean,J.,andHinton,G.E.(2013) Onrectiﬁedlinearunitsfor
speechprocessing.In ICASSP2013460
Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,andTorralba,A.(2015) Objectdetectors
emergeindeepsceneCNNs.ICLR’2015,arXiv:1412.6856.551
Zhou,J.andTroyanskaya,O.G.(2014).Deepsupervisedandconvolutionalgenerative
stochasticnetworkforproteinsecondarystructureprediction.In ICML’2014715
Zhou,Y.andChellappa,R.(1988).Computationofopticalﬂowusinganeuralnetwork

============================================================

=== CHUNK 205 ===
Palavras: 350
Caracteres: 5034
--------------------------------------------------
InNeuralNetworks,1988.,IEEEInternationalConferenceon,pages71–78.IEEE.339 Zöhrer,M.andPernkopf,F.(2014).Generalstochasticnetworksforclassiﬁcation.In NIPS’2014.716 7 7 7 I n d e x 0-1loss,, 1 0 2274 Absolutevaluerectiﬁcation,191 Accuracy,420 Activationfunction,169 Activeconstraint,94 AdaGrad,305 ADALINE, s e eadaptivelinearelement Adam,,307422 Adaptivelinearelement,,,152326 Adversarialexample,265 Adversarialtraining,,,266268526 Aﬃne,109 AIS, s e eannealedimportancesampling Almosteverywhere,70 Almostsureconvergence,128 Ancestralsampling,,576591 ANN, s e eArtiﬁcialneuralnetwork Annealedimportancesampling, ,,621662 711 ApproximateBayesiancomputation,710 Approximateinference,579 Artiﬁcialintelligence,1 Artiﬁcialneuralnetwork, s e eNeuralnet- work ASR, s e eautomaticspeechrecognition Asymptoticallyunbiased,123 Audio,,,101357455 Autoencoder,,,4353 4 9 8 Automaticspeechrecognition,455 Back-propagation,201 Back-propagationthroughtime, 3 8 1 Backprop, s e eback-propagationBagofwords,467 Bagging,252 Batchnormalization,,264422 Bayeserror, 1 1 6 Bayes’rule,69 Bayesianhyperparameteroptimization,433 Bayesian network, s e edirected graphical model Bayesianprobability,54 Bayesianstatistics, 1 3 4 Beliefnetwork, s e edirectedgraphicalmodel Bernoullidistribution,61 BFGS,314 Bias,,123227 Biasparameter,109 Biasedimportancesampling,589 Bigram,458 Binaryrelation,478 BlockGibbssampling,595 Boltzmanndistribution,566 Boltzmannmachine,,566648 BPTT, s e eback-propagationthroughtime Broadcasting,33 Burn-in,593 CAE, s e econtractiveautoencoder Calculusofvariations,178 Categoricaldistribution, s e emultinoullidis- tribution CD, s e econtrastivedivergence Centeringtrick(DBM),667 Centrallimittheorem,63 Chainrule(calculus),203 Chainruleofprobability,58 778 INDEX Chess,2 Chord,575 Chordalgraph,575 Class-basedlanguagemodels,460 Classicaldynamicalsystem,372 Classiﬁcation,99 Cliquepotential, s e efactor(graphicalmodel) CNN, s e econvolutionalneuralnetwork CollaborativeFiltering,474 Collider, s e eexplainingaway Colorimages,357 Complexcell,362 Computationalgraph,202 Computervision,449 Conceptdrift,533 Conditionnumber,277 Conditionalcomputation, s e edynamicstruc- ture Conditionalindependence,,xiii59 Conditionalprobability,58 ConditionalRBM,679 Connectionism,,17440 Connectionisttemporalclassiﬁcation,457 Consistency,,128509 Constrainedoptimization,,92235 Content-basedaddressing,416 Content-basedrecommendersystems,475 Context-speciﬁcindependence,569 Contextualbandits,476 Continuationmethods,324 Contractiveautoencoder,516 Contrast,451 Contrastivedivergence,,,289606666 Convexoptimization,140 Convolution,,327677 Convolutionalnetwork,16 Convolutionalneuralnetwork,,250 3 2 7,,422 456 Coordinatedescent,,319665 Correlation,60 Costfunction, s e eobjectivefunction Covariance,,xiii60 Covariancematrix,61 Coverage,421Criticaltemperature,599 Cross-correlation,329 Cross-entropy,, 7 4131 Cross-validation,121 CTC, s e econnectionisttemporalclassiﬁca- tion Curriculumlearning,326 Curseofdimensionality,153 Cyc,2 D-separation,568 DAE, s e edenoisingautoencoder Datageneratingdistribution,, 1 1 0130 Datageneratingprocess,110 Dataparallelism,444 Dataset,103 Datasetaugmentation,,268454 DBM, s e edeepBoltzmannmachine DCGAN,,,547548695 Decisiontree,, 1 4 4544 Decoder,4 Deepbeliefnetwork,,,,,, 26525626651654 678686, DeepBlue,2 DeepBoltzmannmachine,,,,, 2326525626 647651657666678 ,,,, Deepfeedforwardnetwork,,166422 Deeplearning,,25 Denoisingautoencoder,,506683 Denoisingscorematching,615 Densityestimation,102 Derivative,,xiii82 Designmatrix, 1 0 5 Detectorlayer,336 Determinant,xii Diagonalmatrix,40 Diﬀerentialentropy,,73641 Diracdeltafunction,64 Directedgraphicalmodel,,,, 76503559685 Directionalderivative,84 Discriminativeﬁne-tuning, s e esupervised ﬁne-tuning DiscriminativeRBM,680 Distributedrepresentation,,,17149542 Domainadaptation,532 7 7 9 INDEX Dotproduct,,33139 Doublebackprop,268 Doublyblockcirculantmatrix,330 Dreamsleep,,605647 DropConnect,263 Dropout,,,,,, 2 5 5422427428666683 Dynamicstructure,445 E-step,629 Earlystopping,,,,, 244246270271422 EBM, s e eenergy-basedmodel Echostatenetwork,,,2326401 Eﬀectivecapacity,113 Eigendecomposition,41 Eigenvalue,41 Eigenvector,41 ELBO, s e eevidencelowerbound Element-wiseproduct, s e eHadamardprod- uct, s e eHadamardproduct EM, s e eexpectationmaximization Embedding,512 Empiricaldistribution,65 Empiricalrisk,274 Empiricalriskminimization,274 Encoder,4 Energyfunction,565 Energy-basedmodel,,,, 565591648657 Ensemblemethods,252 Epoch,244 Equalityconstraint,93 Equivariance,335 Errorfunction, s e eobjectivefunction ESN, s e eechostatenetwork Euclideannorm,38 Euler-Lagrangeequation,641 Evidencelowerbound,,628655 Example,98 Expectation,59 Expectationmaximization,629 Expectedvalue, s e eexpectation Explainingaway,,,570626639 Exploitation,477 Exploration,477 Exponentialdistribution, 6 4F-score,420 Factor(graphicalmodel),563 Factoranalysis,486 Factorgraph,575 Factorsofvariation,4 Feature,98 Featureselection,234 Feedforwardneuralnetwork,166 Fine-tuning,321 Finitediﬀerences,436

============================================================

=== CHUNK 206 ===
Palavras: 350
Caracteres: 4992
--------------------------------------------------
Forgetgate,304 Forwardpropagation,201 Fouriertransform,,357359 Fovea,363 FPCD,610 Freeenergy,, 5 6 7674 Freebase,479 Frequentistprobability,54 Frequentiststatistics, 1 3 4 Frobeniusnorm,45 Fully-visibleBayesnetwork,699 Functionalderivatives,640 FVBN, s e efully-visibleBayesnetwork Gaborfunction,365 GANs, s e egenerativeadversarialnetworks Gatedrecurrentunit,422 Gaussiandistribution, s e enormaldistribu- tion Gaussiankernel,140 Gaussianmixture,,66187 GCN, s e eglobalcontrastnormalization GeneOntology,479 Generalization,109 GeneralizedLagrangefunction, s e egeneral- izedLagrangian GeneralizedLagrangian,93 Generativeadversarialnetworks,,683693 Generativemomentmatchingnetworks,696 Generatornetwork,687 Gibbsdistribution,564 Gibbssampling,,577595 Globalcontrastnormalization,451 GPU, s e egraphicsprocessingunit Gradient,83 7 8 0 INDEX Gradientclipping,,287411 Gradientdescent,,8284 Graph,xii Graphicalmodel, s e estructuredprobabilis- ticmodel Graphicsprocessingunit,441 Greedyalgorithm,321 Greedylayer-wiseunsupervisedpretraining, 524 Greedysupervisedpretraining,321 Gridsearch,429 Hadamardproduct,,xii33 Hard,tanh195 Harmonium, s e erestrictedBoltzmannma- chine Harmonytheory,567 Helmholtz freeenergy, s e eevidencelower bound Hessian,221 Hessianmatrix,,xiii86 Heteroscedastic,186 Hiddenlayer,,6166 Hillclimbing,85 Hyperparameteroptimization,429 Hyperparameters,,119427 Hypothesisspace,,111117 i.i.d.assumptions,,,110121265 Identitymatrix,35 ILSVRC, s e eImageNetLargeScaleVisual RecognitionChallenge ImageNetLargeScaleVisualRecognition Challenge,22 Immorality,573 Importancesampling,,,588620691 Importanceweightedautoencoder,691 Independence,,xiii59 Independentandidenticallydistributed, s e e i.i.d.assumptions Independentcomponentanalysis,487 Independentsubspaceanalysis,489 Inequalityconstraint,93 Inference,,,,,,,, 558579626628630633643 646Informationretrieval,520 Initialization,298 Integral,xiii Invariance,339 Isotropic,64 Jacobianmatrix,,,xiii7185 Jointprobability,56 k-means,,361542 k-nearestneighbors,, 1 4 1544 Karush-Kuhn-Tuckerconditions,,94235 Karush–Kuhn–Tucker,93 Kernel(convolution),,328329 Kernelmachine,544 Kerneltrick,139 KKT, s e eKarush–Kuhn–Tucker KKTconditions, s e eKarush-Kuhn-Tucker conditions KLdivergence, s e eKullback-Leiblerdiver- gence Knowledgebase,,2479 Krylovmethods,222 Kullback-Leiblerdivergence,,xiii 7 3 Labelsmoothing,241 Lagrangemultipliers,,93641 Lagrangian, s e egeneralizedLagrangian LAPGAN,695 Laplacedistribution,, 6 4492 Latentvariable,66 Layer(neuralnetwork),166 LCN, s e elocalcontrastnormalization LeakyReLU,191 Leakyunits,404 Learningrate,84 Linesearch,,,848592 Linearcombination,36 Lineardependence,37 Linearfactormodels,485 Linearregression,,, 1 0 6109138 Linkprediction,480 Lipschitzconstant,91 Lipschitzcontinuous,91 Liquidstatemachine,401 7 8 1 INDEX Localconditionalprobabilitydistribution, 560 Localcontrastnormalization,452 Logisticregression,,,3 1 3 8139 Logisticsigmoid,,766 Longshort-termmemory,,,,1824304 4 0 7, 422 Loop,575 Loopybeliefpropagation,581 Lossfunction, s e eobjectivefunction Lpnorm,38 LSTM, s e elongshort-termmemory M-step,629 Machinelearning,2 Machinetranslation,100 Maindiagonal,32 Manifold,159 Manifoldhypothesis,160 Manifoldlearning,160 Manifoldtangentclassiﬁer,268 MAPapproximation,,137501 Marginalprobability,57 Markovchain,591 MarkovchainMonteCarlo,591 Markovnetwork, s e eundirectedmodel Markovrandomﬁeld, s e eundirectedmodel Matrix,,,xixii31 Matrixinverse,35 Matrixproduct,33 Maxnorm,39 Maxpooling,336 Maximumlikelihood, 1 3 0 Maxout,,191422 MCMC, s e eMarkovchainMonteCarlo Meanﬁeld,,,633634666 Meansquarederror,107 Measuretheory,70 Measurezero,70 Memorynetwork,,413415 Methodof steepestdescent, s e egradient descent Minibatch,277 Missinginputs,99 Mixing(Markovchain),597Mixturedensitynetworks,187 Mixturedistribution,65 Mixturemodel,,187506 Mixtureofexperts,,446544 MLP, s e emultilayerperception MNIST,,,2021666 Modelaveraging,252 Modelcompression,444 Modelidentiﬁability,282 Modelparallelism,444 Momentmatching,696 Moore-Penrosepseudoinverse,,44237 Moralizedgraph,573 MP-DBM, s e emulti-predictionDBM MRF(Markov RandomField), s e eundi- rectedmodel MSE, s e emeansquarederror Multi-modallearning,535 Multi-predictionDBM,668 Multi-tasklearning,,242533 Multilayerperception,5 Multilayerperceptron,26 Multinomialdistribution,61 Multinoullidistribution,61 n-gram, 4 5 8 NADE,702 NaiveBayes,3 Nat,72 Naturalimage,555 Naturallanguageprocessing,457 Nearestneighborregression, 1 1 4 Negativedeﬁnite,88 Negativephase,,,466602604 Neocognitron,,,,162326364 Nesterovmomentum,298 NetﬂixGrandPrize,,255475 Neurallanguagemodel,,460472 Neuralnetwork,13 NeuralTuringmachine,415 Neuroscience,15 Newton’smethod,,88309 NLM, s e eneurallanguagemodel NLP, s e enaturallanguageprocessing Nofreelunchtheorem,115 7 8 2 INDEX Noise-contrastiveestimation,616 Non-parametricmodel, 1 1 3 Norm,,xiv38 Normaldistribution,,,6263124 Normalequations,,,, 1 0 8108111232 Normalizedinitialization,301 Numericaldiﬀerentiation, s

============================================================

=== CHUNK 207 ===
Palavras: 350
Caracteres: 5060
--------------------------------------------------
e eﬁnitediﬀer- ences Objectdetection,449 Objectrecognition,449 Objectivefunction,81 OMP-, k s e eorthogonalmatchingpursuit One-shotlearning,534 Operation,202 Optimization,,7981 Orthodoxstatistics, s e efrequentiststatistics Orthogonalmatchingpursuit,,26 2 5 2 Orthogonalmatrix,41 Orthogonality,40 Outputlayer,166 Paralleldistributedprocessing,17 Parameterinitialization,,298403 Parametersharing,,,,, 249332370372386 Parametertying, s e eParametersharing Parametricmodel, 1 1 3 ParametricReLU,191 Partialderivative,83 Partitionfunction,,,564601663 PCA, s e eprincipalcomponentsanalysis PCD, s e estochasticmaximumlikelihood Perceptron,,1526 Persistentcontrastivedivergence, s e estochas- ticmaximumlikelihood Perturbationanalysis, s e ereparametrization trick Pointestimator,121 Policy,476 Pooling,,327677 Positivedeﬁnite,88 Positivephase,,,,, 466602604650662 Precision,420 Precision(ofanormaldistribution),,6264 Predictivesparsedecomposition,519Preprocessing,450 Pretraining,,320524 Primaryvisualcortex,362 Principalcomponentsanalysis,,,, 47145146 486626, Priorprobabilitydistribution, 1 3 4 Probabilisticmaxpooling,677 ProbabilisticPCA,,,486487627 Probabilitydensityfunction,57 Probabilitydistribution,55 Probabilitymassfunction,55 Probabilitymassfunctionestimation,102 Productofexperts,566 Productruleofprobability, s e echainrule ofprobability PSD, s e epredictivesparsedecomposition Pseudolikelihood,611 Quadraturepair,366 Quasi-Newtonmethods,314 Radialbasisfunction,195 Randomsearch,431 Randomvariable,55 Ratiomatching,614 RBF,195 RBM, s e erestrictedBoltzmannmachine Recall,420 Receptiveﬁeld,334 RecommenderSystems,474 Rectiﬁedlinearunit,,,, 170191422503 Recurrentnetwork,26 Recurrentneuralnetwork,375 Regression,99 Regularization,,,,, 1 1 9119176226427 Regularizer,118 REINFORCE,683 Reinforcementlearning,,,, 24105476683 Relationaldatabase,479 Relations,478 Reparametrizationtrick,682 Representationlearning,3 Representationalcapacity,113 RestrictedBoltzmannmachine, , ,353456 475583626650651666670 ,,,,,,, 7 8 3 INDEX 672674677,, Ridgeregression, s e eweightdecay Risk,273 RNN-RBM,679 Saddlepoints,283 Samplemean,124 Scalar,,,xixii30 Scorematching,,509613 Secondderivative,85 Secondderivativetest,88 Self-information,72 Semantichashing,521 Semi-supervisedlearning,241 Separableconvolution,359 Separation(probabilisticmodeling),568 Set,xii SGD, s e estochasticgradientdescent Shannonentropy,,xiii73 Shortlist,462 Sigmoid,,xiv s e elogisticsigmoid Sigmoidbeliefnetwork,26 Simplecell,362 Singularvalue, s e esingularvaluedecompo- sition Singularvaluedecomposition,,,43146475 Singularvector, s e esingularvaluedecom- position Slowfeatureanalysis,489 SML, s e estochasticmaximumlikelihood Softmax,,,182415446 Softplus,,,xiv67195 Spamdetection,3 Sparsecoding,,,,, 319353492626686 Sparseinitialization,,302403 Sparserepresentation,,,,, 145224251501 552 Spearmint,433 Spectralradius,401 Speechrecognition, s e e automaticspeech recognition Sphering, s e ewhitening Spikeand slabrestricted Boltzmannma- chine,674 SPN, s e esum-productnetworkSquarematrix,37 ssRBM, s e espikeandslabrestrictedBoltz- mannmachine Standarddeviation,60 Standarderror,126 Standarderrorofthemean,,126276 Statistic,121 Statisticallearningtheory,109 Steepestdescent, s e egradientdescent Stochasticback-propagation, s e ereparametriza- tiontrick Stochasticgradientdescent,,,, 15149277 2 9 2,666 Stochasticmaximumlikelihood,,608666 Stochasticpooling,263 Structurelearning,578 Structuredoutput,,100679 Structuredprobabilisticmodel,,76554 Sumruleofprobability,57 Sum-productnetwork,549 Supervisedﬁne-tuning,,525656 Supervisedlearning, 1 0 4 Supportvectormachine,139 Surrogatelossfunction,274 SVD, s e esingularvaluedecomposition Symmetricmatrix,,4042 Tangentdistance,267 Tangentplane,511 Tangentprop,267 TDNN, s e etime-delayneuralnetwork Teacherforcing,,379380 Tempering,599 Templatematching,140 Tensor,,,xixii32 Testset,109 Tikhonovregularization, s e eweightdecay Tiledconvolution,349 Time-delayneuralnetwork,,364371 Toeplitzmatrix,330 TopographicICA,489 Traceoperator,45 Trainingerror,109 Transcription,100 Transferlearning,532 7 8 4 INDEX Transpose,,xii32 Triangleinequality,38 Triangulatedgraph, s e echordalgraph Trigram,458 Unbiased,123 Undirectedgraphicalmodel,,76503 Undirectedmodel,562 Uniformdistribution,56 Unigram,458 Unitnorm,40 Unitvector,40 Universalapproximationtheorem,196 Universalapproximator,549 Unnormalizedprobabilitydistribution,563 Unsupervisedlearning,, 1 0 4144 Unsupervisedpretraining,,456524 V-structure, s e eexplainingaway V1,362 VAE, s e evariationalautoencoder Vapnik-Chervonenkisdimension,113 Variance,,,xiii60227 Variationalautoencoder,,683 6 9 0 Variationalderivatives, s e efunctionalderiva- tives Variationalfreeenergy, s e eevidencelower bound VCdimension, s e eVapnik-Chervonenkisdi- mension Vector,,,xixii31 Virtualadversarialexamples,266 Visiblelayer,6 Volumetricdata,357 Wake-sleep,,646655 Weightdecay,,,, 1 1 7176229428 Weightspacesymmetry,282 Weights,,15106 Whitening,452 Wikibase,479 Wikibase,479 Wordembedding,460 Word-sensedisambiguation,480

============================================================

=== CHUNK 208 ===
Palavras: 8
Caracteres: 80
--------------------------------------------------
WordNet,479Zero-datalearning, s e ezero-shotlearning Zero-shotlearning,534 7 8 5

============================================================


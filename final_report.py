#!/usr/bin/env python3
"""
Relat√≥rio final completo do projeto Knowledge Graph.
Gera estat√≠sticas e an√°lises finais do pipeline.
"""

import sys
import os
from pathlib import Path
import pickle
import json
from datetime import datetime

def analyze_files():
    """Analisa todos os arquivos gerados."""
    
    data_dir = Path("data")
    
    files_info = {}
    
    # Arquivos principais
    main_files = [
        "processed_chunks.pkl",
        "extracted_entities.pkl", 
        "normalized_entities.pkl",
        "extracted_relations.pkl",
        "ml_kg.turtle",
        "ml_kg.xml",
        "ml_kg.n3", 
        "ml_kg.json-ld",
        "kg_construction_report.txt"
    ]
    
    total_size = 0
    
    for file_name in main_files:
        file_path = data_dir / file_name
        if file_path.exists():
            size_mb = file_path.stat().st_size / 1024 / 1024
            files_info[file_name] = {
                'exists': True,
                'size_mb': size_mb,
                'path': str(file_path)
            }
            total_size += size_mb
        else:
            files_info[file_name] = {'exists': False}
    
    files_info['total_size_mb'] = total_size
    
    return files_info

def load_pipeline_stats():
    """Carrega estat√≠sticas de cada etapa do pipeline."""
    
    stats = {}
    
    try:
        # Chunks
        with open("data/processed_chunks.pkl", 'rb') as f:
            chunk_data = pickle.load(f)
            stats['chunks'] = {
                'total': len(chunk_data['chunks']),
                'sources': len(set(chunk.source_file for chunk in chunk_data['chunks']))
            }
    except:
        stats['chunks'] = {'total': 'N/A', 'sources': 'N/A'}
    
    try:
        # Entidades extra√≠das
        with open("data/extracted_entities.pkl", 'rb') as f:
            entity_data = pickle.load(f)
            stats['extracted_entities'] = {
                'total': len(entity_data['entities']),
                'unique_texts': len(set(e.text for e in entity_data['entities']))
            }
    except:
        stats['extracted_entities'] = {'total': 'N/A', 'unique_texts': 'N/A'}
    
    try:
        # Entidades normalizadas
        with open("data/normalized_entities.pkl", 'rb') as f:
            norm_data = pickle.load(f)
            stats['normalized_entities'] = {
                'total': len(norm_data['normalized_entities']),
                'reduction_ratio': round(stats['extracted_entities']['total'] / len(norm_data['normalized_entities']), 1) if stats['extracted_entities']['total'] != 'N/A' else 'N/A'
            }
    except:
        stats['normalized_entities'] = {'total': 'N/A', 'reduction_ratio': 'N/A'}
    
    try:
        # Rela√ß√µes
        with open("data/extracted_relations.pkl", 'rb') as f:
            rel_data = pickle.load(f)
            stats['relations'] = {
                'total': len(rel_data['relations']),
                'unique_predicates': len(set(r.predicate for r in rel_data['relations']))
            }
    except:
        stats['relations'] = {'total': 'N/A', 'unique_predicates': 'N/A'}
    
    return stats

def generate_final_report():
    """Gera relat√≥rio final completo."""
    
    print("üìä RELAT√ìRIO FINAL - KNOWLEDGE GRAPH PIPELINE")
    print("=" * 60)
    print(f"Data/Hora: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    print()
    
    # An√°lise de arquivos
    print("üìÅ ARQUIVOS GERADOS:")
    print("-" * 30)
    
    files_info = analyze_files()
    
    for file_name, info in files_info.items():
        if file_name == 'total_size_mb':
            continue
            
        if info['exists']:
            print(f"‚úÖ {file_name:<25} ({info['size_mb']:.1f} MB)")
        else:
            print(f"‚ùå {file_name:<25} (n√£o encontrado)")
    
    print(f"\nüìä Tamanho total dos arquivos: {files_info['total_size_mb']:.1f} MB")
    print()
    
    # Estat√≠sticas do pipeline
    print("üî¢ ESTAT√çSTICAS DO PIPELINE:")
    print("-" * 35)
    
    stats = load_pipeline_stats()
    
    if isinstance(stats['chunks']['total'], int):
        print(f"üìö Chunks de texto processados: {stats['chunks']['total']:,}")
    else:
        print(f"üìö Chunks de texto processados: {stats['chunks']['total']}")
    
    print(f"üìñ Fontes (livros): {stats['chunks']['sources']}")
    print()
    
    if isinstance(stats['extracted_entities']['total'], int):
        print(f"üè∑Ô∏è  Entidades extra√≠das: {stats['extracted_entities']['total']:,}")
        print(f"üè∑Ô∏è  Entidades √∫nicas: {stats['extracted_entities']['unique_texts']:,}")
    else:
        print(f"üè∑Ô∏è  Entidades extra√≠das: {stats['extracted_entities']['total']}")
        print(f"üè∑Ô∏è  Entidades √∫nicas: {stats['extracted_entities']['unique_texts']}")
    print()
    
    if isinstance(stats['normalized_entities']['total'], int):
        print(f"‚ú® Entidades normalizadas: {stats['normalized_entities']['total']:,}")
    else:
        print(f"‚ú® Entidades normalizadas: {stats['normalized_entities']['total']}")
        
    if stats['normalized_entities']['reduction_ratio'] != 'N/A':
        reduction_pct = (1 - 1/stats['normalized_entities']['reduction_ratio']) * 100
        print(f"üìâ Redu√ß√£o de entidades: {reduction_pct:.1f}% (fator {stats['normalized_entities']['reduction_ratio']}x)")
    print()
    
    if isinstance(stats['relations']['total'], int):
        print(f"üîó Rela√ß√µes extra√≠das: {stats['relations']['total']:,}")
    else:
        print(f"üîó Rela√ß√µes extra√≠das: {stats['relations']['total']}")
    print(f"üîó Tipos de rela√ß√µes: {stats['relations']['unique_predicates']}")
    print()
    
    # M√©tricas de qualidade
    print("‚≠ê M√âTRICAS DE QUALIDADE:")
    print("-" * 30)
    
    # Calcular m√©tricas
    if isinstance(stats['normalized_entities']['total'], int) and isinstance(stats['chunks']['total'], int):
        entities_per_chunk = stats['normalized_entities']['total'] / stats['chunks']['total']
    else:
        entities_per_chunk = 'N/A'
        
    if isinstance(stats['relations']['total'], int) and isinstance(stats['chunks']['total'], int):
        relations_per_chunk = stats['relations']['total'] / stats['chunks']['total']
    else:
        relations_per_chunk = 'N/A'
        
    if isinstance(stats['relations']['total'], int) and isinstance(stats['normalized_entities']['total'], int):
        relations_per_entity = stats['relations']['total'] / stats['normalized_entities']['total']
    else:
        relations_per_entity = 'N/A'
    
    print(f"üìä Entidades por chunk: {entities_per_chunk:.1f}" if entities_per_chunk != 'N/A' else "üìä Entidades por chunk: N/A")
    print(f"üìä Rela√ß√µes por chunk: {relations_per_chunk:.1f}" if relations_per_chunk != 'N/A' else "üìä Rela√ß√µes por chunk: N/A")
    print(f"üìä Rela√ß√µes por entidade: {relations_per_entity:.1f}" if relations_per_entity != 'N/A' else "üìä Rela√ß√µes por entidade: N/A")
    print()
    
    # Knowledge Graph RDF
    print("üï∏Ô∏è KNOWLEDGE GRAPH RDF:")
    print("-" * 30)
    
    kg_file = Path("data/ml_kg.turtle")
    if kg_file.exists():
        # Tentar carregar para estat√≠sticas
        try:
            from rdflib import Graph
            g = Graph()
            g.parse("data/ml_kg.turtle", format="turtle")
            print(f"üìä Total de triplas RDF: {len(g):,}")
            
            # Contar tipos
            query = """
            SELECT ?class (COUNT(?entity) as ?count)
            WHERE {
                ?entity a ?class .
                FILTER(STRSTARTS(STR(?class), "http://ml-kg.org/ontology/"))
            }
            GROUP BY ?class
            ORDER BY DESC(?count)
            LIMIT 5
            """
            
            results = list(g.query(query))
            print(f"üìä Classes principais:")
            for result in results:
                class_name = str(result[0]).split('/')[-1]
                count = int(result[1])
                print(f"   ‚Ä¢ {class_name}: {count:,}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro carregando RDF: {e}")
            # Estimativa baseada em arquivos
            print(f"üìä Arquivo RDF: {files_info['ml_kg.turtle']['size_mb']:.1f} MB")
    else:
        print("‚ùå Knowledge Graph RDF n√£o encontrado")
    
    print()
    
    # Formatos dispon√≠veis
    print("üìÑ FORMATOS DISPON√çVEIS:")
    print("-" * 30)
    
    rdf_formats = ['ml_kg.turtle', 'ml_kg.xml', 'ml_kg.n3', 'ml_kg.json-ld']
    for fmt in rdf_formats:
        if files_info[fmt]['exists']:
            print(f"‚úÖ {fmt:<15} ({files_info[fmt]['size_mb']:.1f} MB)")
        else:
            print(f"‚ùå {fmt:<15} (n√£o dispon√≠vel)")
    
    print()
    
    # Pr√≥ximos passos
    print("üöÄ PR√ìXIMOS PASSOS SUGERIDOS:")
    print("-" * 35)
    print("1. üìà An√°lise comparativa com abordagem RAG")
    print("2. üîç Consultas SPARQL mais avan√ßadas")
    print("3. üé® Visualiza√ß√£o do grafo com Gephi/Cytoscape")
    print("4. üèóÔ∏è  Deploy em triplestore (Apache Jena, GraphDB)")
    print("5. ü§ñ Desenvolvimento de aplica√ß√µes que consomem o KG")
    print("6. üìä M√©tricas de avalia√ß√£o da qualidade do KG")
    print("7. üîß Refinamento e melhoria do pipeline")
    print()
    
    # Conclus√£o
    print("üéØ CONCLUS√ÉO:")
    print("-" * 15)
    print("‚úÖ Pipeline de constru√ß√£o de Knowledge Graph CONCLU√çDO!")
    print("‚úÖ Dados processados e persistidos com seguran√ßa")
    print("‚úÖ Knowledge Graph em RDF dispon√≠vel em m√∫ltiplos formatos")
    print("‚úÖ Consultas SPARQL funcionando corretamente")
    print()
    
    print("üèÜ Parab√©ns! Voc√™ tem agora um Knowledge Graph completo")
    print("   do dom√≠nio de Machine Learning e Deep Learning!")
    print("=" * 60)

if __name__ == "__main__":
    generate_final_report()